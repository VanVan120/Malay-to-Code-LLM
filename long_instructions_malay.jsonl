{"instruction": "Untuk menganalisis data halaju dan mencari titik ekstrem dan pelana, ikuti langkah -langkah ini:\n\n1. Import Perpustakaan yang diperlukan: YT, matplotlib.pyplot, numpy, dan yt.analysis_modules.halo_finding.api.\n2. Tentukan empat fungsi: derivx, derivy, deriv2x, dan deriv2y untuk mengira derivatif pertama dan kedua halaju berkenaan dengan x dan y.\n3. Tentukan fungsi campuran_deriv untuk mengira derivatif campuran kedua halaju.\n4. Tentukan fungsi Hess untuk membuat matriks Hessian untuk setiap titik.\n5. Tentukan penentu fungsi untuk mencari penentu setiap matriks Hessian.\n6. Tentukan magnitud fungsi untuk mencari magnitud halaju pada setiap titik.\n7. Tentukan fungsi ekstrem untuk mencari mata ekstrem dan pelana menggunakan penentu dan magnitud.\n8. Tentukan fungsi Jacobian untuk membuat matriks Jacobian untuk setiap titik.\n9. Tentukan fungsi evals untuk mendapatkan nilai eigen untuk semua matriks Jacobian mata dan kemudian periksa ekstrem.\n10. Tentukan fungsi utama untuk memuatkan fail dan panggil fungsi Hess.\n11. Panggil fungsi utama untuk menganalisis data halaju dan cari mata ekstrem dan pelana.", "input": "", "output": "import yt\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import pylab\nfrom yt.analysis_modules.halo_finding.api import HaloFinder\nfrom pylab import*\nfrom numpy import ma\nfrom numpy import linalg as LA\n\n\n#derives vel with respect to x\ndef derivx(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelxdx = np.zeros((320,320))\n\t\n\tfor i in range(len(vel)):\n\t\t\n\t\tfor x in range(len(vel)):\n\t\t\t\n\t\t\tif 0 < i < len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-1/2) * vel[i-1][x]) + ((1/2) * vel[i+1][x])\n\t\t\t\n\t\t\telif i == 0:\n\t\t\t\tvelxdx[i,x] = (((-3/2) * vel[i][x]) + (2 * vel[i+1][x]) + ((-1/2) * vel[i+2][x]))\n\t\t\t\n\t\t\telif i == len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-3/2) * vel[i][x]) + (2 * vel[i-1][x]) + ((-1/2) * vel[i-2][x])\n\n\treturn velxdx/distance\n\n\n#derives vel with respect to y\ndef derivy(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelydy = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < x < len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((-1/2) * vel[i][x-1]) + ((1/2) * vel[i][x+1]))\n\t\t\t\n\t\t\telif x == 0:\n\t\t\t\tvelydy[i,x] = (((-3/2)*vel[i][x]) + (2*vel[i][x+1]) + ((-1/2) * vel[i][x + 2]))\n\t\t\t\n\t\t\telif x == len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((-3/2)*vel[i][x]) + (2*vel[i][x-1]) + ((-1/2) * vel[i][x-2]))\n\n\treturn velydy/distance\n\n\n#second derivative of vel with respect to x\ndef deriv2x(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelxdx = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < i < len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = (vel[i-1][x]) + (-2 * vel[i][x]) + (vel[i+1][x])\n\n\t\t\telif i == 0:\n\t\t\t\tvelxdx[i,x] = ((2 * vel[i][x]) + (-5 * vel[i+1][x]) + (4* vel[i+2][x]) + (-1 * vel[i+3][x]))\n\n\t\t\telif i == len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-3/2) * vel[i][x]) + (2 * vel[i-1][x]) + ((-1/2) * vel[i-2][x])\n\n\treturn velxdx/distance\n\n\n#second derivative of vel with respect to y\ndef deriv2y(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelydy = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < x < len(vel) - 1:\n\t\t\t\tvelydy[i,x] = ((vel[i][x-1]) + (-2 * vel[i][x]) + (vel[i][x+1]))\n\n\t\t\telif x == 0:\n\t\t\t\tvelydy[i,x] = (((2)*vel[i][x]) + (-5 * vel[i][x+1]) + ((4) * vel[i][x+2]) + (-1 * vel[i][x+3]))\n\n\t\t\telif x == len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((2) * vel[i][x]) + (-5 * vel[i][x - 1]) + ((4) * vel[i][x-2]) + (-1 * vel[i][x-3]))\n\n\treturn velydy/distance\n\n\n#second derivative of a mixed derivative\ndef mixed_deriv(xcoords, ycoords, vel):\n\tdistx = xcoords[1][0] - xcoords[0][0]\n\tdisty = ycoords[0][1] - ycoords[0][0]\n\tmixed = np.zeros((320,320))\n\tveldx = derivx(vel, xcoords)\n\tveldy = derivy(veldx, xcoords)  #takes deriv of vel with respect to x and derives that in the y direction\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < i < len(vel) - 1 and 0 < x < len(vel) - 1:\n\t\t\t\tmixed[i][x] = ((vel[i+1][x+1]) - (vel[i+1][x-1]) - (vel[i-1][x+1]) + (vel[i-1][x-1]))/(4*distx*disty)\n\n\t\t\t#if on edges derives with respect to x first\n\t\t\telif i == 0 or i == len(vel) - 1 or x == 0 or x == len(vel) - 1:\n\t\t\t\tmixed[i][x]=veldy[i][x]\n\n\treturn mixed\n\n#create hessian matrix for each point\ndef hess(xcoords, ycoords, vel):\n\tveldx = deriv2x(vel, xcoords)  #retrieves the second derivatives of the velocity in the x direction\n\tveldy = deriv2y(vel, xcoords)  #retrieves the second derivatives of the velocity in the y direction\n\tmixed = mixed_deriv(xcoords, ycoords, vel) #retrieves the second mixed derivatives of the velocity \n\thessian = np.zeros((2,2))\n\tallhessian = [[[] for j in range(320)] for i in range(320)]\n\tfor j in range(len(veldx)):\n\n\t\tfor k in range(len(veldx)):\n\n\t\t\tfor i in range(len(hessian)):\n\n\t\t\t\tfor x in range(len(hessian)):\n\n\t\t\t\t\tif i == 0 and x == 1:\n\t\t\t\t\t\thessian[i,x] = mixed[j,k]\n\t\t\t\t\t\thessian[i+1][x-1] = mixed[j,k]\n\n\t\t\t\t\telif x == 0 and i == 0:\n\t\t\t\t\t\thessian[i,x] = veldx[j,k]\n\n\t\t\t\t\telif x == 1 and i == 1:\n\t\t\t\t\t\thessian[i,x] = veldy[j,k] \n\n\t\t\t\tallhessian[j][k] = hessian\n\tallhessian = np.array(allhessian)\n\treturn allhessian\n\n\n#find determinant\ndef determinant(allhessian):\n\tdeters = np.zeros((320,320))\n\n\tfor j in range(len(allhessian)):\n\n\t\tfor k in range(len(allhessian)):\n\n\t\t\tx = allhessian[j,k]\n\n\t\t\tdeters[j,k] = (x[0,0]*x[1,1]) - (x[1,0]*x[0,1])\n\n\treturn deters\n\n\n#find magnitude\ndef magnitude(velx,vely, xcoords):\n\tmag = np.zeros((320,320))\n\tyderiv = derivy(vely, xcoords)\n\txderiv = derivx(velx, xcoords)\n\n\tfor i in range(len(xderiv)):\n\n\t\tfor x in range(len(xderiv)):\n\t\t\tmag[i][x] = (((yderiv[i,x]**2) + (xderiv[i,x]**2))**.5)\n\n\treturn mag\n\n\n#finds extrema and saddlepoints\ndef extrema(allhessian, velx, vely, xcoords):\n\tdeters = determinant(allhessian)\n\textrem = np.zeros((320,320))\n\tmag = magnitude(velx, vely, xcoords)\n\n\tfor j in range(len(extrem)):\n\n\t\tfor k in range(len(extrem)):\n\n\t\t\tif mag[j][k] == 0:\n\t\t\t\tif deters[j,k] < 0:\n\t\t\t\t\textrem[j, k] = -1\n\n\t\t\t\telif deters[j,k] == 0:\n\t\t\t\t\textrem[j,k] = 0\n\n\t\t\t\telse:\n\t\t\t\t\tx = allhessian[j,k]\n\t\t\t\t\tif deter[j,k] > 0 and x[0,0] > 0:\n\t\t\t\t\t\textem[j, k] = -2\n\t\t\t\t\telif deter[j,k] > 0 and x[0,0] < 0:\n\t\t\t\t\t\textrem[j, k] = 2\n\treturn extrem\t\t\t\t\n\n\n#creates jacobia matrix for each point\t\t\t\t\t\t\ndef jacobian(xcoords,velx, vely):\n\txx = derivx(velx, xcoords)\n\txy = derivy(velx, xcoords)\n\tyx = derivx(vely, xcoords)\n\tyy = derivy(vely, xcoords)\n\tjacob = np.zeros ((2,2))\n\talljacob = [[[] for j in range(320)] for i in range(320)]\n\n\tfor j in range(len(xx)):\n\n\t\tfor k in range(len(xx)):\n\n\t\t\tfor i in range(len(jacob)):\n\n\t\t\t\tfor c in range(len(jacob)):\n\n\t\t\t\t\tif c == 0 and i == 0:\n\t\t\t\t\t\tjacob [i][c] = xx[j][k] \n\n\t\t\t\t\telif c == 1 and i == 0:\n\t\t\t\t\t\tjacob[i][c] = xy[j][k]\n\n\t\t\t\t\telif c ==1 and i == 1:\n\t\t\t\t\t\tjacob[i][c] = yy[j][k]\n\n\t\t\t\talljacob[j][k] = jacob\n\talljacob = np.array(alljacob)\t\t\t\n\treturn alljacob\n\n\n#obtains eigenvalues for all points' jacobian matrices and then checks the extrema\ndef evals(alljacob):\n\teigen = [[[] for j in range(320)] for i in range(320)]\n\textrema = np.zeros((320,320))\n\n\tfor j in range(len(alljacob)):\n\n\t\tfor k in range(len(alljacob)):\n\n\t\t\tx = alljacob[j,k]\n\t\t\teigen[j][k] = LA.eigvalsh(x)\n\t\t\ty = eigen [j][k]\n\n\t\t\tif y[0]>0 and y[1] > 0:\n\t\t\t\textrema[j,k] = 2\n\n\t\t\telif y[0]<0 and y[1] <0:\n\t\t\t\textrema[j,k] = -2\n\n\t\t\telif y[0]*y[1] < 0:\n\t\t\t\textrema[j,k] = 3\n\n\treturn extrema\n\n#loads files and calls hess function\ndef main():\n\tds = yt.load(\"kh_mhd_Ma=0.803333333333At=0.0hdf5_chk_0000\")\n\tad = ds.covering_grid(level=0, left_edge=ds.index.grids[0].LeftEdge, \n\t\t\t\t\t\t\t\t\t\tdims=ds.domain_dimensions)\n\n\txcoords = np.array(ad[\"x\"])\n\tycoords = np.array(ad[\"y\"])\n\tvelx = np.array(ad[\"velx\"])\n\tvely = np.array(ad[\"vely\"])\n\n\tds1 = yt.load(\"kh_mhd_Ma=0.803333333333At=0.0hdf5_chk_0001\")\n\tdd = ds1.covering_grid(level=0, left_edge=ds1.index.grids[0].LeftEdge, dims=ds1.domain_dimensions)\n\n\txcoords1 = np.array(dd[\"x\"])\n\tycoords1 = np.array(dd[\"y\"])\n\tvelx1 = np.array(dd[\"velx\"])\n\tvely1 = np.array(dd[\"vely\"])\n\n\t#creates Hessian matrix for x velocity for file 1\n\textrema(hess(xcoords, ycoords, velx), velx, vely, xcoords)\n\n\t#creates Hessian marix for y velocity for file 1\n\t(extrema(hess(xcoords, ycoords, vely), velx, vely, xcoords))\n\n\t#prints extrema for file1\n\tprint(evals(jacobian(xcoords, velx, vely)))\n\t'''plt.figure()\n\tplt.scatter(xcoords, ycoords,c=evals(jacobian(xcoords, velx, vely)), marker= 'o',edgecolor='none')\n\tcb = plt.colorbar()\n\tcb.set_label('Extrema')\n\tplt.show()'''\n\n\nmain()", "instruction_en": "To analyze velocity data and find extrema and saddle points, follow these steps:\n\n1. Import necessary libraries: yt, matplotlib.pyplot, numpy, and yt.analysis_modules.halo_finding.api.\n2. Define four functions: derivx, derivy, deriv2x, and deriv2y to calculate first and second derivatives of velocity with respect to x and y.\n3. Define a function mixed_deriv to calculate the second mixed derivative of velocity.\n4. Define a function hess to create a Hessian matrix for each point.\n5. Define a function determinant to find the determinant of each Hessian matrix.\n6. Define a function magnitude to find the magnitude of velocity at each point.\n7. Define a function extrema to find extrema and saddle points using the determinant and magnitude.\n8. Define a function jacobian to create a Jacobian matrix for each point.\n9. Define a function evals to obtain eigenvalues for all points' Jacobian matrices and then check the extrema.\n10. Define a function main to load files and call the hess function.\n11. Call the main function to analyze the velocity data and find extrema and saddle points.", "original_line_number": 243, "instruction_length": 1111}
{"instruction": "Kod yang diberikan adalah modul python yang menyediakan fungsi untuk berinteraksi dengan peranti Video4Linux2 (V4L2). Untuk menggunakan modul ini, importnya ke dalam skrip Python anda dan panggil fungsi seperti yang diperlukan. Fungsi termasuk:\n\n- `list_devices ()`: Senaraikan semua peranti V4L2 yang kini disambungkan ke sistem.\n- `list_resolutions (peranti)`: Senarai semua resolusi yang disokong untuk peranti V4L2 yang diberikan.\n- `get_brightness (peranti)`: Mendapat nilai kecerahan semasa untuk peranti V4L2 yang diberikan.\n- `set_brightness (peranti, nilai)`: Menetapkan nilai kecerahan untuk peranti V4L2 yang diberikan.\n- `get_contrast (peranti)`: Mendapat nilai kontras semasa untuk peranti V4L2 yang diberikan.\n- `set_contrast (peranti, nilai)`: Menetapkan nilai kontras untuk peranti V4L2 yang diberikan.\n- `get_saturation (peranti)`: Mendapat nilai ketepuan semasa untuk peranti V4L2 yang diberikan.\n- `set_saturation (peranti, nilai)`: Menetapkan nilai tepu untuk peranti V4L2 yang diberikan.\n- `get_hue (peranti)`: Mendapat nilai warna semasa untuk peranti V4L2 yang diberikan.\n- `set_hue (peranti, nilai)`: Tetapkan nilai warna untuk peranti V4L2 yang diberikan.", "input": "", "output": "# Copyright (c) 2013 Calin Crisan\n# This file is part of motionEye.\n#\n# motionEye is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>. \n\nimport fcntl\nimport logging\nimport os.path\nimport re\nimport stat\nimport subprocess\nimport time\nimport utils\n\n\n_resolutions_cache = {}\n_ctrls_cache = {}\n_ctrl_values_cache = {}\n\n_DEV_V4L_BY_ID = '/dev/v4l/by-id/'\n\n\ndef find_v4l2_ctl():\n    try:\n        return subprocess.check_output('which v4l2-ctl', shell=True).strip()\n    \n    except subprocess.CalledProcessError: # not found\n        return None\n\n\ndef list_devices():\n    global _resolutions_cache, _ctrls_cache, _ctrl_values_cache\n    \n    logging.debug('listing v4l2 devices...')\n    \n    try:\n        output = ''\n        started = time.time()\n        p = subprocess.Popen('v4l2-ctl --list-devices 2>/dev/null', shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n        fd = p.stdout.fileno()\n        fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n        fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n        while True:\n            try:\n                data = p.stdout.read(1024)\n                if not data:\n                    break\n            \n            except IOError:\n                data = ''\n                time.sleep(0.01)\n\n            output += data\n\n            if len(output) > 10240:\n                logging.warn('v4l2-ctl command returned more than 10k of output')\n                break\n            \n            if time.time() - started > 3:\n                logging.warn('v4l2-ctl command ran for more than 3 seconds')\n                break\n\n    except subprocess.CalledProcessError:\n        logging.debug('failed to list devices (probably no devices installed)')\n        return []\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n    name = None\n    devices = []\n    for line in output.split('\\n'):\n        if line.startswith('\\t'):\n            device = line.strip()\n            persistent_device = find_persistent_device(device)\n            devices.append((device, persistent_device, name))\n        \n            logging.debug('found device %(name)s: %(device)s, %(persistent_device)s' % {\n                    'name': name, 'device': device, 'persistent_device': persistent_device})\n\n        else:\n            name = line.split('(')[0].strip()\n    \n    # clear the cache\n    _resolutions_cache = {}\n    _ctrls_cache = {}\n    _ctrl_values_cache = {}\n\n    return devices\n\n\ndef list_resolutions(device):\n    global _resolutions_cache\n    \n    device = utils.make_str(device)\n    \n    if device in _resolutions_cache:\n        return _resolutions_cache[device]\n    \n    logging.debug('listing resolutions of device %(device)s...' % {'device': device})\n    \n    resolutions = set()\n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --list-formats-ext | grep -vi stepwise | grep -oE \"[0-9]+x[0-9]+\" || true' % {\n            'device': device}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n\n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n        \n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n    \n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n    \n    except:\n        pass # nevermind\n\n    for pair in output.split('\\n'):\n        pair = pair.strip()\n        if not pair:\n            continue\n        \n        width, height = pair.split('x')\n        width = int(width)\n        height = int(height)\n\n        if (width, height) in resolutions:\n            continue # duplicate resolution\n\n        if width < 96 or height < 96: # some reasonable minimal values\n            continue\n        \n        if width % 16 or height % 16: # ignore non-modulo 16 resolutions\n            continue\n\n        resolutions.add((width, height))\n        \n        logging.debug('found resolution %(width)sx%(height)s for device %(device)s' % {\n                'device': device, 'width': width, 'height': height})\n    \n    if not resolutions:\n        logging.debug('no resolutions found for device %(device)s, using common values' % {'device': device})\n\n        # no resolution returned by v4l2-ctl call, add common default resolutions\n        resolutions = utils.COMMON_RESOLUTIONS\n\n    resolutions = list(sorted(resolutions, key=lambda r: (r[0], r[1])))\n    _resolutions_cache[device] = resolutions\n    \n    return resolutions\n\n\ndef device_present(device):\n    device = utils.make_str(device)\n    \n    try:\n        st = os.stat(device)\n        return stat.S_ISCHR(st.st_mode)\n\n    except:\n        return False\n    \n\ndef find_persistent_device(device):\n    device = utils.make_str(device)\n    \n    try:\n        devs_by_id = os.listdir(_DEV_V4L_BY_ID)\n\n    except OSError:\n        return device\n    \n    for p in devs_by_id:\n        p = os.path.join(_DEV_V4L_BY_ID, p)\n        if os.path.realpath(p) == device:\n            return p\n    \n    return device\n\n\ndef get_brightness(device):\n    return _get_ctrl(device, 'brightness')\n\n\ndef set_brightness(device, value):\n    _set_ctrl(device, 'brightness', value)\n\n\ndef get_contrast(device):\n    return _get_ctrl(device, 'contrast')\n\n\ndef set_contrast(device, value):\n    _set_ctrl(device, 'contrast', value)\n\n\ndef get_saturation(device):\n    return _get_ctrl(device, 'saturation')\n\n\ndef set_saturation(device, value):\n    _set_ctrl(device, 'saturation', value)\n\n\ndef get_hue(device):\n    return _get_ctrl(device, 'hue')\n\n\ndef set_hue(device, value):\n    _set_ctrl(device, 'hue', value)\n\n\ndef _get_ctrl(device, control):\n    global _ctrl_values_cache\n    \n    device = utils.make_str(device)\n    \n    if not device_present(device):\n        return None\n    \n    if device in _ctrl_values_cache and control in _ctrl_values_cache[device]:\n        return _ctrl_values_cache[device][control]\n    \n    controls = _list_ctrls(device)\n    properties = controls.get(control)\n    if properties is None:\n        logging.warn('control %(control)s not found for device %(device)s' % {\n                'control': control, 'device': device})\n        \n        return None\n    \n    value = int(properties['value'])\n    \n    # adjust the value range\n    if 'min' in properties and 'max' in properties:\n        min_value = int(properties['min'])\n        max_value = int(properties['max'])\n        \n        value = int(round((value - min_value) * 100.0 / (max_value - min_value)))\n    \n    else:\n        logging.warn('min and max values not found for control %(control)s of device %(device)s' % {\n                'control': control, 'device': device})\n    \n    logging.debug('control %(control)s of device %(device)s is %(value)s%%' % {\n            'control': control, 'device': device, 'value': value})\n    \n    return value\n\n\ndef _set_ctrl(device, control, value):\n    global _ctrl_values_cache\n    \n    device = utils.make_str(device)\n    \n    if not device_present(device):\n        return\n\n    controls = _list_ctrls(device)\n    properties = controls.get(control)\n    if properties is None:\n        logging.warn('control %(control)s not found for device %(device)s' % {\n                'control': control, 'device': device})\n        \n        return\n    \n    _ctrl_values_cache.setdefault(device, {})[control] = value\n\n    # adjust the value range\n    if 'min' in properties and 'max' in properties:\n        min_value = int(properties['min'])\n        max_value = int(properties['max'])\n        \n        value = int(round(min_value + value * (max_value - min_value) / 100.0))\n    \n    else:\n        logging.warn('min and max values not found for control %(control)s of device %(device)s' % {\n                'control': control, 'device': device})\n    \n    logging.debug('setting control %(control)s of device %(device)s to %(value)s' % {\n            'control': control, 'device': device, 'value': value})\n\n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --set-ctrl %(control)s=%(value)s' % {\n            'device': device, 'control': control, 'value': value}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n        \n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n\n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n\ndef _list_ctrls(device):\n    global _ctrls_cache\n    \n    device = utils.make_str(device)\n\n    if device in _ctrls_cache:\n        return _ctrls_cache[device]\n    \n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --list-ctrls' % {\n            'device': device}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n        \n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n\n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n    controls = {}\n    for line in output.split('\\n'):\n        if not line:\n            continue\n        \n        match = re.match('^\\s*(\\w+)\\s+\\(\\w+\\)\\s+\\:\\s*(.+)', line)\n        if not match:\n            continue\n        \n        (control, properties) = match.groups()\n        properties = dict([v.split('=', 1) for v in properties.split(' ') if v.count('=')])\n        controls[control] = properties\n    \n    _ctrls_cache[device] = controls\n    \n    return controls", "instruction_en": "The given code is a Python module that provides functions to interact with video4linux2 (v4l2) devices. To use this module, import it into your Python script and call the functions as needed. The functions include:\n\n- `list_devices()`: lists all v4l2 devices currently connected to the system.\n- `list_resolutions(device)`: lists all supported resolutions for a given v4l2 device.\n- `get_brightness(device)`: gets the current brightness value for a given v4l2 device.\n- `set_brightness(device, value)`: sets the brightness value for a given v4l2 device.\n- `get_contrast(device)`: gets the current contrast value for a given v4l2 device.\n- `set_contrast(device, value)`: sets the contrast value for a given v4l2 device.\n- `get_saturation(device)`: gets the current saturation value for a given v4l2 device.\n- `set_saturation(device, value)`: sets the saturation value for a given v4l2 device.\n- `get_hue(device)`: gets the current hue value for a given v4l2 device.\n- `set_hue(device, value)`: sets the hue value for a given v4l2 device.", "original_line_number": 480, "instruction_length": 1180}
{"instruction": "Kod yang diberikan mengandungi beberapa kelas yang melaksanakan tugas yang berbeza. Untuk menggunakan kelas ini, anda perlu membuat contoh kelas yang dikehendaki dan memanggil kaedah utamanya. Sebagai contoh, untuk menambah JavaScript ke dokumen PDF, buat contoh kelas Addjavascript, lulus laluan ke fail PDF input sebagai parameter. Kemudian panggil kaedah utama kelas. Fail PDF output dengan JavaScript ditambah akan disimpan dalam direktori yang sama seperti fail input. Begitu juga, anda boleh menggunakan kelas lain untuk melaksanakan tugas yang berbeza seperti menambah jadual kandungan, mengoptimumkan fail PDF, mengeluarkan metadata, menetapkan maklumat dokumen PDF, dll.", "input": "", "output": "__author__ = 'fahadadeel'\r\nimport jpype\r\nimport re\r\nimport datetime\r\n\r\nclass AddJavascript:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n        self.JavascriptAction=jpype.JClass(\"com.aspose.pdf.JavascriptAction\")\r\n\r\n    def main(self):\r\n\r\n        # Open a pdf document.\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'Template.pdf'\r\n\r\n\r\n        # Adding JavaScript at Document Level\r\n        # Instantiate JavascriptAction with desried JavaScript statement\r\n        javaScript = self.JavascriptAction(\"this.print({bUI:true,bSilent:false,bShrinkToFit:true});\");\r\n\r\n        # Assign JavascriptAction object to desired action of Document\r\n        doc.setOpenAction(javaScript)\r\n        js=self.JavascriptAction(\"app.alert('page 2 is opened')\")\r\n\r\n        # Adding JavaScript at Page Level\r\n        doc.getPages.get_Item(2)\r\n        doc.getActions().setOnOpen(js())\r\n        doc.getPages().get_Item(2).getActions().setOnClose(self.JavascriptAction(\"app.alert('page 2 is closed')\"))\r\n\r\n        # Save PDF Document\r\n        doc.save(self.dataDir + \"JavaScript-Added.pdf\")\r\n\r\n        print \"Added JavaScript Successfully, please check the output file.\"\r\n\r\nclass AddToc:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n        self.TocInfo=jpype.JClass(\"com.aspose.pdf.TocInfo\")\r\n        self.TextFragment=jpype.JClass(\"com.aspose.pdf.TextFragment\")\r\n        self.TextSegment=jpype.JClass(\"com.aspose.pdf.TextSegment\")\r\n        self.Heading=jpype.JClass(\"com.aspose.pdf.Heading\")\r\n\r\n    def main(self):\r\n        # Open a pdf document.\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get access to first page of PDF file\r\n        toc_page = doc.getPages().insert(1)\r\n\r\n        # Create object to represent TOC information\r\n        toc_info = self.TocInfo()\r\n        title = self.TextFragment(\"Table Of Contents\")\r\n        title.getTextState().setFontSize(20)\r\n\r\n        # Set the title for TOC\r\n        toc_info.setTitle(title)\r\n        toc_page.setTocInfo(toc_info)\r\n\r\n        # Create string objects which will be used as TOC elements\r\n        titles = [\"First page\", \"Second page\"]\r\n\r\n        i = 0;\r\n        while (i < 2):\r\n            # Create Heading object\r\n            heading2 = self.Heading(1);\r\n\r\n            segment2 = self.TextSegment\r\n            heading2.setTocPage(toc_page)\r\n            heading2.getSegments().add(segment2)\r\n\r\n            # Specify the destination page for heading object\r\n            heading2.setDestinationPage(doc.getPages().get_Item(i + 2))\r\n\r\n            # Destination page\r\n            heading2.setTop(doc.getPages().get_Item(i + 2).getRect().getHeight())\r\n\r\n            # Destination coordinate\r\n            segment2.setText(titles[i])\r\n\r\n            # Add heading to page containing TOC\r\n            toc_page.getParagraphs().add(heading2)\r\n\r\n            i +=1;\r\n\r\n\r\n        # Save PDF Document\r\n        doc.save(self.dataDir + \"TOC.pdf\")\r\n\r\n        print \"Added TOC Successfully, please check the output file.\"\r\n\r\nclass GetDocumentWindow:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get different document properties\r\n        # Position of document's window - Default: false\r\n        print \"CenterWindow :- \" + str(doc.getCenterWindow())\r\n\r\n        # Predominant reading order; determine the position of page\r\n        # when displayed side by side - Default: L2R\r\n        print \"Direction :- \" + str(doc.getDirection())\r\n\r\n        # Whether window's title bar should display document title.\r\n        # If false, title bar displays PDF file name - Default: false\r\n        print \"DisplayDocTitle :- \" + str(doc.getDisplayDocTitle())\r\n\r\n        #Whether to resize the document's window to fit the size of\r\n        #first displayed page - Default: false\r\n        print \"FitWindow :- \" + str(doc.getFitWindow())\r\n\r\n        # Whether to hide menu bar of the viewer application - Default: false\r\n        print \"HideMenuBar :-\" + str(doc.getHideMenubar())\r\n\r\n        # Whether to hide tool bar of the viewer application - Default: false\r\n        print \"HideToolBar :-\" + str(doc.getHideToolBar())\r\n\r\n        # Whether to hide UI elements like scroll bars\r\n        # and leaving only the page contents displayed - Default: false\r\n        print \"HideWindowUI :-\" + str(doc.getHideWindowUI())\r\n\r\n        # The document's page mode. How to display document on exiting full-screen mode.\r\n        print \"NonFullScreenPageMode :-\" + str(doc.getNonFullScreenPageMode())\r\n\r\n        # The page layout i.e. single page, one column\r\n        print \"PageLayout :-\" + str(doc.getPageLayout())\r\n\r\n        #How the document should display when opened.\r\n        print \"pageMode :-\" + str(doc.getPageMode())\r\n\r\nclass GetPdfFileInfo:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get document information\r\n        doc_info = doc.getInfo();\r\n\r\n        # Show document information\r\n        print \"Author:-\" + str(doc_info.getAuthor())\r\n        print \"Creation Date:-\" + str(doc_info.getCreationDate())\r\n        print \"Keywords:-\" + str(doc_info.getKeywords())\r\n        print \"Modify Date:-\" + str(doc_info.getModDate())\r\n        print \"Subject:-\" + str(doc_info.getSubject())\r\n        print \"Title:-\" + str(doc_info.getTitle())\r\n\r\nclass GetXMPMetadata:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get properties\r\n        print \"xmp:CreateDate: \" + str(doc.getMetadata().get_Item(\"xmp:CreateDate\"))\r\n        print \"xmp:Nickname: \" + str(doc.getMetadata().get_Item(\"xmp:Nickname\"))\r\n        print \"xmp:CustomProperty: \" + str(doc.getMetadata().get_Item(\"xmp:CustomProperty\"))\r\n\r\n\r\n\r\nclass Optimize:\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n#            self.OptimizationOptions=jpype.JClass(\"com.aspose.pdf.Document.OptimizationOptions\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            # Optimize for web\r\n            doc.optimize();\r\n\r\n            #Save output document\r\n            doc.save(self.dataDir + \"Optimized_Web.pdf\")\r\n\r\n            print \"Optimized PDF for the Web, please check output file.\"\r\n\r\nclass RemoveMetadata:\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            if (re.findall('/pdfaid:part/',doc.getMetadata())):\r\n                doc.getMetadata().removeItem(\"pdfaid:part\")\r\n\r\n\r\n            if (re.findall('/dc:format/',doc.getMetadata())):\r\n                doc.getMetadata().removeItem(\"dc:format\")\r\n\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"Remove_Metadata.pdf\")\r\n\r\n            print \"Removed metadata successfully, please check output file.\"\r\n\r\nclass SetExpiration:\r\n\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n            self.JavascriptAction=jpype.JClass(\"com.aspose.pdf.JavascriptAction\")\r\n\r\n        def main(self):\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            javascript = self.JavascriptAction(\r\n                \"var year=2014; var month=4;today = new Date();today = new Date(today.getFullYear(), today.getMonth());expiry = new Date(year, month);if (today.getTime() > expiry.getTime())app.alert('The file is expired. You need a new one.');\");\r\n\r\n            doc.setOpenAction(javascript);\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"set_expiration.pdf\");\r\n\r\n            print \"Update document information, please check output file.\"\r\n\r\n\r\nclass SetPdfFileInfo:\r\n\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            # Get document information\r\n            doc_info = doc.getInfo();\r\n\r\n            doc_info.setAuthor(\"Aspose.Pdf for java\");\r\n            doc_info.setCreationDate(datetime.today.strftime(\"%m/%d/%Y\"));\r\n            doc_info.setKeywords(\"Aspose.Pdf, DOM, API\");\r\n            doc_info.setModDate(datetime.today.strftime(\"%m/%d/%Y\"));\r\n            doc_info.setSubject(\"PDF Information\");\r\n            doc_info.setTitle(\"Setting PDF Document Information\");\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"Updated_Information.pdf\")\r\n\r\n            print \"Update document information, please check output file.\"", "instruction_en": "The given code contains several classes that perform different tasks. To use these classes, you need to create an instance of the desired class and call its main method. For example, to add JavaScript to a PDF document, create an instance of the AddJavascript class, passing the path to the input PDF file as a parameter. Then call the main method of the class. The output PDF file with the added JavaScript will be saved in the same directory as the input file. Similarly, you can use other classes to perform different tasks such as adding a table of contents, optimizing a PDF file, removing metadata, setting PDF document information, etc.", "original_line_number": 873, "instruction_length": 679}
{"instruction": "Kod yang diberikan adalah skrip python yang melakukan pelbagai tugas pemprosesan bahasa semulajadi pada data Twitter. Untuk menggunakan skrip ini, anda perlu mengimport perpustakaan yang diperlukan seperti JSON, Sklearn, NLTK, WordCloud, Matplotlib, dan DB. Kemudian, anda boleh menentukan pembolehubah dataset_path untuk menunjuk ke lokasi fail data Twitter. Fungsi calc_frequencies mengira kekerapan perkataan dalam senarai perkataan. Fungsi get_word_clouds menghasilkan awan perkataan untuk setiap pengguna dalam senarai tweet. Fungsi save_wordcloud_image menjimatkan imej awan perkataan ke fail. Fungsi LOAD_EXAMPLE_DATA memuatkan Contoh Data Twitter. Fungsi get_corpus_of_most_active_users mengembalikan korpus tweet dari pengguna yang paling aktif. Akhirnya, fungsi utama memanggil fungsi get_word_clouds dan menjimatkan awan perkataan yang dihasilkan ke pangkalan data.", "input": "", "output": "import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport db\nimport os\n\nDATASET_PATH = os.environ['HOME'] + '/nltk_data/corpora/twitter_samples/tweets.20150430-223406.json'\ndef calc_frequencies(words, words_n=50, lang='german'):\n    words = [word for word in words if len(word) > 1]\n    words = [word for word in words if not word.isnumeric()]\n    words = [word.lower() for word in words]\n    # words = [word for word in words if word not in all_stopwords]\n    # Stemming words seems to make matters worse, disabled\n    # stemmer = nltk.stem.snowball.SnowballStemmer(lang)\n    # words = [stemmer.stem(word) for word in words]\n\n    fdist = nltk.FreqDist(words)\n    return fdist.most_common(words_n)\n\ndef get_word_clouds(tweets, users, words_n=50, lang='english'):\n    default_stopwords = set(nltk.corpus.stopwords.words(lang))\n    stopwords_file = '../data/stopwords.txt'\n    custom_stopwords = set(open(stopwords_file, 'r').read().splitlines())\n    all_stopwords = default_stopwords | custom_stopwords\n\n    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=list(all_stopwords))\n    X = vectorizer.fit_transform(tweets)\n    terms = vectorizer.get_feature_names()\n\n    word_cloud_per_person = {}\n    for doc in range(len(tweets)):\n        feature_index = X[doc, :].nonzero()[1]\n        tfidf_scores = zip(feature_index, [X[doc, x] for x in feature_index])\n        doc_terms = []\n        for word, score in [(terms[i], score) for (i, score) in tfidf_scores]:\n            doc_terms.append((word, score))\n        important_terms = [(word, score) for word, score in sorted(doc_terms, key=lambda x: x[1], reverse=True)][:words_n]\n        word_cloud_per_person[users[doc]] = important_terms\n    return word_cloud_per_person\n\ndef save_wordcloud_image(frequencies, filename):\n    wordcloud = WordCloud(width=1024, height=786, min_font_size=1).fit_words(frequencies)\n    fig = plt.figure()\n    fig.set_figwidth(12)\n    fig.set_figheight(16)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.savefig(filename, facecolor='k', bbox_inches='tight')\n    print('imaged created')\n\ndef load_example_data():\n    tweets = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['text'])\n    return tweets\n\ndef get_corpus_of_most_active_users(n_users=5):\n    tweets = []\n    texts = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['user']['screen_name'])\n            texts.append((json.loads(line)['user']['screen_name'], json.loads(line)['text']))\n\n    users = nltk.FreqDist(tweets).most_common(n_users)\n\n    dict = {}\n    for user, tweet in texts:\n        if user in dict:\n            dict[user] = \" \".join([dict[user],tweet])\n        else:\n            dict[user] = tweet\n\n    corpus = [dict[name] for name, _ in users]\n    user_names = [name for name, _ in users]\n    return  corpus, user_names\n\nif __name__ == \"__main__\":\n    corpus, users = get_corpus_of_most_active_users()\n    word_cloud_per_person = get_word_clouds(corpus, users, words_n=100, lang='english')\n    for user in users:\n        topic_frequencies = word_cloud_per_person[user]\n        print user\n        print topic_frequencies\n        db.save_word_frequencies('test_user_seb', dict(topic_frequencies))\n        exit()\n        # save_wordcloud_image(dict(topic_frequencies), 'plots/word_clouds/' + user + '.png')\n\n\n    # This is an example how to save a word_cloud in the database\n    # user_in_db = 'malechanissen'\n    # db.save_word_frequencies(user_in_db, {'w3':10, 'w4':20})\n    # db.save_word_frequencies(user_in_db, dict(topic_frequencies))\n\n    # db.save_word_frequencies('test_user_seb', {'w3':10, 'w4':20})", "instruction_en": "The given code is a Python script that performs various natural language processing tasks on Twitter data. To use this script, you need to import the required libraries such as json, sklearn, nltk, wordcloud, matplotlib, and db. Then, you can define the DATASET_PATH variable to point to the location of the Twitter data file. The calc_frequencies function calculates the frequency of words in a list of words. The get_word_clouds function generates a word cloud for each user in a list of tweets. The save_wordcloud_image function saves a word cloud image to a file. The load_example_data function loads example Twitter data. The get_corpus_of_most_active_users function returns a corpus of tweets from the most active users. Finally, the main function calls the get_word_clouds function and saves the resulting word clouds to a database.", "original_line_number": 1275, "instruction_length": 876}
{"instruction": "Kod ini mentakrifkan beberapa kelas untuk mewujudkan modul HTML dalam kursus EDX. Kelas -kelas termasuk htmlfields, htmlmodule, htmldescriptor, tentang medan, mengenai perkara, mengenai statictabfields, staticTabmodule, staticTabdescriptor, coursinfofields, courseinfomodule, dan courseinfodescriptor. Kelas -kelas ini menentukan pelbagai bidang dan kaedah untuk membuat dan mengedit kandungan HTML dalam kursus, termasuk memuatkan dan mengeksport kandungan HTML, menentukan nama paparan dan membantu teks, dan menentukan sumber JavaScript dan CSS.", "input": "", "output": "import copy\nfrom fs.errors import ResourceNotFoundError\nimport logging\nimport os\nimport sys\nfrom lxml import etree\nfrom path import path\n\nfrom pkg_resources import resource_string\nfrom xblock.fields import Scope, String, Boolean, List\nfrom xmodule.editing_module import EditingDescriptor\nfrom xmodule.html_checker import check_html\nfrom xmodule.stringify import stringify_children\nfrom xmodule.x_module import XModule\nfrom xmodule.xml_module import XmlDescriptor, name_to_pathname\nimport textwrap\nfrom xmodule.contentstore.content import StaticContent\nfrom xblock.core import XBlock\n\nlog = logging.getLogger(\"edx.courseware\")\n\n\nclass HtmlFields(object):\n    display_name = String(\n        display_name=\"Display Name\",\n        help=\"This name appears in the horizontal navigation at the top of the page.\",\n        scope=Scope.settings,\n        # it'd be nice to have a useful default but it screws up other things; so,\n        # use display_name_with_default for those\n        default=\"Text\"\n    )\n    data = String(help=\"Html contents to display for this module\", default=u\"\", scope=Scope.content)\n    source_code = String(help=\"Source code for LaTeX documents. This feature is not well-supported.\", scope=Scope.settings)\n    use_latex_compiler = Boolean(\n        help=\"Enable LaTeX templates?\",\n        default=False,\n        scope=Scope.settings\n    )\n\n\nclass HtmlModule(HtmlFields, XModule):\n    js = {\n        'coffee': [\n            resource_string(__name__, 'js/src/javascript_loader.coffee'),\n            resource_string(__name__, 'js/src/collapsible.coffee'),\n            resource_string(__name__, 'js/src/html/display.coffee')\n        ],\n        'js': [\n            resource_string(__name__, 'js/src/html/imageModal.js'),\n            resource_string(__name__, 'js/common_static/js/vendor/draggabilly.pkgd.js')\n        ]\n    }\n    js_module_name = \"HTMLModule\"\n    css = {'scss': [resource_string(__name__, 'css/html/display.scss')]}\n\n    def get_html(self):\n        if self.system.anonymous_student_id:\n            return self.data.replace(\"%%USER_ID%%\", self.system.anonymous_student_id)\n        return self.data\n\n\nclass HtmlDescriptor(HtmlFields, XmlDescriptor, EditingDescriptor):\n    \"\"\"\n    Module for putting raw html in a course\n    \"\"\"\n    mako_template = \"widgets/html-edit.html\"\n    module_class = HtmlModule\n    filename_extension = \"xml\"\n    template_dir_name = \"html\"\n\n    js = {'coffee': [resource_string(__name__, 'js/src/html/edit.coffee')]}\n    js_module_name = \"HTMLEditingDescriptor\"\n    css = {'scss': [resource_string(__name__, 'css/editor/edit.scss'), resource_string(__name__, 'css/html/edit.scss')]}\n\n    # VS[compat] TODO (cpennington): Delete this method once all fall 2012 course\n    # are being edited in the cms\n    @classmethod\n    def backcompat_paths(cls, path):\n        if path.endswith('.html.xml'):\n            path = path[:-9] + '.html'  # backcompat--look for html instead of xml\n        if path.endswith('.html.html'):\n            path = path[:-5]  # some people like to include .html in filenames..\n        candidates = []\n        while os.sep in path:\n            candidates.append(path)\n            _, _, path = path.partition(os.sep)\n\n        # also look for .html versions instead of .xml\n        nc = []\n        for candidate in candidates:\n            if candidate.endswith('.xml'):\n                nc.append(candidate[:-4] + '.html')\n        return candidates + nc\n\n    @classmethod\n    def filter_templates(cls, template, course):\n        \"\"\"\n        Filter template that contains 'latex' from templates.\n\n        Show them only if use_latex_compiler is set to True in\n        course settings.\n        \"\"\"\n        return (not 'latex' in template['template_id'] or course.use_latex_compiler)\n\n    def get_context(self):\n        \"\"\"\n        an override to add in specific rendering context, in this case we need to\n        add in a base path to our c4x content addressing scheme\n        \"\"\"\n        _context = EditingDescriptor.get_context(self)\n        # Add some specific HTML rendering context when editing HTML modules where we pass\n        # the root /c4x/ url for assets. This allows client-side substitutions to occur.\n        _context.update({\n            'base_asset_url': StaticContent.get_base_url_path_for_course_assets(self.location) + '/',\n            'enable_latex_compiler': self.use_latex_compiler,\n        })\n        return _context\n\n    # NOTE: html descriptors are special.  We do not want to parse and\n    # export them ourselves, because that can break things (e.g. lxml\n    # adds body tags when it exports, but they should just be html\n    # snippets that will be included in the middle of pages.\n\n    @classmethod\n    def load_definition(cls, xml_object, system, location):\n        '''Load a descriptor from the specified xml_object:\n\n        If there is a filename attribute, load it as a string, and\n        log a warning if it is not parseable by etree.HTMLParser.\n\n        If there is not a filename attribute, the definition is the body\n        of the xml_object, without the root tag (do not want <html> in the\n        middle of a page)\n        '''\n        filename = xml_object.get('filename')\n        if filename is None:\n            definition_xml = copy.deepcopy(xml_object)\n            cls.clean_metadata_from_xml(definition_xml)\n            return {'data': stringify_children(definition_xml)}, []\n        else:\n            # html is special.  cls.filename_extension is 'xml', but\n            # if 'filename' is in the definition, that means to load\n            # from .html\n            # 'filename' in html pointers is a relative path\n            # (not same as 'html/blah.html' when the pointer is in a directory itself)\n            pointer_path = \"{category}/{url_path}\".format(\n                category='html',\n                url_path=name_to_pathname(location.name)\n            )\n            base = path(pointer_path).dirname()\n            # log.debug(\"base = {0}, base.dirname={1}, filename={2}\".format(base, base.dirname(), filename))\n            filepath = \"{base}/{name}.html\".format(base=base, name=filename)\n            # log.debug(\"looking for html file for {0} at {1}\".format(location, filepath))\n\n            # VS[compat]\n            # TODO (cpennington): If the file doesn't exist at the right path,\n            # give the class a chance to fix it up. The file will be written out\n            # again in the correct format.  This should go away once the CMS is\n            # online and has imported all current (fall 2012) courses from xml\n            if not system.resources_fs.exists(filepath):\n                candidates = cls.backcompat_paths(filepath)\n                # log.debug(\"candidates = {0}\".format(candidates))\n                for candidate in candidates:\n                    if system.resources_fs.exists(candidate):\n                        filepath = candidate\n                        break\n\n            try:\n                with system.resources_fs.open(filepath) as file:\n                    html = file.read().decode('utf-8')\n                    # Log a warning if we can't parse the file, but don't error\n                    if not check_html(html) and len(html) > 0:\n                        msg = \"Couldn't parse html in {0}, content = {1}\".format(filepath, html)\n                        log.warning(msg)\n                        system.error_tracker(\"Warning: \" + msg)\n\n                    definition = {'data': html}\n\n                    # TODO (ichuang): remove this after migration\n                    # for Fall 2012 LMS migration: keep filename (and unmangled filename)\n                    definition['filename'] = [filepath, filename]\n\n                    return definition, []\n\n            except (ResourceNotFoundError) as err:\n                msg = 'Unable to load file contents at path {0}: {1} '.format(\n                    filepath, err)\n                # add more info and re-raise\n                raise Exception(msg), None, sys.exc_info()[2]\n\n    # TODO (vshnayder): make export put things in the right places.\n\n    def definition_to_xml(self, resource_fs):\n        ''' Write <html filename=\"\" [meta-attrs=\"...\"]> to filename.xml, and the html\n        string to filename.html.\n        '''\n\n        # Write html to file, return an empty tag\n        pathname = name_to_pathname(self.url_name)\n        filepath = u'{category}/{pathname}.html'.format(\n            category=self.category,\n            pathname=pathname\n        )\n\n        resource_fs.makedir(os.path.dirname(filepath), recursive=True, allow_recreate=True)\n        with resource_fs.open(filepath, 'w') as filestream:\n            html_data = self.data.encode('utf-8')\n            filestream.write(html_data)\n\n        # write out the relative name\n        relname = path(pathname).basename()\n\n        elt = etree.Element('html')\n        elt.set(\"filename\", relname)\n        return elt\n\n    @property\n    def non_editable_metadata_fields(self):\n        non_editable_fields = super(HtmlDescriptor, self).non_editable_metadata_fields\n        non_editable_fields.append(HtmlDescriptor.use_latex_compiler)\n        return non_editable_fields\n\n\nclass AboutFields(object):\n    display_name = String(\n        help=\"Display name for this module\",\n        scope=Scope.settings,\n        default=\"overview\",\n    )\n    data = String(\n        help=\"Html contents to display for this module\",\n        default=\"\",\n        scope=Scope.content\n    )\n\n\n@XBlock.tag(\"detached\")\nclass AboutModule(AboutFields, HtmlModule):\n    \"\"\"\n    Overriding defaults but otherwise treated as HtmlModule.\n    \"\"\"\n    pass\n\n\n@XBlock.tag(\"detached\")\nclass AboutDescriptor(AboutFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = \"about\"\n    module_class = AboutModule\n\n\nclass StaticTabFields(object):\n    \"\"\"\n    The overrides for Static Tabs\n    \"\"\"\n    display_name = String(\n        display_name=\"Display Name\",\n        help=\"This name appears in the horizontal navigation at the top of the page.\",\n        scope=Scope.settings,\n        default=\"Empty\",\n    )\n    data = String(\n        default=textwrap.dedent(\"\"\"\\\n            <p>This is where you can add additional pages to your courseware. Click the 'edit' button to begin editing.</p>\n        \"\"\"),\n        scope=Scope.content,\n        help=\"HTML for the additional pages\"\n    )\n\n\n@XBlock.tag(\"detached\")\nclass StaticTabModule(StaticTabFields, HtmlModule):\n    \"\"\"\n    Supports the field overrides\n    \"\"\"\n    pass\n\n\n@XBlock.tag(\"detached\")\nclass StaticTabDescriptor(StaticTabFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = None\n    module_class = StaticTabModule\n\n\nclass CourseInfoFields(object):\n    \"\"\"\n    Field overrides\n    \"\"\"\n    items = List(\n        help=\"List of course update items\",\n        default=[],\n        scope=Scope.content\n    )\n    data = String(\n        help=\"Html contents to display for this module\",\n        default=\"<ol></ol>\",\n        scope=Scope.content\n    )\n\n\n@XBlock.tag(\"detached\")\nclass CourseInfoModule(CourseInfoFields, HtmlModule):\n    \"\"\"\n    Just to support xblock field overrides\n    \"\"\"\n    # statuses\n    STATUS_VISIBLE = 'visible'\n    STATUS_DELETED = 'deleted'\n\n\n@XBlock.tag(\"detached\")\nclass CourseInfoDescriptor(CourseInfoFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = None\n    module_class = CourseInfoModule", "instruction_en": "This code defines several classes for creating HTML modules in an edX course. The classes include HtmlFields, HtmlModule, HtmlDescriptor, AboutFields, AboutModule, AboutDescriptor, StaticTabFields, StaticTabModule, StaticTabDescriptor, CourseInfoFields, CourseInfoModule, and CourseInfoDescriptor. The classes define various fields and methods for creating and editing HTML content in a course, including loading and exporting HTML content, defining display names and help text, and specifying JavaScript and CSS resources.", "original_line_number": 1571, "instruction_length": 548}
{"instruction": "Kod ini mewujudkan rangkaian ekspresi khusus sampel untuk satu lipatan dan satu ulangi pengesahan silang yang mana indeks lipatan telah dikira. Untuk menggunakannya, jalankan skrip `SetupSubtypestratiedCv_ComputeNetWorks.py` dengan argumen berikut:` aces_dir`, `data_dir`,` lipatan`, dan `mengulang`. Data akan disimpan di bawah `<strata_dir>/ulangi <ulangi idx>` dengan struktur yang ditentukan. Skrip membaca data ekspresi dan label sampel dari fail, dan mencipta rangkaian menggunakan modul `CoExpressionNetwork`.", "input": "", "output": "# @Author \n# Chloe-Agathe Azencott\n# chloe-agathe.azencott@mines-paristech.fr\n# April 2016\n\nimport argparse\nimport h5py\nimport numpy as np\nimport os\nimport sys\n\nimport CoExpressionNetwork\n\ndef main():\n    \"\"\" Create sample-specific co-expression networks for one fold and one repeat\n    of a cross-validation for which fold indices have already been computed.\n\n    The data will be stored under\n        <data_dir>/repeat<repeat idx>\n    with the following structure:\n        edges.gz: \n            Gzipped file containing the list of edges of the co-expression networks.\n            Each line is an undirected edge, formatted as:\n                <index of gene 1> <index of gene 2>\n            By convention, the index of gene 1 is smaller than that of gene 2.\n        For k=0..(numFolds-1):\n            <k>/lioness/edge_weights.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the LIONESS co-expression networks\n                for the training samples.\n            <k>/lioness/edge_weights_te.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the LIONESS co-expression networks\n                for the test samples.\n            <k>/regline/edge_weights.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the Regline co-expression networks\n                for the training samples.\n            <k>/regline/edge_weights_te.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the Regline co-expression networks\n                for the test samples.\n\n    Parameters\n    ----------\n    aces_dir: path\n        Path to the ACES folder.\n    data_dir: path\n        Path to the folder containing fold indices (under <data_dir>/repeat<repeat_idx>/fold<fold_idx>).\n    fold: int\n        Fold index.\n    repeat: int\n        Repeat index.\n\n    Example\n    -------\n        $ python setUpSubTypeStratifiedCV_computeNetworks.py ACES outputs/U133A_combat_RFS/subtype_stratified 0 0\n    \n    Reference\n    ---------\n    Allahyar, A., and Ridder, J. de (2015).\n    FERAL: network-based classifier with application to breast cancer outcome prediction.\n    Bioinformatics 31, i311--i319.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Build sample-specific co-expression networks\" + \\\n                                     \"for a 10-fold sub-type stratified CV on the RFS data\",\n                                     add_help=True)\n    parser.add_argument(\"aces_dir\", help=\"Path to ACES data\")\n    parser.add_argument(\"data_dir\", help=\"Path to the fold indices\")\n    parser.add_argument(\"fold\", help=\"Index of the fold\", type=int)\n    parser.add_argument(\"repeat\", help=\"Index of the repeat\", type=int)\n    args = parser.parse_args()\n\n    outDir = '%s/repeat%d' % (args.data_dir, args.repeat)\n\n    # Get expression data, sample labels.\n    # Do not normalize the data while loading it (so as not to use test data for normalization).\n    f = h5py.File(\"%s/experiments/data/U133A_combat.h5\" % args.aces_dir)\n    expressionData = np.array(f['U133A_combat_RFS']['ExpressionData'])\n    sampleLabels = np.array(f['U133A_combat_RFS']['PatientClassLabels'])\n    f.close()\n    \n    foldNr = args.fold\n    # Output directory\n    foldDir = \"%s/fold%d\" % (outDir, foldNr)\n\n    # Read train indices from file\n    trIndicesF = '%s/train.indices' % foldDir\n    trIndices = np.loadtxt(trIndicesF, dtype=int)\n    sys.stdout.write(\"Read training indices for fold %d from %s\\n\" % (foldNr, trIndicesF))\n\n    # Read test indices from file\n    teIndicesF = '%s/test.indices' % foldDir\n    teIndices = np.loadtxt(teIndicesF, dtype=int)\n    sys.stdout.write(\"Read training indices for fold %d from %s\\n\" % (foldNr, teIndicesF))\n    print teIndices\n    print teIndices.shape\n\n    # Create networks\n    CoExpressionNetwork.run_whole_data(expressionData, sampleLabels, foldDir,\n                                       trIndices=trIndices, teIndices=teIndices)\n\n\n\nif __name__ == \"__main__\":\n    main()", "instruction_en": "This code creates sample-specific co-expression networks for one fold and one repeat of a cross-validation for which fold indices have already been computed. To use it, run the script `setUpSubTypeStratifiedCV_computeNetworks.py` with the following arguments: `aces_dir`, `data_dir`, `fold`, and `repeat`. The data will be stored under `<data_dir>/repeat<repeat idx>` with the specified structure. The script reads expression data and sample labels from a file, and creates networks using the `CoExpressionNetwork` module.", "original_line_number": 2094, "instruction_length": 516}
{"instruction": "Kod ini adalah pengawal untuk pemacu kebezaan. Untuk menggunakannya, anda perlu membuat contoh kelas DiffController, lulus dalam peranti dan nama sebagai argumen. Kemudian, hubungi kaedah permulaan () untuk memulakan pengawal. Akhirnya, hubungi kaedah kemas kini () dalam gelung untuk mengemas kini pengawal dan mengendalikan permintaan pergerakan. Kaedah CMDVELCB () boleh digunakan untuk mengendalikan permintaan pergerakan. Kaedah penutupan () boleh dipanggil untuk menghentikan pengawal. Kaedah getDiagnostics () boleh digunakan untuk mendapatkan status diagnostik.", "input": "", "output": "#!/usr/bin/env python\n\n\"\"\"\n  diff_controller.py - controller for a differential drive\n  Copyright (c) 2010-2011 Vanadium Labs LLC.  All right reserved.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n      * Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n      * Redistributions in binary form must reproduce the above copyright\n        notice, this list of conditions and the following disclaimer in the\n        documentation and/or other materials provided with the distribution.\n      * Neither the name of Vanadium Labs LLC nor the names of its \n        contributors may be used to endorse or promote products derived \n        from this software without specific prior written permission.\n  \n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL VANADIUM LABS BE LIABLE FOR ANY DIRECT, INDIRECT,\n  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n  OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport rospy\n\nfrom math import sin,cos,pi\n\nfrom geometry_msgs.msg import Quaternion\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nfrom diagnostic_msgs.msg import *\nfrom tf.broadcaster import TransformBroadcaster\n\nfrom ax12 import *\nfrom controllers import *\nfrom struct import unpack\n\nclass DiffController(Controller):\n    \"\"\" Controller to handle movement & odometry feedback for a differential \n            drive mobile base. \"\"\"\n    def __init__(self, device, name):\n        Controller.__init__(self, device, name)\n        self.pause = True\n        self.last_cmd = rospy.Time.now()\n\n        # parameters: rates and geometry\n        self.rate = rospy.get_param('~controllers/'+name+'/rate',10.0)\n        self.timeout = rospy.get_param('~controllers/'+name+'/timeout',1.0)\n        self.t_delta = rospy.Duration(1.0/self.rate)\n        self.t_next = rospy.Time.now() + self.t_delta\n        self.ticks_meter = float(rospy.get_param('~controllers/'+name+'/ticks_meter'))\n        self.base_width = float(rospy.get_param('~controllers/'+name+'/base_width'))\n\n        self.base_frame_id = rospy.get_param('~controllers/'+name+'/base_frame_id', 'base_link')\n        self.odom_frame_id = rospy.get_param('~controllers/'+name+'/odom_frame_id', 'odom')\n\n        # parameters: PID\n        self.Kp = rospy.get_param('~controllers/'+name+'/Kp', 5)\n        self.Kd = rospy.get_param('~controllers/'+name+'/Kd', 1)\n        self.Ki = rospy.get_param('~controllers/'+name+'/Ki', 0)\n        self.Ko = rospy.get_param('~controllers/'+name+'/Ko', 50)\n\n        # parameters: acceleration\n        self.accel_limit = rospy.get_param('~controllers/'+name+'/accel_limit', 0.1)\n        self.max_accel = int(self.accel_limit*self.ticks_meter/self.rate)\n\n        # output for joint states publisher\n        self.joint_names = [\"base_l_wheel_joint\",\"base_r_wheel_joint\"]\n        self.joint_positions = [0,0]\n        self.joint_velocities = [0,0]\n\n        # internal data            \n        self.v_left = 0                 # current setpoint velocity\n        self.v_right = 0\n        self.v_des_left = 0             # cmd_vel setpoint\n        self.v_des_right = 0\n        self.enc_left = None            # encoder readings\n        self.enc_right = None\n        self.x = 0                      # position in xy plane\n        self.y = 0\n        self.th = 0\n        self.dx = 0                     # speeds in x/rotation\n        self.dr = 0\n        self.then = rospy.Time.now()    # time for determining dx/dy\n\n        # subscriptions\n        rospy.Subscriber(\"cmd_vel\", Twist, self.cmdVelCb)\n        self.odomPub = rospy.Publisher(\"odom\", Odometry, queue_size=5)\n        self.odomBroadcaster = TransformBroadcaster()\n\t\t\n        rospy.loginfo(\"Started DiffController (\"+name+\"). Geometry: \" + str(self.base_width) + \"m wide, \" + str(self.ticks_meter) + \" ticks/m.\")\n\n    def startup(self):\n        if not self.fake:\n            self.setup(self.Kp,self.Kd,self.Ki,self.Ko) \n    \n    def update(self):\n        now = rospy.Time.now()\n        if now > self.t_next:\n            elapsed = now - self.then\n            self.then = now\n            elapsed = elapsed.to_sec()\n\n            if self.fake:\n                x = cos(self.th)*self.dx*elapsed\n                y = -sin(self.th)*self.dx*elapsed\n                self.x += cos(self.th)*self.dx*elapsed\n                self.y += sin(self.th)*self.dx*elapsed\n                self.th += self.dr*elapsed\n            else:\n                # read encoders\n                try:\n                    left, right = self.status()\n                except Exception as e:\n                    rospy.logerr(\"Could not update encoders: \" + str(e))\n                    return\n                rospy.logdebug(\"Encoders: \" + str(left) +\",\"+ str(right))\n\n                # calculate odometry\n                if self.enc_left == None:\n                    d_left = 0\n                    d_right = 0\n                else:\n                    d_left = (left - self.enc_left)/self.ticks_meter\n                    d_right = (right - self.enc_right)/self.ticks_meter\n                self.enc_left = left\n                self.enc_right = right\n\n                d = (d_left+d_right)/2\n                th = (d_right-d_left)/self.base_width\n                self.dx = d / elapsed\n                self.dr = th / elapsed\n\n                if (d != 0):\n                    x = cos(th)*d\n                    y = -sin(th)*d\n                    self.x = self.x + (cos(self.th)*x - sin(self.th)*y)\n                    self.y = self.y + (sin(self.th)*x + cos(self.th)*y)\n                if (th != 0):\n                    self.th = self.th + th\n\n            # publish or perish\n            quaternion = Quaternion()\n            quaternion.x = 0.0 \n            quaternion.y = 0.0\n            quaternion.z = sin(self.th/2)\n            quaternion.w = cos(self.th/2)\n            self.odomBroadcaster.sendTransform(\n                (self.x, self.y, 0), \n                (quaternion.x, quaternion.y, quaternion.z, quaternion.w),\n                rospy.Time.now(),\n                self.base_frame_id,\n                self.odom_frame_id\n                )\n\n            odom = Odometry()\n            odom.header.stamp = now\n            odom.header.frame_id = self.odom_frame_id\n            odom.pose.pose.position.x = self.x\n            odom.pose.pose.position.y = self.y\n            odom.pose.pose.position.z = 0\n            odom.pose.pose.orientation = quaternion\n            odom.child_frame_id = self.base_frame_id\n            odom.twist.twist.linear.x = self.dx\n            odom.twist.twist.linear.y = 0\n            odom.twist.twist.angular.z = self.dr\n            self.odomPub.publish(odom)\n\n            if now > (self.last_cmd + rospy.Duration(self.timeout)):\n                self.v_des_left = 0\n                self.v_des_right = 0\n\n            # update motors\n            if not self.fake:\n                if self.v_left < self.v_des_left:\n                    self.v_left += self.max_accel\n                    if self.v_left > self.v_des_left:\n                        self.v_left = self.v_des_left\n                else:\n                    self.v_left -= self.max_accel\n                    if self.v_left < self.v_des_left:\n                        self.v_left = self.v_des_left\n                \n                if self.v_right < self.v_des_right:\n                    self.v_right += self.max_accel\n                    if self.v_right > self.v_des_right:\n                        self.v_right = self.v_des_right\n                else:\n                    self.v_right -= self.max_accel\n                    if self.v_right < self.v_des_right:\n                        self.v_right = self.v_des_right\n                self.write(self.v_left, self.v_right)\n\n            self.t_next = now + self.t_delta\n \n    def shutdown(self):\n        if not self.fake:\n            self.write(0,0)\n\n    def cmdVelCb(self,req):\n        \"\"\" Handle movement requests. \"\"\"\n        self.last_cmd = rospy.Time.now()\n        if self.fake:\n            self.dx = req.linear.x        # m/s\n            self.dr = req.angular.z       # rad/s\n        else:\n            # set motor speeds in ticks per 1/30s\n            self.v_des_left = int( ((req.linear.x - (req.angular.z * self.base_width/2.0)) * self.ticks_meter) / 30.0)\n            self.v_des_right = int( ((req.linear.x + (req.angular.z * self.base_width/2.0)) * self.ticks_meter) / 30.0)\n\n    def getDiagnostics(self):\n        \"\"\" Get a diagnostics status. \"\"\"\n        msg = DiagnosticStatus()\n        msg.name = self.name\n        msg.level = DiagnosticStatus.OK\n        msg.message = \"OK\"\n        if not self.fake:\n            msg.values.append(KeyValue(\"Left\", str(self.enc_left)))\n            msg.values.append(KeyValue(\"Right\", str(self.enc_right)))\n        msg.values.append(KeyValue(\"dX\", str(self.dx)))\n        msg.values.append(KeyValue(\"dR\", str(self.dr)))\n        return msg\n\n    ###\n    ### Controller Specification: \n    ###\n    ###  setup: Kp, Kd, Ki, Ko (all unsigned char)\n    ###\n    ###  write: left_speed, right_speed (2-byte signed, ticks per frame)\n    ###\n    ###  status: left_enc, right_enc (4-byte signed)\n    ### \n    \n    def setup(self, kp, kd, ki, ko):\n        success = self.device.execute(253, AX_CONTROL_SETUP, [10, kp, kd, ki, ko])\n\n    def write(self, left, right):\n        \"\"\" Send a closed-loop speed. Base PID loop runs at 30Hz, these values\n                are therefore in ticks per 1/30 second. \"\"\"\n        left = left&0xffff\n        right = right&0xffff\n        success = self.device.execute(253, AX_CONTROL_WRITE, [10, left%256, left>>8, right%256, right>>8])\n\n    def status(self):\n        \"\"\" read 32-bit (signed) encoder values. \"\"\"\n        values = self.device.execute(253, AX_CONTROL_STAT, [10])\n        left_values = \"\".join([chr(k) for k in values[0:4] ])        \n        right_values = \"\".join([chr(k) for k in values[4:] ])\n        try:\n            left = unpack('=l',left_values)[0]\n            right = unpack('=l',right_values)[0]\n            return [left, right]\n        except:\n            return None", "instruction_en": "This code is a controller for a differential drive. To use it, you need to create an instance of the DiffController class, passing in the device and name as arguments. Then, call the startup() method to initialize the controller. Finally, call the update() method in a loop to update the controller and handle movement requests. The cmdVelCb() method can be used to handle movement requests. The shutdown() method can be called to stop the controller. The getDiagnostics() method can be used to get a diagnostics status.", "original_line_number": 3199, "instruction_length": 569}
{"instruction": "Kod yang diberikan adalah skrip python yang mentakrifkan kelas SupportTicket dan beberapa fungsi. Untuk menggunakan kod ini, import modul yang diperlukan dan buat contoh kelas SupportTicket. Kelas ini mempunyai beberapa kaedah yang boleh dipanggil untuk mendapatkan pengirim, subjek, dan kandungan tiket sokongan, serta mengemas kini status dan menghantar pemberitahuan e -mel. Terdapat juga beberapa fungsi penolong untuk menetapkan status tiket, memberikan tiket kepada pengguna, dan mengemukakan semula akaun pengguna. Untuk menggunakan fungsi ini, hubungi mereka dengan hujah -hujah yang sesuai.", "input": "", "output": "# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\n\nfrom erpnext.utilities.transaction_base import TransactionBase\nfrom frappe.utils import now, extract_email_id\nimport json\nimport requests\n\n\nSTANDARD_USERS = (\"Guest\", \"Administrator\")\n\nclass SupportTicket(TransactionBase):\n\n\tdef get_sender(self, comm):\n\t\treturn frappe.db.get_value('Support Email Settings',None,'support_email')\n\n\tdef get_subject(self, comm):\n\t\treturn '[' + self.name + '] ' + (comm.subject or 'No Subject Specified')\n\n\tdef get_content(self, comm):\n\t\tsignature = frappe.db.get_value('Support Email Settings',None,'support_signature')\n\t\tcontent = comm.content\n\t\tif signature:\n\t\t\tcontent += '<p>' + signature + '</p>'\n\t\treturn content\n\n\tdef get_portal_page(self):\n\t\treturn \"ticket\"\n\n\tdef on_update1(self):\n\t\tfrom frappe.utils import get_url, cstr\n\t\tfrappe.errprint(get_url())\n\t\tif get_url()=='http://tailorpad.com':\n\t\t\tpass\n\t\telse:\n\t\t\tpr2 = frappe.db.sql(\"\"\"select name from `tabSupport Ticket`\"\"\")\n\t\t\tfrappe.errprint(pr2)\n\t\t\tfrappe.errprint(\"is feed back saved\")\n\t\t\tif pr2:\n\t\t\t\t# self.login()\n\t\t\t\tfrappe.errprint(\"in if for creation support ticket\")\n\t\t\t\ttest = {}\n\t\t\t\tsupport_ticket = self.get_ticket_details()\n\t\t\t\tself.call_del_keys(support_ticket)\n\t\t\t\t#test['communications'] = []\n\t\t\t\t#self.call_del_keys(support_ticket.get('communications'), test)\n\t\t\t\tself.login()\n\t\t\t\tfrappe.errprint(\"support_ticket\")\n\t\t\t\tfrappe.errprint(support_ticket)\n\t\t\t\tself.tenent_based_ticket_creation(support_ticket)\n\n\t# def on_update(self):\n\t\t# self.send_email()\t\n\n\tdef send_email(self):\n\t\tfrappe.errprint(\"in the sendmail\")\n\t\tfrom frappe.utils.user import get_user_fullname\n\t\tfrom frappe.utils import get_url\n\t\tif self.get(\"__islocal\") and get_url()=='http://tailorpad.com':\n\t\t\t\n\t\t\t# mail_titles = frappe.get_hooks().get(\"login_mail_title\", [])\n\t\t\t# title = frappe.db.get_default('company') or (mail_titles and mail_titles[0]) or \"\"\n\n\t\t\tfull_name = get_user_fullname(frappe.session['user'])\n\t\t\tif full_name == \"Guest\":\n\t\t\t\tfull_name = \"Administrator\"\n\n\t\t\tfirst_name = frappe.db.sql_list(\"\"\"select first_name from `tabUser` where name='%s'\"\"\"%(self.raised_by))\n\t\t\tfrappe.errprint(first_name[0])\n\n\t\t\tmsg=\"Dear  \"+first_name[0]+\"!<br><br>Support Ticket is created successfully for '\"+self.subject+\"'<br><br>Your Support Ticket Number is '\"+self.name+\"' <br><br>Please note for further information. <br><br>Regards, <br>Team TailorPad.\"\n\t\t\tsender = frappe.session.user not in STANDARD_USERS and frappe.session.user or None\n\t\t\tfrappe.sendmail(recipients=self.raised_by, sender=sender, subject=self.subject,\n\t\t\t\tmessage=msg)\n\n\n\tdef login(self):\n\t\tlogin_details = {'usr': 'Administrator', 'pwd': 'admin'}\n\t\turl = 'http://tailorpad.com/api/method/login'\n\t\theaders = {'content-type': 'application/x-www-form-urlencoded'}\n\t\tfrappe.errprint([url, 'data='+json.dumps(login_details)])\n\t\tresponse = requests.post(url, data='data='+json.dumps(login_details), headers=headers)\n\n\tdef get_ticket_details(self):\n\t\t# return frappe.get_doc('Support Ticket', self.name)\n\t\tresponse = requests.get(\"\"\"%(url)s/api/resource/Support Ticket/SUP-00001\"\"\"%{'url':get_url()})\n\t\t\n\t\t# frappe.errprint([\"\"\"%(url)s/api/resource/Support Ticket/%(name)s\"\"\"%{'url':get_url(), 'name':self.name}])\n\t\tfrappe.errprint(response.text)\n\t\treturn eval(response.text).get('data')\n\n\tdef call_del_keys(self, support_ticket):\n\t\tif support_ticket:\n\t\t\tif isinstance(support_ticket, dict):\n\t\t\t\tself.del_keys(support_ticket)\n\n\t\t\tif isinstance(support_ticket, list):\n\t\t\t\tfor comm in support_ticket:\n\t\t\t\t\tself.del_keys(comm)\n\n\tdef del_keys(self, support_ticket):\n\t\tfrappe.errprint(type(support_ticket))\n\t\tdel support_ticket['name']\n\t\tdel support_ticket['creation']\n\t\tdel support_ticket['modified']\n\t\tdel support_ticket['company']\n\n\tdef tenent_based_ticket_creation(self, support_ticket):\n\t\tfrappe.errprint(support_ticket)\n\t\turl = 'http://tailorpad.com/api/resource/Support Ticket'\n\t\t#url = 'http://192.168.5.12:7676/api/method/login'\n\t\theaders = {'content-type': 'application/x-www-form-urlencoded'}\n\t\tfrappe.errprint('data='+json.dumps(support_ticket))\n\t\tresponse = requests.post(url, data='data='+json.dumps(support_ticket), headers=headers)\n\n\t\tfrappe.errprint(response)\n\t\tfrappe.errprint(response.text)\n\n\tdef validate(self):\n\t\t\n\t\tself.update_status()\n\t\tself.set_lead_contact(self.raised_by)\n\n\t\tif self.status == \"Closed\":\n\t\t\tfrom frappe.widgets.form.assign_to import clear\n\t\t\tclear(self.doctype, self.name)\n\t\t#self.on_update1()\n\t\tself.send_email()\n\n\tdef set_lead_contact(self, email_id):\n\t\timport email.utils\n\t\temail_id = email.utils.parseaddr(email_id)\n\t\tif email_id:\n\t\t\tif not self.lead:\n\t\t\t\tself.lead = frappe.db.get_value(\"Lead\", {\"email_id\": email_id})\n\t\t\tif not self.contact:\n\t\t\t\tself.contact = frappe.db.get_value(\"Contact\", {\"email_id\": email_id})\n\n\t\t\tif not self.company:\n\t\t\t\tself.company = frappe.db.get_value(\"Lead\", self.lead, \"company\") or \\\n\t\t\t\t\tfrappe.db.get_default(\"company\")\n\n\tdef update_status(self):\n\t\tstatus = frappe.db.get_value(\"Support Ticket\", self.name, \"status\")\n\t\tif self.status!=\"Open\" and status ==\"Open\" and not self.first_responded_on:\n\t\t\tself.first_responded_on = now()\n\t\tif self.status==\"Closed\" and status !=\"Closed\":\n\t\t\tself.resolution_date = now()\n\t\tif self.status==\"Open\" and status !=\"Open\":\n\t\t\t# if no date, it should be set as None and not a blank string \"\", as per mysql strict config\n\t\t\tself.resolution_date = None\n\n@frappe.whitelist()\ndef set_status(name, status):\n\tst = frappe.get_doc(\"Support Ticket\", name)\n\tst.status = status\n\tst.save()\n\n@frappe.whitelist()\ndef get_admin(name):\n\tadmin = frappe.db.sql(\"select email_id_admin from tabUser  where name='administrator'\")\n\tfrappe.errprint(admin)\n\tfrappe.errprint(frappe.session.get('user'))\n\tif admin:\n\t\tfrappe.errprint(\"if\")\n\t\treturn admin[0][0]\n\telse:\n\t\tfrappe.errprint(\"else\")\n\t  \treturn frappe.session.get('user')\n\n\n@frappe.whitelist()\ndef assing_future(name, assign_in_future,raised_by,assign_to):\n  \tfrappe.errprint(\"in assign future\")\n  \tfrom frappe.utils import get_url, cstr\n\tif get_url()=='http://tailorpad.com':\n\t\tcheck_entry = frappe.db.sql(\"\"\"select assign_to from `tabAssing Master` where name = %s \"\"\", raised_by)\n\t\tfrappe.errprint(\"in assign\")\n\t\tif check_entry :\n\t\t\tfrappe.errprint(\"chk\")\n\t\t\tif assign_in_future=='No':\n\t\t\t\tfrappe.errprint(\"no\")\n\t\t\t\tfrappe.db.sql(\"\"\"delete from `tabAssing Master` where name = %s \"\"\", raised_by)\t\n\t\t\telse :\n\t\t\t\tfrappe.errprint(\"Yes\")\n\t\t\t\tfrappe.db.sql(\"\"\"update `tabAssing Master` set assign_to=%s where name = %s \"\"\",(assign_to,raised_by))\n\t\telse :\n\t\t\tfrappe.errprint(\"not chk\")\n\t\t\tif assign_in_future=='Yes':\n\t\t\t\tfrappe.errprint(\"Yes\")\n\t\t\t\tam = frappe.new_doc(\"Assing Master\")\n\t\t\t\tam.update({\n\t\t\t\t\"name\": raised_by,\n\t\t\t\t\"assign_to\": assign_to,\n\t\t\t\t\"raised_by\":raised_by\n\t\t\t})\n\t\t\tam.insert()\n\ndef auto_close_tickets():\n\tfrappe.db.sql(\"\"\"update `tabSupport Ticket` set status = 'Closed'\n\t\twhere status = 'Replied'\n\t\tand date_sub(curdate(),interval 15 Day) > modified\"\"\")\n\n\n@frappe.whitelist()\ndef reenable(name):\n\tfrappe.errprint(\"calling superadmin\")\n        from frappe.utils import get_url, cstr\n\tfrappe.errprint(get_url())\n\tif get_url()!='http://tailorpad.com':\n\t  frappe.errprint(\"in reenable\")\n  \t  from frappe.utils import get_url, cstr,add_months\n  \t  from frappe import msgprint, throw, _\n  \t  res = frappe.db.sql(\"select validity from `tabUser` where name='Administrator' and no_of_users >0\")\n\t  if  res:\n\t\tres1 = frappe.db.sql(\"select validity_end_date from `tabUser` where '\"+cstr(name)+\"' and validity_end_date <CURDATE()\")\n\t\tif res1:\n\t\t\tbc=\"update `tabUser` set validity_end_date=DATE_ADD((nowdate(), INTERVAL \"+cstr(res[0][0])+\" MONTH) where name = '\"+cstr(name)+\"'\"\n\t\t\tfrappe.db.sql(bc)\n\t  \t\tfrappe.db.sql(\"update `tabUser`set no_of_users=no_of_users-1  where name='Administrator'\")\n\t\telse:\n\t\t\tab=\"update `tabUser` set validity_end_date=DATE_ADD(validity_end_date,INTERVAL \"+cstr(res[0][0])+\" MONTH) where name = '\"+cstr(name)+\"' \"\n\t\t\tfrappe.errprint(ab)\n\t\t\tfrappe.db.sql(ab)\n\t  \t\tfrappe.db.sql(\"update `tabUser`set no_of_users=no_of_users-1  where name='Administrator'\")\n\t  else:\n\t\t    frappe.throw(_(\"Your subscription plan expired .Please purchase an subscription plan and enable user.\"))", "instruction_en": "The given code is a Python script that defines a SupportTicket class and several functions. To use this code, import the necessary modules and create an instance of the SupportTicket class. The class has several methods that can be called to get the sender, subject, and content of a support ticket, as well as to update the status and send an email notification. There are also several helper functions for setting the status of a ticket, assigning tickets to users, and re-enabling user accounts. To use these functions, call them with the appropriate arguments.", "original_line_number": 3244, "instruction_length": 599}
{"instruction": "Kod ini termasuk beberapa kelas dan fungsi untuk melaksanakan model pengekod-decoder dengan mekanisme perhatian. Ia juga termasuk fungsi untuk mengira kehilangan silang entropi bertopeng dan kelas untuk melaksanakan kehilangan fokus. Untuk menggunakan kod ini, import perpustakaan dan kelas yang diperlukan, dan kemudian tentukan saiz input dan output, saiz tersembunyi, bilangan lapisan, dan hyperparameter lain untuk pengekod dan penyahkod. Kemudian, instantiate kelas encoder dan decoder dan melatih model menggunakan fungsi kerugian yang disediakan. Akhirnya, gunakan model terlatih untuk menghasilkan ramalan pada data baru.", "input": "", "output": "import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nUSE_CUDA = False\n\nKERN_SIZE = 3\n\n\ndef set_cuda(val):\n    global USE_CUDA\n    USE_CUDA = val\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_range_expand = Variable(seq_range_expand)\n    # if sequence_length.is_cuda:\n    if USE_CUDA:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (sequence_length.unsqueeze(1)\n                         .expand_as(seq_range_expand))\n    return seq_range_expand < seq_length_expand\n\n\ndef masked_cross_entropy(logits, target, length, gamma=0, eps=1e-7):\n    length = Variable(torch.LongTensor(length))\n    if USE_CUDA:\n        length = length.cuda()\n\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # probs_flat: (batch * max_len, 1)\n    probs_flat = torch.gather(F.softmax(logits_flat), dim=1, index=target_flat)\n    probs_flat = probs_flat.clamp(eps, 1. - eps)   # prob: [0, 1] -> [eps, 1 - eps]\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.log(probs_flat) * (1 - probs_flat) ** gamma  # focal loss\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    return loss\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n        super(EncoderRNN, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n\n    def forward(self, input_seqs, input_lengths, hidden=None):\n        # Note: we run this all at once (over multiple batches of multiple sequences)\n        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n        return outputs, hidden\n\n\nclass CNN(nn.Module):\n    def __init__(self, input_size):\n        super(CNN, self).__init__()\n\n        inp_size = input_size\n\n        self.features = nn.Sequential(\n\n            nn.Conv2d(1, 10, kernel_size=(KERN_SIZE, inp_size)),\n            nn.BatchNorm2d(10),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(10, 10, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(10),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n\n            nn.Conv2d(10, 20, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(20),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(20, 20, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(20),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n\n            nn.Conv2d(20, 40, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(40),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(40, 40, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(40),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n        )\n\n        self.n_pool_layers = 3\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(2), out.size(0), out.size(1) * out.size(3))\n        return out\n\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n        super(EncoderCNN, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.cnn = CNN(input_size)\n        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n\n    def forward(self, input_seqs, input_lengths, hidden=None):\n        input_features = self.cnn(input_seqs.transpose(0, 1).unsqueeze(1))\n        features_length = [(l//(2 ** self.cnn.n_pool_layers)) for l in input_lengths]\n        # features_length = input_lengths\n        # print(input_features.size())\n        # print(features_length)\n        # Note: we run this all at once (over multiple batches of multiple sequences)\n        packed = torch.nn.utils.rnn.pack_padded_sequence(input_features, features_length)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n        return outputs, hidden\n\n\nclass Attn(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n\n        self.method = method\n        self.hidden_size = hidden_size\n\n        if self.method == 'general':\n            self.attn = nn.Linear(self.hidden_size, hidden_size)\n\n        elif self.method == 'concat':\n            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n\n    def forward(self, hidden, encoder_outputs):\n        max_len = encoder_outputs.size(0)\n        this_batch_size = encoder_outputs.size(1)\n\n        # Create variable to store attention energies\n        attn_energies = Variable(torch.zeros(this_batch_size, max_len))  # B x S\n\n        if USE_CUDA:\n            attn_energies = attn_energies.cuda()\n\n        # For each batch of encoder outputs\n        for b in range(this_batch_size):\n            # Calculate energy for each encoder output\n            for i in range(max_len):\n                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n\n        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n        return F.softmax(attn_energies).unsqueeze(1)\n\n    def score(self, hidden, encoder_output):\n\n        if self.method == 'dot':\n            energy = torch.dot(hidden.view(-1), encoder_output.view(-1))\n            return energy\n\n        elif self.method == 'general':\n            energy = self.attn(encoder_output)\n            energy = torch.dot(hidden.view(-1), energy.view(-1))\n            return energy\n\n        elif self.method == 'concat':\n            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n            energy = self.v.dot(energy)\n            return energy\n\n\nclass LuongAttnDecoderRNN(nn.Module):\n\n    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, prior_size=0, dropout=0.1, embedding=None):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.prior_size = prior_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        if np.any(embedding):\n            self.embedding_size = embedding_size = embedding.shape[1]\n            self.embedding = nn.Embedding(output_size, embedding_size)\n            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding).float())\n            self.embedding.requires_grad = True\n        else:\n            self.embedding_size = embedding_size = hidden_size\n            self.embedding = nn.Embedding(output_size, embedding_size)\n\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout)\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size + prior_size, output_size)\n\n        # Choose attention model\n        if attn_model != 'none':\n            self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_seq, last_hidden, encoder_outputs, prior):\n        # Note: we run this one step at a time\n\n        # Get the embedding of the current input word (last output word)\n        batch_size = input_seq.size(0)\n        embedded = self.embedding(input_seq)\n        embedded = self.embedding_dropout(embedded)\n        embedded = embedded.view(1, batch_size, -1)  # S=1 x B x N\n\n        # Get current hidden state from input word and last hidden state\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n\n        # Calculate attention from current RNN state and all encoder outputs;\n        # apply to encoder outputs to get weighted average\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n\n        # Attentional vector using the RNN hidden state and context vector\n        # concatenated together (Luong eq. 5)\n        rnn_output = rnn_output.squeeze(0)  # S=1 x B x N -> B x N\n        context = context.squeeze(1)  # B x S=1 x N -> B x N\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = F.tanh(self.concat(concat_input))\n\n        # Finally predict next token (Luong eq. 6, without softmax)\n        if prior is None:\n            output = self.out(concat_output)\n        else:\n            output = self.out(torch.cat((concat_output, prior), 1))\n\n        # Return final output, hidden state, and attention weights (for visualization)\n        return output, hidden, attn_weights\n\n\n# https://github.com/DingKe/pytorch_workplace/blob/master/focalloss/loss.py\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n\n    return mask.scatter_(1, index, ones)\n\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n\n        return loss.sum()", "instruction_en": "This code includes several classes and functions for implementing an Encoder-Decoder model with attention mechanism. It also includes a function for calculating masked cross-entropy loss and a class for implementing Focal Loss. To use this code, import the necessary libraries and classes, and then define the input and output sizes, hidden size, number of layers, and other hyperparameters for the Encoder and Decoder. Then, instantiate the Encoder and Decoder classes and train the model using the provided loss functions. Finally, use the trained model to generate predictions on new data.", "original_line_number": 3600, "instruction_length": 629}
{"instruction": "Kod ini melakukan analisis hasil pemeriksaan perpustakaan menggunakan pakej PYCAS. Untuk menggunakannya, jalankan skrip dengan argumen yang diperlukan: Laluan Fail Data Input, Kawalan Label Eksperimen, Label Eksperimen Rawatan, kaedah yang akan digunakan dalam analisis, sama ada untuk mempertimbangkan barcode, dan direktori output hasil. Fail data input harus mempunyai lajur bernama \"Gene\", \"Panduan\", dan \"Barcode\". Label eksperimen kawalan dan rawatan harus dipisahkan oleh ruang dan mempunyai panjang yang sama. Kaedah ini mestilah sama ada \"Sunbird\" atau \"MW\". Hasilnya akan disimpan dalam fail CSV bernama \"pycas_analysis_sunbird.csv\" dalam direktori output yang ditentukan atau direktori kerja semasa jika tidak ditentukan.", "input": "", "output": "#! /bin/env python3\n__all__ = ['pycaspattern']\n\n# ------------------\n# Libraries\n# ------------------\nimport argparse\nimport os\nimport sys\nsys.path.append('/gpfs/user/liuzh/Code/crispr')\nimport pandas as pd\nfrom pycas.analysis import Screening\nfrom pycas.utils.decorator import helpstring\nfrom pycas.utils.decorator import AppendHelp\n\n\n# ------------------\n# Functions\n# ------------------\n\ndef pycasanalysis(filename,\n                  ctrl_label,\n                  exp_label,\n                  method,\n                  hasbarcode,\n                  out_dir):\n    file_type = 'csv'\n    if filename.split('.')[-1] == 'txt':\n        sep = '\\t'\n        file_type = 'txt'\n    elif filename.split('.')[-1] == 'csv':\n        sep = ','\n        file_type = 'csv'\n    else:\n        raise ValueError('Input data file should be txt or csv')\n    if file_type == 'csv':\n        data = pd.read_csv(filename, header=0)\n    else:\n        data = pd.read_table(filename, header=0, sep='\\t')\n    for x in ['gene', 'guide', 'barcode']:\n        if x not in data.columns:\n            raise ValueError('Input data file should contain column named as: ' + x)\n    if len(ctrl_label) != len(exp_label):\n        raise ValueError('Input control labels and treatment labels should be of the same length.')\n    if out_dir != '' and not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    if method not in ['sunbird', 'mw']:\n        raise ValueError('The test method should be in: sunbird mw.')\n    analysis = Screening(data, ctrl_label, exp_label, hasbarcode=hasbarcode)\n    if method == 'sunbird':\n        analysis.sunbird(10)\n        analysis.test['sunbird'].to_csv(\n            os.path.join(out_dir, 'pycas_analysis_sunbird.csv')\n        )\n    else:\n        pass\n\n\n\n# ------------------\n# Main\n# ------------------\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        prog='pycasanalysis',\n        description='Analysis the library screening result.'\n    )\n    parser.add_argument(\n        '-i', '--input', required=True,\n        help='Input data file path, with columns: gene, guide <, barcode>, [labels]. (column names should be in the csv file).'\n    )\n    parser.add_argument(\n        '-c', '--control-label', nargs='+',\n        help='Control experiment labels, separeted by space.'\n    )\n    parser.add_argument(\n        '-t', '--treat-label', nargs='+',\n        help='Treatment experiment labels, separeted by space.'\n    )\n    parser.add_argument(\n        '-m', '--method', default='sunbird',\n        help='Method to be used in the analysis: sunbird, mw.'\n    )\n    parser.add_argument(\n        '--has-barcode', action='store_true',\n        help='Input data should be tested consider barcodes.'\n    )\n    parser.add_argument(\n        '--out-dir', default='',\n        help='Result output directorydefault is current work directory.'\n    )\n    args = parser.parse_args()\n    def analysis(**args):\n        pycasanalysis(\n            filename=args['input'],\n            ctrl_label=args['control_label'],\n            exp_label=args['treat_label'],\n            method=args['method'],\n            hasbarcode=args['has_barcode'],\n            out_dir=args['out_dir']\n        )\n    analysis(**vars(args))\n\n\n# ------------------\n# EOF\n# ------------------", "instruction_en": "This code performs library screening result analysis using the pycas package. To use it, run the script with the required arguments: input data file path, control experiment labels, treatment experiment labels, method to be used in the analysis, whether to consider barcodes, and the result output directory. The input data file should have columns named \"gene\", \"guide\", and \"barcode\". The control and treatment experiment labels should be separated by space and have the same length. The method should be either \"sunbird\" or \"mw\". The result will be saved in a csv file named \"pycas_analysis_sunbird.csv\" in the specified output directory or the current work directory if not specified.", "original_line_number": 3602, "instruction_length": 732}
{"instruction": "Sambung ke pangkalan data menggunakan PymssQL dengan membuat kelas DAO dengan kaedah untuk menyambung, membuat, memasukkan, memilih, dan menutup sambungan pangkalan data. Kaedah Connect harus mengambil hos, pengguna, kata laluan, pangkalan data, dan charset sebagai parameter dan mewujudkan sambungan ke pangkalan data. Kaedah CREATE harus melaksanakan pernyataan SQL yang diberikan untuk membuat jadual dalam pangkalan data. Kaedah memasukkan hendaklah melaksanakan pernyataan SQL yang diberikan untuk memasukkan data ke dalam jadual. Kaedah SELECT harus melaksanakan pernyataan SQL yang diberikan untuk mendapatkan data dari jadual dan mengembalikan hasilnya. Akhirnya, kaedah dekat harus menutup sambungan pangkalan data.", "input": "", "output": "# -*- coding: utf-8 -*-\n\nimport pymssql\n\n\nclass Dao:\n    def __init__(self):\n        self.conn = None\n        self.cur = None\n\n    def connect(self):\n        # \n        self.conn = pymssql.connect(host=\"localhost:59318\", user=\"eachen\", password=\"123456\", database=\"mydata\",\n                                    charset=\"utf8\")\n        # host = \"localhost:59318\", user = \"eachen\", pwd = \"123456\", db = \"mydata\"\n        self.cur = self.conn.cursor()\n        if not self.cur:\n            raise (NameError, \"\")\n        else:\n            print(\"\")\n\n    def create(self, sql):\n        # print(sql)\n        try:\n            self.cur.execute(sql)\n            self.conn.commit()\n        except:\n            print('create failed')\n        else:\n            print('create succeed')\n\n    def insert(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n    def select(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        # fetchall()\n        return self.cur.fetchall()\n\n    def close(self):\n        self.conn.close()", "instruction_en": "Connect to a database using pymssql by creating a Dao class with methods to connect, create, insert, select, and close the database connection. The connect method should take in the host, user, password, database, and charset as parameters and establish a connection to the database. The create method should execute a given SQL statement to create a table in the database. The insert method should execute a given SQL statement to insert data into the table. The select method should execute a given SQL statement to retrieve data from the table and return the results. Finally, the close method should close the database connection.", "original_line_number": 3655, "instruction_length": 724}
{"instruction": "Muat turun tweet dari senarai pengguna dan simpannya ke fail CSV. \n\n1. Pasang perpustakaan Tweepy dan Unicodecsv di Python.\n2. Buat fail bernama \"Users.txt\" dan tambahkan senarai pengguna yang tweet yang ingin anda muat turun.\n3. Jalankan skrip Python dengan arahan berikut: Python Script.py [maxtweets] [fname] [sejak itu] [last_id]\n4. Gantikan [Maxtweets] dengan bilangan maksimum tweet yang anda mahu muat turun.\n5. Gantikan [fname] dengan nama fail CSV di mana anda ingin menyimpan tweet.\n6. Gantikan [sejak itu] dan [last_id] dengan ID Tweet jika anda ingin memuat turun tweet antara julat tertentu. Jika tidak, gunakan \"-1\" untuk kedua-duanya.\n7. Tunggu skrip selesai memuat turun tweet dan menyimpannya ke fail CSV.", "input": "", "output": "#!/usr/bin/python\nimport tweepy\nimport sys\nimport os\nimport codecs\nimport unicodecsv as csv\n\n# API and ACCESS KEYS\nAPI_KEY = 'jz3feMK2gN0kaN377FsTXY7uY'\nAPI_SECRET = 'sGfCEayfwORloC9SvHy6BmDjifUsUEIF0EF51SgiYUgs054n7H'\n\n# Don't buffer stdout, so we can tail the log output redirected to a file\nsys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n\n# Max Tweets\nmaxTweets = int(sys.argv[1])\n\n# Filename\nfName = sys.argv[2]\n\ntweetsPerQry = 200\n\n# List of users read from users.txt\nusers = []\n\n#open users.txt file and gets the list of users\nwith open('users.txt', 'r') as f:\n  for line in f:\n    users.append(line.strip())\n\nsinceId = None\n\nif(len(sys.argv) > 3):\n  if(sys.argv[3] != '-1'):\n    sinceId = sys.argv[3]\n\nlast_id = -1L\n\nif(len(sys.argv) > 4):\n  last_id = long(sys.argv[4])\n\ndef getHashtags(hashes):\n  hashStr = ''\n  for i, h in enumerate(hashes):\n    if i == len(hashes)-1:\n      hashStr = hashStr + h['text']\n    else:\n      hashStr = hashStr + h['text'] + ','\n  return hashStr\n\ndef getMentions(mentions):\n  mentionStr = ''\n  for i, m in enumerate(mentions):\n    if i == len(mentions)-1:\n      mentionStr = mentionStr + m['screen_name']\n    else:\n      mentionStr = mentionStr + m['screen_name'] + ','\n  return mentionStr\n\nauth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\nif (not api):\n  print (\"Can't Authenticate Bye!\")\n  sys.exit(-1)\n\ntweetCount = 0\nprint(\"Downloading max {0} tweets\".format(maxTweets))\n\nwith open(fName, 'ab') as f:\n  writer = csv.writer(f, encoding='utf-8')\n  writer.writerow(['Username','Created_at','ID','Tweet','RetweetCount','Name','Location','URL','Description','TweetCount','FollowersCount','FriendsCount','hashtags','mentions'])\n  for user in users:\n    tweetCount = 0\n    last_id = 0\n    while tweetCount < maxTweets:\n      print 'User is ' + user + ' Tweet count ' + str(tweetCount) + ' max Tweets ' + str(maxTweets) + ' SinceId ' + str(sinceId) + ' last_id ' + str(last_id)\n      try:\n        if (last_id <= 0):\n          if (not sinceId):\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry)\n          else:\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, since_id=sinceId)\n\n        else:\n          if (not sinceId):\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, max_id=str(last_id - 1))\n          else:\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, max_id=str(last_id - 1), since_id=sinceId)\n\n        if not new_tweets:\n          print(\"No more tweets found\")\n          break\n        \n        for tweet in new_tweets:\n          try: \n            hashTags = getHashtags(tweet.entities.get('hashtags'))\n            mentions = getMentions(tweet.entities.get('user_mentions'))\n\n            writer.writerow([tweet.user.screen_name,tweet.created_at,tweet.id_str,tweet.text,str(tweet.retweet_count),tweet.user.name, tweet.user.location, str(tweet.user.url),tweet.user.description,str(tweet.user.statuses_count),str(tweet.user.followers_count),str(tweet.user.friends_count),hashTags,mentions])\n          except tweepy.TweepError as e:\n            print(\"some error : \" + str(e) + \" for user: \" + user)\n            break\n\n        tweetCount += len(new_tweets)\n        print(\"Downloaded {0} tweets\".format(tweetCount))\n        last_id = new_tweets[-1].id\n      except tweepy.TweepError as e:\n        # Just exit if any error\n        print(\"some error : \" + str(e))\n        break\n\nprint (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))", "instruction_en": "Download tweets from a list of users and save them to a CSV file. \n\n1. Install tweepy and unicodecsv libraries in Python.\n2. Create a file named \"users.txt\" and add the list of users whose tweets you want to download.\n3. Run the Python script with the following command: python script.py [maxTweets] [fName] [sinceId] [last_id]\n4. Replace [maxTweets] with the maximum number of tweets you want to download.\n5. Replace [fName] with the name of the CSV file where you want to save the tweets.\n6. Replace [sinceId] and [last_id] with the tweet IDs if you want to download tweets between a specific range. Otherwise, use \"-1\" for both.\n7. Wait for the script to finish downloading the tweets and saving them to the CSV file.", "original_line_number": 3770, "instruction_length": 722}
{"instruction": "Buat skrip tersuai untuk mengesan dan memasang semua kebergantungan metatoolkit pada Ubuntu. Gunakan kod yang diberikan sebagai rujukan. Skrip harus menyemak keperluan python dan pakej yang diperlukan seperti numpy, sympy, xlrd, xlwt, matplotlib, pyqt4, bio, hidung, docx, libsbml, cbmpy, java, perl, dan blastall. Ia juga harus menyemak pakej pilihan/disyorkan seperti Git, Cython, Libxml2-Dev, Libxslt-Dev, Ipython, Ipython-Notebook, Python-Suds, dan Python-Flask. Skrip harus menghasilkan skrip bash yang boleh dijalankan untuk memasang kebergantungan. Skrip bash harus meminta pengguna untuk pengesahan sebelum memasang kebergantungan.", "input": "", "output": "# Detect all MetaToolKit depencies on Ubuntu and create a custom script to install them.\n# Tested on Ubuntu 14.04, 16.04\n# Author Brett G. Olivier (bgoli@users.sourceforge.net)\n# (C) All rights reserved, Brett G. Olivier, Amsterdam 2016.\n\nimport os, subprocess, itertools, stat\n\nUBUNTU = CONDA = False\ntry:\n    print(os.sys.argv)\n    arg = os.sys.argv[1]\nexcept:\n    arg = 'UBUNTU'\nif arg == 'UBUNTU':\n    UBUNTU = True\nelif arg == 'CONDA':\n    CONDA = True\nelse:\n    print('\\nPlease call script with CONDA as argument for Anaconda install script, defaulting to UBUNTU')\n    UBUNTU = True\n\nres = {'Required' : {},\\\n       'Optional' : {}\n       }\n\n# First lets check for some Python essentials\nreqcnt = itertools.count(1,1)\noptcnt = itertools.count(1,1)\n# this should just be there for any sane python build environment\nif UBUNTU:\n    res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install build-essential g++ gfortran python python-dev'\nif CONDA:\n    pass\n    #res['Required'][reqcnt.next()] = 'conda update -y conda # if this is the only required package ignore it'\ntry:\n    import pip\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-pip'\ntry:\n    import numpy\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-numpy'\ntry:\n    import sympy\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-sympy'\ntry:\n    import xlrd\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-xlrd'\ntry:\n    import xlwt\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-xlwt'\ntry:\n    import matplotlib\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-matplotlib'\ntry:\n    import PyQt4\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-qt4'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -y pyqt=4.11.4'\ntry:\n    import Bio\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-biopython'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -y biopython'\ntry:\n    import nose\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-nose'\ntry:\n    import docx\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install docx'    \n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'pip install docx'    \ntry:\n    import libsbml\n    if libsbml.LIBSBML_VERSION < 51201:\n        print('\\nPlease update to the latest version of libSBML.\\n')\n        raise ImportError\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install libxml2 libxml2-dev libxslt-dev zlib1g zlib1g-dev bzip2 libbz2-dev'\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install --upgrade python-libsbml'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -c SBMLTeam -y python-libsbml'\ntry:\n    import cbmpy\nexcept:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install --upgrade cbmpy'\n        res['Required'][reqcnt.next()] = 'sudo python -c \"import cbmpy\"'\n    if CONDA:\n        res['Required'][reqcnt.next()] = 'pip install cbmpy'\n        res['Required'][reqcnt.next()] = 'python -c \"import cbmpy\"'\ntry:\n    out = subprocess.call(['java', '-version'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install default-jre'\n\ntry:\n    out = subprocess.call(['perl', '-v'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install perl'\np_script = \"\"\"\\\nmy $rc = 0;\n$rc = eval\n{\n  require XML::Parser;\n  XML::Parser->import();\n  1;\n};\nif ($rc){\n    exit 0\n    } else {\n    exit 1\n    }\n\"\"\"\ntry:\n    PF = file('_test.pl', 'w')\n    PF.write(p_script)\n    PF.close()\n    out = int(subprocess.call(['perl', '_test.pl']))\n    if out:\n        raise OSError\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install libxml-parser-perl'\ntry:\n    out = subprocess.call(['blastall'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install blast2'\n\n# Optional/recommended\n# https://github.com/bgoli/cbmpy-glpk\nif UBUNTU:\n    res['Optional'][optcnt.next()] = 'sudo apt-get -y install git cython libxml2-dev libxslt-dev'\ntry:\n    import IPython\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install ipython ipython-notebook'\ntry:\n    import suds\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install python-suds'\n    elif CONDA:\n        res['Optional'][optcnt.next()] = 'pip install suds'\ntry:\n    import flask\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install python-flask'\n\nif UBUNTU or CONDA:\n    bash_script=\"\"\"\\\n    while true; do\n        read -p \"Do you wish to install *{}* MetaToolkit dependencies? [y/n]: \" yn\n        case $yn in\n            [Yy]* ) echo \"Installing ...\"; break;;\n            [Nn]* ) exit;;\n            * ) echo \"Please enter y/n.\";;\n        esac\n    done\n    \n    \"\"\"\n    \n    bash_script=\"\"\"\\\n    \n    # {}\n    \n    \"\"\"\n    \n    output = '#!/bin/sh\\n\\n'\n    output += '#MetaToolkit: Ubuntu system requirements check\\n'\n    output += '#=============================================\\n\\n'\n    \n    REQUIRE_USER_INPUT = False\n    \n    for r in res:\n        if len(res[r]) > 0:\n            if REQUIRE_USER_INPUT:\n                output += bash_script.format(r)\n            output += '#{}\\n#{}\\n\\n'.format(r, '-'*len(r))\n            resk = list(res[r])\n            resk.sort()\n            for k in resk:\n                if k != None:\n                    output += '{}\\n'.format(res[r][k])\n            output += '\\n'\n    output += 'exit\\n\\n'\n    fname = 'metatoolkit_install_dependencies.sh'\n    F = file(fname, 'w')\n    F.write(output)\n    F.close()\n    os.chmod(fname, stat.S_IRWXU)\n    \n    print('')\n    print(output)\n    print('\\n\\nInstall script (shown above) saved as file: {}\\nplease examine it carefully and run. Alternatively install individual dependencies manually').format(fname)", "instruction_en": "Create a custom script to detect and install all MetaToolKit dependencies on Ubuntu. Use the given code as a reference. The script should check for Python essentials and required packages such as numpy, sympy, xlrd, xlwt, matplotlib, PyQt4, Bio, nose, docx, libsbml, cbmpy, java, perl, and blastall. It should also check for optional/recommended packages such as git, cython, libxml2-dev, libxslt-dev, ipython, ipython-notebook, python-suds, and python-flask. The script should generate a bash script that can be run to install the dependencies. The bash script should prompt the user for confirmation before installing the dependencies.", "original_line_number": 3955, "instruction_length": 639}
{"instruction": "Untuk menjalankan bahagian pelayan papan pemuka pakej, ikuti langkah -langkah ini:\n\n1. Pasang perpustakaan yang diperlukan:\n   - Google.appengine.api.mail\n   - Google.appengine.api.memcache\n   - Google.Appengine.api.Taskqueue\n   - Google.appengine.api.urlfetch\n   - Google.appengine.api.users\n   - Google.appengine.ext.db\n   - Google.appengine.ext.WebApp\n   - Google.appengine.ext.webapp.template\n   - Google.appengine.ext.webapp.util\n   - DateTime\n   - Pembalakan\n   - OS\n   - Re\n   - set\n   - urllib2\n\n2. Import Modul Auth dan Modul Const.\n\n3. Tentukan kelas pakej, yang merupakan model penyimpanan untuk maklumat pakej yang direkodkan pada pelayan.\n\n4. Tentukan kelas projek, yang merupakan model penyimpanan untuk maklumat projek yang direkodkan pada pelayan.\n\n5. Tentukan fungsi VC_TO_WEB, yang menukarkan laluan sistem kawalan versi ke URL web dan URL cek.\n\n6. Tentukan fungsi web_to_vc, yang menukarkan URL web ke laluan sistem kawalan versi.\n\n7. Tentukan kelas PacketPage, yang mengendalikan permintaan yang berkaitan dengan pakej.\n\n8. Tentukan kelas ProjectPage, yang mengendalikan permintaan yang berkaitan dengan projek.\n\n9. Tentukan fungsi utama, yang mewujudkan aplikasi WSGI dan menjalankannya.\n\n10. Jalankan fungsi utama untuk memulakan pelayan.", "input": "", "output": "# Copyright 2010 The Go Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style\n# license that can be found in the LICENSE file.\n\n# This is the server part of the package dashboard.\n# It must be run by App Engine.\n\nfrom google.appengine.api import mail\nfrom google.appengine.api import memcache\nfrom google.appengine.api import taskqueue\nfrom google.appengine.api import urlfetch\nfrom google.appengine.api import users\nfrom google.appengine.ext import db\nfrom google.appengine.ext import webapp\nfrom google.appengine.ext.webapp import template\nfrom google.appengine.ext.webapp.util import run_wsgi_app\nimport datetime\nimport logging\nimport os\nimport re\nimport sets\nimport urllib2\n\n# local imports\nfrom auth import auth\nimport toutf8\nimport const\n\ntemplate.register_template_library('toutf8')\n\n# Storage model for package info recorded on server.\nclass Package(db.Model):\n    path = db.StringProperty()\n    web_url = db.StringProperty()           # derived from path\n    count = db.IntegerProperty()            # grand total\n    week_count = db.IntegerProperty()       # rolling weekly count\n    day_count = db.TextProperty(default='') # daily count\n    last_install = db.DateTimeProperty()\n\n    # data contributed by gobuilder\n    info = db.StringProperty()  \n    ok = db.BooleanProperty()\n    last_ok = db.DateTimeProperty()\n\n    def get_day_count(self):\n        counts = {}\n        if not self.day_count:\n            return counts\n        for d in str(self.day_count).split('\\n'):\n            date, count = d.split(' ')\n            counts[date] = int(count)\n        return counts\n\n    def set_day_count(self, count):\n        days = []\n        for day, count in count.items():\n            days.append('%s %d' % (day, count))\n        days.sort(reverse=True)\n        days = days[:28]\n        self.day_count = '\\n'.join(days)\n\n    def inc(self):\n        count = self.get_day_count()\n        today = str(datetime.date.today())\n        count[today] = count.get(today, 0) + 1\n        self.set_day_count(count)\n        self.update_week_count(count)\n        self.count += 1\n\n    def update_week_count(self, count=None):\n        if count is None:\n            count = self.get_day_count()\n        total = 0\n        today = datetime.date.today()\n        for i in range(7):\n            day = str(today - datetime.timedelta(days=i))\n            if day in count:\n                total += count[day]\n        self.week_count = total\n\n\n# PackageDaily kicks off the daily package maintenance cron job\n# and serves the associated task queue.\nclass PackageDaily(webapp.RequestHandler):\n\n    def get(self):\n        # queue a task to update each package with a week_count > 0\n        keys = Package.all(keys_only=True).filter('week_count >', 0)\n        for key in keys:\n            taskqueue.add(url='/package/daily', params={'key': key.name()})\n\n    def post(self):\n        # update a single package (in a task queue)\n        def update(key):\n            p = Package.get_by_key_name(key)\n            if not p:\n                return\n            p.update_week_count()\n            p.put()\n        key = self.request.get('key')\n        if not key:\n            return\n        db.run_in_transaction(update, key)\n \n\nclass Project(db.Model):\n    name = db.StringProperty(indexed=True)\n    descr = db.StringProperty()\n    web_url = db.StringProperty()\n    package = db.ReferenceProperty(Package)\n    category = db.StringProperty(indexed=True)\n    tags = db.ListProperty(str)\n    approved = db.BooleanProperty(indexed=True)\n\n\nre_bitbucket = re.compile(r'^(bitbucket\\.org/[a-z0-9A-Z_.\\-]+/[a-zA-Z0-9_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?$')\nre_googlecode = re.compile(r'^[a-z0-9\\-]+\\.googlecode\\.com/(svn|hg|git)(/[a-z0-9A-Z_.\\-/]+)?$')\nre_github = re.compile(r'^github\\.com/[a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)+$')\nre_launchpad = re.compile(r'^launchpad\\.net/([a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)?|~[a-z0-9A-Z_.\\-]+/(\\+junk|[a-z0-9A-Z_.\\-]+)/[a-z0-9A-Z_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?$')\n\ndef vc_to_web(path):\n    if re_bitbucket.match(path):\n        m = re_bitbucket.match(path)\n        check_url = 'http://' + m.group(1) + '/?cmd=heads'\n        web = 'http://' + m.group(1) + '/'\n    elif re_github.match(path):\n        m = re_github_web.match(path)\n        check_url = 'https://raw.github.com/' + m.group(1) + '/' + m.group(2) + '/master/'\n        web = 'http://github.com/' + m.group(1) + '/' + m.group(2) + '/'\n    elif re_googlecode.match(path):\n        m = re_googlecode.match(path)\n        check_url = 'http://'+path\n        if not m.group(2):  # append / after bare '/hg' or '/git'\n            check_url += '/'\n        web = 'http://code.google.com/p/' + path[:path.index('.')]\n    elif re_launchpad.match(path):\n        check_url = web = 'https://'+path\n    else:\n        return False, False\n    return web, check_url\n\nre_bitbucket_web = re.compile(r'bitbucket\\.org/([a-z0-9A-Z_.\\-]+)/([a-z0-9A-Z_.\\-]+)')\nre_googlecode_web = re.compile(r'code.google.com/p/([a-z0-9\\-]+)')\nre_github_web = re.compile(r'github\\.com/([a-z0-9A-Z_.\\-]+)/([a-z0-9A-Z_.\\-]+)')\nre_launchpad_web = re.compile(r'launchpad\\.net/([a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)?|~[a-z0-9A-Z_.\\-]+/(\\+junk|[a-z0-9A-Z_.\\-]+)/[a-z0-9A-Z_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?')\nre_striphttp = re.compile(r'https?://(www\\.)?')\n\ndef find_googlecode_vcs(path):\n    # Perform http request to path/hg or path/git to check if they're\n    # using mercurial or git.  Otherwise, assume svn.\n    for vcs in ['git', 'hg']:\n        try:\n            response = urlfetch.fetch('http://'+path+vcs, deadline=1)\n            if response.status_code == 200:\n                return vcs\n        except: pass\n    return 'svn'\n\ndef web_to_vc(url):\n    url = re_striphttp.sub('', url)\n    m = re_bitbucket_web.match(url)\n    if m:\n        return 'bitbucket.org/'+m.group(1)+'/'+m.group(2)\n    m = re_github_web.match(url)\n    if m:\n        return 'github.com/'+m.group(1)+'/'+m.group(2)\n    m = re_googlecode_web.match(url)\n    if m:\n        path = m.group(1)+'.googlecode.com/'\n        vcs = find_googlecode_vcs(path)\n        return path + vcs\n    m = re_launchpad_web.match(url)\n    if m:\n        return m.group(0)\n    return False\n\nMaxPathLength = 100\nCacheTimeout = 3600\n\nclass PackagePage(webapp.RequestHandler):\n    def get(self):\n        if self.request.get('fmt') == 'json':\n            return self.json()\n\n        html = memcache.get('view-package')\n        if not html:\n            tdata = {}\n\n            q = Package.all().filter('week_count >', 0)\n            q.order('-week_count')\n            tdata['by_week_count'] = q.fetch(50)\n\n            q = Package.all()\n            q.order('-last_install')\n            tdata['by_time'] = q.fetch(20)\n\n            q = Package.all()\n            q.order('-count')\n            tdata['by_count'] = q.fetch(100)\n\n            path = os.path.join(os.path.dirname(__file__), 'package.html')\n            html = template.render(path, tdata)\n            memcache.set('view-package', html, time=CacheTimeout)\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        self.response.out.write(html)\n\n    def json(self):\n        json = memcache.get('view-package-json')\n        if not json:\n            q = Package.all()\n            s = '{\"packages\": ['\n            sep = ''\n            for r in q.fetch(1000):\n                s += '%s\\n\\t{\"path\": \"%s\", \"last_install\": \"%s\", \"count\": \"%s\"}' % (sep, r.path, r.last_install, r.count)\n                sep = ','\n            s += '\\n]}\\n'\n            json = s\n            memcache.set('view-package-json', json, time=CacheTimeout)\n        self.response.set_status(200)\n        self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n        self.response.out.write(json)\n\n    def can_get_url(self, url):\n        try:\n            urllib2.urlopen(urllib2.Request(url))\n            return True\n        except:\n            return False\n\n    def is_valid_package_path(self, path):\n        return (re_bitbucket.match(path) or\n            re_googlecode.match(path) or\n            re_github.match(path) or\n            re_launchpad.match(path))\n\n    def record_pkg(self, path):\n        # sanity check string\n        if not path or len(path) > MaxPathLength or not self.is_valid_package_path(path):\n            return False\n\n        # look in datastore\n        key = 'pkg-' + path\n        p = Package.get_by_key_name(key)\n        if p is None:\n            # not in datastore - verify URL before creating\n            web, check_url = vc_to_web(path)\n            if not web:\n                logging.error('unrecognized path: %s', path)\n                return False\n            if not self.can_get_url(check_url):\n                logging.error('cannot get %s', check_url)\n                return False\n            p = Package(key_name = key, path = path, count = 0, web_url = web)\n\n        if auth(self.request):\n            # builder updating package metadata\n            p.info = self.request.get('info')\n            p.ok = self.request.get('ok') == \"true\"\n            if p.ok:\n                p.last_ok = datetime.datetime.utcnow()\n        else:\n            # goinstall reporting an install\n            p.inc()\n            p.last_install = datetime.datetime.utcnow()\n\n        # update package object\n        p.put()\n        return True\n\n    def post(self):\n        path = self.request.get('path')\n        ok = db.run_in_transaction(self.record_pkg, path)\n        if ok:\n            self.response.set_status(200)\n            self.response.out.write('ok')\n        else:\n            logging.error('invalid path in post: %s', path)\n            self.response.set_status(500)\n            self.response.out.write('not ok')\n\nclass ProjectPage(webapp.RequestHandler):\n\n    def get(self):\n        admin = users.is_current_user_admin()\n        if self.request.path == \"/project/login\":\n            self.redirect(users.create_login_url(\"/project\"))\n        elif self.request.path == \"/project/logout\":\n            self.redirect(users.create_logout_url(\"/project\"))\n        elif self.request.path == \"/project/edit\" and admin:\n            self.edit()\n        elif self.request.path == \"/project/assoc\" and admin:\n            self.assoc()\n        else:\n            self.list()\n\n    def assoc(self):\n        projects = Project.all()\n        for p in projects:\n            if p.package:\n                continue\n            path = web_to_vc(p.web_url)\n            if not path:\n                continue\n            pkg = Package.get_by_key_name(\"pkg-\"+path)\n            if not pkg:\n                self.response.out.write('no: %s %s<br>' % (p.web_url, path))\n                continue\n            p.package = pkg\n            p.put()\n            self.response.out.write('yes: %s %s<br>' % (p.web_url, path))\n\n    def post(self):\n        if self.request.path == \"/project/edit\":\n            self.edit(True)\n        else:\n            data = dict(map(lambda x: (x, self.request.get(x)), [\"name\",\"descr\",\"web_url\"]))\n            if reduce(lambda x, y: x or not y, data.values(), False):\n                data[\"submitMsg\"] = \"You must complete all the fields.\"\n                self.list(data)\n                return\n            p = Project.get_by_key_name(\"proj-\"+data[\"name\"])\n            if p is not None:\n                data[\"submitMsg\"] = \"A project by this name already exists.\"\n                self.list(data)\n                return\n            p = Project(key_name=\"proj-\"+data[\"name\"], **data)\n            p.put()\n\t\t\n            path = os.path.join(os.path.dirname(__file__), 'project-notify.txt')\n            mail.send_mail(\n                sender=const.mail_from,\n                to=const.mail_submit_to,\n                subject=const.mail_submit_subject,\n                body=template.render(path, {'project': p}))\n\n            self.list({\"submitMsg\": \"Your project has been submitted.\"})\n\n    def list(self, additional_data={}):\n        cache_key = 'view-project-data'\n        tag = self.request.get('tag', None)\n        if tag:\n            cache_key += '-'+tag\n        data = memcache.get(cache_key)\n        admin = users.is_current_user_admin()\n        if admin or not data:\n            projects = Project.all().order('category').order('name')\n            if not admin:\n                projects = projects.filter('approved =', True)\n            projects = list(projects)\n\n            tags = sets.Set()\n            for p in projects:\n                for t in p.tags:\n                    tags.add(t)\n\n            if tag:\n                projects = filter(lambda x: tag in x.tags, projects)\n\n            data = {}\n            data['tag'] = tag\n            data['tags'] = tags\n            data['projects'] = projects \n            data['admin']= admin\n            if not admin:\n                memcache.set(cache_key, data, time=CacheTimeout)\n\n        for k, v in additional_data.items():\n            data[k] = v\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        path = os.path.join(os.path.dirname(__file__), 'project.html')\n        self.response.out.write(template.render(path, data))\n\n    def edit(self, save=False):\n        if save:\n            name = self.request.get(\"orig_name\")\n        else:\n            name = self.request.get(\"name\")\n\n        p = Project.get_by_key_name(\"proj-\"+name)\n        if not p:\n            self.response.out.write(\"Couldn't find that Project.\")\n            return\n\n        if save:\n            if self.request.get(\"do\") == \"Delete\":\n                p.delete()\n            else:\n                pkg_name = self.request.get(\"package\", None)\n                if pkg_name:\n                    pkg = Package.get_by_key_name(\"pkg-\"+pkg_name)\n                    if pkg:\n                        p.package = pkg.key()\n                for f in ['name', 'descr', 'web_url', 'category']:\n                    setattr(p, f, self.request.get(f, None))\n                p.approved = self.request.get(\"approved\") == \"1\"\n                p.tags = filter(lambda x: x, self.request.get(\"tags\", \"\").split(\",\"))\n                p.put()\n            memcache.delete('view-project-data')\n            self.redirect('/project')\n            return\n\n        # get all project categories and tags\n        cats, tags = sets.Set(), sets.Set()\n        for r in Project.all():\n            cats.add(r.category)\n            for t in r.tags:\n                tags.add(t)\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        path = os.path.join(os.path.dirname(__file__), 'project-edit.html')\n        self.response.out.write(template.render(path, { \n            \"taglist\": tags, \"catlist\": cats, \"p\": p, \"tags\": \",\".join(p.tags) }))\n\n    def redirect(self, url):\n        self.response.set_status(302)\n        self.response.headers.add_header(\"Location\", url)\n\ndef main():\n    app = webapp.WSGIApplication([\n        ('/package', PackagePage),\n        ('/package/daily', PackageDaily),\n        ('/project.*', ProjectPage),\n        ], debug=True)\n    run_wsgi_app(app)\n\nif __name__ == '__main__':\n    main()", "instruction_en": "To run the server part of the package dashboard, follow these steps:\n\n1. Install the required libraries:\n   - google.appengine.api.mail\n   - google.appengine.api.memcache\n   - google.appengine.api.taskqueue\n   - google.appengine.api.urlfetch\n   - google.appengine.api.users\n   - google.appengine.ext.db\n   - google.appengine.ext.webapp\n   - google.appengine.ext.webapp.template\n   - google.appengine.ext.webapp.util\n   - datetime\n   - logging\n   - os\n   - re\n   - sets\n   - urllib2\n\n2. Import the auth module and the const module.\n\n3. Define the Package class, which is the storage model for package info recorded on the server.\n\n4. Define the Project class, which is the storage model for project info recorded on the server.\n\n5. Define the vc_to_web function, which converts a version control system path to a web URL and a check URL.\n\n6. Define the web_to_vc function, which converts a web URL to a version control system path.\n\n7. Define the PackagePage class, which handles requests related to packages.\n\n8. Define the ProjectPage class, which handles requests related to projects.\n\n9. Define the main function, which creates a WSGI application and runs it.\n\n10. Run the main function to start the server.", "original_line_number": 4362, "instruction_length": 1260}
{"instruction": "Kod ini menyediakan kelas rollbackdebiangrepository yang boleh melakukan rollbacks. Untuk menggunakannya, buat contoh RollbackDebiangRepository dan panggil kaedahnya untuk melaksanakan operasi Git. Untuk mengingati ref untuk rollback, hubungi kaedah RRR () dengan nama refname, tindakan, dan reftype sebagai argumen. Untuk melakukan rollback lengkap, hubungi kaedah rollback (). Sesetengah operasi Git dibalut untuk mengingati ref untuk rollback secara automatik, seperti create_tag (), komit_dir (), create_branch (), dan gabungan ().", "input": "", "output": "# vim: set fileencoding=utf-8 :\n#\n# (C) 2018 Guido Gnther <agx@sigxcpu.org>\n\"\"\"A git repository for Debian packages that can roll back operations\"\"\"\n\n\nfrom .. import log\nfrom .. git import GitRepositoryError\nfrom . git import DebianGitRepository\n\n\nclass RollbackError(GitRepositoryError):\n    \"\"\"\n    Error raised if the rollback failed\n    \"\"\"\n    def __init__(self, errors):\n        self.msg = \"Automatic rollback failed\"\n        super(RollbackError, self).__init__(self.msg)\n        self.errors = errors\n\n    def __str__(self):\n        return \"%s %s\" % (self.msg, self.errors)\n\n\nclass RollbackDebianGitRepository(DebianGitRepository):\n    \"\"\"\n    Like a DebianGitRepository but can also perform rollbacks and knows\n    about some of the inner workings upstream vcs_tag, \n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.rollbacks = []\n        self.rollback_errors = []\n        DebianGitRepository.__init__(self, *args, **kwargs)\n\n    def has_rollbacks(self):\n        return len(self.rollbacks) > 0\n\n    def rrr(self, refname, action, reftype):\n        \"\"\"\n        Remember ref for rollback\n\n        @param refname: ref to roll back\n        @param action: the rollback action (delete, reset, ...)\n        @param reftype: the reference type (tag, branch, ...)\n        \"\"\"\n        sha = None\n\n        if action == 'reset':\n            try:\n                sha = self.rev_parse(refname)\n            except GitRepositoryError as err:\n                log.warn(\"Failed to rev-parse '%s': %s\" % (refname, err))\n        elif action == 'delete':\n            pass\n        elif action == 'abortmerge':\n            pass\n        else:\n            raise GitRepositoryError(\"Unknown action '%s' for %s '%s'\" % (action, reftype, refname))\n        self.rollbacks.append((refname, reftype, action, sha))\n\n    def rrr_branch(self, branchname, action='reset-or-delete'):\n        if action == 'reset-or-delete':\n            if self.has_branch(branchname):\n                return self.rrr(branchname, 'reset', 'branch')\n            else:\n                return self.rrr(branchname, 'delete', 'branch')\n        else:\n            return self.rrr(branchname, action, 'branch')\n\n    def rrr_tag(self, tagname, action='delete'):\n        return self.rrr(tagname, action, 'tag')\n\n    def rrr_merge(self, commit, action='abortmerge'):\n        return self.rrr(commit, action, 'commit')\n\n    def rollback(self):\n        \"\"\"\n        Perform a complete rollback\n\n        Try to roll back as much as possible and remember what failed.\n        \"\"\"\n        for (name, reftype, action, sha) in self.rollbacks:\n            try:\n                if action == 'delete':\n                    log.info(\"Rolling back %s '%s' by deleting it\" % (reftype, name))\n                    if reftype == 'tag':\n                        self.delete_tag(name)\n                    elif reftype == 'branch':\n                        self.delete_branch(name)\n                    else:\n                        raise GitRepositoryError(\"Don't know how to delete %s '%s'\" % (reftype, name))\n                elif action == 'reset' and reftype == 'branch':\n                    log.info('Rolling back branch %s by resetting it to %s' % (name, sha))\n                    self.update_ref(\"refs/heads/%s\" % name, sha, msg=\"gbp import-orig: failure rollback of %s\" % name)\n                elif action == 'abortmerge':\n                    if self.is_in_merge():\n                        log.info('Rolling back failed merge of %s' % name)\n                        self.abort_merge()\n                    else:\n                        log.info(\"Nothing to rollback for merge of '%s'\" % name)\n                else:\n                    raise GitRepositoryError(\"Don't know how to %s %s '%s'\" % (action, reftype, name))\n            except GitRepositoryError as e:\n                self.rollback_errors.append((name, reftype, action, sha, e))\n        if self.rollback_errors:\n            raise RollbackError(self.rollback_errors)\n\n    # Wrapped methods for rollbacks\n    def create_tag(self, *args, **kwargs):\n        name = kwargs['name']\n        ret = super(RollbackDebianGitRepository, self).create_tag(*args, **kwargs)\n        self.rrr_tag(name)\n        return ret\n\n    def commit_dir(self, *args, **kwargs):\n        import_branch = kwargs['branch']\n        self.rrr_branch(import_branch)\n        return super(RollbackDebianGitRepository, self).commit_dir(*args, **kwargs)\n\n    def create_branch(self, *args, **kwargs):\n        branch = kwargs['branch']\n        ret = super(RollbackDebianGitRepository, self).create_branch(*args, **kwargs)\n        self.rrr_branch(branch, 'delete')\n        return ret\n\n    def merge(self, *args, **kwargs):\n        commit = args[0] if args else kwargs['commit']\n        try:\n            return super(RollbackDebianGitRepository, self).merge(*args, **kwargs)\n        except GitRepositoryError:\n            # Only cleanup in the error case to undo working copy\n            # changes. Resetting the refs handles the other cases.\n            self.rrr_merge(commit)\n            raise", "instruction_en": "This code provides a RollbackDebianGitRepository class that can perform rollbacks. To use it, create an instance of RollbackDebianGitRepository and call its methods to perform Git operations. To remember a ref for rollback, call the rrr() method with the refname, action, and reftype as arguments. To perform a complete rollback, call the rollback() method. Some Git operations are wrapped to automatically remember the ref for rollback, such as create_tag(), commit_dir(), create_branch(), and merge().", "original_line_number": 4389, "instruction_length": 535}
{"instruction": "Untuk menggunakan kod tersebut, panggilnya dengan hujah -hujah berikut: <filename> <pragma_xml_file> <xecutable_name> <cofiling_iterations> true/false (untuk output). Kod ini akan menjalankan executable untuk bilangan lelaran yang ditentukan dan mengagregatkan maklumat dalam executable_profile.xml. Ia kemudiannya akan menghasilkan graf bersarang dan aliran untuk setiap fungsi, serta grafik panggilan lengkap. Kod ini akan mencari jadual optimum menggunakan algoritma selari atau berurutan dengan kekangan masa. Jika jadual yang boleh dilaksanakan dijumpai, ia akan disimpan dalam fail jadual.", "input": "", "output": "import sys\nimport pargraph as par\nimport copy\nimport schedule as sched\nimport profiler as pro\nimport time\nimport multiprocessing\nimport itertools\nimport random\nimport threading\n\n\n\n\n\"\"\" Usage: call with <filename> <pragma_xml_file> <executable_name> <profiling_interations> True/False (for output) \"\"\"\n\nif __name__ == \"__main__\":\n\t\n\tpragma_xml = sys.argv[1]\n\texecutable = sys.argv[2]\n\tcount = int(sys.argv[3])\n\toutput = sys.argv[4]\n\texecution_time = float(sys.argv[5])\n\tdeadline = float(sys.argv[6])\n\tmulti = sys.argv[7]\n\n\t#runs count time the executable and aggregates the informations in executable_profile.xml. The single profile outputs are saved as profile+iter.xml\n\tprofile_xml = pro.profileCreator(count, executable)\n\n\t#return the nested dot graphs in code style (one for each function)\n\tvisual_nested_graphs = par.getNesGraph(pragma_xml, profile_xml)\n\n\t#returns the graphs to be visualized and the object graphs in flow style (one for each function)\n\t(visual_flow_graphs, flow_graphs) = par.getParalGraph(pragma_xml, profile_xml) \n\n\ti = 0\n\n\tfor g in visual_nested_graphs:\n\t\tg.write_pdf('graphs/%s_code.pdf'%flow_graphs[i].type)\n\t\tg.write_dot('graphs/%s_code.dot'%flow_graphs[i].type)\n\t\ti += 1\n\n\ti = 0\n\tfor g in visual_flow_graphs:\n\t\tg.write_pdf('graphs/%s_flow.pdf'%flow_graphs[i].type)\n\t\tg.write_dot('graphs/%s_flow.dot'%flow_graphs[i].type)\n\t\ti += 1\n\n\t#creates the flow type graph --> flow.xml\n\tpar.dump_graphs(flow_graphs)\n\t#adding to the original xml the profiling informations --> code.xml\n\tpro.add_profile_xml(profile_xml, pragma_xml)\n\t#creating the total graph with the call-tree\n\tfunc_graph = par.create_complete_graph(visual_flow_graphs, profile_xml)\n\t#creating the graphs with the function calls\n\tfunc_graph.write_pdf('graphs/function_graphs.pdf')\n\tfunc_graph.write_dot('graphs/function_graphs.dot')\n\n\t#creating the expanded graph where the functions are inserted in the flow graph\n\texp_flows = copy.deepcopy(flow_graphs)\n\tpar.explode_graph(exp_flows)\n\tmain_flow = sched.get_main(exp_flows)\n\t\n\t#creating a generator for the expanded graph\n\tgen = sched.generate_task(main_flow)\n\t\n\t#creating a new generator for the expanded graph\n\tsched.make_white(main_flow)\n\n\t#getting the number of physical cores of the machine profiled\n\tmax_flows = sched.get_core_num(profile_xml)\n\tmax_flows = 4\n\t#getting cores of the actual machine, but the problem is multithreading\n\tcores = multiprocessing.cpu_count()\n\tif cores == 1:\n\t\tcores = 2\n\t\n\t#initializing all the lists for the parallel scheduling algorithm\n\ttasks_list = []\n\ttask_list = []\n\tflows_list = []\n\toptimal_flow_list = []\n\tp_list = []\n\tqueue_list = []\n\tresults = []\n\tnum_tasks = 0\n\n\t#getting the number of tasks in the expanded graph and creating a list of task\n\tfor task in gen:\n\t\ttask_list.append(task)\n\t\tnum_tasks += 1\n\n\tif output == 'True':\n\t\tsched.make_white(main_flow)\n\t\tpar.scanGraph(main_flow)\n\t\n\t#starting the parallel or sequential search of the best solution with a timing constrain\n\tif multi == 'parallel':\n\t\tfor core in range(cores):\n\t\t\ttmp = []\n\t\t\toptimal_flow_list.append(tmp)\n\t\t\ttmp_2 = []\n\t\t\tflows_list.append(tmp_2)\n\t\t\trandom.shuffle(task_list)\n\t\t\ttasks_list.append(copy.deepcopy(task_list))\n\t\t\tq = sched.Queue()\n\t\t\tqueue_list.append(q)\n\t\t\tp_list.append(multiprocessing.Process(target = sched.get_optimal_flow, args = (flows_list[core], tasks_list[core], 0, optimal_flow_list[core], num_tasks, max_flows, execution_time, queue_list[core],  )))\n\t\t\tprint \"starting core: \",core\n\t\t\tp_list[core].start()\n\t\t#getting the results from the processes\n\t\tfor queue in queue_list:\n\t\t\tt = queue.q.get()\n\t\t\tresults.append(t)\n\t\t#joining all the processes\n\t\ti = 0\n\t\tfor p in p_list:\n\t\t\tp.join()\n\t\t\tprint \"core \", i, \" joined\"\n\t\t\ti += 1\n\t\t#getting the best result\n\t\toptimal_flow = results[0]\n\t\tbest = 0\n\t\tfor i in range(len(results)):\n\t\t\tprint \"result:\"\n\t\t\tfor flow in results[i]:\n\t\t\t\tflow.dump()\n\t\t\t\tif sched.get_cost(results[i]) < sched.get_cost(optimal_flow):\n\t\t\t\t\tbest = i\n\t\toptimal_flow = results[best]\n\telse:\n\t\t\toptimal_flow = []\n\t\t\tflow_list = []\n\t\t\texecution_time += time.clock()\n\t\t\tprint \"searching best schedule\"\n\t\t\tsched.get_optimal_flow_single(flow_list, task_list, 0, optimal_flow, num_tasks, max_flows, execution_time )\n\n\t\n\n\t#printing the best result\t\n\tprint \"solution:\"\n\tfor flow in optimal_flow:\n\t\tflow.dump(\"\\t\")\n\t\tprint \"\\ttime:\",flow.time\n\n\n\n\t#substitutes \"for tasks\" with splitted versions if present in the optimal flows\n\tpar.add_new_tasks(optimal_flow, main_flow)\n\tsched.make_white(main_flow)\n\tgen_ = sched.generate_task(main_flow)\n\n\n\tt_list = []\n\tfor t in gen_:\n\t\tt_list.append(t)\n\t\t\"\"\"\n\t\tprint t.type,\" @ \", t.start_line, \" has parents:\"\n\t\tfor p in t.parent:\n\t\t\tprint \"\\t \",p.type,\" @ \", p.start_line\n\t\tprint \"and children:\"\n\t\tfor c in t.children:\n\t\t\tprint \"\\t \",c.type,\" @ \", c.start_line\n\t\tprint\n\t\t\"\"\"\n\t\n\t#adds id's to all the tasks to retrive the flow to which they belong\n\tpar.add_flow_id(optimal_flow, t_list)\n\n\t#sets arrival times and deadlines using a modified version of the chetto algorithm\n\tsched.chetto(main_flow, deadline, optimal_flow)\n\t\n\t#checks if the schedule is feasible and in case creates the schedule file\n\tif sched.check_schedule(main_flow):\n\t\tsched.create_schedule(main_flow, len(optimal_flow))\n\t\tsched.make_white(main_flow)\n\t\t#sched.print_schedule(main_flow)\n\telse:\n\t\tprint \"tasks not schedulable, try with more search time\"\n\n\t#prints extended info of the entire pragma graph", "instruction_en": "To use the code, call it with the following arguments: <filename> <pragma_xml_file> <executable_name> <profiling_iterations> True/False (for output). The code will run the executable for the specified number of iterations and aggregate the information in executable_profile.xml. It will then create nested and flow graphs for each function, as well as a complete call-tree graph. The code will then search for the optimal schedule using a parallel or sequential algorithm with a timing constraint. If a feasible schedule is found, it will be saved in a schedule file.", "original_line_number": 4401, "instruction_length": 595}
{"instruction": "Buat kelas paket yang menghasilkan paket binari dari disediakan init_packet_fields. Kelas harus memasukkan kaedah yang dipanggil Generate_packet yang mengembalikan rentetan yang mewakili init_packet (dalam binari). Kelas juga harus termasuk kamus yang dipanggil init_packet_fields yang mengandungi medan paket. Kelas harus menggunakan format Little Endian dengan saiz standard untuk Python. Kelas harus termasuk enum yang dipanggil Packetfield yang mengandungi nilai medan paket. Kelas ini harus termasuk pemalar untuk Int pendek, tidak ditandatangani yang tidak ditandatangani, char unsigned, dan array char. Kelas harus memasukkan kaedah peribadi yang dipanggil __generate_struct_format_string yang menghasilkan rentetan format struct berdasarkan kunci yang terdapat di init_packet_fields. Kelas harus mengendalikan medan yang memerlukan panjang dan medan yang memerlukan format tertentu.", "input": "", "output": "# Copyright (c) 2015, Nordic Semiconductor\r\n# All rights reserved.\r\n#\r\n# Redistribution and use in source and binary forms, with or without\r\n# modification, are permitted provided that the following conditions are met:\r\n#\r\n# * Redistributions of source code must retain the above copyright notice, this\r\n#   list of conditions and the following disclaimer.\r\n#\r\n# * Redistributions in binary form must reproduce the above copyright notice,\r\n#   this list of conditions and the following disclaimer in the documentation\r\n#   and/or other materials provided with the distribution.\r\n#\r\n# * Neither the name of Nordic Semiconductor ASA nor the names of its\r\n#   contributors may be used to endorse or promote products derived from\r\n#   this software without specific prior written permission.\r\n#\r\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\r\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\r\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\r\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\r\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\r\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\r\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\r\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\r\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\nfrom enum import Enum\r\nimport struct\r\n\r\n\r\nINIT_PACKET_USES_CRC16 = 0\r\nINIT_PACKET_USES_HASH = 1\r\nINIT_PACKET_EXT_USES_ECDS = 2 \r\n\r\n\r\nclass PacketField(Enum):\r\n    PACKET_VERSION = 1\r\n    COMPRESSION_TYPE = 2\r\n    DEVICE_TYPE = 3\r\n    DEVICE_REVISION = 4\r\n    APP_VERSION = 5\r\n    REQUIRED_SOFTDEVICES_ARRAY = 6\r\n    OPT_DATA = 7\r\n    NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID = 8\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH = 9\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH = 10\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16 = 11\r\n    NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS = 12\r\n\r\n\r\nclass Packet(object):\r\n    \"\"\"\r\n    Class that implements the INIT packet format.\r\n    http://developer.nordicsemi.com/nRF51_SDK/doc/7.1.0/s110/html/a00065.html\r\n    \"\"\"\r\n\r\n    UNSIGNED_SHORT = \"H\"\r\n    UNSIGNED_INT = \"I\"\r\n    UNSIGNED_CHAR = \"B\"\r\n    CHAR_ARRAY = \"s\"\r\n\r\n    def __init__(self, init_packet_fields):\r\n        \"\"\"\r\n\r\n            :param init_packet_fields: Dictionary with packet fields\r\n        \"\"\"\r\n        self.init_packet_fields = init_packet_fields\r\n\r\n    def generate_packet(self):\r\n        \"\"\"\r\n        Generates a binary packet from provided init_packet_fields provided in constructor.\r\n        This version includes the extended data\r\n\r\n        :return str: Returns a string representing the init_packet (in binary)\r\n\r\n        \"\"\"\r\n        # Create struct format string based on keys that are\r\n        # present in self.init_packet_fields\r\n        format_string = self.__generate_struct_format_string()\r\n        args = []\r\n\r\n        for key in sorted(self.init_packet_fields.keys(), key=lambda x: x.value):\r\n            # Add length to fields that required that\r\n            if key in [PacketField.REQUIRED_SOFTDEVICES_ARRAY,\r\n                       PacketField.OPT_DATA]:\r\n                args.append(len(self.init_packet_fields[key]))\r\n                args.extend(self.init_packet_fields[key])\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID]:\r\n                args.append(self.init_packet_fields[key]) # Extended packet id format\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH]:\r\n                args.append(self.init_packet_fields[key]) # Length of firmware image\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16]:\r\n                args.append(self.init_packet_fields[key])  # CRC-16\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH]:\r\n                args.append(self.init_packet_fields[key])  # SHA-256 hash of firmware image\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS]:\r\n                args.append(self.init_packet_fields[key])  # ECDS of base init packet using Curve P-256 amd SHA-256\r\n            else:\r\n                args.append(self.init_packet_fields[key])\r\n\r\n        return struct.pack(format_string, *args)\r\n\r\n    def __generate_struct_format_string(self):\r\n        format_string = \"<\"  # Use little endian format with standard sizes for python,\r\n        # see https://docs.python.org/2/library/struct.html\r\n\r\n        for key in sorted(self.init_packet_fields.keys(), key=lambda x: x.value):\r\n            if key in [PacketField.PACKET_VERSION,\r\n                       PacketField.COMPRESSION_TYPE,\r\n                       PacketField.DEVICE_TYPE,\r\n                       PacketField.DEVICE_REVISION,\r\n                       ]:\r\n                format_string += Packet.UNSIGNED_SHORT\r\n\r\n            elif key in [PacketField.APP_VERSION]:\r\n                format_string += Packet.UNSIGNED_INT\r\n            elif key in [PacketField.REQUIRED_SOFTDEVICES_ARRAY]:\r\n                array_elements = self.init_packet_fields[key]\r\n                format_string += Packet.UNSIGNED_SHORT  # Add length field to format packet\r\n\r\n                for _ in range(len(array_elements)):\r\n                    format_string += Packet.UNSIGNED_SHORT\r\n            elif key in [PacketField.OPT_DATA]:\r\n                format_string += Packet.UNSIGNED_SHORT  # Add length field to optional data\r\n                format_string += \"{0}{1}\".format(len(self.init_packet_fields[key]), Packet.CHAR_ARRAY)\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID]:\r\n                format_string += Packet.UNSIGNED_INT # Add the extended packet id field\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH:\r\n                format_string += Packet.UNSIGNED_INT # Add the firmware length field\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH:\r\n                format_string += \"32{0}\".format(Packet.CHAR_ARRAY)  # SHA-256 requires 32 bytes\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16:\r\n                format_string += Packet.UNSIGNED_SHORT\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS:\r\n                format_string += \"64{0}\".format(Packet.CHAR_ARRAY)  # ECDS based on P-256 using SHA-256 requires 64 bytes\r\n\r\n        return format_string", "instruction_en": "Create a Packet class that generates a binary packet from provided init_packet_fields. The class should include a method called generate_packet that returns a string representing the init_packet (in binary). The class should also include a dictionary called init_packet_fields that contains the packet fields. The class should use little endian format with standard sizes for python. The class should include an enum called PacketField that contains the packet field values. The class should include constants for unsigned short, unsigned int, unsigned char, and char array. The class should include a private method called __generate_struct_format_string that generates a struct format string based on keys that are present in init_packet_fields. The class should handle fields that require length and fields that require a specific format.", "original_line_number": 4867, "instruction_length": 890}
{"instruction": "Untuk menggunakan rangka kerja acara kecil, import modul yang diperlukan dan gunakan penghias `@event` untuk mendaftarkan fungsi sebagai pengendali acara. Gunakan `add_event_handler` untuk mendaftarkan fungsi sebagai pengendali acara tanpa menggunakan penghias. Gunakan `fire_event` untuk mencetuskan peristiwa dengan nama yang diberikan dan lulus sebarang hujah yang diperlukan. Gunakan `Rove_event_handlers` untuk mengeluarkan semua pengendali untuk nama acara yang diberikan dan` Rove_event_handler` untuk mengeluarkan fungsi tertentu sebagai pengendali untuk nama acara tertentu.", "input": "", "output": "\"\"\"\nProvides small event framework\n\"\"\"\nfrom typing import Callable, List, Dict, Any\n\nfrom loguru import logger\n\nlogger = logger.bind(name='event')\n\n\nclass Event:\n    \"\"\"Represents one registered event.\"\"\"\n\n    def __init__(self, name: str, func: Callable, priority: int = 128) -> None:\n        self.name = name\n        self.func = func\n        self.priority = priority\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n    def __eq__(self, other):\n        return self.priority == other.priority\n\n    def __lt__(self, other):\n        return self.priority < other.priority\n\n    def __gt__(self, other):\n        return self.priority > other.priority\n\n    def __str__(self):\n        return f'<Event(name={self.name},func={self.func.__name__},priority={self.priority})>'\n\n    __repr__ = __str__\n\n    def __hash__(self):\n        return hash((self.name, self.func, self.priority))\n\n\n_events: Dict[str, List[Event]] = {}\n\n\ndef event(name: str, priority: int = 128) -> Callable[[Callable], Callable]:\n    \"\"\"Register event to function with a decorator\"\"\"\n\n    def decorator(func: Callable) -> Callable:\n        add_event_handler(name, func, priority)\n        return func\n\n    return decorator\n\n\ndef get_events(name: str) -> List[Event]:\n    \"\"\"\n    :param String name: event name\n    :return: List of :class:`Event` for *name* ordered by priority\n    \"\"\"\n    if name not in _events:\n        raise KeyError('No such event %s' % name)\n    _events[name].sort(reverse=True)\n    return _events[name]\n\n\ndef add_event_handler(name: str, func: Callable, priority: int = 128) -> Event:\n    \"\"\"\n    :param string name: Event name\n    :param function func: Function that acts as event handler\n    :param priority: Priority for this hook\n    :return: Event created\n    :rtype: Event\n    :raises Exception: If *func* is already registered in an event\n    \"\"\"\n    events = _events.setdefault(name, [])\n    for event in events:\n        if event.func == func:\n            raise ValueError(\n                '%s has already been registered as event listener under name %s'\n                % (func.__name__, name)\n            )\n    logger.trace('registered function {} to event {}', func.__name__, name)\n    event = Event(name, func, priority)\n    events.append(event)\n    return event\n\n\ndef remove_event_handlers(name: str) -> None:\n    \"\"\"Removes all handlers for given event `name`.\"\"\"\n    _events.pop(name, None)\n\n\ndef remove_event_handler(name: str, func: Callable) -> None:\n    \"\"\"Remove `func` from the handlers for event `name`.\"\"\"\n    for e in list(_events.get(name, [])):\n        if e.func is func:\n            _events[name].remove(e)\n\n\ndef fire_event(name: str, *args, **kwargs) -> Any:\n    \"\"\"\n    Trigger an event with *name*. If event is not hooked by anything nothing happens. If a function that hooks an event\n    returns a value, it will replace the first argument when calling next function.\n\n    :param name: Name of event to be called\n    :param args: List of arguments passed to handler function\n    :param kwargs: Key Value arguments passed to handler function\n    \"\"\"\n    if name in _events:\n        for event in get_events(name):\n            result = event(*args, **kwargs)\n            if result is not None:\n                args = (result,) + args[1:]\n    return args and args[0]", "instruction_en": "To use the small event framework, import the necessary modules and use the `@event` decorator to register a function as an event handler. Use `add_event_handler` to register a function as an event handler without using the decorator. Use `fire_event` to trigger an event with the given name and pass any necessary arguments. Use `remove_event_handlers` to remove all handlers for a given event name and `remove_event_handler` to remove a specific function as a handler for a given event name.", "original_line_number": 4911, "instruction_length": 583}
{"instruction": "Kod yang diberikan menyediakan fungsi untuk menganggarkan persamaan pasangan dari model yang diberikan dan pemprosesan perpecahan merentasi pelbagai pemproses. Untuk menggunakan fungsi ini, seseorang perlu menyediakan fail objek pangkalan data BayESDB, nama jadual yang mengandungi data mentah, nama model untuk menganggarkan dari, dan nama jadual untuk memasukkan hasil persamaan ke dalam. Fungsi ini juga mempunyai parameter pilihan seperti bilangan pemproses untuk digunakan, bilangan baris untuk menganggarkan persamaan pasangan, dan sama ada untuk menulis ganti sim_table jika sudah ada. Fungsi ini memecah pemprosesan secara merata di antara bilangan pemproses yang ditentukan dan mengira saiz dan pengiraan mengimbangi untuk setiap pertanyaan pekerja. Ia kemudian mewujudkan jadual kesamaan dan memasukkan hasil anggaran ke dalam jadual.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n#   Copyright (c) 2010-2016, MIT Probabilistic Computing Project\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n\"\"\"Speed up BDB queries by parallelizing them.\n\nIntuition\n---------\n\n``ESTIMATE SIMILARITY FROM PAIRWISE t`` will run in ``O(n^2 m v)`` time, where\n``n`` is the number of rows, ``m`` is the number of models, and ``v`` is the\naverage number of views per model. While this query is reasonable for small\ndatasets, on medium-large datasets the query slows down significantly and can\nbecome intractable.  Splitting the processing up among multiple cores can\ngreatly reduce computation time; this module provides functionality to assist\nthis multiprocessing.\n\nCurrently, a multiprocessing equivalent is provided only for\n``ESTIMATE PAIRWISE SIMILARITY``. In fact, this is a query that is most likely\nto require multiprocessing, as datasets frequently have many more rows than\ncolumns.\n\nExample\n-------\n\nFollowing are (very) informal timing statistics with a 200 rows by 4 column\n.cvs file, run on a late 2012 MacBook Pro with a 2.5 GHz 2-core Intel Core i5::\n\n    id,one,two,three,four\n    0,2,3,4,two\n    1,1,5,4,three\n    2,5,1,5,one\n    ...\n    197,0,5,0,five\n    198,5,3,0,three\n    199,4,5,2,three\n\nAfter inserting this .csv data into a table ``t`` and analyzing it quickly::\n\n    bdb.execute('''\n        CREATE GENERATOR t_cc FOR t USING crosscat (\n            GUESS(*),\n            id IGNORE\n        )\n    ''')\n    bdb.execute('INITIALIZE 3 MODELS FOR t_cc')\n    bdb.execute('ANALYZE t_cc MODELS 0-2 FOR 10 ITERATIONS WAIT')\n\nThe corresponding similarity table thus has 200^2 = 40000 rows::\n\n    In [72]: %timeit -n 10 cursor_to_df(bdb.execute('ESTIMATE SIMILARITY FROM PAIRWISE t_cc'))\n    10 loops, best of 3: 9.56 s per loop\n\n    In [73]: %timeit -n 10 parallel.estimate_pairwise_similarity(bdb_file.name, 't', 't_cc', overwrite=True)\n    10 loops, best of 3: 5.16 s per loop  # And values are located in the t_similarity table.\n\nThe approximate 2x speed up is what would be expected from dividing the work\namong two cores. Further speed increases are likely with more powerful\nmachines.\n\n----\n\"\"\"\n\nfrom bayeslite.exception import BayesLiteException as BLE\nfrom bdbcontrib.bql_utils import cursor_to_df\nimport multiprocessing as mp\nfrom bayeslite import bayesdb_open, bql_quote_name\nfrom bayeslite.util import cursor_value\n\n\ndef _query_into_queue(query_string, params, queue, bdb_file):\n    \"\"\"\n    Estimate pairwise similarity of a certain subset of the bdb according to\n    query_string; place it in the multiprocessing Manager.Queue().\n\n    For two technical reasons, this function is defined as a toplevel class and\n    independently creates a bdb handle:\n\n    1) Multiprocessing workers must be pickleable, and thus must be\n       declared as toplevel functions;\n    2) Multiple threads cannot access the same bdb handle, lest concurrency\n       issues arise with corrupt data.\n\n    Parameters\n    ----------\n    query_string : str\n        Name of the query to execute, determined by estimate_similarity_mp.\n    queue : multiprocessing.Manager.Queue\n        Queue to place results into\n    bdb_file : str\n        File location of the BayesDB database. This function will\n        independently open a new BayesDB handler.\n    \"\"\"\n    bdb = bayesdb_open(pathname=bdb_file)\n    res = bdb.execute(query_string, params)\n    queue.put(cursor_to_df(res))\n\n\ndef _chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n\n\ndef estimate_pairwise_similarity(bdb_file, table, model, sim_table=None,\n                                 cores=None, N=None, overwrite=False):\n    \"\"\"\n    Estimate pairwise similarity from the given model, splitting processing\n    across multiple processors, and save results into sim_table.\n\n    Because called methods in this function must also open up separate BayesDB\n    instances, this function accepts a BayesDB filename, rather than an actual\n    bayeslite.BayesDB object.\n\n    Parameters\n    ----------\n    bdb_file : str\n        File location of the BayesDB database object. This function will\n        handle opening the file with bayeslite.bayesdb_open.\n    table : str\n        Name of the table containing the raw data.\n    model : str\n        Name of the metamodel to estimate from.\n    sim_table : str\n        Name of the table to insert similarity results into. Defaults to\n        table name + '_similarity'.\n    cores : int\n        Number of processors to use. Defaults to the number of cores as\n        identified by multiprocessing.num_cores.\n    N : int\n        Number of rows for which to estimate pairwise similarities (so\n        N^2 calculations are done). Should be used just to test small\n        batches; currently, there is no control over what specific pairwise\n        similarities are estimated with this parameter.\n    overwrite : bool\n        Whether to overwrite the sim_table if it already exists. If\n        overwrite=False and the table exists, function will raise\n        sqlite3.OperationalError. Default True.\n    \"\"\"\n    bdb = bayesdb_open(pathname=bdb_file)\n\n    if cores is None:\n        cores = mp.cpu_count()\n\n    if cores < 1:\n        raise BLE(ValueError(\n            \"Invalid number of cores {}\".format(cores)))\n\n    if sim_table is None:\n        sim_table = table + '_similarity'\n\n    # Get number of occurrences in the database\n    count_cursor = bdb.execute(\n        'SELECT COUNT(*) FROM {}'.format(bql_quote_name(table))\n    )\n    table_count = cursor_value(count_cursor)\n    if N is None:\n        N = table_count\n    elif N > table_count:\n        raise BLE(ValueError(\n            \"Asked for N={} rows but {} rows in table\".format(N, table_count)))\n\n    # Calculate the size (# of similarities to compute) and\n    # offset (where to start) calculation for each worker query.\n\n    # Divide sizes evenly, and make the last job finish the remainder\n    sizes = [(N * N) / cores for i in range(cores)]\n    sizes[-1] += (N * N) % cores\n\n    total = 0\n    offsets = [total]\n    for size in sizes[:-1]:\n        total += size\n        offsets.append(total)\n\n    # Create the similarity table. Assumes original table has rowid column.\n    # XXX: tables don't necessarily have an autoincrementing primary key\n    # other than rowid, which is implicit and can't be set as a foreign key.\n    # We ought to ask for an optional user-specified foreign key, but\n    # ESTIMATE SIMILARITY returns numerical values rather than row names, so\n    # changing numerical rownames into that foreign key would be finicky. For\n    # now, we eliminate REFERENCE {table}(foreign_key) from the rowid0 and\n    # rowid1 specs.\n    sim_table_q = bql_quote_name(sim_table)\n    if overwrite:\n        bdb.sql_execute('DROP TABLE IF EXISTS {}'.format(sim_table_q))\n\n    bdb.sql_execute('''\n        CREATE TABLE {} (\n            rowid0 INTEGER NOT NULL,\n            rowid1 INTEGER NOT NULL,\n            value DOUBLE NOT NULL\n        )\n    '''.format(sim_table_q))\n\n    # Define the helper which inserts data into table in batches\n    def insert_into_sim(df):\n        \"\"\"\n        Use the main thread bdb handle to successively insert results of\n        ESTIMATEs into the table.\n        \"\"\"\n        rows = map(list, df.values)\n        insert_sql = '''\n            INSERT INTO {} (rowid0, rowid1, value) VALUES (?, ?, ?)\n        '''.format(sim_table_q)\n        # Avoid sqlite3 500-insert limit by grouping insert statements\n        # into one transaction.\n        with bdb.transaction():\n            for row in rows:\n                bdb.sql_execute(insert_sql, row)\n\n    pool = mp.Pool(processes=cores)\n\n    manager = mp.Manager()\n    queue = manager.Queue()\n\n    # Construct the estimate query template.\n    q_template = '''\n        ESTIMATE SIMILARITY FROM PAIRWISE {} LIMIT ? OFFSET ?\n    ''' .format(bql_quote_name(model))\n\n    for so in zip(sizes, offsets):\n        pool.apply_async(\n            _query_into_queue, args=(q_template, so, queue, bdb_file)\n        )\n\n    # Close pool and wait for processes to finish\n    # FIXME: This waits for all processes to finish before inserting\n    # into the table, which means that memory usage is potentially very\n    # high!\n    pool.close()\n    pool.join()\n\n    # Process returned results\n    while not queue.empty():\n        df = queue.get()\n        insert_into_sim(df)", "instruction_en": "The given code provides a function to estimate pairwise similarity from a given model and split processing across multiple processors. To use this function, one needs to provide the BayesDB database object file, table name containing the raw data, model name to estimate from, and the name of the table to insert similarity results into. The function also has optional parameters such as the number of processors to use, the number of rows for which to estimate pairwise similarities, and whether to overwrite the sim_table if it already exists. The function splits the processing evenly among the specified number of processors and calculates the size and offset calculation for each worker query. It then creates the similarity table and inserts the results of ESTIMATEs into the table.", "original_line_number": 5252, "instruction_length": 844}
{"instruction": "Kod yang diberikan adalah modul python untuk melaksanakan Google Recaptcha. Untuk menggunakannya, import modul dan fungsi yang diperlukan, dan kemudian panggil fungsi seperti yang diperlukan. Untuk memaparkan HTML Recaptcha, hubungi fungsi \"DisplayHTML\" dan lulus dalam kunci awam dan mana -mana parameter pilihan. Untuk mengemukakan respons Recaptcha untuk pengesahan, hubungi fungsi \"Hantar\" dan lulus dalam parameter yang diperlukan. Untuk menyemak tindak balas Recaptcha dalam paparan Django, hubungi fungsi \"check_captcha\" dan lulus dalam objek permintaan. Untuk menghasilkan HTML Recaptcha baru berdasarkan hasil pengesahan, hubungi fungsi \"new_captcha_html\" dan lulus dalam objek hasil.", "input": "", "output": "import urllib2, urllib\nfrom   proj.settings import *\n\n\nAPI_SSL_SERVER=\"https://www.google.com/recaptcha/api\"\nAPI_SERVER=\"http://www.google.com/recaptcha/api\"\nVERIFY_SERVER=\"www.google.com\"\n\nclass RecaptchaResponse(object):\n    def __init__(self, is_valid, error_code=None):\n        self.is_valid = is_valid\n        self.error_code = error_code\n\ndef displayhtml (public_key,\n                 use_ssl = False,\n                 error = None):\n    \"\"\"Gets the HTML to display for reCAPTCHA\n\n    public_key -- The public api key\n    use_ssl -- Should the request be sent over ssl?\n    error -- An error message to display (from RecaptchaResponse.error_code)\"\"\"\n\n    error_param = ''\n    if error:\n        error_param = '&error=%s' % error\n\n    if use_ssl:\n        server = API_SSL_SERVER\n    else:\n        server = API_SERVER\n\n    return \"\"\"<script type=\"text/javascript\" src=\"%(ApiServer)s/challenge?k=%(PublicKey)s%(ErrorParam)s\"></script>\n\n<noscript>\n  <iframe src=\"%(ApiServer)s/noscript?k=%(PublicKey)s%(ErrorParam)s\" height=\"300\" width=\"500\" frameborder=\"0\"></iframe><br />\n  <textarea name=\"recaptcha_challenge_field\" rows=\"3\" cols=\"40\"></textarea>\n  <input type='hidden' name='recaptcha_response_field' value='manual_challenge' />\n</noscript>\n\"\"\" % {\n        'ApiServer' : server,\n        'PublicKey' : public_key,\n        'ErrorParam' : error_param,\n        }\n\n\ndef submit (recaptcha_challenge_field,\n            recaptcha_response_field,\n            private_key,\n            remoteip):\n    \"\"\"\n    Submits a reCAPTCHA request for verification. Returns RecaptchaResponse\n    for the request\n\n    recaptcha_challenge_field -- The value of recaptcha_challenge_field from the form\n    recaptcha_response_field -- The value of recaptcha_response_field from the form\n    private_key -- your reCAPTCHA private key\n    remoteip -- the user's ip address\n    \"\"\"\n\n    if not (recaptcha_response_field and recaptcha_challenge_field and\n            len (recaptcha_response_field) and len (recaptcha_challenge_field)):\n        return RecaptchaResponse (is_valid = False, error_code = 'incorrect-captcha-sol')\n    \n\n    def encode_if_necessary(s):\n        if isinstance(s, unicode):\n            return s.encode('utf-8')\n        return s\n\n    params = urllib.urlencode ({\n            'privatekey': encode_if_necessary(private_key),\n            'remoteip' :  encode_if_necessary(remoteip),\n            'challenge':  encode_if_necessary(recaptcha_challenge_field),\n            'response' :  encode_if_necessary(recaptcha_response_field),\n            })\n\n    request = urllib2.Request (\n        url = \"http://%s/recaptcha/api/verify\" % VERIFY_SERVER,\n        data = params,\n        headers = {\n            \"Content-type\": \"application/x-www-form-urlencoded\",\n            \"User-agent\": \"reCAPTCHA Python\"\n            }\n        )\n    \n    httpresp = urllib2.urlopen (request)\n\n    return_values = httpresp.read ().splitlines ();\n    httpresp.close();\n\n    return_code = return_values [0]\n\n    if (return_code == \"true\"):\n        return RecaptchaResponse (is_valid=True)\n    else:\n        return RecaptchaResponse (is_valid=False, error_code = return_values [1])\n\ndef check_captcha(request):\n    \n    captcha_challenge = request.POST.get('recaptcha_challenge_field')\n    captcha_response = request.POST.get('recaptcha_response_field')\n    captcha_result = None\n    ip = None\n    if 'HTTP_X_FORWARDED_FOR' in request.META:\n        ip = request.META['HTTP_X_FORWARDED_FOR']\n    elif 'REMOTE_ADDR' in request.META:\n        ip = request.META['REMOTE_ADDR']\n    if captcha_response is not None and captcha_challenge is not None:\n        captcha_result = submit(captcha_challenge,\n                                captcha_response,\n                                recaptcha_private_key,\n                                ip)\n    return captcha_result\n\ndef new_captcha_html(captcha_result):\n    if captcha_result is None:\n        captcha_html = displayhtml(recaptcha_public_key, use_ssl=True)\n    else:\n        captcha_html = displayhtml(recaptcha_public_key, use_ssl=True, error = captcha_result.error_code)\n    return captcha_html", "instruction_en": "The given code is a Python module for implementing Google reCAPTCHA. To use it, import the necessary modules and functions, and then call the functions as needed. To display the reCAPTCHA HTML, call the \"displayhtml\" function and pass in the public key and any optional parameters. To submit the reCAPTCHA response for verification, call the \"submit\" function and pass in the necessary parameters. To check the reCAPTCHA response in a Django view, call the \"check_captcha\" function and pass in the request object. To generate new reCAPTCHA HTML based on the result of the verification, call the \"new_captcha_html\" function and pass in the result object.", "original_line_number": 5575, "instruction_length": 693}
{"instruction": "Kod ini mengandungi logik untuk `AQ Tambah Network_Device`. Untuk menambah peranti rangkaian, anda perlu menyediakan parameter yang diperlukan berikut: \"Network_Device\", \"Model\", \"Type\", \"IP\", \"Interface\", dan \"IFType\". Anda juga boleh menyediakan parameter pilihan seperti \"label\", \"mac\", \"vendor\", \"siri\", \"komen\", \"pengeksport\", \"casis\", \"slot\", \"archetype\", \"domain\", \"sandbox\", \"pengguna\", \"justifikasi\", dan \"alasan\". Kod ini akan membuat objek rangkaian baru dengan maklumat yang disediakan dan menambahkannya ke pangkalan data. Ia juga akan membuat objek hos baru dan mengemas kini DSDB dengan maklumat peranti rangkaian baru. Akhirnya, ia akan menjana templat pleno untuk peranti rangkaian dan objek tuan rumah.", "input": "", "output": "# -*- cpy-indent-level: 4; indent-tabs-mode: nil -*-\n# ex: set expandtab softtabstop=4 shiftwidth=4:\n#\n# Copyright (C) 2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018  Contributor\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Contains the logic for `aq add network_device`.\"\"\"\n\nfrom sqlalchemy.orm import subqueryload\n\nfrom aquilon.exceptions_ import ArgumentError\nfrom aquilon.aqdb.model import NetworkDevice, Model, Archetype, Chassis, NetworkDeviceChassisSlot\nfrom aquilon.aqdb.model.network import get_net_id_from_ip\nfrom aquilon.worker.broker import BrokerCommand\nfrom aquilon.worker.dbwrappers.dns import grab_address\nfrom aquilon.worker.dbwrappers.location import get_location\nfrom aquilon.worker.dbwrappers.interface import (get_or_create_interface,\n                                                 assign_address,\n                                                 check_netdev_iftype)\nfrom aquilon.worker.dbwrappers.host import create_host\nfrom aquilon.worker.processes import DSDBRunner\nfrom aquilon.worker.templates.switchdata import PlenarySwitchData\nfrom aquilon.worker.dbwrappers.change_management import ChangeManagement\n\n\nclass CommandAddNetworkDevice(BrokerCommand):\n    requires_plenaries = True\n\n    required_parameters = [\"network_device\", \"model\", \"type\",\n                           \"ip\", \"interface\", \"iftype\"]\n\n    def render(self, session, logger, plenaries, network_device, label, model, type, ip,\n               interface, iftype, mac, vendor, serial, comments, exporter, chassis, slot,\n               archetype, domain, sandbox, user, justification, reason, **arguments):\n        dbmodel = Model.get_unique(session, name=model, vendor=vendor,\n                                   compel=True)\n\n        if not dbmodel.model_type.isNetworkDeviceType():\n            raise ArgumentError(\"This command can only be used to \"\n                                \"add network devices.\")\n\n        dblocation = get_location(session, query_options=[subqueryload('parents')], **arguments)\n\n        if chassis:\n            dbchassis = Chassis.get_unique(session, chassis, compel=True)\n            if slot is None:\n                raise ArgumentError(\"The --chassis option requires a --slot.\")\n            if dblocation and dblocation != dbchassis.location:\n                raise ArgumentError(\"{0} conflicts with chassis location \"\n                                    \"{1}.\".format(dblocation, dbchassis.location))\n            dblocation = dbchassis.location\n        elif slot is not None:\n            raise ArgumentError(\"The --slot option requires a --chassis.\")\n\n        dbdns_rec, _ = grab_address(session, network_device, ip,\n                                    allow_restricted_domain=True,\n                                    allow_reserved=True, preclude=True,\n                                    exporter=exporter, require_grn=False)\n        if not label:\n            label = dbdns_rec.fqdn.name\n            try:\n                NetworkDevice.check_label(label)\n            except ArgumentError:\n                raise ArgumentError(\"Could not deduce a valid hardware label \"\n                                    \"from the network device name.  Please specify \"\n                                    \"--label.\")\n\n        # FIXME: What do the error messages for an invalid enum (switch_type)\n        # look like?\n        dbnetdev = NetworkDevice(label=label, switch_type=type,\n                                 location=dblocation, model=dbmodel,\n                                 serial_no=serial, comments=comments)\n        session.add(dbnetdev)\n\n        if chassis:\n            dbslot = session.query(NetworkDeviceChassisSlot).filter_by(chassis=dbchassis,\n                                                                       slot_number=slot).first()\n            if dbslot and dbslot.network_device:\n                raise ArgumentError(\"{0} slot {1} already has network device \"\n                                    \"{2}.\".format(dbchassis, slot,\n                                                  dbslot.network_device.label))\n            if not dbslot:\n                dbslot = NetworkDeviceChassisSlot(chassis=dbchassis, slot_number=slot)\n            dbslot.network_device = dbnetdev\n            session.add(dbslot)\n\n        dbnetdev.primary_name = dbdns_rec\n\n        check_netdev_iftype(iftype)\n\n        dbinterface = get_or_create_interface(session, dbnetdev,\n                                              name=interface, mac=mac,\n                                              interface_type=iftype)\n        dbnetwork = get_net_id_from_ip(session, ip)\n        # TODO: should we call check_ip_restrictions() here?\n        assign_address(dbinterface, ip, dbnetwork, logger=logger)\n\n        if not archetype:\n            hw_section = 'hardware_network_device'\n            if not self.config.has_option(hw_section, 'default_archetype'):\n                raise ArgumentError(\"Cannot determine the archetype for \"\n                                    \"network devices.  Please specify \"\n                                    \"--archetype.\")\n            archetype = self.config.get(hw_section, 'default_archetype')\n\n        dbarchetype = Archetype.get_unique(session, archetype, compel=True)\n\n        dbhost = create_host(session, logger, self.config, dbnetdev,\n                             dbarchetype, domain=domain, sandbox=sandbox,\n                             **arguments)\n\n        session.flush()\n\n        # Validate ChangeManagement\n        cm = ChangeManagement(session, user, justification, reason, logger, self.command, **arguments)\n        cm.consider(dbhost)\n        cm.validate()\n\n        # Add the legacy template separately\n        plenaries.add(dbnetdev, cls=PlenarySwitchData)\n        plenaries.add(dbnetdev)\n        plenaries.add(dbhost)\n\n        with plenaries.transaction():\n            dsdb_runner = DSDBRunner(logger=logger)\n            dsdb_runner.update_host(dbnetdev, None)\n            dsdb_runner.commit_or_rollback(\"Could not add network device to DSDB\")\n\n        return", "instruction_en": "This code contains the logic for `aq add network_device`. To add a network device, you need to provide the following required parameters: \"network_device\", \"model\", \"type\", \"ip\", \"interface\", and \"iftype\". You can also provide optional parameters such as \"label\", \"mac\", \"vendor\", \"serial\", \"comments\", \"exporter\", \"chassis\", \"slot\", \"archetype\", \"domain\", \"sandbox\", \"user\", \"justification\", and \"reason\". The code will create a new NetworkDevice object with the provided information and add it to the database. It will also create a new Host object and update the DSDB with the new network device information. Finally, it will generate plenary templates for the network device and host objects.", "original_line_number": 6383, "instruction_length": 720}
{"instruction": "Buat aplikasi web Django dengan paparan untuk memaparkan halaman 404, halaman utama, topik, sumber, senarai pos blog, paparan pos blog, papan pendahulu, pendaftaran pengguna, dan dua titik akhir API untuk memeriksa pemegang dan e -mel GitHub. Permohonan ini harus menggunakan model dan bentuk dari fail yang ditentukan, dan mengimport modul yang diperlukan seperti permintaan, HTTP, dan JsonResponse. Pandangan papan pendahulu harus mengambil data dari API GitHub dan mengemas kini model pengguna dengan sewajarnya. Paparan pendaftaran hendaklah menggunakan RegisterUserForm dan mengalihkan ke halaman utama apabila penyerahan yang berjaya. Titik akhir API harus menyemak pegangan GitHub dan e -mel dalam model pengguna dan mengembalikan respons yang sesuai.", "input": "", "output": "from django import views\nfrom django.shortcuts import render, get_object_or_404\nfrom django.views.generic import TemplateView\nfrom django.views.generic.edit import CreateView\nfrom .models import *\nfrom .forms import *\nimport requests\nimport http\nfrom django.urls import reverse_lazy\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\n\nclass Template404(TemplateView):\n    template_name = \"404.html\"\n\nclass Home(TemplateView):\n    template_name = 'home/home.html'\n\nclass Topics(views.View):\n    def get(self, request, *args, **kwargs):\n        return render(request, \"home/resources/topics.html\", {'topics': Topic.objects.all()})\n\nclass Resources(views.View):\n    def get(self, request, pk, *args, **kwargs):\n        topic = get_object_or_404(Topic, pk=pk)\n        return render(request, \"home/resources/resources.html\", {'resources': topic.resource_set.all(), 'topic' : topic})\n\nclass BlogPostList(views.View):\n    def get(self, request, *args, **kwargs):\n        posts = BlogPost.objects.all()\n        return render(request, \"home/blog/index.html\", {'posts': posts})\n\nclass BlogPostView(views.View):\n    def get(self, request, pk, *args, **kwargs):\n        post = get_object_or_404(BlogPost, pk=pk)\n        return render(request, \"home/blog/blog_post.html\", {'post': post})\n\nclass Leaderboard(views.View):\n    def get(self, request, *args, **kwargs):\n        users = Users.objects.all()\n        for user in users:\n            connected = False\n            while not connected:\n                try:\n                    user_name = user.github_handle\n                    response = requests.get('https://api.github.com/search/issues?sort=created&q=author:{}&type:pr&per_page=100'.format(user_name), verify = False).json()\n                    pr_count = 0\n                    print(response)\n                    for obj in response['items']:\n                        if('pull_request' in obj):\n                            if('2018-09-30T00:00:00Z'<obj['created_at']<'2018-10-31T23:59:59Z'):\n                                pr_count += 1\n                    user.pr_count = pr_count\n                    user.save()\n                    connected = True\n                except:\n                    pass\n        return render(request, 'home/leaderboard.html', {'users': users})\n\nclass RegisterUser(CreateView):\n    form_class = RegisterUserForm\n    template_name = \"home/registeruser.html\"\n    success_url = reverse_lazy('home:home')\n\n@csrf_exempt\ndef GithubEmailCheck(request):\n    github_handle = request.POST.get('github_handle')\n    email = request.POST.get('email')\n    print(\"Received \", github_handle)\n    users = Users.objects.all()\n    for user in users:\n        if user.github_handle == github_handle:\n            return JsonResponse({'message' : 'Duplicate Github Handle'})\n        if user.email == email:\n            return JsonResponse({'message' : 'Duplicate Email'})\n    return JsonResponse({'message' : 'New'})\n\n@csrf_exempt\ndef GithubCheck(request):\n    github_handle = request.POST.get('github_handle')\n    response = requests.get(\"https://api.github.com/users/{}\".format(github_handle), verify = False).json()\n    print(\"https://api.github.com/users/{}\".format(github_handle))\n    if ('login' in response):\n        print(\"Found\")\n        return JsonResponse({'message' : 'Found'})\n    else:\n        return JsonResponse({'message' : 'Not Found'})", "instruction_en": "Create a Django web application with views for displaying a 404 page, home page, topics, resources, blog post list, blog post view, leaderboard, user registration, and two API endpoints for checking Github handles and emails. The application should use models and forms from the specified files, and import necessary modules such as requests, http, and JsonResponse. The leaderboard view should retrieve data from the Github API and update the user model accordingly. The registration view should use the RegisterUserForm and redirect to the home page upon successful submission. The API endpoints should check for duplicate Github handles and emails in the Users model and return appropriate responses.", "original_line_number": 6521, "instruction_length": 758}
{"instruction": "Kod ini membina kit VoltDB untuk Linux dan Mac OSX. Untuk membina kit, jalankan skrip dengan hujah -hujah berikut: komit, tag, atau cawangan untuk repositori VoltDB, komit, tag, atau cawangan untuk repositori pro, dan komit, tag, atau cawangan untuk repositori Rabbitmq. Secara pilihan, gunakan bendera `--nomac` untuk melangkau membina kit Mac OSX dan bendera`-nocommunity` untuk melangkau membina kit komuniti. Skrip ini akan membuat direktori pelepasan dan menyalin fail yang diperlukan untuknya, termasuk lesen percubaan, checksum SHA256, dan balang Maven. Akhirnya, skrip akan membuat calon Symlink dan sandarkan direktori pelepasan.", "input": "", "output": "#!/usr/bin/env python\n\nimport argparse, datetime, getpass, os, sys, shutil, traceback\nfrom fabric.api import run, cd, local, get, settings, lcd, put\nfrom fabric_ssh_config import getSSHInfoForHost\nfrom fabric.context_managers import shell_env\nfrom fabric.utils import abort\n\n#Login as user test, but build in a directory by real username\nusername = 'test'\nbuilddir = \"/tmp/\" + getpass.getuser() + \"Kits/buildtemp\"\nversion = \"UNKNOWN\"\nnativelibdir = \"/nativelibs/obj\"  #  ~test/libs/... usually\ndefaultlicensedays = 70 #default trial license length\n\n\n################################################\n# CHECKOUT CODE INTO A TEMP DIR\n################################################\n\ndef checkoutCode(voltdbGit, proGit, rbmqExportGit, gitloc):\n    global buildir\n    # clean out the existing dir\n    run(\"rm -rf \" + builddir)\n    # make the build dir again\n    run(\"mkdir -p \" + builddir)\n    # change to it\n    with cd(builddir):\n        # do the checkouts, collect checkout errors on both community &\n        # pro repos so user gets status on both checkouts\n        message = \"\"\n        run(\"git clone -q %s/voltdb.git\" % gitloc)\n        result = run(\"cd voltdb; git checkout %s\" % voltdbGit, warn_only=True)\n        if result.failed:\n            message = \"VoltDB checkout failed. Missing branch %s.\" % rbmqExportGit\n\n        run(\"git clone -q %s/pro.git\" % gitloc)\n        result = run(\"cd pro; git checkout %s\" % proGit, warn_only=True)\n        if result.failed:\n            message += \"\\nPro checkout failed. Missing branch %s.\" % rbmqExportGit\n\n        #rabbitmq isn't mirrored internally, so don't use gitloc\n        run(\"git clone -q git@github.com:VoltDB/export-rabbitmq.git\")\n        result = run(\"cd export-rabbitmq; git checkout %s\" % rbmqExportGit, warn_only=True)\n        # Probably ok to use master for export-rabbitmq.\n        if result.failed:\n            print \"\\nExport-rabbitmg branch %s checkout failed. Defaulting to master.\" % rbmqExportGit\n\n        if len(message) > 0:\n            abort(message)\n\n        return run(\"cat voltdb/version.txt\").strip()\n\n################################################\n# MAKE A RELEASE DIR\n################################################\n\ndef makeReleaseDir(releaseDir):\n    # handle the case where a release dir exists for this version\n    if os.path.exists(releaseDir):\n        shutil.rmtree(releaseDir)\n    # create a release dir\n    os.makedirs(releaseDir)\n    print \"Created dir: \" + releaseDir\n\n\n################################################\n# BUILD THE COMMUNITY VERSION\n################################################\n\ndef buildCommunity():\n    if build_mac:\n        packageMacLib=\"true\"\n    else:\n        packageMacLib=\"false\"\n    with cd(builddir + \"/voltdb\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\")\n        run(\"ant -Djmemcheck=NO_MEMCHECK -Dkitbuild=%s %s clean default dist\" % (packageMacLib,  build_args))\n\n################################################\n# BUILD THE ENTERPRISE VERSION\n################################################\n\ndef buildEnterprise():\n    if build_mac:\n        packageMacLib=\"true\"\n    else:\n        packageMacLib=\"false\"\n    with cd(builddir + \"/pro\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\")\n        run(\"VOLTCORE=../voltdb ant -f mmt.xml -Djmemcheck=NO_MEMCHECK -Dallowreplication=true -DallowDrActiveActive=true -Dlicensedays=%d -Dkitbuild=%s %s clean dist.pro\" % (defaultlicensedays, packageMacLib, build_args))\n\n################################################\n# BUILD THE PRO VERSION\n################################################\n\n#\ndef packagePro(version):\n    print \"Making license\"\n    makeTrialLicense(days=defaultlicensedays, dr_and_xdcr=False, nodes=3)\n    print \"Repacking pro kit\"\n    with cd(builddir + \"/pro/obj/pro\"):\n        run(\"mkdir pro_kit_staging\")\n    with cd(builddir + \"/pro/obj/pro/pro_kit_staging\"):\n        run(\"tar xf ../voltdb-ent-%s.tar.gz\" % version)\n        run(\"mv voltdb-ent-%s voltdb-pro-%s\" % (version, version))\n        run(\"cp %s/pro/trial_*.xml voltdb-pro-%s/voltdb/license.xml\" % (builddir, version))\n        run(\"tar cvfz ../voltdb-pro-%s.tar.gz voltdb-pro-%s\" % (version, version))\n\n################################################\n# BUILD THE RABBITMQ EXPORT CONNECTOR\n################################################\n#Build rabbit MQ Exporter\ndef buildRabbitMQExport(version, dist_type):\n    # Paths to the final kit for unpacking/repacking with rmq export\n    paths = {\n        'community': builddir + \"/voltdb/obj/release\",\n        'ent' : builddir + \"/pro/obj/pro/\"\n        }\n    # Untar\n    with cd(paths[dist_type]):\n        run (\"pwd\")\n        run (\"mkdir -p restage\")\n        run (\"tar xf voltdb-%s-%s.tar.gz -C restage\" % (dist_type, version))\n        run (\"rm -f voltdb-%s-%s.tar.gz\" % (dist_type, version))\n\n    # Build RabbitMQ export jar and put it into the untarred kit\n    with cd(builddir + \"/export-rabbitmq\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\", warn_only=True)\n        run(\"VOLTDIST=%s/restage/voltdb-%s-%s ant\" % (paths[dist_type], dist_type, version))\n\n    # Retar\n    with cd(paths[dist_type]):\n        run(\"pwd\")\n        run(\"tar -C restage -czf voltdb-%s-%s.tar.gz voltdb-%s-%s\" % (dist_type, version, dist_type, version))\n        run (\"rm -Rf restage\")\n\n################################################\n# MAKE AN ENTERPRISE TRIAL LICENSE\n################################################\n\n# Must be called after buildEnterprise has been done\ndef makeTrialLicense(days=30, dr_and_xdcr=\"true\", nodes=12):\n    with cd(builddir + \"/pro/tools\"):\n        run(\"./make_trial_licenses.pl -t %d -H %d -W %s\" % (days, nodes, dr_and_xdcr ))\n\n################################################\n# MAKE A SHA256 checksum\n################################################\n\ndef makeSHA256SUM(version, type):\n    with cd(builddir + \"/pro/obj/pro\"):\n        kitname=\"voltdb-\" +  type + \"-\" + version\n        run(\"sha256sum -b %s.tar.gz > %s.SHA256SUM\" % (kitname, kitname))\n\n################################################\n# MAKE AN JAR FILES NEEDED TO PUSH TO MAVEN\n################################################\n\ndef makeMavenJars():\n    with cd(builddir + \"/voltdb\"):\n        run(\"VOLTCORE=../voltdb ant -f build-client.xml maven-jars\")\n\n################################################\n# COPY FILES\n################################################\n\ndef copyFilesToReleaseDir(releaseDir, version, type=None):\n    print \"Copying files to releaseDir\"\n    if type:\n        typeString=\"-\" + type\n    else:\n        typeString=\"\"\n    get(\"%s/pro/obj/pro/voltdb%s-%s.tar.gz\" % (builddir, typeString, version),\n        \"%s/voltdb%s-%s.tar.gz\" % (releaseDir, typeString, version))\n    get(\"%s/pro/obj/pro/voltdb%s-%s.SHA256SUM\" % (builddir, typeString, version),\n        \"%s/voltdb%s-%s.SHA256SUM\" % (releaseDir, typeString, version))\n\ndef copyCommunityFilesToReleaseDir(releaseDir, version, operatingsys):\n    get(\"%s/voltdb/obj/release/voltdb-community-%s.tar.gz\" % (builddir, version),\n        \"%s/voltdb-community-%s.tar.gz\" % (releaseDir, version))\n\n    # add stripped symbols\n    if operatingsys == \"LINUX\":\n        os.makedirs(releaseDir + \"/other\")\n        get(\"%s/voltdb/obj/release/voltdb-%s.sym\" % (builddir, version),\n            \"%s/other/%s-voltdb-voltkv-%s.sym\" % (releaseDir, operatingsys, version))\n\ndef copyTrialLicenseToReleaseDir(releaseDir):\n    get(\"%s/pro/trial_*.xml\" % (builddir),\n        \"%s/license.xml\" % (releaseDir))\n\ndef copyMavenJarsToReleaseDir(releaseDir, version):\n    #The .jars and upload file must be in a directory called voltdb - it is the projectname\n    mavenProjectDir = releaseDir + \"/mavenjars/voltdb\"\n    if not os.path.exists(mavenProjectDir):\n        os.makedirs(mavenProjectDir)\n\n    #Get the voltdbclient-n.n.jar from the recently built community build\n    get(\"%s/voltdb/obj/release/dist-client-java/voltdb/voltdbclient-%s.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s.jar\" % (mavenProjectDir, version))\n    #Get the upload.gradle file\n    get(\"%s/voltdb/tools/kit_tools/upload.gradle\" % (builddir),\n        \"%s/upload.gradle\" % (mavenProjectDir))\n    #Get the src and javadoc .jar files\n    get(\"%s/voltdb/obj/release/voltdbclient-%s-javadoc.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s-javadoc.jar\" % (mavenProjectDir, version))\n    get(\"%s/voltdb/obj/release/voltdbclient-%s-sources.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s-sources.jar\" % (mavenProjectDir, version))\n\n################################################\n# CREATE CANDIDATE SYMLINKS\n################################################\n\ndef createCandidateSysmlink(releaseDir):\n    candidateDir =  os.getenv('HOME') + \"/releases/candidate\";\n    local(\"rm -rf \" + candidateDir)\n    local(\"ln -s %s %s\" % (releaseDir, candidateDir))\n\n################################################\n# BACKUP RELEASE DIR\n################################################\n\ndef backupReleaseDir(releaseDir,archiveDir,version):\n    if not os.path.exists(archiveDir):\n        os.makedirs(archiveDir)\n    # make a backup with the timstamp of the  build\n    timestamp = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n    local(\"tar -czf %s/%s-%s.tgz %s\" \\\n          % (archiveDir, version, timestamp, releaseDir))\n\n################################################\n# REMOVE NATIVE LIBS FROM SHARED DIRECTORY\n################################################\n\ndef rmNativeLibs():\n    # local(\"ls -l ~\" + username + nativelibdir)\n    local(\"rm -rf ~\" + username + nativelibdir)\n\n\n\n################################################\n# GET THE GIT TAGS OR SHAS TO BUILD FROM\n################################################\n\nparser = argparse.ArgumentParser(description = \"Create a full kit. With no args, will do build of master\")\nparser.add_argument('voltdb_sha', nargs=\"?\", default=\"master\", help=\"voltdb repository commit, tag or branch\" )\nparser.add_argument('pro_sha', nargs=\"?\", default=\"master\", help=\"pro repository commit, tag or branch\" )\nparser.add_argument('rabbitmq_sha', nargs=\"?\", default=\"master\", help=\"rabbitmq repository commit, tag or branch\" )\nparser.add_argument('-g','--gitloc', default=\"git@github.com:VoltDB\", help=\"Repository location. For example: /home/github-mirror\")\nparser.add_argument('--nomac', action='store_true', help=\"Don't build Mac OSX\")\nparser.add_argument('--nocommunity', action='store_true', help=\"Don't build community\")\nargs = parser.parse_args()\n\nproTreeish = args.pro_sha\nvoltdbTreeish = args.voltdb_sha\nrbmqExportTreeish = args.rabbitmq_sha\n\nprint args\n\nbuild_community = not args.nocommunity\nbuild_mac = not args.nomac\n\n#If anything is missing we're going to dump this in oneoffs dir.\nbuild_all = build_community and build_mac\nif voltdbTreeish != proTreeish or not build_all:\n    oneOff = True\nelse:\n    oneOff  = False\n\nrmNativeLibs()\n\ntry:\n    build_args = os.environ['VOLTDB_BUILD_ARGS']\nexcept:\n    build_args=\"\"\n\nprint \"Building with pro: %s and voltdb: %s\" % (proTreeish, voltdbTreeish)\n\nbuild_errors=False\n\nversionCentos = \"unknown\"\nversionMac = \"unknown\"\nreleaseDir = \"unknown\"\n\n# get ssh config [key_filename, hostname]\nCentosSSHInfo = getSSHInfoForHost(\"volt15a\")\nMacSSHInfo = getSSHInfoForHost(\"voltmini\")\nUbuntuSSHInfo = getSSHInfoForHost(\"volt12d\")\n\n# build community kit on the mini so that .so can be picked up for unified kit\nif build_mac or build_community:\n    try:\n        with settings(user=username,host_string=MacSSHInfo[1],disable_known_hosts=True,key_filename=MacSSHInfo[0]):\n            versionMac = checkoutCode(voltdbTreeish, proTreeish, rbmqExportTreeish, args.gitloc)\n            buildCommunity()\n    except Exception as e:\n        print traceback.format_exc()\n        print \"Could not build MAC kit. Exception: \" + str(e) + \", Type: \" + str(type(e))\n        build_errors=True\n\n# build kits on 15f\ntry:\n    with settings(user=username,host_string=CentosSSHInfo[1],disable_known_hosts=True,key_filename=CentosSSHInfo[0]):\n        versionCentos = checkoutCode(voltdbTreeish, proTreeish, rbmqExportTreeish, args.gitloc)\n        if build_mac:\n            assert versionCentos == versionMac\n\n        if oneOff:\n            releaseDir = \"%s/releases/one-offs/%s-%s-%s\" % \\\n                (os.getenv('HOME'), versionCentos, voltdbTreeish, proTreeish)\n        else:\n            releaseDir = os.getenv('HOME') + \"/releases/\" + voltdbTreeish\n        makeReleaseDir(releaseDir)\n        print \"VERSION: \" + versionCentos\n        if build_community:\n            buildCommunity()\n            buildRabbitMQExport(versionCentos, \"community\")\n            copyCommunityFilesToReleaseDir(releaseDir, versionCentos, \"LINUX\")\n        buildEnterprise()\n        buildRabbitMQExport(versionCentos, \"ent\")\n        makeSHA256SUM(versionCentos,\"ent\")\n        copyFilesToReleaseDir(releaseDir, versionCentos, \"ent\")\n        packagePro(versionCentos)\n        makeSHA256SUM(versionCentos,\"pro\")\n        copyFilesToReleaseDir(releaseDir, versionCentos, \"pro\")\n        makeTrialLicense()\n        copyTrialLicenseToReleaseDir(releaseDir)\n        makeMavenJars()\n        copyMavenJarsToReleaseDir(releaseDir, versionCentos)\n\nexcept Exception as e:\n    print traceback.format_exc()\n    print \"Could not build LINUX kit. Exception: \" + str(e) + \", Type: \" + str(type(e))\n    build_errors=True\n\nrmNativeLibs()      # cleanup imported native libs so not picked up unexpectedly by other builds\n\nexit (build_errors)\n#archiveDir = os.path.join(os.getenv('HOME'), \"releases\", \"archive\", voltdbTreeish, versionCentos)\n#backupReleaseDir(releaseDir, archiveDir, versionCentos)", "instruction_en": "This code builds VoltDB kits for Linux and Mac OSX. To build the kits, run the script with the following arguments: the commit, tag, or branch for the VoltDB repository, the commit, tag, or branch for the Pro repository, and the commit, tag, or branch for the RabbitMQ repository. Optionally, use the `--nomac` flag to skip building the Mac OSX kit and the `--nocommunity` flag to skip building the community kit. The script will create a release directory and copy the necessary files to it, including the trial license, SHA256 checksum, and Maven jars. Finally, the script will create a candidate symlink and backup the release directory.", "original_line_number": 6929, "instruction_length": 638}
{"instruction": "Kod ini mentakrifkan beberapa bentuk untuk Pengesahan Pengguna dan Pengurusan Profil dalam aplikasi Django. Untuk menggunakan borang ini, importnya dari modul yang sepadan dan instantiate mereka seperti yang diperlukan. Borang ini termasuk AuthenticationForm, kata laluanChangeForm, passwordReSetform, setPasswordForm, UserCreationForm, UserUpdateForm, dan ProfileUpdateForm. Kod ini juga termasuk beberapa pandangan untuk pendaftaran pengguna, pengesahan, pengurusan profil, dan penghapusan akaun. Untuk menggunakan pandangan ini, masukkannya dalam konfigurasi URL aplikasi Django.", "input": "", "output": "####################################################################################################\n#\n# GroupedPurchaseOrder - A Django Application.\n# Copyright (C) 2014 Fabrice Salvaire\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU Affero General Public License as\n#  published by the Free Software Foundation, either version 3 of the\n#  License, or (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU Affero General Public License for more details.\n#\n#  You should have received a copy of the GNU Affero General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n####################################################################################################\n\n####################################################################################################\n\n# from django.forms.widgets import HiddenInput\nfrom django.contrib import messages\nfrom django.contrib.auth import forms\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.forms import ModelForm\nfrom django.http import HttpResponseRedirect\nfrom django.shortcuts import get_object_or_404, render\nfrom django.utils import translation\nfrom django.utils.translation import ugettext as _\n\n####################################################################################################\n\nfrom GroupedPurchaseOrder.models import Profile\nfrom GroupedPurchaseOrder.utils import send_localized_mail\n\n####################################################################################################\n\nclass AuthenticationForm(forms.AuthenticationForm):\n\n    \"\"\"Override the default AuthenticationForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(AuthenticationForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['password'].widget.attrs['class'] = 'form-control'\n        self.fields['password'].widget.attrs['placeholder'] = _('Password')\n        self.fields['username'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['username'].widget.attrs['class'] = 'form-control'\n        self.fields['username'].widget.attrs['placeholder'] = _('Username')\n\n####################################################################################################\n\nclass PasswordChangeForm(forms.PasswordChangeForm):\n\n    \"\"\"Override the default PasswordChangeForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\n        super(PasswordChangeForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['new_password1'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password1'].widget.attrs['placeholder'] = _('New password')\n        self.fields['new_password2'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password2'].widget.attrs['placeholder'] = _('New password')\n        self.fields['old_password'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['old_password'].widget.attrs['class'] = 'form-control'\n        self.fields['old_password'].widget.attrs['placeholder'] = _('Old password')\n\n####################################################################################################\n\nclass PasswordResetForm(forms.PasswordResetForm):\n\n    \"\"\"Override the default PasswordResetForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(PasswordResetForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['email'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['email'].widget.attrs['class'] = 'form-control'\n        self.fields['email'].widget.attrs['placeholder'] = _('email')\n\n####################################################################################################\n\nclass SetPasswordForm(forms.SetPasswordForm):\n\n    \"\"\"Override the default SetPasswordForm in order to add HTML5 attributes.  This is the only change\n    done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(SetPasswordForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['new_password1'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['new_password1'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password1'].widget.attrs['placeholder'] = _('New password')\n        self.fields['new_password2'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password2'].widget.attrs['placeholder'] = _('New password')\n\n####################################################################################################\n\nclass UserCreationForm(forms.UserCreationForm):\n\n    \"\"\"Override the default UserCreationForm in order to add HTML5 attributes.\n\n    \"\"\"\n\n    ##############################################\n\n    class Meta:\n        model = User\n        fields = ('username', 'email', 'password1', 'password2', 'first_name', 'last_name')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(UserCreationForm, self).__init__(*args, **kwargs)\n\n        # email, first_name and last_name are required\n        self.fields['email'].required = True\n        self.fields['first_name'].required = True\n\n        # Add HTML5 attributes\n        self.fields['email'].widget.attrs['class'] = 'form-control'\n        self.fields['first_name'].widget.attrs['class'] = 'form-control'\n        self.fields['last_name'].widget.attrs['class'] = 'form-control'\n        self.fields['password1'].widget.attrs['class'] = 'form-control'\n        self.fields['password1'].widget.attrs['placeholder'] = _('Password')\n        self.fields['password2'].widget.attrs['class'] = 'form-control'\n        self.fields['password2'].widget.attrs['placeholder'] = _('Password')\n        self.fields['username'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['username'].widget.attrs['class'] = 'form-control'\n        self.fields['username'].widget.attrs['placeholder'] = _('Username')\n\n    ##############################################\n\n    def save(self, commit=True):\n\n        \"\"\"Create the new User and the associated Profile The User is not activated until the\n        register_confirm url has been visited\n\n        \"\"\"\n\n        if not commit:\n            raise NotImplementedError('Cannot create Profile and User without commit')\n        user = super(UserCreationForm, self).save(commit=False)\n        user.is_active = False\n        user.save()\n        profile = Profile(user=user)\n        profile.save()\n        return user\n\n####################################################################################################\n\nclass UserUpdateForm(ModelForm):\n\n    class Meta:\n        model = User\n        fields = ('first_name', 'last_name')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(UserUpdateForm, self).__init__(*args, **kwargs)\n\n        # first_name and last_name are required\n        self.fields['first_name'].required = True\n\n        self.fields['first_name'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['first_name'].widget.attrs['class'] = 'form-control'\n        self.fields['last_name'].widget.attrs['class'] = 'form-control'\n\n####################################################################################################\n\nclass ProfileUpdateForm(ModelForm):\n\n    class Meta:\n        model = Profile\n        fields = ('phone_number', 'language', 'timezone')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(ProfileUpdateForm, self).__init__(*args, **kwargs)\n\n        self.fields['language'].required = True\n        self.fields['timezone'].required = True\n\n        self.fields['language'].widget.attrs['class'] = 'form-control'\n        self.fields['phone_number'].widget.attrs['class'] = 'form-control'\n        self.fields['timezone'].widget.attrs['class'] = 'form-control'\n\n####################################################################################################\n\ndef register(request):\n\n    if request.method == 'POST':\n        user_form = UserCreationForm(request.POST)\n        if user_form.is_valid():\n            new_user = user_form.save()\n            send_localized_mail(new_user, _('Subscription to G.P.O.'),\n                                'GroupedPurchaseOrder/account/register_email.html',\n                                {'URL': request.build_absolute_uri(reverse('accounts.register.confirm',\n                                                                           args=[new_user.pk,\n                                                                                 new_user.profile.hash_id])),\n                                 'fullname': new_user.get_full_name()})\n            return render(request, 'GroupedPurchaseOrder/account/register_end.html')\n        else:\n            messages.error(request, _(\"Some information are missing or mistyped\"))\n    else:\n        user_form = UserCreationForm()\n\n    return render(request, 'GroupedPurchaseOrder/account/register.html', {'user_form': user_form})\n\n####################################################################################################\n\ndef register_confirm(request, user_id, user_hash):\n\n    \"\"\"Check that the User and the Hash are correct before activating the User\n\n    \"\"\"\n\n    user = get_object_or_404(User, pk=user_id, profile__hash_id=user_hash)\n    user.is_active = True\n    user.save()\n\n    return render(request, 'GroupedPurchaseOrder/account/confirm.html', {'user': user})\n\n####################################################################################################\n\n@login_required\ndef profile(request):\n\n    # Force the user to provide language and timezone\n    if not request.user.profile.language or request.user.profile.timezone == 'UTC':\n        messages.error(request, _(\"You should update your timezone. Without it G.P.O. will not work as expected.\"))\n        return HttpResponseRedirect(reverse('accounts.profile.update'))\n\n    return render(request, 'GroupedPurchaseOrder/account/profile.html')\n\n####################################################################################################\n\n@login_required\ndef update(request):\n\n    profile = get_object_or_404(Profile, user__pk=request.user.pk)\n\n    if request.method == 'POST':\n        user_form = UserUpdateForm(request.POST, instance=request.user)\n        profile_form = ProfileUpdateForm(request.POST, instance=profile)\n        if user_form.is_valid() and profile_form.is_valid():\n            user_form.save()\n            profile = profile_form.save()\n            # Update the language code and activate it for the message\n            if profile.language:\n                request.session['django_language'] = profile.language\n                translation.activate(profile.language)\n            # Update the timezone if needed\n            if profile.timezone:\n                request.session['django_timezone'] = profile.timezone\n            # Print the message\n            messages.success(request, _(\"Personnal information updated\"))\n            return HttpResponseRedirect(reverse('accounts.profile'))\n    else:\n        user_form = UserUpdateForm(instance=request.user)\n        profile_form = ProfileUpdateForm(instance=profile)\n\n        return render(request, 'GroupedPurchaseOrder/account/update.html',\n                      {'user_form': user_form, 'profile_form': profile_form})\n\n####################################################################################################\n\n@login_required\ndef password_change_done(request):\n\n    messages.success(request, _('Password changed successfully'))\n    return HttpResponseRedirect(reverse('accounts.profile'))\n\n####################################################################################################\n\ndef password_reset_done(request):\n\n    return render(request, 'GroupedPurchaseOrder/account/password_reset_done.html')\n\n####################################################################################################\n\n@login_required\ndef delete(request):\n\n    request.user.delete()\n    return HttpResponseRedirect(reverse('index'))\n\n####################################################################################################\n# \n# End\n# \n####################################################################################################", "instruction_en": "This code defines several forms for user authentication and profile management in a Django application. To use these forms, import them from the corresponding modules and instantiate them as needed. The forms include AuthenticationForm, PasswordChangeForm, PasswordResetForm, SetPasswordForm, UserCreationForm, UserUpdateForm, and ProfileUpdateForm. The code also includes several views for user registration, confirmation, profile management, and account deletion. To use these views, include them in the URL configuration of the Django application.", "original_line_number": 6955, "instruction_length": 582}
{"instruction": "Untuk menggunakan kod ini, jalankan fungsi `utama ()`. Kod ini menyediakan antara muka baris arahan untuk membina perisian menggunakan komponen yang boleh digunakan semula. Subcommands yang ada adalah `carian`,` init`, `pemasangan`,` binaan`, `versi`,` link`, `link-target`,` update`, `sasaran`,` debug`, `test`,` start`, `publish`,` tidak `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` `` ` `Uninstall`,` keluarkan`, `pemilik`,` lesen`, `bersih`,` config`, dan `shrinkwrap`. Gunakan `yotta <subcommand> --elp` untuk bantuan lebih terperinci pada setiap subkommandan. Pilihan boleh diserahkan kepada alat membina asas dengan lulus mereka selepas `--`.", "input": "", "output": "# Copyright 2014-2015 ARM Limited\n#\n# Licensed under the Apache License, Version 2.0\n# See LICENSE file for details.\n\nfrom yotta.lib import lazyregex #pylint: disable=unused-import\nfrom yotta.lib import errors #pylint: disable=unused-import\n\n# NOTE: argcomplete must be first!\n# argcomplete, pip install argcomplete, tab-completion for argparse, Apache-2\nimport argcomplete\n\n# standard library modules, , ,\nimport argparse\nimport sys\nimport os\n\n# globalconf, share global arguments between modules, internal\nimport yotta.lib.globalconf as globalconf\n\n# hook to support coverage information when yotta runs itself during tests:\nif 'COVERAGE_PROCESS_START' in os.environ:\n    import coverage\n    coverage.process_startup()\n\n# set __version__ using the same file that's read by setup.py when installing:\nwith open(os.path.join(os.path.dirname(__file__), 'version.txt')) as _version_f:\n    __version__ = _version_f.read().strip()\n\ndef splitList(l, at_value):\n    r = [[]]\n    for x in l:\n        if x == at_value:\n            r.append(list())\n        else:\n            r[-1].append(x)\n    return r\n\ndef _handleUnhandledReqestExceptions(fn):\n    import functools\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):\n        # requests, apache2\n        import requests\n        try:\n            return fn(*args, **kwargs)\n        except requests.exceptions.RequestException as e:\n            import logging\n            if e.request is not None:\n                logging.critical('%s %s failed with status %s', e.request.method, e.request.url, e.response.status_code)\n                sys.exit(1)\n            else:\n                raise\n    return wrapped\n\ndef _exitSilentlyOnUnhandledPipeError(fn):\n    import functools\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):\n        try:\n            return fn(*args, **kwargs)\n        except IOError as e:\n            import errno\n            if e.errno == errno.EPIPE:\n                # unhandled pipe error -> exit silently, but with an error code\n                sys.exit(1)\n            else:\n                raise\n    return wrapped\n\n@_exitSilentlyOnUnhandledPipeError\n@_handleUnhandledReqestExceptions\ndef main():\n    # standard library modules, , ,\n    import logging\n    from functools import reduce\n\n    # logging setup, , setup the logging system, internal\n    from yotta.lib import logging_setup\n    # options, , common argument parser options, internal\n    import yotta.options as options\n\n    logging_setup.init(level=logging.INFO, enable_subsystems=None, plain=False)\n\n    # we override many argparse things to make options more re-usable across\n    # subcommands, and allow lazy loading of subcommand modules:\n    parser = options.parser.ArgumentParser(\n        formatter_class=argparse.RawTextHelpFormatter,\n        description='Build software using re-usable components.\\n'+\n        'For more detailed help on each subcommand, run: yotta <subcommand> --help'\n    )\n    subparser = parser.add_subparsers(dest='subcommand_name', metavar='<subcommand>')\n\n    parser.add_argument('--version', action='version', version=__version__,\n        help='display the version'\n    )\n\n    # add re-usable top-level options which subcommands may also accept\n    options.verbosity.addTo(parser)\n    options.debug.addTo(parser)\n    options.plain.addTo(parser)\n    options.noninteractive.addTo(parser)\n    options.registry.addTo(parser)\n    options.target.addTo(parser)\n    options.config.addTo(parser)\n\n    def addParser(name, module_name, description, help=None):\n        if help is None:\n            help = description\n        def onParserAdded(parser):\n            import importlib\n            module = importlib.import_module('.' + module_name, 'yotta')\n            module.addOptions(parser)\n            parser.set_defaults(command=module.execCommand)\n        subparser.add_parser_async(\n            name, description=description, help=help,\n            formatter_class=argparse.RawTextHelpFormatter,\n            callback=onParserAdded\n        )\n\n    addParser('search', 'search',\n        'Search for open-source modules and targets that have been published '+\n        'to the yotta registry (with yotta publish). See help for `yotta '+\n        'install` for installing modules, and for `yotta target` for '+\n        'switching targets.',\n        'Search for published modules and targets'\n    )\n    addParser('init', 'init', 'Create a new module.')\n    addParser('install', 'install',\n        'Add a specific module as a dependency, and download it, or install all '+\n        'dependencies for the current module. Use yotta install '+\n        'modulename@version to install a specific version.'\n    )\n    addParser('build', 'build',\n        'Build the current module. Options can be passed to the underlying '+\n        'build tool by passing them after --, e.g. to do a verbose build '+\n        'which will display each command as it is run, use:\\n'+\n        '  yotta build -- -v\\n\\n'+\n        'The programs or libraries to build can be specified (by default '+\n        'only the libraries needed by the current module and the current '+\n        \"module's own tests are built). For example, to build the tests of \"+\n        'all dependencies, run:\\n  yotta build all_tests\\n\\n',\n        'Build the current module.'\n    )\n    addParser('version', 'version', 'Bump the module version, or (with no arguments) display the current version.')\n    addParser('link', 'link',\n        'Symlink a module to be used into another module.\\n\\n'+\n        'Use: \"yotta link\" in a module to link it globally, then use \"yotta '+\n        'link <modulename>\" to link it into the module where you want to use '+\n        'it.\\n\\n'+\n        '\"yotta link ../path/to/module\" is also supported, which will create '+\n        'the global link and a link into the current module in a single step.',\n        'Symlink a module'\n    )\n    addParser('link-target', 'link_target',\n        'Symlink a target to be used into another module.\\n\\n'+\n        'Use: \"yotta link\" in a target to link it globally, then use \"yotta '+\n        'link-target <targetname>\" to link it into the module where you want to use '+\n        'it.\\n\\n'+\n        '\"yotta link ../path/to/target\" is also supported, which will create '+\n        'the global link and a link into the current module in a single step.',\n        'Symlink a target'\n    )\n    addParser('update', 'update', 'Update dependencies for the current module, or a specific module.')\n    addParser('target', 'target', 'Set or display the target device.')\n    addParser('debug', 'debug', 'Attach a debugger to the current target.  Requires target support.')\n    addParser('test', 'test_subcommand',\n        'Run the tests for the current module on the current target. A build '+\n        'will be run first, and options to the build subcommand are also '+\n        'accepted by test.\\nThis subcommand requires the target to provide a '+\n        '\"test\" script that will be used to run each test. Modules may also '+\n        'define a \"testReporter\" script, which will be piped the output from '+\n        'each test, and may produce a summary.',\n        'Run the tests for the current module on the current target. Requires target support for cross-compiling targets.'\n    )\n    addParser('start', 'start',\n        'Launch the compiled program (available for executable modules only). Requires target support for cross-compiling targets.'\n    )\n    addParser('publish', 'publish', 'Publish a module or target to the public registry.')\n    addParser('unpublish', 'unpublish', 'Un-publish a recently published module or target.')\n    addParser('login', 'login', 'Authorize for access to private github repositories and publishing to the yotta registry.')\n    addParser('logout', 'logout', 'Remove saved authorization token for the current user.')\n    addParser('whoami', 'whoami', 'Display who the currently logged in user is (if any).')\n    addParser('list', 'list', 'List the dependencies of the current module, or the inherited targets of the current target.')\n    addParser('outdated', 'outdated', 'Display information about dependencies which have newer versions available.')\n    addParser('uninstall', 'uninstall', 'Remove a specific dependency of the current module, both from module.json and from disk.')\n    addParser('remove', 'remove',\n        'Remove the downloaded version of a dependency module or target, or '+\n        'un-link a linked module or target (see yotta link --help for details '+\n        'of linking). This command does not modify your module.json file.',\n        'Remove or unlink a dependency without removing it from module.json.'\n    )\n    addParser('owners', 'owners', 'Add/remove/display the owners of a module or target.')\n    addParser('licenses', 'licenses', 'List the licenses of the current module and its dependencies.')\n    addParser('clean', 'clean', 'Remove files created by yotta and the build.')\n    addParser('config', 'config', 'Display the target configuration info.')\n    addParser('shrinkwrap', 'shrinkwrap', 'Create a yotta-shrinkwrap.json file to freeze dependency versions.')\n\n    # short synonyms, subparser.choices is a dictionary, so use update() to\n    # merge in the keys from another dictionary\n    short_commands = {\n                'up':subparser.choices['update'],\n                'in':subparser.choices['install'],\n                'ln':subparser.choices['link'],\n                 'v':subparser.choices['version'],\n                'ls':subparser.choices['list'],\n                'rm':subparser.choices['remove'],\n            'unlink':subparser.choices['remove'],\n     'unlink-target':subparser.choices['remove'],\n             'owner':subparser.choices['owners'],\n              'lics':subparser.choices['licenses'],\n               'who':subparser.choices['whoami'],\n               'run':subparser.choices['start']\n    }\n    subparser.choices.update(short_commands)\n\n    # split the args into those before and after any '--'\n    # argument - subcommands get raw access to arguments following '--', and\n    # may pass them on to (for example) the build tool being used\n    split_args = splitList(sys.argv, '--')\n    following_args = reduce(lambda x,y: x + ['--'] + y, split_args[1:], [])[1:]\n\n    # complete all the things :)\n    argcomplete.autocomplete(\n         parser,\n        exclude = list(short_commands.keys()) + ['-d', '--debug', '-v', '--verbose']\n    )\n\n    # when args are passed directly we need to strip off the program name\n    # (hence [:1])\n    args = parser.parse_args(split_args[0][1:])\n\n    # set global arguments that are shared everywhere and never change\n    globalconf.set('interactive', args.interactive)\n    globalconf.set('plain', args.plain)\n\n    # finally, do stuff!\n    if 'command' not in args:\n        parser.print_usage()\n        sys.exit(0)\n\n    try:\n        status = args.command(args, following_args)\n    except KeyboardInterrupt:\n        logging.warning('interrupted')\n        status = -1\n\n    sys.exit(status or 0)", "instruction_en": "To use this code, run the `main()` function. The code provides a command-line interface for building software using re-usable components. The available subcommands are `search`, `init`, `install`, `build`, `version`, `link`, `link-target`, `update`, `target`, `debug`, `test`, `start`, `publish`, `unpublish`, `login`, `logout`, `whoami`, `list`, `outdated`, `uninstall`, `remove`, `owners`, `licenses`, `clean`, `config`, and `shrinkwrap`. Use `yotta <subcommand> --help` for more detailed help on each subcommand. Options can be passed to the underlying build tool by passing them after `--`.", "original_line_number": 6995, "instruction_length": 736}
{"instruction": "Untuk menggunakan kod yang diberikan, import perpustakaan yang diperlukan termasuk psycopg2, io, os, datetime, masa, pembalakan, tornado, dan urlparse. Kemudian, buat kelas yang dipanggil ListenOseConfhandler yang mewarisi dari kelas RequestHandler. Di dalam kelas, tentukan kaedah inisialisasi yang mengambil pangkalan data dan IPS sebagai parameter dan menetapkannya sebagai pembolehubah contoh. Tentukan kaedah GET yang mengambil data dari pangkalan data dan menjadikan halaman HTML menggunakan data yang diambil. Buat dua lagi kelas yang dipanggil InputeNoSeConFighandler dan Actionenoseconfighandler yang mewarisi dari kelas RequestHandler. Kelas -kelas ini juga mempunyai kaedah inisialisasi yang mengambil pangkalan data dan IPS sebagai parameter dan menetapkannya sebagai pembolehubah contoh. Kelas InputeNoSeConFighandler mempunyai kaedah GET yang menjadikan halaman HTML. Kelas ActionEnoseConFighandler mempunyai kaedah pos yang mengambil data dari bentuk dan memasukkannya ke dalam pangkalan data.", "input": "", "output": "## database\nimport psycopg2\n\n## system libraries\nimport io, os\nimport datetime, time\nimport logging\n\n## web libraries\nimport tornado\nimport tornado.auth\nimport tornado.escape\nimport tornado.gen\nimport tornado.httpserver\nimport urlparse\nimport threading\nimport functools\nfrom tornado.ioloop import IOLoop\nfrom tornado.web import asynchronous, RequestHandler, Application\nfrom tornado.httpclient import AsyncHTTPClient\n\n## custom libraries\nimport faroreDB\n\nrootdir = os.path.dirname(__file__)\n\n\n\n\nclass listEnoseConfHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.db = database\n        self.IPs = IPs\n        return\n\n    def get(self):\n\n        if self.request.remote_ip[:11] in self.IPs :\n\n            miolo = '<div class=\"page-header\">' + \\\n                    '<table class=\"table table-striped\">' + \\\n                    '<thead><tr><th width=500px colspan=2>enose ID</th><th width=150px colspan=3>Location</th><th width=150px colspan=3>Date</th><th width=50px></th><th width=50px></th></tr></thead>'+ \\\n                    '<tbody>\\n'\n\n            # Retrieving data from inductions\n            db = self.db\n            listConfs = self.db.getEnoseConfs( )\n\n            for conf in listConfs:\n                miolo += \"<tr><td colspan=2>hal\" + str(conf[-1]) + \"k</td>\\n\"\n                miolo += \"<td colspan=3>\" + str(conf[-2]) + \"</td>\\n\"\n                miolo += \"<td colspan=5>\" + str(conf[1]) + \"</td>\"\n\n                miolo += \"</tr><tr>\"\n                for j in range(10):\n                    miolo += \"<td>\" + str(conf[2+j]) + \"</td>\"\n\n                miolo += \"</tr>\"\n\n            miolo += '</tbody></table></div>'\n            self.render(rootdir+'/pagess/index.html', title=\"Current list of ENoses\", miolo = miolo,\n                        top=file(rootdir+\"/pagess/top.html\").read(), bottom=file(rootdir+\"/pagess/bottom.html\").read())\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to list_inductions from outside IP list: ' + str(self.request.remote_ip) )\n\n        return\n\n\n\n\nclass inputEnoseConfigHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.IPs = IPs\n        return\n\n    def get(self):\n\n        if self.request.remote_ip[:-2] == self.IPs[0] or self.request.remote_ip[:7] == self.IPs[1]:\n            miolo = file(rootdir+'/pagess/input_enose_config.html').read()\n            self.render(rootdir+'/pagess/index.html', title=\"Farore's wind\", miolo = miolo,\n                        top=file(rootdir+\"/pagess/top.html\").read(), bottom=file(rootdir+\"/pagess/bottom.html\").read())\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to input_metadata from outside IP list: ' + str(self.request.remote_ip) )\n\n        return\n\n\n\n\n\n\nclass actionEnoseConfigHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.db = database\n        self.IPs = IPs\n        return\n\n    def post(self):\n\n        if self.request.remote_ip[:11] in self.IPs :\n            self.render(rootdir+'/pagess/metadata_action.html')\n\n            date = self.get_argument('date', '')\n\n\n            S = []\n            for j in range(1,11):\n                S.append( self.get_argument('S'+str(j), '') )\n\n            T = []\n            for j in range(1,9):\n                T.append( self.get_argument('T'+str(j), '') )\n\n            location = self.get_argument('location', '')\n            enose =  self.get_argument('enose', '')\n\n            if len(enose) > 1:\n                enose = enose[3]\n\n            self.db.insertEnoseConf(enose, date, S, T, location)\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to metadata_action from outside IP list: ' + str(self.request.remote_ip) )\n\n        return", "instruction_en": "To use the given code, import the necessary libraries including psycopg2, io, os, datetime, time, logging, tornado, and urlparse. Then, create a class called listEnoseConfHandler that inherits from the RequestHandler class. Inside the class, define the initialize method that takes in database and IPs as parameters and sets them as instance variables. Define the get method that retrieves data from the database and renders an HTML page using the retrieved data. Create two more classes called inputEnoseConfigHandler and actionEnoseConfigHandler that inherit from the RequestHandler class. These classes also have the initialize method that takes in database and IPs as parameters and sets them as instance variables. The inputEnoseConfigHandler class has a get method that renders an HTML page. The actionEnoseConfigHandler class has a post method that retrieves data from a form and inserts it into the database.", "original_line_number": 7020, "instruction_length": 1008}
{"instruction": "Untuk menjalankan simulasi MCMC menggunakan emcee, ikuti langkah -langkah ini:\n\n1. Import perpustakaan yang diperlukan: emcee, pearce.emulator, pearce.mocks, numpy, dan os.\n2. Tentukan laluan fail latihan dan parameter tetap.\n3. Buat objek emulator menggunakan kelas asalRecipe dari pearce.emulator.\n4. Tentukan fungsi log-kemungkinan negatif, NLL, yang mengambil vektor parameter dan mengembalikan kemungkinan log negatif emulator yang diberikan data yang diturunkan.\n5. Tentukan fungsi terdahulu, lnprior, yang mengambil vektor parameter dan mengembalikan logaritma semulajadi taburan kebarangkalian terdahulu.\n6. Tentukan fungsi log-posterior, LNPROB, yang mengambil vektor parameter dan sebarang hujah tambahan, dan mengembalikan logaritma semulajadi taburan kebarangkalian posterior.\n7. Tetapkan vektor parameter awal, p0, ke titik permulaan yang munasabah.\n8. Tetapkan bilangan pejalan kaki, nwalkers, dan bilangan langkah, NSTEPS, untuk simulasi MCMC.\n9. Tetapkan bilangan langkah terbakar, NBURN, hingga 0.\n10. Tetapkan bilangan teras untuk digunakan, ncores, untuk penyesuaian.\n11. Tentukan Direktori Simpan dan Nama Fail Rantai.\n12. Tulis nama parameter ke fail rantai.\n13. Inisialisasi objek ensemblesampler dari emcee.\n14. Jalankan simulasi MCMC menggunakan kaedah sampel () objek ensemblesampler, lulus dalam vektor kedudukan awal, POS0, dan bilangan lelaran, NSTEPS.", "input": "", "output": "import emcee as mc\nfrom pearce.emulator import OriginalRecipe, ExtraCrispy\nfrom pearce.mocks import cat_dict\nimport numpy as np\nfrom os import path\ntraining_file = '/home/users/swmclau2/scratch/PearceRedMagicXiCosmo.hdf5'\na = 1.0\nz = 1./a-1.0\n\nfixed_params = {'z':z, 'r':24.06822623}\nn_leaves, n_overlap = 10, 2\nem_method = 'gp'\nemu = OriginalRecipe(training_file, method = em_method, fixed_params=fixed_params, downsample_factor = 0.1)\n\n# TODO downsample sampling?\ndef nll(p):\n    emu._emulator.set_parameter_vector(p)\n\n    ll = emu._emulator.lnlikelihood(emu.downsample_y, quiet=True)\n\n    return -ll if np.isfinite(ll) else 1e25\n\ndef lnprior(theta):\n    return -np.inf if np.any(np.logical_or(theta < -15, theta > 15)) else 0\n\ndef lnprob(theta, *args):\n    lp = lnprior(theta)\n    if not np.isfinite(lp):\n        return -np.inf\n\n    return lp - nll(theta, *args)\n\n#p0 = emu._emulator.get_parameter_vector()\n\n#p0 = np.array([  0.       ,  10.6161248,   1.8339794,   7.342365 ,  10.6371797,\n     #    2.2441632,  13.8155106,  11.3512804,   3.1795786,   4.6846614,\n     #             1.       ,   5.0188608,   3.7658774,  -1.5383083])\n\np0 = np.array([-12.0550382,   0.1054246,   0.2661017,   5.6407612,   0.2408568,   1.1295944,\n   0.3643993,  11.5649985,   4.9071932,   4.7031938,   1.,         11.7621938,\n     10.6279446,   0.,         10.6161248,   1.8339794,   7.342365   10.6371797,\n        2.2441632,  13.8155106,  11.3512804   3.1795786,   4.6846614   1.,\n           5.0188608,   3.7658774,  -1.5383083])\n\nnwalkers = 100 \nnsteps = 2000\nnburn = 0\nnum_params = p0.shape[0]#len(emu.get_param_names())+1\npos0 = p0+np.random.randn(nwalkers, num_params)\nncores = 16 \n\n\nsavedir = '/home/users/swmclau2/scratch/'\nchain_fname = path.join(savedir, '%d_walkers_%d_steps_cosmo_hyperparams.npy'%(nwalkers, nsteps))\n\npnames = ['amp']\npnames.extend(emu.get_param_names())\nwith open(chain_fname, 'w') as f:\n    f.write('#' + '\\t'.join(pnames)+'\\t'+ '\\t'.join(pnames)+'\\tamp'+'\\n')\n\nsampler = mc.EnsembleSampler(nwalkers, num_params, lnprob, threads=ncores)\nfor result in sampler.sample(pos0, iterations = nsteps, storechain=False):\n\n    with open(chain_fname, 'a') as f:\n        np.savetxt(f, result[0])", "instruction_en": "To run an MCMC simulation using emcee, follow these steps:\n\n1. Import the necessary libraries: emcee, pearce.emulator, pearce.mocks, numpy, and os.\n2. Define the training file path and fixed parameters.\n3. Create an emulator object using the OriginalRecipe class from pearce.emulator.\n4. Define the negative log-likelihood function, nll, which takes in a parameter vector and returns the negative log-likelihood of the emulator given the downsampled data.\n5. Define the prior function, lnprior, which takes in a parameter vector and returns the natural logarithm of the prior probability distribution.\n6. Define the log-posterior function, lnprob, which takes in a parameter vector and any additional arguments, and returns the natural logarithm of the posterior probability distribution.\n7. Set the initial parameter vector, p0, to a reasonable starting point.\n8. Set the number of walkers, nwalkers, and the number of steps, nsteps, for the MCMC simulation.\n9. Set the number of burn-in steps, nburn, to 0.\n10. Set the number of cores to use, ncores, for parallelization.\n11. Define the save directory and chain file name.\n12. Write the parameter names to the chain file.\n13. Initialize the EnsembleSampler object from emcee.\n14. Run the MCMC simulation using the sample() method of the EnsembleSampler object, passing in the initial position vector, pos0, and the number of iterations, nsteps.", "original_line_number": 8053, "instruction_length": 1380}
{"instruction": "Buat dua fungsi, satu untuk membuat pelayan soket dan satu untuk membuat klien soket. Fungsi pelayan harus mengambil hos dan pelabuhan sebagai argumen dan mengembalikan objek soket. Fungsi pelanggan harus mengambil hos dan pelabuhan sebagai argumen dan mengembalikan objek soket. Di samping itu, buat fungsi yang dipanggil ke hadapan yang mengambil paket, alamat, dan senarai tupel yang mengandungi alamat dan pelabuhan. Jika paket bukan paket ralat, beralih melalui senarai tuple dan untuk setiap tuple, jika alamat tidak sama dengan alamat tuple semasa, buat klien soket dan hantar paket ke alamat dan port tuple semasa.", "input": "", "output": "import socket\n\n####### SOCKET\n\ndef create_socket_server(myHost, port):\n\ts = None\n\tfor res in socket.getaddrinfo(None, int(port), socket.AF_UNSPEC,socket.SOCK_STREAM, 0, socket.AI_PASSIVE):\n\t    af, socktype, proto, canonname, sa = res\n\t    try:\n\t    \ts = socket.socket(af, socktype, proto)\n\t    except socket.error as msg:\n\t    \ts = None\n\t    \tcontinue\n\t    try:\n\t    \ts.bind(sa)\n\t    \ts.listen(1000)\n\t    except socket.error as msg:\n\t    \ts.close()\n\t    \ts = None\n\t    \tcontinue\n\t    break\n\treturn s\n\ndef create_socket_client(myHost, port):\n\ts = None\n\tfor res in socket.getaddrinfo(myHost, int(port), socket.AF_UNSPEC, socket.SOCK_STREAM):\n\t    af, socktype, proto, canonname, sa = res\n\t    try:\n\t        s = socket.socket(af, socktype, proto)\n\t    except socket.error as msg:\n\t        s = None\n\t        continue\n\t    try:\n\t        s.connect(sa)\n\t    except socket.error as msg:\n\t        s.close()\n\t        s = None\n\t        continue\n\t    break\n\treturn s\n\ndef forward(pk, addr, l): \n\tif pk != bytes(const.ERROR_PKT, \"ascii\"):\n\t\tfor x in l:\n\t\t\tif addr != x[0]:\n\t\t\t\ts = func.create_socket_client(func.roll_the_dice(x[0]), x[1])\n\t\t\t\tif not(s is None):\n\t\t\t\t\ts.sendall(pk)\n\t\t\t\t\t#write_daemon_success(\"Daemon\", \"-\", \"Forward da \" + addr + \" a \" + x[0])\n\t\t\t\t\ts.close()", "instruction_en": "Create two functions, one to create a socket server and one to create a socket client. The server function should take in a host and port as arguments and return a socket object. The client function should take in a host and port as arguments and return a socket object. Additionally, create a function called forward that takes in a packet, an address, and a list of tuples containing addresses and ports. If the packet is not an error packet, iterate through the list of tuples and for each tuple, if the address is not equal to the current tuple's address, create a socket client and send the packet to the current tuple's address and port.", "original_line_number": 8088, "instruction_length": 622}
{"instruction": "Kod ini memeriksa jika fail berubah dalam git sepadan dengan spesifikasi format .clang. Untuk menggunakannya, jalankan fungsi `cli ()` dengan hujah -hujah berikut:\n-`-File-Extensions`: Senarai sambungan fail yang dipisahkan koma untuk memeriksa (lalai adalah` .cpp, .h, .cxx, .hxx, .hpp, .cc, .ipp`)\n-`--exclude`: senarai fail atau direktori untuk dikecualikan dari cek\n-`--clang-format-bin`: jalan ke binari format clang (lalai adalah` clang-format`)\n-`--git-bin`: jalan ke binari git (lalai adalah` git`)", "input": "", "output": "#!/usr/bin/env python\n\nimport argparse\nimport os\nimport sys\nimport subprocess\n\n\ndef check_file(filename, excludes, extensions):\n    \"\"\"\n    Check if a file should be included in our check\n    \"\"\"\n    name, ext = os.path.splitext(filename)\n\n    if len(ext) > 0 and ext in extensions:\n        if len(excludes) == 0:\n            return True\n\n        for exclude in excludes:\n            if exclude in filename:\n                return False\n\n        return True\n\n    return False\n\n\ndef check_directory(directory, excludes, extensions):\n    output = []\n\n    if len(excludes) > 0:\n        for exclude in excludes:\n            if exclude in directory:\n                directory_excluded = False\n                return output\n\n    for root, _, files in os.walk(directory):\n        for file in files:\n            filename = os.path.join(root, file)\n            if check_file(filename, excludes, extensions):\n                print(\"Will check file [{}]\".format(filename))\n                output.append(filename)\n    return output\n\ndef get_git_root(git_bin):\n    cmd = [git_bin, \"rev-parse\", \"--show-toplevel\"]\n    try:\n        return subprocess.check_output(cmd).strip()\n    except subprocess.CalledProcessError, e:\n        print(\"Error calling git [{}]\".format(e))\n        raise\n\ndef clean_git_filename(line):\n    \"\"\"\n    Takes a line from git status --porcelain and returns the filename\n    \"\"\"\n    file = None\n    git_status = line[:2]\n    # Not an exhaustive list of git status output but should\n    # be enough for this case\n    # check if this is a delete\n    if 'D' in git_status:\n        return None\n    # ignored file\n    if '!' in git_status:\n        return None\n    # Covers renamed files\n    if '->' in line:\n        file = line[3:].split('->')[-1].strip()\n    else:\n        file = line[3:].strip()\n\n    return file\n\n\ndef get_changed_files(git_bin, excludes, file_extensions):\n    \"\"\"\n    Run git status and return the list of changed files\n    \"\"\"\n    extensions = file_extensions.split(\",\")\n    # arguments coming from cmake will be *.xx. We want to remove the *\n    for i, extension in enumerate(extensions):\n        if extension[0] == '*':\n            extensions[i] = extension[1:]\n\n    git_root = get_git_root(git_bin)\n\n    cmd = [git_bin, \"status\", \"--porcelain\", \"--ignore-submodules\"]\n    print(\"git cmd = {}\".format(cmd))\n    output = []\n    returncode = 0\n    try:\n        cmd_output = subprocess.check_output(cmd)\n        for line in cmd_output.split('\\n'):\n            if len(line) > 0:\n                file = clean_git_filename(line)\n                if not file:\n                    continue\n                file = os.path.join(git_root, file)\n\n                if file[-1] == \"/\":\n                    directory_files = check_directory(\n                        file, excludes, file_extensions)\n                    output = output + directory_files\n                else:\n                    if check_file(file, excludes, file_extensions):\n                        print(\"Will check file [{}]\".format(file))\n                        output.append(file)\n\n    except subprocess.CalledProcessError, e:\n        print(\"Error calling git [{}]\".format(e))\n        returncode = e.returncode\n\n    return output, returncode\n\n\ndef run_clang_format(clang_format_bin, changed_files):\n    \"\"\"\n    Run clang format on a list of files \n    @return 0 if formatted correctly.\n    \"\"\"\n    if len(changed_files) == 0:\n        return 0\n    cmd = [clang_format_bin, \"-style=file\",\n           \"-output-replacements-xml\"] + changed_files\n    print(\"clang-format cmd = {}\".format(cmd))\n    try:\n        cmd_output = subprocess.check_output(cmd)\n        if \"replacement offset\" in cmd_output:\n            print(\"ERROR: Changed files don't match format\")\n            return 1\n    except subprocess.CalledProcessError, e:\n        print(\"Error calling clang-format [{}]\".format(e))\n        return e.returncode\n\n    return 0\n\n\ndef cli():\n    # global params\n    parser = argparse.ArgumentParser(prog='clang-format-check-changed',\n                                     description='Checks if files chagned in git match the .clang-format specification')\n    parser.add_argument(\"--file-extensions\", type=str,\n                        default=\".cpp,.h,.cxx,.hxx,.hpp,.cc,.ipp\",\n                        help=\"Comma separated list of file extensions to check\")\n    parser.add_argument('--exclude', action='append', default=[],\n                        help='Will not match the files / directories with these in the name')\n    parser.add_argument('--clang-format-bin', type=str, default=\"clang-format\",\n                        help=\"The clang format binary\")\n    parser.add_argument('--git-bin', type=str, default=\"git\",\n                        help=\"The git binary\")\n    args = parser.parse_args()\n\n    # Run gcovr to get the .gcda files form .gcno\n    changed_files, returncode = get_changed_files(\n        args.git_bin, args.exclude, args.file_extensions)\n    if returncode != 0:\n        return returncode\n\n    return run_clang_format(args.clang_format_bin, changed_files)\n\nif __name__ == '__main__':\n    sys.exit(cli())", "instruction_en": "This code checks if files changed in git match the .clang-format specification. To use it, run the `cli()` function with the following arguments:\n- `--file-extensions`: a comma-separated list of file extensions to check (default is `.cpp,.h,.cxx,.hxx,.hpp,.cc,.ipp`)\n- `--exclude`: a list of files or directories to exclude from the check\n- `--clang-format-bin`: the path to the clang-format binary (default is `clang-format`)\n- `--git-bin`: the path to the git binary (default is `git`)", "original_line_number": 8106, "instruction_length": 506}
{"instruction": "Untuk menjalankan pengesanan pejalan kaki pada fail video atau suapan kamera, import pakej yang diperlukan dengan menjalankan kod yang diberikan. Kemudian, gunakan ArgParse untuk menambah argumen untuk fail video dan saiz kawasan minimum. Jika tiada fail video ditentukan, suapan kamera akan digunakan. Jika tidak, fail video akan dibaca. Kod ini menggunakan pengesan deskriptor/orang untuk mengesan orang dalam imej dan menggunakan penindasan bukan maxima ke kotak terikat. Kod ini juga menggunakan penapis Kalman untuk mengesan pejalan kaki yang dikesan. Untuk menjalankan kod, laksanakan gelung sementara dan paparkan imej output. Kod ini akan memaparkan bilangan pejalan kaki yang dijumpai dan saiz giliran. Tekan kekunci ESC untuk keluar dari program. Pada akhir program, masa berlalu dan bingkai anggaran sesaat akan dipaparkan, bersama dengan objek yang dikesan.", "input": "", "output": "# import the necessary packages\nfrom __future__ import print_function\nfrom non_max_suppression import non_max_suppression\nfrom myqueue import myqueue\nfrom frames import frames\nfrom object import Object\nimport numpy as np\nimport argparse\nimport datetime\nimport imutils\nimport cv2\nimport time\n\nap = argparse.ArgumentParser()\nap.add_argument(\"-v\", \"--video\", help=\"path to the video file\")\nap.add_argument(\"-a\", \"--min-area\", type=int, default=500, help=\"minimum area size\")\nargs = vars(ap.parse_args())\n\nif args.get(\"video\", None) is None:\n    camera = cv2.VideoCapture(0)\n# otherwise, we are reading from a video file\nelse:\n    print(\"[INFO] starting video file thread...\")\n    camera = myqueue(args[\"video\"]).start()\n    time.sleep(1.0)\n\ni = 0\ncenterX = 0\ncenterY = 0\nobjList = []\nmeas = []\npred = []\nmp = np.array((2, 1), np.float32)  # measurement\ntp = np.zeros((2, 1), np.float32)  # tracked / prediction\n\nkalman = cv2.KalmanFilter(4, 2)\nkalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\nkalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\nkalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.03\n\n\ndef onPed(x, y):\n    global mp, meas\n    mp = np.array([[np.float32(x)], [np.float32(y)]])\n    meas.append((x, y))\n\n\ndef kalPredict(mp):\n    global tp, pred\n    kalman.correct(mp)\n    tp = kalman.predict()\n    pred.append((int(tp[0]), int(tp[1])))\n\n\ndef paint(tp, xA, yA, xB, yB):\n    global frame, pred\n    # cv2.circle(frame, ((tp[0]), (tp[1])), 3, (0, 0, 255), -1)\n    cv2.rectangle(frame, ((tp[0]) - ((xB - xA) / 2), (tp[1]) + (yB - yA) / 2),\n                  (((tp[0]) + ((xB - xA) / 2)), ((tp[1]) - (yB - yA) / 2)), (0, 0, 255), 2)\n\nfps = frames().start()\n# initialize the HOG descriptor/person detector\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n# loop over the image paths\nwhile camera.more():\n    frame = camera.read()\n    frame = imutils.resize(frame, width=600)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # start = datetime.datetime.now()\n    # detect people in the image\n    (rects, weights) = hog.detectMultiScale(frame, winStride=(8, 8),\n                                            padding=(32, 32), scale=1.05)\n    # print(\"[INFO] detection took: {}\".format(\n    #(datetime.datetime.now() - start).total_seconds()))\n    # apply non-maxima suppression to the bounding boxes using a\n    # fairly large overlap threshold to try to maintain overlapping\n    # boxes that are still people\n    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n\n    # draw the final bounding boxes\n    for (xA, yA, xB, yB) in pick:\n        i = i+1\n        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n        centerX = (xB + xA) / 2\n        centerY = (yB + yA) / 2\n        obj = Object(centerX, centerY, i)\n        objList.append(obj)\n        onPed(centerX, centerY)\n        kalPredict(mp)\n        paint(tp, xA, yA, xB, yB)\n\n    cv2.putText(frame, \"Queue Size: {}\".format(camera.Q.qsize()),\n                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    peds_found = \"Found \" + str(len(pick)) + \" Pedestrians\"\n    cv2.putText(frame, peds_found, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    # show the output images\n    cv2.imshow(\"HOG\", frame)\n    cv2.waitKey(1)\n    fps.update()\n    k = cv2.waitKey(27) & 0xff\n    if k == 27:\n        break\nfps.stop()\nfor objects in range(len(objList) - 1):\n    print(str(objList[objects]))\nprint(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\nprint(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\ncv2.destroyAllWindows()\ncamera.stop()", "instruction_en": "To run pedestrian detection on a video file or camera feed, import the necessary packages by running the given code. Then, use argparse to add arguments for the video file and minimum area size. If no video file is specified, the camera feed will be used. Otherwise, the video file will be read. The code uses a HOG descriptor/person detector to detect people in the image and applies non-maxima suppression to the bounding boxes. The code also uses a Kalman filter to track the detected pedestrians. To run the code, execute the while loop and display the output images. The code will display the number of pedestrians found and the queue size. Press the ESC key to exit the program. At the end of the program, the elapsed time and approximate frames per second will be displayed, along with the tracked objects.", "original_line_number": 8301, "instruction_length": 869}
{"instruction": "Kod ini mentakrifkan kelas yang dipanggil Googleapicaller yang mengendalikan memanggil API Google untuk mengira jadual perjalanan. Untuk menggunakan kelas ini, buat contohnya dengan meluluskan objek permintaan dengan atribut dari_x, dari_y, to_x, dan to_y. Kemudian, hubungi kaedah get_times () untuk mendapatkan masa perjalanan untuk mod pengangkutan yang berlainan, atau hubungi kaedah get_itineraries () untuk mendapatkan jadual perjalanan untuk mod pengangkutan yang berbeza. Mod pengangkutan yang ada memandu, berjalan, berbasikal, dan transit.", "input": "", "output": "import json\nimport re\nfrom bs4 import BeautifulSoup\nfrom model.Transport.Walk import Walk\nfrom model.Transport.PublicTransport import PublicTransport\nfrom model.Transport.Drive import Drive\nfrom model.Transport.Bicycle import Bicycle\nfrom model.Possibilities import Possibilities\nfrom webservice_caller.TransportAPICaller import TransportAPICaller\nfrom webservice_caller.call_url import call_url, APICallError\n\nclass GoogleAPICaller(TransportAPICaller):\n    '''\n    Class that handles calling google api to compute itiniraries\n    ''' \n    _url = 'https://maps.googleapis.com/maps/api/directions/json?'\n    _key = 'AIzaSyCqgwlzgUDYYF7xnePerJZaapgUWmyGYjc'\n\n    def __init__ (self, request):\n        '''\n        Create the different parameters that we will need for the API url\n        '''\n        self._origin = request.from_x, request.from_y\n        self._destination = request.to_x, request.to_y\n        self._modes = {'driving':Drive,'walking':Walk,'bicycling':Bicycle,'transit':PublicTransport}\n\n    @property\n    def modes(self):\n        return self._modes\n\n    def get_times(self):    \n        '''\n        Get the different times related to the travel modes and returns \n        a list of objects corresponding to each travel mode'\n        '''\n        times = {}\n    \n        for mode, mode_class in self._modes.items():\n            url_final = GoogleAPICaller._url + \"origin=\" + \",\".join(str (e) for e in self._origin) + \"&destination=\" + \",\".join(str(f) for f in self._destination) + \"&mode=\" + mode + \"&key=\" + GoogleAPICaller._key\n            response = call_url(url_final)\n            data = json.loads(response.content)  \n            try:\n                travel_time = data[\"routes\"][0][\"legs\"][0][\"duration\"][\"value\"]\n            except IndexError:\n                raise APICallError\n            except KeyError:\n                raise APICallError\n            times[mode] = travel_time\n        return times\n    \n\n    def get_itineraries(self):\n        '''\n        Get the different itineraries related to the travel modes\n        '''\n        itineraries = {}\n        for mode, mode_class in self._modes.items():\n            url_final = GoogleAPICaller._url + \"origin=\" + \",\".join(str (e) for e in self._origin) + \"&destination=\" + \",\".join(str(f) for f in self._destination) + \"&mode=\" + mode + \"&key=\" + GoogleAPICaller._key\n            response = call_url(url_final)\n            data = json.loads(response.content)  \n            try:\n                instruction = data[\"routes\"][0][\"legs\"][0][\"steps\"]\n            except IndexError:\n                raise APICallError\n            except KeyError:\n                raise APICallError    \n            itinerary = \"\"\n            for i in range(len(instruction)):\n                itinerary += instruction[i][\"html_instructions\"] + \", \"\n            clean_itinerary = BeautifulSoup(itinerary,\"html.parser\").text\n            itineraries[mode] = clean_itinerary  \n        return itineraries", "instruction_en": "This code defines a class called GoogleAPICaller that handles calling the Google API to compute itineraries. To use this class, create an instance of it by passing in a request object with from_x, from_y, to_x, and to_y attributes. Then, call the get_times() method to get the travel times for different modes of transportation, or call the get_itineraries() method to get the itineraries for different modes of transportation. The modes of transportation available are driving, walking, bicycling, and transit.", "original_line_number": 8401, "instruction_length": 549}
{"instruction": "Kod yang diberikan adalah modul python untuk membaca dan mengesahkan fail konfigurasi untuk perkhidmatan rangkaian OpenNSA. Untuk menggunakan modul ini, importnya ke dalam kod python anda dan panggil fungsi `ReadConfig` dengan laluan ke fail konfigurasi anda sebagai hujah. Fungsi ini akan mengembalikan objek `configparser.safeconfigparser yang mengandungi data konfigurasi. Untuk mengesahkan konfigurasi, hubungi fungsi `ReadVerifyConfig` dengan objek` SafeConFigParser` sebagai hujah. Fungsi ini akan mengembalikan kamus yang mengandungi data konfigurasi yang disahkan.", "input": "", "output": "\"\"\"\nConfiguration reader and defaults.\n\nAuthor: Henrik Thostrup Jensen <htj@nordu.net>\nCopyright: NORDUnet (2011)\n\"\"\"\n\nimport os\nimport configparser\n\nfrom opennsa import constants as cnt\n\n\n\n# defaults\nDEFAULT_CONFIG_FILE     = '/etc/opennsa.conf'\nDEFAULT_LOG_FILE        = '/var/log/opennsa.log'\nDEFAULT_TLS             = 'true'\nDEFAULT_TOPOLOGY_FILE   = '/usr/local/share/nsi/topology.owl'\nDEFAULT_TCP_PORT        = 9080\nDEFAULT_TLS_PORT        = 9443\nDEFAULT_VERIFY          = True\nDEFAULT_CERTIFICATE_DIR = '/etc/ssl/certs' # This will work on most mordern linux distros\n\n\n# config blocks and options\nBLOCK_SERVICE    = 'service'\nBLOCK_DUD        = 'dud'\nBLOCK_JUNIPER_EX = 'juniperex'\nBLOCK_JUNIPER_VPLS = 'junipervpls'\nBLOCK_FORCE10    = 'force10'\nBLOCK_BROCADE    = 'brocade'\nBLOCK_NCSVPN     = 'ncsvpn'\nBLOCK_PICA8OVS   = 'pica8ovs'\nBLOCK_JUNOSMX    = 'junosmx'\nBLOCK_JUNOSEX    = 'junosex'\nBLOCK_JUNOSSPACE = 'junosspace'\nBLOCK_OESS       = 'oess'\nBLOCK_CUSTOM_BACKEND = 'custombackend'\n\n# service block\nDOMAIN           = 'domain'      # mandatory\nNETWORK_NAME     = 'network'     # legacy, used to be mandatory\nLOG_FILE         = 'logfile'\nHOST             = 'host'\nPORT             = 'port'\nTLS              = 'tls'\nREST             = 'rest'\nNRM_MAP_FILE     = 'nrmmap'\nPEERS            = 'peers'\nPOLICY           = 'policy'\nPLUGIN           = 'plugin'\nSERVICE_ID_START = 'serviceid_start'\n\n# database\nDATABASE                = 'database'    # mandatory\nDATABASE_USER           = 'dbuser'      # mandatory\nDATABASE_PASSWORD       = 'dbpassword'  # can be none (os auth)\nDATABASE_HOST           = 'dbhost'      # can be none (local db)\n\n# tls\nKEY                     = 'key'         # mandatory, if tls is set\nCERTIFICATE             = 'certificate' # mandatory, if tls is set\nCERTIFICATE_DIR         = 'certdir'     # mandatory (but dir can be empty)\nVERIFY_CERT             = 'verify'\nALLOWED_HOSTS           = 'allowedhosts' # comma seperated list\n\n# generic stuff\n_SSH_HOST               = 'host'\n_SSH_PORT               = 'port'\n_SSH_HOST_FINGERPRINT   = 'fingerprint'\n_SSH_USER               = 'user'\n_SSH_PASSWORD           = 'password'\n_SSH_PUBLIC_KEY         = 'publickey'\n_SSH_PRIVATE_KEY        = 'privatekey'\n\nAS_NUMBER              = 'asnumber'\n\n# TODO: Don't do backend specifics for everything, it causes confusion, and doesn't really solve anything\n\n# juniper block - same for mx / ex backends\nJUNIPER_HOST                = _SSH_HOST\nJUNIPER_PORT                = _SSH_PORT\nJUNIPER_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nJUNIPER_USER                = _SSH_USER\nJUNIPER_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nJUNIPER_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\n\n# force10 block\nFORCE10_HOST            = _SSH_HOST\nFORCE10_PORT            = _SSH_PORT\nFORCE10_USER            = _SSH_USER\nFORCE10_PASSWORD        = _SSH_PASSWORD\nFORCE10_HOST_FINGERPRINT = _SSH_HOST_FINGERPRINT\nFORCE10_SSH_PUBLIC_KEY  = _SSH_PUBLIC_KEY\nFORCE10_SSH_PRIVATE_KEY = _SSH_PRIVATE_KEY\n\n# Brocade block\nBROCADE_HOST              = _SSH_HOST\nBROCADE_PORT              = _SSH_PORT\nBROCADE_HOST_FINGERPRINT  = _SSH_HOST_FINGERPRINT\nBROCADE_USER              = _SSH_USER\nBROCADE_SSH_PUBLIC_KEY    = _SSH_PUBLIC_KEY\nBROCADE_SSH_PRIVATE_KEY   = _SSH_PRIVATE_KEY\nBROCADE_ENABLE_PASSWORD   = 'enablepassword'\n\n# Pica8 OVS\nPICA8OVS_HOST                = _SSH_HOST\nPICA8OVS_PORT                = _SSH_PORT\nPICA8OVS_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nPICA8OVS_USER                = _SSH_USER\nPICA8OVS_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nPICA8OVS_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\nPICA8OVS_DB_IP               = 'dbip'\n\n\n# NCS VPN Backend\nNCS_SERVICES_URL        = 'url'\nNCS_USER                = 'user'\nNCS_PASSWORD            = 'password'\n\n# JUNOS block\nJUNOS_HOST                = _SSH_HOST\nJUNOS_PORT                = _SSH_PORT\nJUNOS_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nJUNOS_USER                = _SSH_USER\nJUNOS_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nJUNOS_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\nJUNOS_ROUTERS             = 'routers'\n\n#Junosspace backend\nSPACE_USER              = 'space_user'\nSPACE_PASSWORD          = 'space_password'\nSPACE_API_URL           = 'space_api_url'\nSPACE_ROUTERS           = 'routers'\nSPACE_CONFIGLET_ACTIVATE_LOCAL = 'configlet_activate_local'  \nSPACE_CONFIGLET_ACTIVATE_REMOTE = 'configlet_activate_remote'\nSPACE_CONFIGLET_DEACTIVATE_LOCAL = 'configlet_deactivate_local'\nSPACE_CONFIGLET_DEACTIVATE_REMOTE = 'configlet_deactivate_remote'\n\n# OESS\nOESS_URL                = 'url'\nOESS_USER               = 'username'\nOESS_PASSWORD           = 'password'\nOESS_WORKGROUP          = 'workgroup'\n\n\nclass ConfigurationError(Exception):\n    \"\"\"\n    Raised in case of invalid/inconsistent configuration.\n    \"\"\"\n\n\nclass Peer(object):\n\n    def __init__(self, url, cost):\n        self.url = url\n        self.cost = cost\n\n\n\ndef readConfig(filename):\n\n    cfg = configparser.SafeConfigParser()\n\n    cfg.add_section(BLOCK_SERVICE)\n    cfg.read( [ filename ] )\n\n    return cfg\n\n\n\ndef readVerifyConfig(cfg):\n    \"\"\"\n    Read a config and verify that things are correct. Will also fill in\n    default values where applicable.\n\n    This is supposed to be used during application creation (before service\n    start) to ensure that simple configuration errors do not pop up efter\n    daemonization.\n\n    Returns a \"verified\" config, which is a dictionary.\n    \"\"\"\n\n    vc = {}\n\n    # Check for deprecated / old invalid stuff\n\n    try:\n        cfg.get(BLOCK_SERVICE, NRM_MAP_FILE)\n        raise ConfigurationError('NRM Map file should be specified under backend')\n    except configparser.NoOptionError:\n        pass\n\n    # check / extract\n\n    try:\n        vc[DOMAIN] = cfg.get(BLOCK_SERVICE, DOMAIN)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No domain name specified in configuration file (mandatory, see docs/migration)')\n\n    try:\n        cfg.get(BLOCK_SERVICE, NETWORK_NAME)\n        raise ConfigurationError('Network name no longer used, use domain (see docs/migration)')\n    except configparser.NoOptionError:\n        pass\n\n    try:\n        vc[LOG_FILE] = cfg.get(BLOCK_SERVICE, LOG_FILE)\n    except configparser.NoOptionError:\n        vc[LOG_FILE] = DEFAULT_LOG_FILE\n\n    try:\n        nrm_map_file = cfg.get(BLOCK_SERVICE, NRM_MAP_FILE)\n        if not os.path.exists(nrm_map_file):\n            raise ConfigurationError('Specified NRM mapping file does not exist (%s)' % nrm_map_file)\n        vc[NRM_MAP_FILE] = nrm_map_file\n    except configparser.NoOptionError:\n        vc[NRM_MAP_FILE] = None\n\n    try:\n        vc[REST] = cfg.getboolean(BLOCK_SERVICE, REST)\n    except configparser.NoOptionError:\n        vc[REST] = False\n\n    try:\n        peers_raw = cfg.get(BLOCK_SERVICE, PEERS)\n        vc[PEERS] = [ Peer(purl.strip(), 1) for purl in  peers_raw.split('\\n') ]\n    except configparser.NoOptionError:\n        vc[PEERS] = None\n\n    try:\n        vc[HOST] = cfg.get(BLOCK_SERVICE, HOST)\n    except configparser.NoOptionError:\n        vc[HOST] = None\n\n    try:\n        vc[TLS] = cfg.getboolean(BLOCK_SERVICE, TLS)\n    except configparser.NoOptionError:\n        vc[TLS] = DEFAULT_TLS\n\n    try:\n        vc[PORT] = cfg.getint(BLOCK_SERVICE, PORT)\n    except configparser.NoOptionError:\n        vc[PORT] = DEFAULT_TLS_PORT if vc[TLS] else DEFAULT_TCP_PORT\n\n    try:\n        policies = cfg.get(BLOCK_SERVICE, POLICY).split(',')\n        for policy in policies:\n            if not policy in (cnt.REQUIRE_USER, cnt.REQUIRE_TRACE, cnt.AGGREGATOR, cnt.ALLOW_HAIRPIN):\n                raise ConfigurationError('Invalid policy: %s' % policy)\n        vc[POLICY] = policies\n    except configparser.NoOptionError:\n        vc[POLICY] = []\n\n    try:\n        vc[PLUGIN] = cfg.get(BLOCK_SERVICE, PLUGIN)\n    except configparser.NoOptionError:\n        vc[PLUGIN] = None\n\n    # database\n    try:\n        vc[DATABASE] = cfg.get(BLOCK_SERVICE, DATABASE)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No database specified in configuration file (mandatory)')\n\n    try:\n        vc[DATABASE_USER] = cfg.get(BLOCK_SERVICE, DATABASE_USER)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No database user specified in configuration file (mandatory)')\n\n    try:\n        vc[DATABASE_PASSWORD] = cfg.get(BLOCK_SERVICE, DATABASE_PASSWORD)\n    except configparser.NoOptionError:\n        vc[DATABASE_PASSWORD] = None\n\n    try:\n        vc[DATABASE_HOST] = cfg.get(BLOCK_SERVICE, DATABASE_HOST)\n    except configparser.NoOptionError:\n        vc[DATABASE_HOST] = None\n\n    try:\n        vc[SERVICE_ID_START] = cfg.get(BLOCK_SERVICE, SERVICE_ID_START)\n    except configparser.NoOptionError:\n        vc[SERVICE_ID_START] = None\n\n    # we always extract certdir and verify as we need that for performing https requests\n    try:\n        certdir = cfg.get(BLOCK_SERVICE, CERTIFICATE_DIR)\n        if not os.path.exists(certdir):\n            raise ConfigurationError('Specified certdir does not exist (%s)' % certdir)\n        vc[CERTIFICATE_DIR] = certdir\n    except configparser.NoOptionError:\n        vc[CERTIFICATE_DIR] = DEFAULT_CERTIFICATE_DIR\n    try:\n        vc[VERIFY_CERT] = cfg.getboolean(BLOCK_SERVICE, VERIFY_CERT)\n    except configparser.NoOptionError:\n        vc[VERIFY_CERT] = DEFAULT_VERIFY\n\n    # tls\n    if vc[TLS]:\n        try:\n            hostkey  = cfg.get(BLOCK_SERVICE, KEY)\n            hostcert = cfg.get(BLOCK_SERVICE, CERTIFICATE)\n\n            if not os.path.exists(hostkey):\n                raise ConfigurationError('Specified hostkey does not exist (%s)' % hostkey)\n            if not os.path.exists(hostcert):\n                raise ConfigurationError('Specified hostcert does not exist (%s)' % hostcert)\n\n            vc[KEY] = hostkey\n            vc[CERTIFICATE] = hostcert\n\n            try:\n                allowed_hosts_cfg = cfg.get(BLOCK_SERVICE, ALLOWED_HOSTS)\n                vc[ALLOWED_HOSTS] = allowed_hosts_cfg.split(',')\n            except:\n                pass\n\n        except configparser.NoOptionError as e:\n            # Not enough options for configuring tls context\n            raise ConfigurationError('Missing TLS option: %s' % str(e))\n\n\n    # backends\n    backends = {}\n\n    for section in cfg.sections():\n\n        if section == 'service':\n            continue\n\n        if ':' in section:\n            backend_type, name = section.split(':',2)\n        else:\n            backend_type = section\n            name = ''\n\n        if name in backends:\n            raise ConfigurationError('Can only have one backend named \"%s\"' % name)\n\n        if backend_type in (BLOCK_DUD, BLOCK_JUNIPER_EX, BLOCK_JUNIPER_VPLS, BLOCK_JUNOSMX, BLOCK_FORCE10, BLOCK_BROCADE,\n                            BLOCK_NCSVPN, BLOCK_PICA8OVS, BLOCK_OESS, BLOCK_JUNOSSPACE, BLOCK_JUNOSEX,\n                            BLOCK_CUSTOM_BACKEND, 'asyncfail'):\n            backend_conf = dict( cfg.items(section) )\n            backend_conf['_backend_type'] = backend_type\n            backends[name] = backend_conf\n\n    vc['backend'] = backends\n\n    return vc", "instruction_en": "The given code is a Python module for reading and verifying configuration files for the OpenNSA network service. To use this module, import it into your Python code and call the `readConfig` function with the path to your configuration file as an argument. The function will return a `configparser.SafeConfigParser` object containing the configuration data. To verify the configuration, call the `readVerifyConfig` function with the `SafeConfigParser` object as an argument. The function will return a dictionary containing the verified configuration data.", "original_line_number": 8518, "instruction_length": 572}
{"instruction": "Kod yang diberikan adalah pelaksanaan algoritma pokok keputusan. Untuk menggunakan algoritma, anda perlu menyediakan dataset dalam fail bernama 'Decision_Tree_Example.txt' dalam format yang dipisahkan tab. Untuk membina pokok keputusan, hubungi fungsi 'BuildTree' dan lulus dataset sebagai hujah. Untuk mengklasifikasikan pemerhatian, hubungi fungsi 'Missing_Value_Classify' dan lulus pemerhatian dan nod akar pokok keputusan sebagai argumen. Fungsi ini akan mengembalikan kamus dengan kebarangkalian kelas.", "input": "", "output": "'''\nCreated on Feb 21, 2013\n@author: Administrator\n'''\nfrom collections import defaultdict\nfrom math import log\n\ndef readDataFromFile(filename='decision_tree_example.txt'):\n    with open(filename, 'rt') as f:\n        data = []\n        for line in f:\n            data.append(line.strip().split('\\t'))\n        \n    return data\n\ndef uniquecounts(rows):\n    results = defaultdict(int)\n    for row in rows:\n        r = row[len(row)-1]\n        results[r] += 1\n        \n    return results\n\ndef gini_impurity(rows):\n    total = len(rows)\n    counts = uniquecounts(rows)\n    imp = 0\n    for k1 in counts:\n        p1 = float(counts[k1])/total\n        for k2 in counts:\n            if k1 == k2: continue\n            p2 = float(counts[k2])/total\n            imp += p1*p2\n            \n    return imp\n\ndef entropy(rows):\n    log2 = lambda x: log(x)/log(2)\n    results = uniquecounts(rows)\n    ent = 0.0\n    total = len(rows)\n    for r in results:\n        p = float(results[r])/total\n        ent -= p*log2(p)\n        \n    return ent\n\ndef divide_set(rows, col, value):\n    split_function = None\n    if isinstance(value, int) or isinstance(value, float):\n        split_function = lambda row: row[col] >= value\n    else:\n        split_function = lambda row: row[col] == value\n        \n    set1 = [row for row in rows if split_function(row)]\n    set2 = [row for row in rows if not split_function(row)]\n    return (set1, set2)\n\nclass treenode(object):\n    def __init__(self, col=-1, value=None, results=None, tb=None, fb=None):\n        self.col = col\n        self.value = value\n        self.results = results\n        self.fb = fb\n        self.tb = tb\n        \ndef buildtree(rows, score_function=entropy):\n    if len(rows) == 0: return treenode()\n    current_score = score_function(rows)\n    \n    best_gain = 0.0\n    best_criteria = None\n    best_sets = None\n    column_cnt = len(rows[0])-1 # excluding the last column\n    for col in range(0, column_cnt):\n        col_values = {}\n        for row in rows:\n            col_values[row[col]] = 1\n        for value in col_values:\n            (set1, set2) = divide_set(rows, col, value)\n            len1 = len(set1)\n            total = len(rows)\n            p = float(len1)/total\n            gain = current_score - p*score_function(set1) - (1-p)*score_function(set2)\n            if gain > best_gain and len(set1) > 0 and len(set2) > 0:\n                best_gain = gain\n                best_criteria = (col, value)\n                best_sets = (set1, set2)\n    if best_gain > 0:\n        trueBranch = buildtree(best_sets[0], score_function)\n        falseBranch = buildtree(best_sets[1], score_function)\n        return treenode(col=best_criteria[0], value=best_criteria[1], tb=trueBranch, fb=falseBranch)\n    else:\n        return treenode(results=uniquecounts(rows))\n    \ndef print_tree(node, indent=''):\n    if node.results != None:\n        print str(node.results)\n    else:\n        print str(node.col) + ':' + str(node.value) + '?'\n        print indent + 'T->',\n        print_tree(node.tb, indent+' ')\n        print indent + 'F->',\n        print_tree(node.fb, indent+' ')\n        \ndef getWidth(node):\n    if node.tb == None and node.fb == None:\n        return 1\n    return getWidth(node.fb) + getWidth(node.tb)\n        \ndef getHeight(node):\n    if node.tb == None and node.fb == None:\n        return 1\n    return getHeight(node.tb) + getHeight(node.fb) + 1\n\nfrom PIL import Image, ImageDraw\n\ndef drawNode(draw, node, x, y):\n    if node.results == None:\n        w1 = getWidth(node.fb)*100\n        w2 = getWidth(node.tb)*100\n        \n        left = x-(w1+w2)/2\n        right = x+(w1+w2)/2\n        \n        draw.text((x-20,y-10),str(node.col)+':'+str(node.value),(0,0,0))\n        \n        draw.line((x, y, left+w1/2, y+100), fill=(255,0,0))\n        draw.line((x, y, right-w2/2, y+100), fill=(255,0,0))\n        \n        drawNode(draw, node.fb, left+w1/2, y+100)\n        drawNode(draw, node.tb, right-w2/2, y+100)\n    else:\n        txt = ' \\n'.join(['%s:%d' %v for v in node.results.items()])\n        draw.text((x-20,y), txt, (0,0,0))\n\ndef drawTree(node, jpeg='tree.jpg'):\n    w = getWidth(node)*100\n    h = getHeight(node)*100+120\n    \n    img = Image.new('RGB', (w,h), (255,255,255))\n    draw = ImageDraw.Draw(img)\n    \n    drawNode(draw, node, w/2, 20)\n    img.save(jpeg, 'JPEG')\n    \ndef classify(observation, node):\n    if node.results != None:\n        return node.results\n    else:\n        v = observation[node.col]\n        branch = None\n        if isinstance(v,int) or isinstance(v,float):\n            if v >= node.value: branch = node.tb\n            else: branch = node.fb\n        else:\n            if v == node.value: branch = node.tb\n            else: branch = node.fb\n        return classify(observation, branch) \n    \ndef prune(node, mingain):\n    if node.tb.results == None:\n        prune(node.tb, mingain)\n    if node.fb.results == None:\n        prune(node.fb, mingain)\n        \n    if node.tb.results != None and node.fb.results != None:\n        tb, fb = [], []\n        for v, c in node.tb.results.items():\n            tb.extend([[v]]*c)\n        for v, c in node.fb.results.items():\n            fb.extend([[v]]*c)\n            \n        delta = entropy(tb+fb) - (entropy(tb) + entropy(fb))/2\n        \n        if delta < mingain:\n            node.tb, node.fb = None, None\n            node.results = uniquecounts(tb+fb)\n\ndef missing_value_classify(observation, node):\n    if node.results != None:\n        return node.results\n    else:\n        v = observation[node.col]\n        if v == None:\n            tr, fr = missing_value_classify(observation, node.tb), missing_value_classify(observation, node.fb)\n            tcount = sum(tr.values())\n            fcount = sum(fr.values())\n            tw = float(tcount)/(tcount+fcount)\n            fw = 1-tw\n            result = {}\n            for k,v in tr.items():\n                result[k] = v*tw\n            for k,v in fr.items():\n                if k not in result: result[k] = 0\n                result[k] += v*fw\n            return result\n        else:\n            if isinstance(v, int) or isinstance(v, float):\n                if v >= node.value: branch = node.tb\n                else: branch = node.fb\n            else:\n                if v == node.value: branch = node.tb\n                else: branch = node.fb\n            return missing_value_classify(observation, branch)\n        \n\nif __name__ == '__main__':\n    data = readDataFromFile()\n    root = buildtree(data)\n    print missing_value_classify(['google',None,'yes',None], root)", "instruction_en": "The given code is an implementation of a decision tree algorithm. To use the algorithm, you need to provide a dataset in a file named 'decision_tree_example.txt' in tab-separated format. To build a decision tree, call the function 'buildtree' and pass the dataset as the argument. To classify an observation, call the function 'missing_value_classify' and pass the observation and the root node of the decision tree as arguments. The function will return a dictionary with the class probabilities.", "original_line_number": 8815, "instruction_length": 507}
{"instruction": "Kod ini menyediakan fungsi untuk pengurusan senarai APK yang dipasang. Untuk menggunakannya, hubungi fungsi yang dikehendaki dengan hujah -hujah yang sesuai. Fungsi yang ada ialah:\n- create_pkgdict (): Mewujudkan kamus untuk mencari jalan cepat dari /data/system/packages.xml.\n- list_installed_pkgs (args): Menyenaraikan ahli -ahli kategori pakej yang diberikan.\n- list_installed_pkgs_nougat (args): Menggunakan arahan CMD Nougat untuk menanyakan perkhidmatan pakej (lebih cepat).\n- check_substratum (nougat): cek jika enjin substratum dipasang.\n- Exclude_overlays (AAPT, PKGDICT, PKGS): Tidak termasuk overlays substratum dari pakej untuk mengekstrak.\n- EXCLUDE_ARCUS_VARIANTS (PKGS): Tidak termasuk varian tema Arcus dari pakej untuk mengekstrak.\n- check_already_extracted (pkgpath, md5sums): cek jika APK telah diekstrak.", "input": "", "output": "\"\"\"Module with functions for management of installed APK lists.\"\"\"\n\nimport glob\nimport re\nimport subprocess\n\nimport apkutils    # needed for AndroidManifest.xml dump\nimport utils    # needed for sudo\n\n# Creates a APK/path dictionary to avoid the sluggish \"pm path\"\ndef create_pkgdict():\n    \"\"\"Creates a dict for fast path lookup from /data/system/packages.xml; returns dict.\"\"\"\n    \n    (out, err) = utils.sudo(\"cat /data/system/packages.xml\")\n    \n    if err: return False\n    \n    xml_dump = [i for i in out.decode(\"utf-8\").split(\"\\n\") if \"<package name=\" in i]\n    pkgdict = {}\n    \n    for i in xml_dump:\n        pkgname = re.findall(\"<package name=\\\"(.*?)\\\"\", i)[0]\n        pkgpath = re.findall(\"codePath=\\\"(.*?)\\\"\", i)[0]\n        \n        # Normalizes each entry\n        if not pkgpath.endswith(\".apk\"):\n            try:\n                pkgpath = glob.glob(pkgpath + \"/*.apk\")[0]\n            except:\n                continue\n        \n        pkgdict[pkgname] = pkgpath\n    \n    return pkgdict\n\ndef list_installed_pkgs(args):\n    \"\"\"Lists the members of a given category of packages; returns list.\"\"\"\n    \n    prefix = \"pm list packages\"\n    \n    if args.user:\n        suffix = \"-3\"\n    elif args.system:\n        suffix = \"-s\"\n    elif args.disabled:\n        suffix = \"-d\"\n    else:\n        suffix = \"\"\n    \n    pkgs = [i[8:] for i in subprocess.Popen(\"{0} {1}\".format(prefix, suffix), stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell = True).communicate()[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    return pkgs\n\ndef list_installed_pkgs_nougat(args):\n    \"\"\"Uses Nougat's cmd command to query the package service (faster); returns list.\"\"\"\n    \n    prefix = \"cmd package list packages\"\n    \n    if args.user:\n        suffix = \"-3\"\n    elif args.system:\n        suffix = \"-s\"\n    elif args.disabled:\n        suffix = \"-d\"\n    else:\n        suffix = \"\"\n    \n    pkgs = [i[8:] for i in utils.sudo(\"{0} {1}\".format(prefix, suffix))[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    return pkgs\n\ndef check_substratum(nougat):\n    \"\"\"Checks if the Substratum engine is installed; returns bool.\"\"\"\n    \n    if nougat:\n        user_pkgs = [i[8:] for i in utils.sudo(\"cmd package list packages -3\")[0].decode(\"utf-8\").split(\"\\n\") if i]\n    else:\n        user_pkgs = [i[8:] for i in subprocess.Popen(\"pm list packages -3\", stdout = subprocess.PIPE, shell = True).communicate()[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    substratum_installed = True if \"projekt.substratum\" in user_pkgs else False\n    \n    return substratum_installed\n\ndef exclude_overlays(aapt, pkgdict, pkgs):\n    \"\"\"Excludes Substratum overlays from the packages to extract; returns nothing.\"\"\"\n    \n    for i in pkgs:\n        pkgpath = pkgdict.get(i)\n        out = apkutils.get_pkgxml(aapt, pkgpath)[0].decode(\"utf-8\")\n        \n        if \"Substratum_Parent\" in out: pkgs.remove(i)\n\ndef exclude_arcus_variants(pkgs):\n    \"\"\"Excludes Arcus theme variants from the packages to extract; returns nothing.\"\"\"\n    \n    for i in pkgs:\n        if \"pixkart.arcus.user\" in i: pkgs.remove(i)\n\ndef check_already_extracted(pkgpath, md5sums):\n    \"\"\"Checks if an APK has already been extracted; returns bool, str.\"\"\"\n    \n    pkgsum = utils.compute_md5sum(pkgpath)\n    already_extracted = True if pkgsum in md5sums else False\n    \n    return already_extracted, pkgsum", "instruction_en": "This code provides functions for management of installed APK lists. To use it, call the desired function with the appropriate arguments. The available functions are:\n- create_pkgdict(): creates a dictionary for fast path lookup from /data/system/packages.xml.\n- list_installed_pkgs(args): lists the members of a given category of packages.\n- list_installed_pkgs_nougat(args): uses Nougat's cmd command to query the package service (faster).\n- check_substratum(nougat): checks if the Substratum engine is installed.\n- exclude_overlays(aapt, pkgdict, pkgs): excludes Substratum overlays from the packages to extract.\n- exclude_arcus_variants(pkgs): excludes Arcus theme variants from the packages to extract.\n- check_already_extracted(pkgpath, md5sums): checks if an APK has already been extracted.", "original_line_number": 8903, "instruction_length": 824}
{"instruction": "Kod ini adalah skrip autodiscovery IBM Storwize V7000 untuk Zabbix. Untuk menggunakannya, jalankan svc_perf_discovery_sender.py dengan pilihan berikut: --debug (untuk membolehkan output debug),-svc1> [, <svc2> ...] --Discovery-types <ype1>, [type2] (jenis penemuan: 'volum-mdisk', 'volume', 'mdisk', 'kolam'). Skrip menghantar data volum/mdisk/pool lld json ke item lld trapper \"svc.discovery. <Volume-mdisk | volume | mdisk | pool>\" dan harus digunakan dengan template \"_special_storwize_perf\" Zabbix. Lihat juga http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery.", "input": "", "output": "#!/usr/bin/python\n# -*- coding: utf-8 -*- # coding: utf-8\n#\n# IBM Storwize V7000 autodiscovery script for Zabbix\n#\n# 2013 Matvey Marinin\n#\n# Sends volume/mdisk/pool LLD JSON data to LLD trapper items \"svc.discovery.<volume-mdisk|volume|mdisk|pool>\"\n# Use with \"_Special_Storwize_Perf\" Zabbix template\n#\n# See also http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery\n#\n# Usage:\n# svc_perf_discovery_sender.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd>\n#\n#   --debug    = Enable debug output\n#   --clusters = Comma-separated Storwize node list\n#   --user     = Storwize V7000 user account with Administrator role (it seems that Monitor role is not enough)\n#   --password = User password\n#\nimport pywbem\nimport getopt, sys\nfrom zbxsend import Metric, send_to_zabbix\nimport logging\n\ndef usage():\n  print >> sys.stderr, \"Usage: svc_perf_discovery_sender_zabbix.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd> --discovery-types <type1>,[type2]\"\n  print >> sys.stderr, \"Discovery types: 'volume-mdisk','volume','mdisk','pool'\"\n\n\ntry:\n  opts, args = getopt.gnu_getopt(sys.argv[1:], \"-h\", [\"help\", \"clusters=\", \"user=\", \"password=\", \"debug\", \"discovery-types=\"])\nexcept getopt.GetoptError, err:\n  print >> sys.stderr, str(err)\n  usage()\n  sys.exit(2)\n\ndebug = False\nclusters = []\nDISCOVERY_TYPES = []\nuser = None\npassword = None\nfor o, a in opts:\n  if o == \"--clusters\" and not a.startswith('--'):\n    clusters.extend( a.split(','))\n  elif o == \"--user\" and not a.startswith('--'):\n    user = a\n  elif o == \"--password\" and not a.startswith('--'):\n    password = a\n  elif o == \"--debug\":\n    debug = True\n  elif o == \"--discovery-types\":\n    DISCOVERY_TYPES.extend( a.split(','))\n  elif o in (\"-h\", \"--help\"):\n    usage()\n    sys.exit()\n\nif not clusters:\n  print >> sys.stderr, '--clusters option must be set'\n  usage()\n  sys.exit(2)\n\nif not DISCOVERY_TYPES:\n  print >> sys.stderr, '--discovery-types option must be set'\n  usage()\n  sys.exit(2)\n\nif not user or not password:\n  print >> sys.stderr, '--user and --password options must be set'\n  usage()\n  sys.exit(2)\n\ndef debug_print(message):\n  if debug:\n    print message\n\nfor cluster in clusters:\n  debug_print('Connecting to: %s' % cluster)\n  conn = pywbem.WBEMConnection('https://'+cluster, (user, password), 'root/ibm')\n  conn.debug = True\n\n  for discovery in DISCOVERY_TYPES:\n    output = []\n\n    if discovery == 'volume-mdisk' or discovery == 'volume':\n      for vol in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_StorageVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('volume', vol.properties['ElementName'].value, vol.properties['DeviceID'].value) )\n\n    if discovery == 'volume-mdisk' or discovery == 'mdisk':\n      for mdisk in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_BackendVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('mdisk', mdisk.properties['ElementName'].value, mdisk.properties['DeviceID'].value) )\n\n    if discovery == 'pool':\n      for pool in conn.ExecQuery('WQL', 'select PoolID, ElementName from IBMTSSVC_ConcreteStoragePool'):\n        output.append( '{\"{#TYPE}\":\"%s\",\"{#NAME}\":\"%s\",\"{#ID}\":\"%s\"}' % ('pool', pool.properties['ElementName'].value, pool.properties['PoolID'].value) )\n\n    json = []\n    json.append('{\"data\":[')\n\n    for i, v in enumerate( output ):\n      if i < len(output)-1:\n        json.append(v+',')\n      else:\n        json.append(v)\n    json.append(']}')\n\n    json_string = ''.join(json)\n    print(json_string)\n\n    trapper_key = 'svc.discovery.%s' % discovery\n    debug_print('Sending to host=%s, key=%s' % (cluster, trapper_key))\n\n    #send json to LLD trapper item with zbxsend module\n    if debug:\n      logging.basicConfig(level=logging.INFO)\n    else:\n      logging.basicConfig(level=logging.WARNING)\n    send_to_zabbix([Metric(cluster, trapper_key, json_string)], 'localhost', 10051)\n    debug_print('')", "instruction_en": "This code is an IBM Storwize V7000 autodiscovery script for Zabbix. To use it, run svc_perf_discovery_sender.py with the following options: --debug (to enable debug output), --clusters <svc1>[,<svc2>...] (comma-separated Storwize node list), --user <username> (Storwize V7000 user account with Administrator role), --password <pwd> (user password), and --discovery-types <type1>,[type2] (discovery types: 'volume-mdisk','volume','mdisk','pool'). The script sends volume/mdisk/pool LLD JSON data to LLD trapper items \"svc.discovery.<volume-mdisk|volume|mdisk|pool>\" and should be used with \"_Special_Storwize_Perf\" Zabbix template. See also http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery.", "original_line_number": 9553, "instruction_length": 591}
{"instruction": "Kemas kini modul Tetapan Django ke \"MS2LDAVIZ.SETTINGS_SIMON\". Buat set ciri yang dinamakan \"Binned_005\" jika ia tidak wujud. Dapatkan eksperimen yang dinamakan \"Massbank_Binned_005\" dan tetapkan ciri -cirinya yang ditetapkan kepada \"Binned_005\". Untuk ciri -ciri dalam \"Massbank_binned_005\" tanpa set ciri, tetapkan ciri mereka ditetapkan kepada \"binned_005\". Dapatkan semua ciri yang terikat dengan ciri set \"binned_005\". Dapatkan percubaan lain yang ditentukan oleh hujah baris arahan dan tetapkan ciri yang ditetapkan kepada \"Binned_005\". Untuk ciri -ciri dalam eksperimen lain dengan nama yang sama seperti ciri -ciri dalam \"Massbank_binned_005\", gantikannya dengan ciri yang sepadan dari \"Massbank_binned_005\". Untuk ciri -ciri dalam eksperimen lain dengan nama yang tidak dalam \"Massbank_binned_005\", buat ciri baru dengan nama yang sama dan tetapkan ciri yang ditetapkan kepada \"Binned_005\". Padam ciri -ciri dalam eksperimen lain yang tidak mempunyai contoh ciri, contoh Mass2Motif, atau peta ciri.", "input": "", "output": "import os\nimport sys\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"ms2ldaviz.settings_simon\")\n\nimport django\ndjango.setup()\n\nfrom basicviz.models import *\n\nif __name__ == '__main__':\n\n\n\tother_experiment_name = sys.argv[1]\n\n\tfs,status = BVFeatureSet.objects.get_or_create(name = 'binned_005')\n\tif status:\n\t\tprint(\"Created feature set\")\n\telse:\n\t\tprint(\"Featureset already exists\")\n\n\tmbe = Experiment.objects.get(name = 'massbank_binned_005')\n\tprint(\"Got \" + str(mbe))\n\tif mbe.featureset == None:\n\t\tmbe.featureset = fs\n\t\tmbe.save()\n\n\tmbe_features = Feature.objects.filter(experiment = mbe)\n\tprint(\"Got {} features\".format(len(mbe_features)))\n\n\tmbe_features_sub = Feature.objects.filter(experiment = mbe,featureset = None)\n\tprint(\"{} have no featureset\".format(len(mbe_features_sub)))\n\tfor f in mbe_features_sub:\n\t\tf.featureset = fs\n\t\tf.save()\n\n\n\t# Now get the features as tied to the feature set\n\tmbe_features = Feature.objects.filter(featureset = fs)\n\tprint(\"Got {} features\".format(len(mbe_features)))\n\n\tfnames = set([f.name for f in mbe_features])\n\n\t# get another experiment\n\ti93 = Experiment.objects.get(name = other_experiment_name)\n\ti93.featureset = fs\n\ti93.save()\n\tprint(\"Got \" + str(i93))\n\n\ti93_features = Feature.objects.filter(experiment = i93)\n\n\tprint(\"Got {} features\".format(len(i93_features)))\n\tfor f in i93_features:\n\t\tif f.name in fnames:\n\t\t\t# Find all the instances\n\t\t\tfis = FeatureInstance.objects.filter(feature = f)\n\t\t\tgfeature = [g for g in mbe_features if g.name == f.name][0]\n\t\t\tfor fi in fis:\n\t\t\t\tfi.feature = gfeature\n\t\t\t\tfi.save()\n\t\t\tmis = Mass2MotifInstance.objects.filter(feature = f)\n\t\t\tfor ms in mis:\n\t\t\t\tms.feature = gfeature\n\t\t\t\tms.save()\n\n\n\t\telse:\n\t\t\tnew_feature = Feature.objects.create(name = f.name,featureset = fs,min_mz = f.min_mz,max_mz = f.max_mz)\n\t\t\tfis = FeatureInstance.objects.filter(feature = f)\n\t\t\tfor fi in fis:\n\t\t\t\tfi.feature = new_feature\n\t\t\t\tfi.save()\n\t\t\tmis = Mass2MotifInstance.objects.filter(feature = f)\n\t\t\tfor ms in mis:\n\t\t\t\tms.feature = new_feature\n\t\t\t\tms.save()\n\n\n\tfor f in i93_features:\n\t\tif len(f.featureinstance_set.all()) == 0 and len(f.mass2motifinstance_set.all()) == 0 and len(f.featuremap_set.all()) == 0:\n\t\t\tf.delete()\n\t\telse:\n\t\t\tprint(f)", "instruction_en": "Update the Django settings module to \"ms2ldaviz.settings_simon\". Create a feature set named \"binned_005\" if it does not exist. Get an experiment named \"massbank_binned_005\" and set its feature set to \"binned_005\". For features in \"massbank_binned_005\" without a feature set, set their feature set to \"binned_005\". Get all features tied to feature set \"binned_005\". Get another experiment specified by the command line argument and set its feature set to \"binned_005\". For features in the other experiment with the same name as features in \"massbank_binned_005\", replace them with the corresponding feature from \"massbank_binned_005\". For features in the other experiment with a name not in \"massbank_binned_005\", create a new feature with the same name and set its feature set to \"binned_005\". Delete features in the other experiment that have no feature instances, mass2motif instances, or feature maps.", "original_line_number": 9570, "instruction_length": 1007}
{"instruction": "Kod ini adalah aplikasi konsol yang membolehkan mencari stesen kereta api dan mendapatkan masa berlepas. Untuk menggunakan aplikasi ini, anda boleh menjalankannya dalam persekitaran Python dan memanggil fungsi yang ditakrifkan dalam kod. Fungsi utama adalah \"do_stations\", \"do_departures\", dan \"do_roadmap\". Fungsi \"do_stations\" mencari stesen kereta api berdasarkan corak yang diberikan. Fungsi \"do_departures\" menyenaraikan semua pelepasan untuk stesen tertentu. Fungsi \"do_roadmap\" memaparkan peta jalan untuk perjalanan dari berlepas ke stesen ketibaan. Untuk menggunakan fungsi \"DO_ROADMAP\", anda boleh lulus stesen berlepas dan ketibaan sebagai argumen dan secara pilihan menentukan masa berlepas dan ketibaan menggunakan pilihan \"-JPARURURE-TIME\" dan \"--Rival-time\".", "input": "", "output": "# -*- coding: utf-8 -*-\n\n# Copyright(C) 2010-2011 Romain Bignon, Julien Hbert\n#\n# This file is part of weboob.\n#\n# weboob is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# weboob is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with weboob. If not, see <http://www.gnu.org/licenses/>.\n\n\nimport sys\nfrom datetime import datetime\nimport logging\n\nfrom weboob.capabilities.travel import ICapTravel, RoadmapFilters\nfrom weboob.tools.application.repl import ReplApplication, defaultcount\n\n\n__all__ = ['Traveloob']\n\n\nclass Traveloob(ReplApplication):\n    APPNAME = 'traveloob'\n    VERSION = '0.h'\n    COPYRIGHT = 'Copyright(C) 2010-2011 Romain Bignon'\n    DESCRIPTION = \"Console application allowing to search for train stations and get departure times.\"\n    SHORT_DESCRIPTION = \"search for train stations and departures\"\n    CAPS = ICapTravel\n    DEFAULT_FORMATTER = 'table'\n\n    def add_application_options(self, group):\n        group.add_option('--departure-time')\n        group.add_option('--arrival-time')\n\n    @defaultcount(10)\n    def do_stations(self, pattern):\n        \"\"\"\n        stations PATTERN\n\n        Search stations.\n        \"\"\"\n        for backend, station in self.do('iter_station_search', pattern):\n            self.format(station)\n\n    @defaultcount(10)\n    def do_departures(self, line):\n        \"\"\"\n        departures STATION [ARRIVAL]\n\n        List all departures for a given station.\n        \"\"\"\n        station, arrival = self.parse_command_args(line, 2, 1)\n\n        station_id, backend_name = self.parse_id(station)\n        if arrival:\n            arrival_id, backend_name2 = self.parse_id(arrival)\n            if backend_name and backend_name2 and backend_name != backend_name2:\n                logging.error('Departure and arrival aren\\'t on the same backend')\n                return 1\n        else:\n            arrival_id = backend_name2 = None\n\n        if backend_name:\n            backends = [backend_name]\n        elif backend_name2:\n            backends = [backend_name2]\n        else:\n            backends = None\n\n        for backend, departure in self.do('iter_station_departures', station_id, arrival_id, backends=backends):\n            self.format(departure)\n\n    def do_roadmap(self, line):\n        \"\"\"\n        roadmap DEPARTURE ARRIVAL\n\n        Display the roadmap to travel from DEPARTURE to ARRIVAL.\n\n        Command-line parameters:\n           --departure-time TIME    requested departure time\n           --arrival-time TIME      requested arrival time\n\n        TIME might be in form \"yyyy-mm-dd HH:MM\" or \"HH:MM\".\n\n        Example:\n            > roadmap Puteaux Aulnay-sous-Bois --arrival-time 22:00\n        \"\"\"\n        departure, arrival = self.parse_command_args(line, 2, 2)\n\n        filters = RoadmapFilters()\n        try:\n            filters.departure_time = self.parse_datetime(self.options.departure_time)\n            filters.arrival_time = self.parse_datetime(self.options.arrival_time)\n        except ValueError as e:\n            print >>sys.stderr, 'Invalid datetime value: %s' % e\n            print >>sys.stderr, 'Please enter a datetime in form \"yyyy-mm-dd HH:MM\" or \"HH:MM\".'\n            return 1\n\n        for backend, route in self.do('iter_roadmap', departure, arrival, filters):\n            self.format(route)\n\n    def parse_datetime(self, text):\n        if text is None:\n            return None\n\n        try:\n            date = datetime.strptime(text, '%Y-%m-%d %H:%M')\n        except ValueError:\n            try:\n                date = datetime.strptime(text, '%H:%M')\n            except ValueError:\n                raise ValueError(text)\n            date = datetime.now().replace(hour=date.hour, minute=date.minute)\n\n        return date", "instruction_en": "This code is a console application that allows searching for train stations and getting departure times. To use this application, you can run it in a Python environment and call the functions defined in the code. The main functions are \"do_stations\", \"do_departures\", and \"do_roadmap\". The \"do_stations\" function searches for train stations based on a given pattern. The \"do_departures\" function lists all departures for a given station. The \"do_roadmap\" function displays the roadmap to travel from a departure to an arrival station. To use the \"do_roadmap\" function, you can pass the departure and arrival stations as arguments and optionally specify the departure and arrival times using the \"--departure-time\" and \"--arrival-time\" options.", "original_line_number": 9765, "instruction_length": 773}
{"instruction": "Buat kelas PlayerThread yang memanjangkan benang dan mengambil parameter GameTheRead, Nama Pengguna, dan IA. Dalam pembina, memulakan atribut GameThread, pengguna, IA, dan Joueur. Meningkatkan Pihak -pihak Atribut Objek BarrierTours GameThread. Tambahkan atribut Joueur ke senarai JoueursaAdd GameThread. Meningkatkan atribut NBTH GameTheRead. Melaksanakan kaedah larian yang melaksanakan gelung tak terhingga. Di dalam gelung, tunggu objek BarrierTours GameThread. Kemudian, laksanakan kaedah Executeia. Tunggu objek halangan GameTheRead. Kirakan kedudukan pemain. Tunggu objek halangan GameTheRead. Sertailah pemain. Tunggu objek halangan GameTheRead. Memperoleh objek Lockmanger GameThread. Tambahkan pembolehubah Agraille ke senarai Amanger GameThread. Lepaskan objek Lockmanger GameThread. Tunggu objek barriermanger gameTheRead. Sekiranya atribut Poidtotal Joueur kurang daripada atau sama dengan 0 dan atribut akhir Joueur adalah palsu, tetapkan atribut akhir Joueur menjadi benar. Cetak mesej yang menunjukkan bahawa pemain telah hilang. Kemas kini skor pengguna yang berkaitan dengan pemain. Melaksanakan kaedah Executeia yang tidak melakukan apa -apa. Melaksanakan kaedah Calculepos yang mengemas kini kedudukan dan halaju sfera pemain. Sekiranya normevitesse sfera lebih besar daripada normevitessemax sfera, mengurangkan komponen x dan y dari atribut vectvitesse sfera sebanyak 0.9. Jika atribut taille sfera lebih besar daripada 50000 dan integer rawak antara 1 dan 300 adalah sama dengan 1, berpecah sfera dan mengemas kini atribut poidtotal pemain. Melaksanakan kaedah gabungan yang menyertai bidang pemain dengan bidang pemain lain. Kembalikan kamus yang memetakan nama pengguna pemain lain ke sfera yang disertai.", "input": "", "output": "from Server.Game import *\nfrom threading import Thread\nfrom CodIa.tuto.models import User\nfrom CodIa.tuto.app import db\nimport threading\nimport time\nimport random\n\nclass PlayerThread(Thread):\n\n    def __init__(self, GameThread, username, ia):\n        Thread.__init__(self)\n        self.GameThread = GameThread\n        self.username = username\n        GameThread.barrierTours._parties += 1\n        self.ia = ia\n        self.joueur = Player(ia,username,GameThread.game.gamesize)\n        #GameThread.game.joueurs[username]=Player(ia,username,GameThread.game.gamesize)\n        GameThread.joueursAAdd.append(self.joueur)\n\n        GameThread.nbth += 1\n\n\n    def run(self):\n        while True:\n            #attend le dbut du tours\n            # print(\"Barriere debut de tours \"+str(threading.current_thread().name))\n            # print(self.GameThread.barrierTours.parties)\n            self.GameThread.barrierTours.wait()\n            #execute le code de l'IA\n            self.executeIa()\n\n\n            #print(self.GameThread.barrierEtape.parties)\n            self.GameThread.barrierEtape.wait()\n\n            self.calculePos()\n\n            self.GameThread.barrierEtape.wait()\n\n            agraille = self.join()\n\n            #print(\"avant acquire\")\n            self.GameThread.barrierEtape.wait()\n            self.GameThread.lockmanger.acquire()\n            self.GameThread.aManger.append(agraille)\n            #print(\"pendant\")\n            self.GameThread.lockmanger.release()\n            #print(\"aprs release\")\n            self.GameThread.barrierManger.wait()\n            if self.joueur.poidTotal<=0 and not self.joueur.end:\n                self.joueur.end = True\n                print(\"\\033[91m Le Joueur \"+self.joueur.username +\"  perdu \\033[0m\")\n                user = User.query.filter_by(pseudo=self.joueur.username).first()\n                if user is not None:\n                    # print(\"\\033[91m Zbra \\033[0m\")\n\n                    user.score += self.joueur.score\n                    db.session.commit()\n            # time.sleep(1/60)\n        # self.GameThread.nbth-=1\n        # self.GameThread.barrierTours._parties -= 1\n\n\n\n\n\n    def executeIa(self):\n        pass\n\n    def calculePos(self):\n        # print(\"\\033[91m caca \\033[0m\")\n        # print(str(self.joueur.spheres[0].normeVitesse()) +\"    \"+ str(self.joueur.spheres[0].normeVitesseMax()))\n        res=0\n        for sphere in  self.joueur.spheres:\n            sphere.vectVitesse = sphere.vitesseNextTick()\n            if sphere.normeVitesse() > sphere.normeVitesseMax():\n                # print(\"\\033[91m caca \\033[0m\")\n                sphere.vectVitesse[0] *= 0.9\n                sphere.vectVitesse[1] *= 0.9\n            # else :\n            #     print(\"\\033[92m non caca \\033[0m\")\n            sphere.vectPos = sphere.posNextTick()\n\n            rand = random.randint(1,300)\n            if sphere.taille > 50000 and rand==1:\n                sphere.split(self.joueur)\n                somme = 0\n\n                #print(\"=======================================================\")\n\n                for joueur in self.GameThread.game.joueurs.values():\n                    somme += joueur.poidTotal\n                #print(\"somme2\"+str(somme))\n            #print(\"taille sphere max: \"+str((sphere.taille)))\n            #pass\n        self.joueur.updateScore()\n\n    def join(self):\n        try:\n            listjoueur = dict()\n            for sphere in  self.joueur.spheres:\n                for joueur2 in self.GameThread.game.joueurs.values():\n                    for sphere2 in joueur2.spheres:\n                        res = sphere.join(sphere2,joueur2)\n                        if(res != None):\n                            # if(not (listjoueur[res[0].username] in locals)):\n                            #     listjoueur[res[0].username] = []\n                            try:\n                                listjoueur[res[0].username].append(res[1])\n                            except KeyError:\n                                listjoueur[res[0].username] = []\n                                listjoueur[res[0].username].append(res[1])\n        except RuntimeError:\n             print(\"\\033[91m Nb de Thread :\"+str(self.GameThread.barrierManger._parties)+\", \"+str(self.GameThread.nbth)+\" \\033[0m\")\n\n        return listjoueur", "instruction_en": "Create a PlayerThread class that extends Thread and takes in parameters GameThread, username, and ia. In the constructor, initialize the GameThread, username, ia, and joueur attributes. Increment the parties attribute of the barrierTours object of GameThread. Append the joueur attribute to the joueursAAdd list of GameThread. Increment the nbth attribute of GameThread. Implement the run method that executes an infinite loop. Inside the loop, wait for the barrierTours object of GameThread. Then, execute the executeIa method. Wait for the barrierEtape object of GameThread. Calculate the position of the player. Wait for the barrierEtape object of GameThread. Join the player. Wait for the barrierEtape object of GameThread. Acquire the lockmanger object of GameThread. Append the agraille variable to the aManger list of GameThread. Release the lockmanger object of GameThread. Wait for the barrierManger object of GameThread. If the poidTotal attribute of joueur is less than or equal to 0 and the end attribute of joueur is False, set the end attribute of joueur to True. Print a message indicating that the player has lost. Update the score of the user associated with the player. Implement the executeIa method that does nothing. Implement the calculePos method that updates the position and velocity of the spheres of the player. If the normeVitesse of a sphere is greater than the normeVitesseMax of the sphere, reduce the x and y components of the vectVitesse attribute of the sphere by 0.9. If the taille attribute of a sphere is greater than 50000 and a random integer between 1 and 300 is equal to 1, split the sphere and update the poidTotal attribute of the player. Implement the join method that joins the spheres of the player with the spheres of other players. Return a dictionary that maps the usernames of the other players to the spheres that were joined.", "original_line_number": 10097, "instruction_length": 1730}
{"instruction": "Paparkan spanduk klasifikasi untuk sesi XWindows dengan menjalankan skrip Python \"klasifikasi-banner.py\". Banner memaparkan tahap klasifikasi dan boleh disesuaikan dengan pelbagai pilihan seperti warna latar depan dan latar belakang, muka fon dan saiz, dan kelegapan. Banner juga boleh menunjukkan pengguna dan nama hos dan boleh tersembunyi buat sementara waktu dengan menekan kekunci ESC. Skrip ini boleh dikonfigurasikan dengan fail konfigurasi global yang terletak di \"/etc/klasifikasi-Banner\". Untuk melancarkan spanduk, jalankan skrip dan pastikan pembolehubah persekitaran paparan ditetapkan.", "input": "", "output": "#!/usr/bin/python\n# Classification Banner\n#\n# Last update was 02 July 2018\n#\n# Script: classification-banner.py\n# Description: Displays a Classification for an Xwindows session\n# Copyright: Frank Caviggia, 2017\n# Author: Frank Caviggia <fcaviggia (at) gmail.com>\n# Version: 1.6.6\n# License: GPLv2\n\nimport sys,os,optparse,time\nfrom socket import gethostname\n\n# Check if DISPLAY variable is set\ntry:\n\tos.environ[\"DISPLAY\"]\n\timport pygtk,gtk\nexcept:\n\ttry:\n\t\timport Gtk\n\texcept:\n\t\tprint(\"Error: DISPLAY environment variable not set.\")\n\t\tsys.exit(1)\n\n# Global Configuration File\nCONF_FILE = \"/etc/classification-banner\"\n\n# Returns Username\ndef get_user():\n    try:\n        user = os.getlogin()\n    except:\n        user = ''\n        pass\n    return user\n\n# Returns Hostname\ndef get_host():\n    host = gethostname()\n    host = host.split('.')[0]\n    return host\n\n# Classification Banner Class\nclass Classification_Banner:\n    \"\"\"Class to create and refresh the actual banner.\"\"\"\n\n    def __init__(self, message=\"UNCLASSIFIED\", fgcolor=\"#000000\",\n                 bgcolor=\"#00CC00\", face=\"liberation-sans\", size=\"small\",\n                 weight=\"bold\", x=0, y=0, esc=True, opacity=0.75, sys_info=False):\n\n        \"\"\"Set up and display the main window\n\n        Keyword arguments:\n        message -- The classification level to display\n        fgcolor -- Foreground color of the text to display\n        bgcolor -- Background color of the banner the text is against\n        face    -- Font face to use for the displayed text\n        size    -- Size of font to use for text\n        weight  -- Bold or normal\n        hres    -- Horizontal Screen Resolution (int) [ requires vres ]\n        vres    -- Vertical Screen Resolution (int) [ requires hres ]\n        opacity -- Opacity of window (float) [0 .. 1, default 0.75]\n        \"\"\"\n        self.hres = x\n        self.vres = y\n\n        # Dynamic Resolution Scaling\n        self.monitor = gtk.gdk.Screen()\n        self.monitor.connect(\"size-changed\", self.resize)\n\n        # Newer versions of pygtk have this method\n        try:\n            self.monitor.connect(\"monitors-changed\", self.resize)\n        except:\n            pass\n\n        # Create Main Window\n        self.window = gtk.Window()\n        self.window.set_position(gtk.WIN_POS_CENTER)\n        self.window.connect(\"hide\", self.restore)\n        self.window.connect(\"key-press-event\", self.keypress)\n        self.window.modify_bg(gtk.STATE_NORMAL, gtk.gdk.color_parse(bgcolor))\n        self.window.set_property('skip-taskbar-hint', True)\n        self.window.set_property('skip-pager-hint', True)\n        self.window.set_property('destroy-with-parent', True)\n        self.window.stick()\n        self.window.set_decorated(False)\n        self.window.set_keep_above(True)\n        self.window.set_app_paintable(True)\n\n        try:\n            self.window.set_opacity(opacity)\n        except:\n            pass\n\n        # Set the default window size\n        self.window.set_default_size(int(self.hres), 5)\n\n        # Create Main Horizontal Box to Populate\n        self.hbox = gtk.HBox()\n\n        # Create the Center Vertical Box\n        self.vbox_center = gtk.VBox()\n        self.center_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, message))\n        self.center_label.set_use_markup(True)\n        self.center_label.set_justify(gtk.JUSTIFY_CENTER)\n        self.vbox_center.pack_start(self.center_label, True, True, 0)\n\n        # Create the Right-Justified Vertical Box to Populate for hostname\n        self.vbox_right = gtk.VBox()\n        self.host_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, get_host()))\n        self.host_label.set_use_markup(True)\n        self.host_label.set_justify(gtk.JUSTIFY_RIGHT)\n        self.host_label.set_width_chars(20)\n\n        # Create the Left-Justified Vertical Box to Populate for user\n        self.vbox_left = gtk.VBox()\n        self.user_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, get_user()))\n        self.user_label.set_use_markup(True)\n        self.user_label.set_justify(gtk.JUSTIFY_LEFT)\n        self.user_label.set_width_chars(20)\n        \n        # Create the Right-Justified Vertical Box to Populate for ESC message\n        self.vbox_esc_right = gtk.VBox()\n        self.esc_label = gtk.Label(\n            \"<span font_family='liberation-sans' weight='normal' foreground='%s' size='xx-small'>  (ESC to hide temporarily)  </span>\" %\n            (fgcolor))\n        self.esc_label.set_use_markup(True)\n        self.esc_label.set_justify(gtk.JUSTIFY_RIGHT)\n        self.esc_label.set_width_chars(20)\n\n        # Empty Label for formatting purposes\n        self.vbox_empty = gtk.VBox()\n        self.empty_label = gtk.Label(\n            \"<span font_family='liberation-sans' weight='normal'>                 </span>\")\n        self.empty_label.set_use_markup(True)\n        self.empty_label.set_width_chars(20)\n\n        if not esc:\n            if not sys_info:\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n            else:\n                self.vbox_right.pack_start(self.host_label, True, True, 0)\n                self.vbox_left.pack_start(self.user_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_right, False, True, 20)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_left, False, True, 20)\n\n        else:\n            if esc and not sys_info:\n                self.empty_label.set_justify(gtk.JUSTIFY_LEFT)\n                self.vbox_empty.pack_start(self.empty_label, True, True, 0)\n                self.vbox_esc_right.pack_start(self.esc_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_esc_right, False, True, 0)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_empty, False, True, 0)\n\n        if sys_info:\n                self.vbox_right.pack_start(self.host_label, True, True, 0)\n                self.vbox_left.pack_start(self.user_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_right, False, True, 20)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_left, False, True, 20)\n\n        self.window.add(self.hbox)\n        self.window.show_all()\n        self.width, self.height = self.window.get_size()\n\n    # Restore Minimized Window\n    def restore(self, widget, data=None):\n        self.window.deiconify()\n        self.window.present()\n\n        return True\n\n    # Destroy Classification Banner Window on Resize (Display Banner Will Relaunch)\n    def resize(self, widget, data=None):\n        self.window.destroy()\n\n        return True\n\n    # Press ESC to hide window for 15 seconds\n    def keypress(self, widget, event=None):\n        if event.keyval == 65307:\n            if not gtk.events_pending():\n                self.window.iconify()\n                self.window.hide()\n                time.sleep(15)\n                self.window.show()\n                self.window.deiconify()\n                self.window.present()\n\n        return True\n\n\nclass Display_Banner:\n\n    \"\"\"Display Classification Banner Message\"\"\"\n    def __init__(self):\n        # Dynamic Resolution Scaling\n        self.monitor = gtk.gdk.Screen()\n        self.monitor.connect(\"size-changed\", self.resize)\n\n        # Newer versions of pygtk have this method\n        try:\n            self.monitor.connect(\"monitors-changed\", self.resize)\n        except:\n            pass\n\n        # Launch Banner\n        self.config, self.args = self.configure()\n        self.execute(self.config)\n\n    # Read Global configuration\n    def configure(self):\n        config = {}\n        try:\n            execfile(CONF_FILE, config)\n        except:\n            pass\n\n        defaults = {}\n        defaults[\"message\"] = config.get(\"message\", \"UNCLASSIFIED\")\n        defaults[\"fgcolor\"] = config.get(\"fgcolor\", \"#FFFFFF\")\n        defaults[\"bgcolor\"] = config.get(\"bgcolor\", \"#007A33\")\n        defaults[\"face\"] = config.get(\"face\", \"liberation-sans\")\n        defaults[\"size\"] = config.get(\"size\", \"small\")\n        defaults[\"weight\"] = config.get(\"weight\", \"bold\")\n        defaults[\"show_top\"] = config.get(\"show_top\", True)\n        defaults[\"show_bottom\"] = config.get(\"show_bottom\", True)\n        defaults[\"hres\"] = config.get(\"hres\", 0)\n        defaults[\"vres\"] = config.get(\"vres\", 0)\n        defaults[\"sys_info\"] = config.get(\"sys_info\", False)\n        defaults[\"opacity\"] = config.get(\"opacity\", 0.75)\n        defaults[\"esc\"] = config.get(\"esc\", True)\n        defaults[\"spanning\"] = config.get(\"spanning\", False)\n\n        # Use the global config to set defaults for command line options\n        parser = optparse.OptionParser()\n        parser.add_option(\"-m\", \"--message\", default=defaults[\"message\"],\n                          help=\"Set the Classification message\")\n        parser.add_option(\"-f\", \"--fgcolor\", default=defaults[\"fgcolor\"],\n                          help=\"Set the Foreground (text) color\")\n        parser.add_option(\"-b\", \"--bgcolor\", default=defaults[\"bgcolor\"],\n                          help=\"Set the Background color\")\n        parser.add_option(\"-x\", \"--hres\", default=defaults[\"hres\"], type=\"int\",\n                          help=\"Set the Horizontal Screen Resolution\")\n        parser.add_option(\"-y\", \"--vres\", default=defaults[\"vres\"], type=\"int\",\n                          help=\"Set the Vertical Screen Resolution\")\n        parser.add_option(\"-o\", \"--opacity\", default=defaults[\"opacity\"],\n                          type=\"float\", dest=\"opacity\",\n                          help=\"Set the window opacity for composted window managers\")\n        parser.add_option(\"--face\", default=defaults[\"face\"], help=\"Font face\")\n        parser.add_option(\"--size\", default=defaults[\"size\"], help=\"Font size\")\n        parser.add_option(\"--weight\", default=defaults[\"weight\"],\n                          help=\"Set the Font weight\")\n        parser.add_option(\"--disable-esc-msg\", default=defaults[\"esc\"],\n                          dest=\"esc\", action=\"store_false\",\n                          help=\"Disable the 'ESC to hide' message\")\n        parser.add_option(\"--hide-top\", default=defaults[\"show_top\"],\n                          dest=\"show_top\", action=\"store_false\",\n                          help=\"Disable the top banner\")\n        parser.add_option(\"--hide-bottom\", default=defaults[\"show_bottom\"],\n                          dest=\"show_bottom\", action=\"store_false\",\n                          help=\"Disable the bottom banner\")\n        parser.add_option(\"--system-info\", default=defaults[\"sys_info\"],\n                          dest=\"sys_info\", action=\"store_true\",\n                          help=\"Show user and hostname in the top banner\")\n        parser.add_option(\"--enable-spanning\", default=defaults[\"spanning\"],\n                          dest=\"spanning\", action=\"store_true\",\n                          help=\"Enable banner(s) to span across screens as a single banner\")\n\n        options, args = parser.parse_args()\n        return options, args\n\n    # Launch the Classification Banner Window(s)\n    def execute(self, options):\n        self.num_monitor = 0\n\n        if options.hres == 0 or options.vres == 0:\n            # Try Xrandr to determine primary monitor resolution\n            try:\n                self.screen = os.popen(\"xrandr | grep ' connected ' | awk '{ print $3 }'\").readlines()[0]\n                self.x = self.screen.split('x')[0]\n                self.y = self.screen.split('x')[1].split('+')[0]\n\n            except:\n                try:\n                    self.screen = os.popen(\"xrandr | grep ' current ' | awk '{ print $8$9$10+0 }'\").readlines()[0]\n                    self.x = self.screen.split('x')[0]\n                    self.y = self.screen.split('x')[1].split('+')[0]\n\n                except:\n                    self.screen = os.popen(\"xrandr | grep '^\\*0' | awk '{ print $2$3$4 }'\").readlines()[0]\n                    self.x = self.screen.split('x')[0]\n                    self.y = self.screen.split('x')[1].split('+')[0]\n\n                else:\n                    # Fail back to GTK method\n                    self.display = gtk.gdk.display_get_default()\n                    self.screen = self.display.get_default_screen()\n                    self.x = self.screen.get_width()\n                    self.y = self.screen.get_height()\n        else:\n            # Resoultion Set Staticly\n            self.x = options.hres\n            self.y = options.vres\n\n        if not options.spanning and self.num_monitor > 1:\n            for monitor in range(self.num_monitor):\n                mon_geo = self.screen.get_monitor_geometry(monitor)\n                self.x_location, self.y_location, self.x, self.y = mon_geo\n                self.banners(options)\n        else:\n            self.x_location = 0\n            self.y_location = 0\n            self.banners(options)\n\n    def banners(self, options):\n            if options.show_top:\n                top = Classification_Banner(\n                    options.message,\n                    options.fgcolor,\n                    options.bgcolor,\n                    options.face,\n                    options.size,\n                    options.weight,\n                    self.x,\n                    self.y,\n                    options.esc,\n                    options.opacity,\n                    options.sys_info)\n                top.window.move(self.x_location, self.y_location)\n\n            if options.show_bottom:\n                bottom = Classification_Banner(\n                    options.message,\n                    options.fgcolor,\n                    options.bgcolor,\n                    options.face,\n                    options.size,\n                    options.weight,\n                    self.x,\n                    self.y,\n                    options.esc,\n                    options.opacity)\n                bottom.window.move(self.x_location, int(bottom.vres))\n\n    # Relaunch the Classification Banner on Screen Resize\n    def resize(self, widget, data=None):\n        self.config, self.args = self.configure()\n        self.execute(self.config)\n        return True\n\n\n# Main Program Loop\nif __name__ == \"__main__\":\n    run = Display_Banner()\n    gtk.main()", "instruction_en": "Display a Classification banner for an Xwindows session by running the Python script \"classification-banner.py\". The banner displays the classification level and can be customized with various options such as foreground and background colors, font face and size, and opacity. The banner can also show the user and hostname and can be temporarily hidden by pressing the ESC key. The script can be configured with a global configuration file located at \"/etc/classification-banner\". To launch the banner, run the script and ensure that the DISPLAY environment variable is set.", "original_line_number": 10203, "instruction_length": 599}
{"instruction": "Memandangkan rentetan s dan kamus kata -kata dict, tulis fungsi `wordbreak` yang mengembalikan semua ayat yang mungkin di mana setiap perkataan adalah perkataan kamus yang sah. Fungsi ini harus mengambil dua argumen, rentetan `s` dan satu set rentetan` worddict`. Fungsi ini harus menggunakan DP dan DFS untuk menghilangkan TLE. Algoritma DP harus menggunakan DP ke belakang. Algoritma DFS harus bermula dari akhir rentetan dan periksa secara rekursif jika rentetan selebihnya boleh pecah. Fungsi ini harus mengembalikan senarai rentetan.", "input": "", "output": "# 140. Word Break II  QuestionEditorial Solution  My Submissions\n# Total Accepted: 68516\n# Total Submissions: 321390\n# Difficulty: Hard\n# Given a string s and a dictionary of words dict, add spaces in s to construct a sentence where each word is a valid dictionary word.\n# \n# Return all such possible sentences.\n# \n# For example, given\n# s = \"catsanddog\",\n# dict = [\"cat\", \"cats\", \"and\", \"sand\", \"dog\"].\n# \n# A solution is [\"cats and dog\", \"cat sand dog\"].\n# \n# Subscribe to see which companies asked this question\n\n# Notes:\n# Forward DP or Backward DP?\n# WB1 is forward DP. DP[i] means s[:i+1] is breakable\n# WB2 is backward. DP[i] means s[i:] is breakable\n# Since DFS is to check remaining string is \n\n# 12.08.2016 Rewrite. DP + DFS\nclass Solution(object):\n    dp = []\n\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        n = len(s)\n        self.dp = [ False for x in xrange(len(s)+1) ]\n        self.dp[0] = True\n        \n        for i in xrange(n):\n            for j in xrange(i+1):\n                tmp = s[j:i+1]\n                if tmp in wordDict and self.dp[j]:\n                    self.dp[i+1] = True\n                    break\n        \n        if not self.dp[-1]: return []\n        res = []\n        self.dfs(res, \"\", s, n-1, wordDict)\n        return res\n    \n    def dfs(self, res, line, s, end, wordDict):\n        if end == -1:\n            res.append(line[:-1])\n            return\n        \n        for start in xrange(end, -1, -1):\n            tmp = s[start:end+1]\n            if tmp in wordDict and self.dp[start]:\n                self.dfs(res, tmp + \" \" + line, s, start - 1, wordDict)\n\n# DP + DFS can get rid of TLE\nclass Solution(object):\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        n = len(s)\n        print(s)\n        print(wordDict)\n        res = []\n\n        dp = [False for x in xrange(n+1)]\n        dp[n] = True\n        for i in xrange(n-1, -1, -1):\n            for j in xrange(n-1, i-1, -1):      # Better loop. i start index. j end index\n                if dp[j+1] and s[i:j+1] in wordDict:\n                    dp[i] = True\n                    break\n\n#        for i in xrange(n-1, -1, -1):\n#            for j in xrange(i, -1, -1):\n#                if dp[i+1] and s[j:i+1] in wordDict:\n#                    dp[j] = True\n#                    continue\n        \n        def dfs(start, line):\n            if not dp[start]:\n                return\n            \n            if start == len(s):\n                res.append(line[1:])\n                return\n    \n            for i in xrange(start, len(s)):\n                if dp[i+1] and s[start:i+1] in wordDict:\n                    dfs(i+1, line + \" \" + s[start:i+1])   \n\n        dfs(0, \"\")\n        return res\n        \nif __name__  == \"__main__\":\n#    s = \"catsanddog\"\n#    d = [\"cat\",\"cats\",\"and\",\"sand\",\"dog\"]\n#    Solution().wordBreak(s, d)\n#    s = \"goalspecial\"\n#    d = [\"go\",\"goal\",\"goals\",\"special\"]\n    s = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n    d = [\"a\",\"aa\",\"aaa\",\"aaaa\",\"aaaaa\",\"aaaaaa\",\"aaaaaaa\",\"aaaaaaaa\",\"aaaaaaaaa\",\"aaaaaaaaaa\"]\n    print(Solution().wordBreak(s, d))\n#    Solution().wordBreak(s, d)\n#    s1 = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabaabaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n#    d1 = [\"aa\",\"aaa\",\"aaaa\",\"aaaaa\",\"aaaaaa\",\"aaaaaaa\",\"aaaaaaaa\",\"aaaaaaaaa\",\"aaaaaaaaaa\",\"ba\"]\n#    Solution().wordBreak(s1, d1)\n\nexit\n\n# If DFS only. get TLE \nclass Solution2(object):\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        res = []\n\n        # Precheck to get rid of TLE\n        set_s = set(s)\n        set_dict = set(\"\".join(wordDict))\n        for char in set_s:\n            if set_s not in set_dict:\n                return []\n\n        self.dfs(s, wordDict, res, \"\")\n        return res\n        \n    def dfs(self, s, wordDict, res, line):\n        if not s:\n            print(line)\n            res.append(line)\n            return\n        \n        for i in xrange(1, len(s)+1):\n            if s[:i] in wordDict:\n                if not line:\n                    self.dfs(s[i:], wordDict, res, s[:i])\n                else:\n                    self.dfs(s[i:], wordDict, res, line + \" \" + s[:i])", "instruction_en": "Given a string s and a dictionary of words dict, write a function `wordBreak` that returns all possible sentences where each word is a valid dictionary word. The function should take two arguments, a string `s` and a set of strings `wordDict`. The function should use DP and DFS to get rid of TLE. The DP algorithm should use backward DP. The DFS algorithm should start from the end of the string and recursively check if the remaining string is breakable. The function should return a list of strings.", "original_line_number": 10368, "instruction_length": 538}
{"instruction": "Kelas SimulasiClient digunakan untuk mengira geodesik tempatan menggunakan kaedah BFGS yang diubahsuai. Untuk menggunakan kelas ini, buat contohnya dengan argumen yang diperlukan: simulasi_client_id (str), server_host (str), server_port (int), authKey (str, optional), metric_server_addresses (senarai), configuration_file (str), logfile ( Kemudian hubungi kaedah start_client () untuk memulakan pengkomputeran geodesik tempatan. SimulationClient akan berkomunikasi dengan contoh Simulationserver yang sedang berjalan untuk menerima tugas dan hasil pulangan. SimulationClient juga akan berkomunikasi dengan contoh -contoh simulasi yang berjalan untuk mengira nilai metrik yang diperlukan untuk geodesik. Sekiranya SimulationClient tidak dapat berkomunikasi dengan contoh Simulationserver atau Simulasi, ia akan ditutup dan menghantar isyarat kepada contoh lain untuk ditutup juga.", "input": "", "output": "from multiprocessing.connection import Client\nimport time\nimport logging\nimport socket\n\nimport numpy as np\n\nfrom SimulationUtilities import Configuration_Processing\nfrom SimulationUtilities.Communication_Codes import comm_code\nimport LinearAlgebra as la\nfrom CustomBFGS import find_geodesic_midpoint\nfrom MetricValues import shutdown_metric\n\n\nclass SimulationClient:\n    \"\"\"\n\n    The purpose of this object is to compute local geodesics using a modified BFGS method. The object receives a pair of\n    end points to compute the local geodesic between. The simulation client then returns a new position for the node.\n    The simulation client needs the value of the potential and it's gradient function, in order to achieve this it makes\n    calls to it's assigned SimulationPotential servers.\n\n    Attributes:\n      CONFIGURATION (dict) :\n          A dictionary containing the parsed values from the file in configuration_file.\n      CURVE_ADDRESS (str, int) :\n          A tuple containing a string representing the hostname/IP and an integer for the running SimulationServer.\n      AUTHKEY (str) :\n          A string containing the authorisation key for the listener method.\n      DELAY (float) :\n          The length of time the SimulationClient should wait if there is no new available jobs, before attempting to\n          contact the SimulationServer again.\n      METRIC_SERVERS :\n          A list containing tuples of addresses for SimulationPotential instances.\n      ID (str) :\n          A string that uniquely identifies the client amongst all other clients in the computation.\n      MASS_MATRIX (numpy.array) :\n          A NumPy matrix containing the mass matrix of the molecular system. Produced automatically from the Atomistic\n          Simulation Environment.\n\n    \"\"\"\n    def __init__(self, simulation_client_id, server_host, server_port, authkey, metric_server_addresses,\n                 configuration_file, logfile=None, log_level=logging.INFO, callback_delay=1.0):\n        \"\"\"The constructor for the SimulationClient class.\n\n        Note:\n          This class is intended to be used in conjunction with running SimulationServer and SimulationPotential\n          objects. It will cause a runtime exception if this condition isn't satisfied.\n\n        Args:\n          simulation_client_id (str) :\n              A string that uniquely identifies the client amongst all other clients in the computation.\n          server_host (str) :\n              The TCP/IP hostname of the running SimulationServer instance.\n          server_port (int) :\n              The port number that the SimulationServer instance is communicating on.\n          authkey (str, optional) :\n              Authentication key used to secure process communications. Default to None for local computations to\n              increase speed.\n          metric_server_addresses :\n              A list containing tuples of the type (str, int) containing the hostnames and ports for the running\n              SimulationPotential instances.\n          configuration_file (str) :\n              Directory and filename of the configuration file.\n          logfile (str, optional) :\n              Directory and filename of the log file. Is created if doesn't exist, overwritten if it does.\n          log_level (int, optional) :\n              Specify level of logging required as described in the logging package documentation.\n          callback_delay (float) :\n              The length of time the SimulationClient should wait if there is no new available jobs, before attempting\n               to contact the SimulationServer again.\n\n        \"\"\"\n        # Set the SimulationClient log output to write to logfile at prescribed log level if specified. Otherwise write\n        # to console output. Setting to DEBUG will cause poor performance and should only be used to debug.\n        if logfile is not None:\n            logging.basicConfig(filename=logfile, level=log_level, filemode='w')\n        else:\n            logging.basicConfig(level=logging.INFO)\n\n        # Read configuration from configuration_file and store in SimulationPotential's CONFIGURATION attribute.\n        self.CONFIGURATION = Configuration_Processing.read_configuration_file(configuration_file)\n\n        # Set ADDRESS and AUTHKEY attributes for Client object in the start_client method.\n        self.CURVE_ADDRESS = (server_host, server_port)\n        self.AUTHKEY = authkey\n\n        # Set the callback delay as described in the attributes.\n        self.DELAY = callback_delay\n\n        # Store the ADDRESS and AUTHKEY attributes for Client objects in the start_client method used to compute the\n        # metric values.\n        self.METRIC_SERVERS = metric_server_addresses\n\n        # Set the client's unique identifier.\n        self.ID = simulation_client_id\n\n        # Compute the mass matrix for the molecular system.\n        self.MASS_MATRIX = np.diag(np.dstack((self.CONFIGURATION['molecule'].get_masses(),) *\n                                             (self.CONFIGURATION['dimension'] /\n                                              len(self.CONFIGURATION['molecule'].get_masses()))).flatten())\n\n\n    def start_client(self):\n        \"\"\"Start the instance of SimulationClient and begin computing local geodesics.\n\n        \"\"\"\n\n        # Define a flag to indicate if contact with the SimulationServer instance is possible.\n        connection_made = False\n\n        # Create a response to send to the SimulationServer indicating that this is the first time this SimulationClient\n        # has attempted to get a task.\n        client_response = {'status_code': comm_code('CLIENT_FIRST_CONTACT'),\n                           'client_name': self.ID}\n\n        # Attempt to connect to the SimulationServer instance.\n        try:\n            # Create a Client object that communicates with the listener on CURVE_ADDRESS using password AUTHKEY.\n            server = Client(self.CURVE_ADDRESS, authkey=self.AUTHKEY)\n\n            # When a connection is made send the client message.\n            server.send(client_response)\n\n            # The client assumes the server will respond with a message, either a local geodesic to compute or a message\n            # asking the client to try again after DELAY seconds.\n            server_response = server.recv()\n\n            # Interpret the servers response by first extracting the status_code variable from the response.\n            server_response_code = server_response['status_code']\n\n            # Close the connection to the server at this point to allow other clients to communicate with the\n            # SimulationServer.\n            server.close()\n\n            # Store in the connection_made flag that it was possible to create a connection.\n            connection_made = True\n\n        # If it isn't possible to connect to the server than a socket.error exception is raised.\n        except socket.error:\n            # Write an error to the log for this client indicating that the connection couldn't be made.\n            logging.warning('Failed to Make Connection to SimulationServer. Shutting down client.')\n\n            # Send a signal to the running instances of SimulationPotential that the SimulationClient would have used\n            # indicating that they should also shutdown.\n            shutdown_metric(self.METRIC_SERVERS, self.AUTHKEY)\n\n        # This is the main loop of the SimulationClient - the program stops running when it is no longer possible to\n        # communicate with the SimulationServer. This is decided by the connection_made flag.\n        while connection_made:\n\n            # At this point in the code a new server_response should have been received. How the SimulationClient reacts\n            # depends on the communication code received.\n\n            # If the server has indicated it is giving the SimulationClient a new geodesic to compute then...\n            if server_response_code == comm_code('SERVER_GIVES_NEW_TASK'):\n\n                # Compute the rescaled tangent direction of the curve as store as a NumPy array.\n                tangent_direction = (1 / float(self.CONFIGURATION['local_number_of_nodes'] + 1)) * \\\n                    np.subtract(server_response['right_end_point'], server_response['left_end_point'], dtype='float64')\n\n                # Compute the local geodesic using the BFGS method and store the NumPy array in result\n                result = \\\n                    find_geodesic_midpoint(server_response['left_end_point'],\n                                                server_response['right_end_point'],\n                                                self.CONFIGURATION['local_number_of_nodes'],\n                                                la.orthonormal_tangent_basis(tangent_direction,\n                                                                          self.CONFIGURATION['dimension']),\n                                                tangent_direction, self.CONFIGURATION['codimension'],\n                                                self.METRIC_SERVERS,\n                                                self.MASS_MATRIX,\n                                                self.AUTHKEY)\n\n                # If the function find_geodesic_midpoint returned a None object then it couldn't contact it's\n                # SimulationPotential instances and should be restarted.\n                if result is None:\n                    # Tell the user via the log that the SimulationPotential instances couldn't be contacted.\n                    logging.warning('Failed to Make Connection to SimulationPotential. Shutting down client.')\n\n                    # Exit the main loop of the SimulationClient.\n                    break\n\n                # If there is a midpoint then construct a client response to tell the server which node has which new\n                # position.\n                client_response = {'status_code': comm_code('CLIENT_HAS_MIDPOINT_DATA'),\n                                   'node_number': server_response['node_number'],\n                                   'new_node_position': result,\n                                   'client_name': self.ID\n                                   }\n\n            # Otherwise if the server has asked the SimulationClient to try again later...\n            elif server_response_code == comm_code('SERVER_REQUEST_CALLBACK'):\n                # Make the SimulationClient wait for DELAY seconds\n                time.sleep(self.DELAY)\n\n                # Create a response to tell the SimulationServer that the SimulationClient would like a new job.\n                client_response = {'status_code': comm_code('CLIENT_HAS_NO_TASK'), 'client_name': self.ID}\n\n            # Attempt to connect to the SimulationServer instance.\n            try:\n                # Create a Client object that communicates with the listener on CURVE_ADDRESS using password AUTHKEY.\n                server = Client(self.CURVE_ADDRESS, authkey=self.AUTHKEY)\n\n                # When a connection is made send the client message.\n                server.send(client_response)\n\n                # The client assumes the server will respond with a message, either a local geodesic to compute or a\n                # message asking the client to try again after DELAY seconds.\n                server_response = server.recv()\n\n                # Interpret the servers response by first extracting the status_code variable from the response.\n                server_response_code = server_response['status_code']\n\n                # Close the connection to the server at this point to allow other clients to communicate with the\n                # SimulationServer.\n                server.close()\n\n            # If it isn't possible to connect to the server than a socket.error exception is raised.\n            except (socket.error, EOFError):\n                # Write an error to the log for this client indicating that the connection couldn't be made.\n                logging.warning('Failed to Make Connection to SimulationServer. Shutting down client.')\n\n                # Send a signal to the running instances of SimulationPotential that the SimulationClient would have\n                # used indicating that they should also shutdown.\n                shutdown_metric(self.METRIC_SERVERS, self.AUTHKEY)\n\n                # Exit the main loop of the SimulationClient.\n                break", "instruction_en": "The SimulationClient class is used to compute local geodesics using a modified BFGS method. To use this class, create an instance of it with the required arguments: simulation_client_id (str), server_host (str), server_port (int), authkey (str, optional), metric_server_addresses (list), configuration_file (str), logfile (str, optional), log_level (int, optional), and callback_delay (float). Then call the start_client() method to begin computing local geodesics. The SimulationClient will communicate with a running SimulationServer instance to receive tasks and return results. The SimulationClient will also communicate with running SimulationPotential instances to compute the metric values needed for the geodesics. If the SimulationClient is unable to communicate with the SimulationServer or SimulationPotential instances, it will shut down and send a signal to the other instances to shut down as well.", "original_line_number": 10402, "instruction_length": 880}
{"instruction": "Untuk membina pengedaran kedudukan klip lembut dalam fail BAM, ikuti langkah -langkah ini:\n\n1. Pasang modul pysam.\n2. Gunakan ArgParse untuk menentukan fail BAM input, kualiti pemetaan minimum bacaan, saiz bin kedudukan, kedalaman minimum untuk output, dan output nama fail pengedaran klip lembut.\n3. Tentukan fungsi FilterReadqualy untuk menapis bacaan berdasarkan kualiti pemetaan.\n4. Tentukan fungsi HassoftClip untuk memeriksa sama ada bacaan mempunyai klip lembut.\n5. Baca fail BAM input menggunakan pysam dan gelung melalui setiap bacaan.\n6. Penapis berbunyi berdasarkan kualiti pemetaan, panjang nama rujukan, dan kriteria lain.\n7. Buat kamus untuk menyimpan taburan kedudukan klip lembut untuk setiap kromosom.\n8. Kirakan kedudukan setiap baca dan kemas kini kedudukan yang sepadan dalam kamus dengan bilangan klip lembut dan bacaan lain.\n9. Tulis pengedaran kedudukan klip lembut ke fail output.\n10. Selesai.", "input": "", "output": "import pysam\r\nimport argparse\r\nimport sys\r\nimport logging\r\nimport os\r\nfrom asyncore import read\r\n\r\nparser = argparse.ArgumentParser(description=\"Build soft clip position distribution in BAM file.\",\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\nDEBUG=False\r\nNOT_DEBUG = not DEBUG\r\n\r\nparser.add_argument('-i', '--input', action='store', nargs='?', help='Input BAM file', required=NOT_DEBUG)\r\nparser.add_argument('--min-mapq', action='store', nargs='?', type=int, default=10, help=\"Minimum mapping quality of read\")\r\nparser.add_argument('--binsize', action='store', nargs='?', type=int, default=1000, help=\"Bin size of position\")\r\nparser.add_argument('--min-depth', action='store', nargs='?', type=int, default=100, help=\"Minimum depth for output\")\r\nparser.add_argument('-o', '--output', action='store', nargs='?', help=\"Output soft clip distribution file name\", required=NOT_DEBUG)\r\n\r\nif NOT_DEBUG and len(sys.argv)==1:\r\n  parser.print_help()\r\n  sys.exit(1)\r\n  \r\nargs = parser.parse_args()\r\n\r\nif DEBUG:\r\n  args.input = \"/scratch/cqs/shengq2/jennifer/20190906_lindsay_exomeseq_3772_hg38/softclip/P_175_06.indel.recal.TP53.bam\"\r\n  args.output = \"/scratch/cqs/shengq2/jennifer/20190906_lindsay_exomeseq_3772_hg38/softclip/P_175_06.softclip.position.tsv\"\r\n\r\nlogger = logging.getLogger('bamSoftClipPosition')\r\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)-8s - %(message)s')\r\n\r\ndef filterReadQuality(read, min_mapq):\r\n  return(read.is_unmapped or read.mapping_quality < min_mapq or read.is_secondary or read.is_qcfail or read.is_duplicate or read.is_supplementary)\r\n\r\ndef hasSoftClip(read):\r\n  return(\"S\" in read.cigarstring)\r\n\r\nchrPositionMap = {}\r\nprocessed = 0\r\nlogger.info(\"reading %s\" % args.input)\r\nwith pysam.Samfile(args.input, \"rb\") as samfile:\r\n  for read in samfile.fetch(until_eof=True):\r\n    processed += 1\r\n    if processed % 1000000 == 0:\r\n      logger.info(\"processed %d\" % processed)\r\n      #break\r\n      \r\n    if filterReadQuality(read, args.min_mapq):\r\n      continue\r\n    \r\n    if len(read.reference_name) > 5:\r\n      continue\r\n\r\n    if not read.reference_name in chrPositionMap:\r\n      chrPositionMap[read.reference_name] = {}\r\n    positionMap = chrPositionMap[read.reference_name]\r\n\r\n    position = int(read.reference_start / args.binsize)\r\n    if not position in positionMap:\r\n      positionMap[position] = [0, 0]\r\n    posvalues = positionMap[position]\r\n\r\n    if hasSoftClip(read):\r\n      posvalues[0] = posvalues[0] + 1\r\n    else:\r\n      posvalues[1] = posvalues[1] + 1\r\n\r\nwith open(args.output, \"wt\") as sw:  \r\n  sw.write(\"Chr\\tStartPosition\\tSoftClipRead\\tOtherRead\\tSoftClipPerc\\n\")\r\n  for chr in chrPositionMap.keys():\r\n    positionMap = chrPositionMap[chr]\r\n    positions = sorted(positionMap.keys())\r\n    for pos in positions:\r\n      posvalues = positionMap[pos]\r\n      sread = posvalues[0]\r\n      oread = posvalues[1]\r\n      allread = sread + oread\r\n      if allread >= args.min_depth:\r\n        sw.write(\"%s\\t%d\\t%d\\t%d\\t%.2f\\n\" % (chr, pos * args.binsize, sread, oread, sread * 1.0 / allread) )\r\n\r\nlogger.info(\"done.\")", "instruction_en": "To build a soft clip position distribution in a BAM file, follow these steps:\n\n1. Install the pysam module.\n2. Use argparse to define the input BAM file, minimum mapping quality of read, bin size of position, minimum depth for output, and output soft clip distribution file name.\n3. Define the filterReadQuality function to filter reads based on mapping quality.\n4. Define the hasSoftClip function to check if a read has a soft clip.\n5. Read the input BAM file using pysam and loop through each read.\n6. Filter reads based on mapping quality, reference name length, and other criteria.\n7. Create a dictionary to store the soft clip position distribution for each chromosome.\n8. Calculate the position of each read and update the corresponding position in the dictionary with the number of soft clip and other reads.\n9. Write the soft clip position distribution to the output file.\n10. Done.", "original_line_number": 10573, "instruction_length": 917}
{"instruction": "Kod ini mentakrifkan model untuk kempen dan kupon dalam platform e-dagang shuup. Untuk menggunakan kod tersebut, buat aplikasi Django baru dan tambahkan kod ini ke fail model.py. Kemudian, jalankan migrasi untuk membuat jadual pangkalan data yang diperlukan. Model termasuk Catalogcampaign, BasketCampaign, Coupon, dan Couponusage. Catalogcampaign dan BasketCampaign adalah subclass kempen, yang merupakan kelas asas abstrak. Catalogcampaign mewakili kempen yang digunakan untuk satu set produk, manakala BasketCampaign mewakili kempen yang digunakan untuk seluruh bakul. Kupon mewakili kod diskaun yang boleh digunakan untuk bakul. Couponusage mewakili penggunaan kupon dalam perintah. Model -model ini mempunyai pelbagai bidang dan kaedah untuk menguruskan kempen dan kupon, termasuk memeriksa jika kempen atau kupon tersedia, melampirkan kupon ke kempen, dan memeriksa jika kupon boleh digunakan oleh pelanggan. Model -model ini juga termasuk fungsi pembalakan melalui penggunaan fungsi define_log_model.", "input": "", "output": "# This file is part of Shuup.\n#\n# Copyright (c) 2012-2017, Shoop Commerce Ltd. All rights reserved.\n#\n# This source code is licensed under the OSL-3.0 license found in the\n# LICENSE file in the root directory of this source tree.\nimport random\nimport string\n\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django.db import models\nfrom django.db.models import Q\nfrom django.utils.encoding import force_text, python_2_unicode_compatible\nfrom django.utils.timezone import now\nfrom django.utils.translation import ugettext_lazy as _\nfrom enumfields import Enum\nfrom parler.models import TranslatableModel, TranslatedFields\n\nfrom shuup.campaigns.consts import (\n    CAMPAIGNS_CACHE_NAMESPACE, CATALOG_FILTER_CACHE_NAMESPACE,\n    CONTEXT_CONDITION_CACHE_NAMESPACE\n)\nfrom shuup.campaigns.models.basket_conditions import (\n    CategoryProductsBasketCondition, ProductsInBasketCondition\n)\nfrom shuup.campaigns.utils.campaigns import get_product_ids_and_quantities\nfrom shuup.campaigns.utils.matcher import get_matching_for_product\nfrom shuup.core import cache\nfrom shuup.core.fields import InternalIdentifierField\nfrom shuup.core.models import Category, Order, Shop\nfrom shuup.core.utils import context_cache\nfrom shuup.utils.analog import define_log_model\nfrom shuup.utils.properties import MoneyPropped\n\n\nclass CampaignType(Enum):\n    CATALOG = 1\n    BASKET = 2\n\n\nclass CampaignQueryset(models.QuerySet):\n    def available(self, shop=None):\n        query = Q(\n            Q(active=True) &\n            (Q(start_datetime__isnull=True) | Q(start_datetime__lte=now())) &\n            (Q(end_datetime__isnull=True) | Q(end_datetime__gte=now()))\n        )\n        if shop:\n            query &= Q(shop=shop)\n        return self.filter(query)\n\n\nclass Campaign(MoneyPropped, TranslatableModel):\n    admin_url_suffix = None\n\n    shop = models.ForeignKey(Shop, verbose_name=_(\"shop\"), help_text=_(\"The shop where the campaign is active.\"))\n    name = models.CharField(max_length=120, verbose_name=_(\"name\"), help_text=_(\"The name for this campaign.\"))\n\n    # translations in subclass\n    identifier = InternalIdentifierField(unique=True)\n\n    active = models.BooleanField(default=False, verbose_name=_(\"active\"), help_text=_(\n        \"Check this if the campaign is currently active. Please also set a start and end date.\"\n    ))\n    start_datetime = models.DateTimeField(null=True, blank=True, verbose_name=_(\"start date and time\"), help_text=_(\n        \"The date and time the campaign starts. This is only applicable if the campaign is marked as active.\"\n    ))\n    end_datetime = models.DateTimeField(null=True, blank=True, verbose_name=_(\"end date and time\"), help_text=_(\n        \"The date and time the campaign ends. This is only applicable if the campaign is marked as active.\"\n    ))\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    objects = CampaignQueryset.as_manager()\n\n    class Meta:\n        abstract = True\n        verbose_name = _('Campaign')\n        verbose_name_plural = _('Campaigns')\n\n    def save(self, *args, **kwargs):\n        super(Campaign, self).save(*args, **kwargs)\n        cache.bump_version(CAMPAIGNS_CACHE_NAMESPACE)\n        cache.bump_version(CONTEXT_CONDITION_CACHE_NAMESPACE)\n        cache.bump_version(CATALOG_FILTER_CACHE_NAMESPACE)\n\n    def is_available(self):\n        if not self.active:  # move to manager?\n            return False\n        if self.start_datetime and self.end_datetime:\n            if self.start_datetime <= now() <= self.end_datetime:\n                return True\n            return False\n        elif self.start_datetime and not self.end_datetime:\n            if self.start_datetime > now():\n                return False\n        elif not self.start_datetime and self.end_datetime:\n            if self.end_datetime < now():\n                return False\n        return True\n\n    @property\n    def type(self):\n        return CampaignType.BASKET if isinstance(self, BasketCampaign) else CampaignType.CATALOG\n\n\nclass CatalogCampaign(Campaign):\n    _queryset = None\n\n    admin_url_suffix = \"catalog_campaign\"\n    conditions = models.ManyToManyField('ContextCondition', blank=True, related_name='campaign')\n    filters = models.ManyToManyField('CatalogFilter', blank=True, related_name='campaign')\n\n    translations = TranslatedFields(public_name=models.CharField(max_length=120, blank=True, help_text=_(\n        \"The campaign name to show in the store front.\"\n    )))\n\n    def __str__(self):\n        return force_text(_(\"Catalog Campaign: %(name)s\" % dict(name=self.name)))\n\n    def save(self, *args, **kwargs):\n        super(CatalogCampaign, self).save(*args, **kwargs)\n        self.filters.update(active=self.active)\n        for f in self.filters.all():\n            for matching_product in f.get_matching_shop_products():\n                context_cache.bump_cache_for_shop_product(matching_product)\n        self.conditions.update(active=self.active)\n\n    def rules_match(self, context, shop_product, matching_catalog_filters, matching_context_conditions):\n        if not self.is_available():\n            return False\n\n        # If rule has filters, all of them has to match\n        for filter_pk in self.filters.values_list(\"pk\", flat=True):\n            if filter_pk not in matching_catalog_filters:\n                return False\n\n        # All filters match so let's check that also all the conditions match\n        for condition_pk in self.conditions.values_list(\"pk\", flat=True):\n            if condition_pk not in matching_context_conditions:\n                return False\n        return True\n\n    @classmethod\n    def get_for_product(cls, shop_product):\n        matching_filters = get_matching_for_product(shop_product, provide_category=\"campaign_catalog_filter\")\n        matching_conditions = get_matching_for_product(shop_product, provide_category=\"campaign_context_condition\")\n        query_filter = Q(Q(filters__in=matching_filters) | Q(conditions__in=matching_conditions))\n        return cls.objects.available(shop=shop_product.shop).filter(query_filter).distinct()\n\n    @classmethod\n    def get_matching(cls, context, shop_product):\n        prod_ctx_cache_elements = dict(\n            customer=context.customer.pk or 0,\n            shop=context.shop.pk,\n            product_id=shop_product.pk)\n        namespace = CAMPAIGNS_CACHE_NAMESPACE\n        key = \"%s:%s\" % (namespace, hash(frozenset(prod_ctx_cache_elements.items())))\n        cached_matching = cache.get(key, None)\n        if cached_matching is not None:\n            return cached_matching\n\n        from shuup.campaigns.models.matching import get_matching_context_conditions, get_matching_catalog_filters\n        matching_context_conditions = get_matching_context_conditions(context)\n        matching_catalog_filters = get_matching_catalog_filters(shop_product)\n\n        if not (matching_context_conditions or matching_catalog_filters):\n            return []\n\n        # Get all possible campaign id's for matching context_conditions\n        campaigns_based_on_conditions = set(\n            cls.objects.filter(\n                active=True,\n                shop=context.shop,\n                conditions__id__in=matching_context_conditions\n            ).values_list(\"pk\", flat=True)\n        )\n\n        campaigns_based_on_catalog_filters = set()\n        if hasattr(cls, \"filters\"):\n            # Get all possible campaigns for matching catalog_filters\n            campaigns_based_on_catalog_filters = set(\n                cls.objects.filter(\n                    active=True,\n                    shop=context.shop,\n                    filters__id__in=matching_catalog_filters\n                ).values_list(\"pk\", flat=True)\n            )\n\n        all_possible_campaigns_ids = (campaigns_based_on_conditions | campaigns_based_on_catalog_filters)\n        matching = []\n        for campaign in cls.objects.filter(id__in=all_possible_campaigns_ids):\n            if campaign.rules_match(context, shop_product, matching_catalog_filters, matching_context_conditions):\n                matching.append(campaign)\n        cache.set(key, matching, timeout=None)\n        return matching\n\n\nclass BasketCampaign(Campaign):\n    admin_url_suffix = \"basket_campaign\"\n\n    basket_line_text = models.CharField(\n        max_length=120, verbose_name=_(\"basket line text\"), help_text=_(\"This text will be shown in basket.\"))\n\n    conditions = models.ManyToManyField('BasketCondition', blank=True, related_name='campaign')\n    coupon = models.OneToOneField('Coupon', null=True, blank=True, related_name='campaign', verbose_name=_(\"coupon\"))\n\n    translations = TranslatedFields(\n        public_name=models.CharField(max_length=120, verbose_name=_(\"public name\"), help_text=_(\n            \"The campaign name to show in the store front.\"\n        ))\n    )\n\n    def __str__(self):\n        return force_text(_(\"Basket Campaign: %(name)s\" % dict(name=self.name)))\n\n    def save(self, *args, **kwargs):\n        if self.coupon:\n            code_count_for_shop = BasketCampaign.objects.filter(\n                active=True, shop_id=self.shop.id, coupon__code=self.coupon.code)\n            if not self.id and code_count_for_shop.exists():\n                raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n            if self.id and code_count_for_shop.exclude(coupon_id=self.coupon.id).exists():\n                raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n        super(BasketCampaign, self).save(*args, **kwargs)\n        self.conditions.update(active=self.active)\n\n    @classmethod\n    def get_for_product(cls, shop_product):\n        matching_conditions = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_condition\")\n        matching_effects = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_discount_effect_form\")\n        matching_line_effects = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_line_effect_form\")\n        effects_q = Q(Q(line_effects__id__in=matching_line_effects) | Q(discount_effects__id__in=matching_effects))\n        matching_q = Q(Q(conditions__in=matching_conditions) | effects_q)\n        return cls.objects.available(shop=shop_product.shop).filter(matching_q).distinct()\n\n    @classmethod\n    def get_matching(cls, basket, lines):\n        matching = []\n        exclude_condition_ids = set()\n        product_id_to_qty = get_product_ids_and_quantities(basket)\n\n        # Get ProductsInBasketCondition's that can't match with the basket\n        products_in_basket_conditions_to_check = set(\n            ProductsInBasketCondition.objects.filter(\n                products__id__in=product_id_to_qty.keys()\n            ).values_list(\"id\", flat=True)\n        )\n        exclude_condition_ids |= set(\n            ProductsInBasketCondition.objects.exclude(\n                id__in=products_in_basket_conditions_to_check\n            ).values_list(\"id\", flat=True)\n        )\n\n        # Get CategoryProductsBasketCondition's that can't match with the basket\n        categories = set(Category.objects.filter(\n            shop_products__product_id__in=product_id_to_qty.keys()).values_list(\"id\", flat=True))\n        category_products_in_basket_to_check = set(\n            CategoryProductsBasketCondition.objects.filter(categories__in=categories).values_list(\"id\", flat=True)\n        )\n        exclude_condition_ids |= set(\n            CategoryProductsBasketCondition.objects.exclude(\n                id__in=category_products_in_basket_to_check\n            ).values_list(\"id\", flat=True)\n        )\n\n        queryset = cls.objects.filter(active=True, shop=basket.shop)\n        if exclude_condition_ids:\n            queryset = queryset.exclude(conditions__id__in=exclude_condition_ids)\n        for campaign in queryset.prefetch_related(\"conditions\"):\n            if campaign.rules_match(basket, lines):\n                matching.append(campaign)\n        return matching\n\n    def rules_match(self, basket, lines):\n        \"\"\"\n        Check if basket rules match.\n\n        They will not match if\n        1) The campaign is not active\n        2) The campaign has attached coupon\n           which doesn't match or is not active\n        3) Any of the attached rules doesn't match\n        \"\"\"\n        if not self.is_available():\n            return False\n\n        if self.coupon and not (self.coupon.active and self.coupon.code.upper() in [c.upper() for c in basket.codes]):\n            return False\n\n        for rule in self.conditions.all():\n            if not rule.matches(basket, lines):\n                return False\n        return True\n\n\nclass CouponUsage(models.Model):\n    coupon = models.ForeignKey('Coupon', related_name='usages')\n    order = models.ForeignKey(Order, related_name='coupon_usages')\n\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    @classmethod\n    def add_usage(cls, order, coupon):\n        return cls.objects.create(order=order, coupon=coupon)\n\n\n@python_2_unicode_compatible\nclass Coupon(models.Model):\n    admin_url_suffix = \"coupon\"\n    name_field = \"code\"  # TODO: Document me\n    search_fields = [\"code\"]    # used by Select2Multiple to know which fields use to search by\n\n    code = models.CharField(max_length=12)\n\n    usage_limit_customer = models.PositiveIntegerField(\n        blank=True, null=True,\n        verbose_name=_(\"usage limit per customer\"), help_text=_(\"Limit the amount of usages per a single customer.\"))\n    usage_limit = models.PositiveIntegerField(\n        blank=True, null=True,\n        verbose_name=_(\"usage limit\"),\n        help_text=_(\"Set the absolute limit of usages for this coupon. \"\n                    \"If the limit is zero (0) coupon cannot be used.\"))\n\n    active = models.BooleanField(default=False, verbose_name=_(\"is active\"))\n\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    def save(self, **kwargs):\n        campaign = BasketCampaign.objects.filter(active=True, coupon_id=self.id).first()\n        if campaign and BasketCampaign.objects.filter(\n                active=True, shop_id=campaign.shop.id, coupon__code=self.code).exclude(id=campaign.id).exists():\n            raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n        return super(Coupon, self).save(**kwargs)\n\n    @classmethod\n    def generate_code(cls, length=6):\n        if length > 12:\n            length = 12\n        return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n\n    @property\n    def exhausted(self):\n        val = bool(self.usage_limit and self.usages.count() >= self.usage_limit)\n        return val\n\n    @property\n    def attached(self):\n        return BasketCampaign.objects.filter(coupon=self).exists()\n\n    def attach_to_campaign(self, campaign):\n        if not self.attached:\n            self.campaign = campaign\n\n    @classmethod\n    def is_usable(cls, code, customer):\n        try:\n            code = cls.objects.get(code__iexact=code, active=True)\n            return code.can_use_code(customer)\n        except cls.DoesNotExist:\n            return False\n\n    def can_use_code(self, customer):\n        \"\"\"\n        Check if customer can use the code\n\n        :param customer:\n        :type customer: `Contact` or None\n        :rtype: True|False\n        \"\"\"\n        if not self.active:\n            return False\n\n        if not self.attached:\n            return False\n\n        if self.usage_limit_customer:\n            if not customer or customer.is_anonymous:\n                return False\n            if (self.usages.filter(order__customer=customer, coupon=self).count() >= self.usage_limit_customer):\n                return False\n\n        return not self.exhausted\n\n    def use(self, order):\n        return CouponUsage.add_usage(order=order, coupon=self)\n\n    def increase_customer_usage_limit_by(self, amount):\n        if self.usage_limit_customer:\n            new_limit = self.usage_limit_customer + amount\n        else:\n            new_limit = self.usages.count() + amount\n        self.usage_limit_customer = new_limit\n\n    def increase_usage_limit_by(self, amount):\n        self.usage_limit = self.usage_limit + amount if self.usage_limit else (self.usages.count() + amount)\n\n    def has_been_used(self, usage_count=1):\n        \"\"\" See if code is used the times given \"\"\"\n        return CouponUsage.objects.filter(coupon=self).count() >= usage_count\n\n    def __str__(self):\n        return self.code\n\n\nCatalogCampaignLogEntry = define_log_model(CatalogCampaign)\nBasketCampaignLogEntry = define_log_model(BasketCampaign)\nCouponLogEntry = define_log_model(Coupon)\nCouponUsageLogEntry = define_log_model(CouponUsage)", "instruction_en": "This code defines models for campaigns and coupons in a Shuup e-commerce platform. To use the code, create a new Django app and add this code to the models.py file. Then, run migrations to create the necessary database tables. The models include CatalogCampaign, BasketCampaign, Coupon, and CouponUsage. CatalogCampaign and BasketCampaign are subclasses of Campaign, which is an abstract base class. CatalogCampaign represents a campaign that applies to a set of products, while BasketCampaign represents a campaign that applies to the entire basket. Coupon represents a discount code that can be applied to a basket. CouponUsage represents the usage of a coupon in an order. The models have various fields and methods for managing campaigns and coupons, including checking if a campaign or coupon is available, attaching a coupon to a campaign, and checking if a coupon can be used by a customer. The models also include logging functionality through the use of the define_log_model function.", "original_line_number": 11010, "instruction_length": 1007}
{"instruction": "Buat kelas rangkaian saraf dengan kaedah untuk memainkan langkah dan tetapkan semula rangkaian. Gunakan perpustakaan `neurolab` untuk membuat rangkaian saraf feedforward dengan 64 neuron input dan 1 neuron output. Kaedah `play_move` mengambil objek permainan dan nilai epsilon pilihan, menghasilkan kemungkinan bergerak, dan mengembalikan langkah terbaik berdasarkan ramalan rangkaian. Kaedah `Reset` melatih rangkaian pada data yang dikumpulkan dan menetapkan semula pembolehubah kelas. Kaedah `reset_without_train` menetapkan semula pembolehubah kelas tanpa melatih rangkaian. Kod ini juga termasuk fungsi utama yang memainkan permainan terhadap pemain yang sistematik dan kemudian menentang dirinya dengan nilai Epsilon yang berkurangan, dan mengeluarkan jumlah kemenangan.", "input": "", "output": "import neurolab as nl\nimport game2\nimport othello\nimport ntuplesystematic as nts\nimport time\nimport random\n\nclass nn:\n\tdef __init__(self):\n\t\tself.x = [[-1,1] for x in range(64)]\n\t\tself.net = nl.net.newff(self.x,[1])\n\t\t#self.net.trainf = nl.train.train_gd\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\n\tdef play_move(self,game,epsilon = 0):\n\t\tmoves = game.generate_moves()\n\t\tnum = random.uniform(0,1)\n\t\tif(num <= epsilon):\n\t\t\ttemp = game.copy()\n\t\t\tif(game.player==-1):\n\t\t\t\tif(self.moveb == 0):\n\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\tv=[]\n\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\tv2 = [v]\n\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\tself.moveb = self.moveb+1\n\t\t\t\t\tself.last_vb = v\n\t\t\t\t\treturn (v1[0][0], move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 0\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv=[]\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 1\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (v1[0],move)\n\t\t\telse:\n\t\t\t\tif(self.movew == 0):\n\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\tv=[]\n\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\tv2 = [v]\n\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\tself.movew = self.movew+1\n\t\t\t\t\tself.last_vw = v\n\t\t\t\t\treturn (v1[0][0], move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 2\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv=[]\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 3\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (v1[0],move)\n\t\telse:\n\t\t\tif(game.player == -1):\n\t\t\t\tif(self.moveb==0):\n\t\t\t\t\tj=0\n\t\t\t\t\tmax1 = 0\n\t\t\t\t\tbest_v = 0\n\t\t\t\t\tbest_move = None\n\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t#print v\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\telif(v1[0][0]>max1):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\tj = j+1\n\t\t\t\t\tself.moveb = self.moveb+1\n\t\t\t\t\tself.last_vb = best_v\n\t\t\t\t\treturn (max1, best_move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 4\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tj=0\n\t\t\t\t\t\tmax1 = 0\n\t\t\t\t\t\tbest_v = 0\n\t\t\t\t\t\tbest_move = 0\n\t\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\t\tv = []\n\t\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t\t#print v\n\t\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\telif(v1[0][0]>max1):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\t\tj = j+1\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(best_move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv2 = [best_v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 5\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = best_v\n\t\t\t\t\t\treturn (max1,best_move)\n\t\t\t\t\n\t\t\telse:\n\t\t\t\tif(self.movew==0):\n\t\t\t\t\tj=0\n\t\t\t\t\tmax1 = 0\n\t\t\t\t\tbest_v = 0\n\t\t\t\t\tbest_move = 0\n\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t#print v\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\telif(v1[0][0]<max1):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\tj = j+1\n\t\t\t\t\tself.movew = self.movew+1\n\t\t\t\t\tself.last_vw = best_v\n\t\t\t\t\treturn (max1,best_move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 6\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tj=0\n\t\t\t\t\t\tmax1 = 0\n\t\t\t\t\t\tbest_v = 0\n\t\t\t\t\t\tbest_move = 0\n\t\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\t\tv = []\n\t\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t\t#print v\n\t\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\telif(v1[0][0]<max1):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\t\tj = j+1\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(best_move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv2 = [best_v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 7\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = best_v\n\t\t\t\t\t\treturn (max1,best_move)\n\n\tdef reset(self):\n\t\t#print self.fin_v\n\t\t#print self.fin_val\n\t\terror = self.net.train(self.fin_v,self.fin_val,epochs=5,show=1)\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\n\tdef reset_without_train(self):\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tCreates a main player\n\t\"\"\"\n\tplayernew = nn()\n\tnTuplesSystematicObject = nts.nTuplesSystematic()\n\tgame2.play(othello.game(), game2.player(lambda x: playernew.play_move(x)),game2.player(lambda x: nTuplesSystematicObject.play_next_move(x)), True)\n\tplayernew.reset_without_train()\n\ttime.sleep(5)\n\tk = 100\n\tfor i in range(k):\n\t\tprint(i)\n\t\tgame2.play(othello.game(), game2.player(lambda x: playernew.play_move(x,0.3)),game2.player(lambda x: playernew.play_move(x,0.3)), False)\n\t\tplayernew.reset()\n\n\twins = [0, 0]\n\tfor i in range(100):\n\t\twinner = game2.play(othello.game(), game2.player_epsilon(lambda x: playernew.play_move(x)),game2.player_epsilon(lambda x: nTuplesSystematicObject.play_next_move(x)), False)\n\t\tif winner == 1:\n\t\t\twins[0] += 1\n\t\telif winner == 2:\n\t\t\twins[1] += 1\n\t\twinner = game2.play(othello.game(),game2.player_epsilon(lambda x: nTuplesSystematicObject.play_next_move(x)), game2.player_epsilon(lambda x: playernew.play_move(x)), False)\n\t\tif winner == 2:\n\t\t\twins[0] += 1\n\t\telif winner == 1:\n\t\t\twins[1] += 1\n\n\tprint wins\n\tf = open('results','a')\n\tval = (k,0.001,'epsilon',wins)\n\tval = str(val)", "instruction_en": "Create a neural network class with methods to play moves and reset the network. Use the `neurolab` library to create a feedforward neural network with 64 input neurons and 1 output neuron. The `play_move` method takes a game object and an optional epsilon value, generates possible moves, and returns the best move based on the network's predictions. The `reset` method trains the network on the collected data and resets the class variables. The `reset_without_train` method resets the class variables without training the network. The code also includes a main function that plays games against a systematic player and then against itself with a decreasing epsilon value, and outputs the win counts.", "original_line_number": 11160, "instruction_length": 776}
{"instruction": "Arahan:\n1. Pastikan anda mempunyai kebergantungan yang diperlukan, termasuk utils, lemmatizer, iGraph, numpy, dan cpickle.\n2. Tetapkan nilai yang dikehendaki untuk pembolehubah berikut: rgx, min_length, min_occ, max_freq, min_size, max_size, data_dir, pickle_dir1, dan pickle_dir2.\n3. Jalankan program dengan melaksanakan kod di bawah jika __name__ == '__main__': pernyataan.\n4. Program ini akan memaksimumkan fail dalam direktori data yang ditentukan, menapisnya berdasarkan kriteria yang ditentukan, dan menjana matriks collocation untuk fail yang layak.\n5. Perangkaan token yang terhasil dan matriks collocation akan disimpan sebagai fail acar dalam direktori yang ditentukan.", "input": "", "output": "#\n# This file is part of DF.\n# \n# DF is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation;\n# either version 3 of the License, or (at your option) any\n# later version.\n#\n# Latassan is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied\n# warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR\n# PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public\n# License along with DF; see the file COPYING.  If not\n# see <http://www.gnu.org/licenses/>.\n# \n# Copyright (C) 2014-2019 Jimmy Dubuisson <jimmy.dubuisson@gmail.com>\n#\n\nfrom __future__ import division\nfrom utils import *\nfrom lemmatizer import *\nfrom igraph import Graph\nfrom numpy import dot\nimport cPickle as pickle\n\nif __name__ == '__main__':\n        rgx = '\\w+'\n        #punct = '\\',.!?'\n        min_length = 3\n        # min number of occurences\n        min_occ = 3\n        # max frequency (between 0 and 1)\n        max_freq = 1\n\t# min number of tokens\n        min_size = 100\n        # max number of tokens\n        max_size = 1000\n\t# folder path\n\tdata_dir = sys.argv[1]\n\tpickle_dir1 = sys.argv[2]\n\tpickle_dir2 = sys.argv[3]\n\t# collocation metrics instance to be used\n        #cmetrics = CollocationMetrics(CollocationMetrics.decreasing_exp,(1,1),CollocationMetrics.do_nothing,())\n        cmetrics = CollocationMetrics(CollocationMetrics.decreasing_exp,(1,1),CollocationMetrics.information,())\n\t# batch replace arrays\n\tvold = ['</?blog>','</?Blog>','</?post>','<date>.*</date>','nbsp','urlLink']\n\tvnew = ['','','','','','']\n\t\t\n\tfnames = FileUtils.get_files_list(data_dir)\n\n\tcounter = 1\n\tmax_count = 2000\n\tsuccess_count = 0\n\tfor p in fnames:\n\t\tif success_count == max_count:\n\t\t\tbreak\n\t\tprint counter, '- Tokenizing: ', p \n\t\tcounter += 1\n\t\ttxt = FileUtils.read_text_file(data_dir + '/' + p)\n\t\ttxt = FileUtils.batch_replace(txt,vold,vnew)\n        \tdoc = DocStats(txt, rgx, min_length, min_occ, max_freq, cmetrics)\n\t\tprint '# tokens: ', len(doc.token_set) \n        \tif len(doc.token_set) >= min_size and len(doc.token_set) <= max_size:\n\t\t\tmat = doc.get_collocation_mat()\n\t\t\tprint '# rows: ', mat.dim\n\t\t\tprint '# nnz entries: ', mat.vmat.nnz\n\t\t\tif mat:\n\t\t\t\tsuccess_count += 1\n\t\t\t\tpickle.dump(doc.token_stats, open(pickle_dir1 + '/' + p.replace('.xml','') + \".p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n\t\t\t\tpickle.dump(mat, open(pickle_dir2 + '/' + p.replace('.xml','') + \".p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n\t\tprint '---'", "instruction_en": "Instructions:\n1. Make sure you have the necessary dependencies installed, including utils, lemmatizer, igraph, numpy, and cPickle.\n2. Set the desired values for the following variables: rgx, min_length, min_occ, max_freq, min_size, max_size, data_dir, pickle_dir1, and pickle_dir2.\n3. Run the program by executing the code under the if __name__ == '__main__': statement.\n4. The program will tokenize the files in the specified data directory, filter them based on the specified criteria, and generate collocation matrices for qualifying files.\n5. The resulting token statistics and collocation matrices will be saved as pickle files in the specified directories.", "original_line_number": 11357, "instruction_length": 679}
{"instruction": "Kod ini adalah pengurus kunci/kata laluan mini yang ditulis dalam Python menggunakan algoritma penyulitan AES. Untuk menggunakannya, buat contoh kelas Keybox dengan laluan fail untuk menyimpan kunci. Anda kemudian boleh menyenaraikan, mencari, menambah, melihat, mengubah suai, atau memadam kekunci. Untuk mengimport kunci dari fail teks, gunakan sub-komitmen \"import\" diikuti dengan laluan fail. Untuk mengeksport kunci ke fail, gunakan sub-komando \"Eksport\" diikuti dengan laluan fail pilihan. Untuk menetapkan semula kata laluan induk, gunakan sub-komando \"Reset\".", "input": "", "output": "#!/usr/bin/env python\n\n\"\"\"A mini key/password manager written in python using the AES encryption algorithm.\"\"\"\n\nimport os\nimport sys\nimport time\nimport os.path\nimport random\nimport sqlite3\nimport hashlib\nimport getpass\nimport argparse\n\nimport Crypto.Cipher.AES\n\n\nclass KeyBox(object):\n    TABLE_NAME = \"keybox\"\n    MASTER_KEY_TITLE = \"<MASTER>\"\n\n    def __init__(self, a_file):\n        # the AES key of the master password, to encrypt key content\n        self.aes_key = None\n\n        self.conn = sqlite3.connect(a_file)\n        # Use 8-bit string instead of unicode string, in order to read/write\n        # international characters like Chinese\n        self.conn.text_factory = str\n        # The following line would use unicode string\n        # self.conn.text_factory = lambda x: unicode(x, 'utf-8', 'ignore')\n        self.cursor = self.conn.cursor()\n        self.cursor.execute('CREATE TABLE IF NOT EXISTS %s (title TEXT PRIMARY KEY, time LONG, content BLOB)' %\n                            KeyBox.TABLE_NAME)\n        self.conn.commit()\n\n    def list(self):\n        title_time_list = []\n        self.cursor.execute('SELECT title,time FROM %s ORDER BY time DESC' % KeyBox.TABLE_NAME)\n        for row in self.cursor:\n            if row[0] != KeyBox.MASTER_KEY_TITLE:\n                title_time_list.append((row[0], row[1]))\n        return title_time_list\n\n    def search(self, keywords):\n        keywords_lower = {keyword.lower() for keyword in keywords}\n        matching_title_time_list = []\n        for title, mod_time in self.list():\n            title_lower = title.lower()\n            match = True\n            for keyword in keywords_lower:\n                if title_lower.find(keyword) == -1:\n                    match = False\n                    break\n            if match:\n                matching_title_time_list.append((title, mod_time))\n        return matching_title_time_list\n\n    def exists(self, title):\n        self.cursor.execute(\"SELECT time FROM %s WHERE title=?\" % KeyBox.TABLE_NAME, (title,))\n        return self.cursor.fetchone() is not None\n\n    def init_master_password(self, table=TABLE_NAME):\n        password = input_password(\"Create a new master password: \")\n        if password == input_password(\"Confirm the master password: \"):\n            self.aes_key = hashlib.sha256(password).digest()\n            # the hash of the AES key, stored in db for master password verification\n            key_hash = hashlib.sha256(self.aes_key).hexdigest()\n            self.cursor.execute(\"INSERT OR REPLACE INTO %s VALUES (?,?,?)\" % table,\n                                (KeyBox.MASTER_KEY_TITLE, time.time(), key_hash))\n            self.conn.commit()\n        else:\n            exit_with_error(\"Error: password not match, please retry\")\n\n    def verify_master_password(self):\n        # get the stored key hash\n        self.cursor.execute(\"SELECT content FROM %s WHERE title=?\"\n                            % KeyBox.TABLE_NAME, (KeyBox.MASTER_KEY_TITLE,))\n        stored_key_hash = self.cursor.fetchone()[0]\n        # input master password\n        password = input_password(\"Master password: \")\n        self.aes_key = hashlib.sha256(password).digest()\n        # compare key hash\n        if hashlib.sha256(self.aes_key).hexdigest() != stored_key_hash:\n            exit_with_error(\"Error: incorrect master password, please retry\")\n\n    def view(self, title):\n        self.cursor.execute(\"SELECT time, content FROM %s WHERE title=?\"\n                            % KeyBox.TABLE_NAME, (title,))\n        mod_time, encrypted = self.cursor.fetchone()\n        return mod_time, decrypt(encrypted, self.aes_key)\n\n    def set(self, title, plain, mod_time=time.time(), table=TABLE_NAME):\n        # for better print effect\n        if plain[-1] != \"\\n\": plain += \"\\n\"\n        encrypted = encrypt(plain, self.aes_key)\n        self.cursor.execute(\"INSERT OR REPLACE INTO %s VALUES (?,?,?)\" % table,\n                            (title, mod_time, sqlite3.Binary(encrypted)))\n        self.conn.commit()\n\n    def delete(self, title):\n        mod_time, plain = self.view(title)\n        self.cursor.execute(\"DELETE FROM %s WHERE title=?\" % KeyBox.TABLE_NAME, (title,))\n        self.conn.commit()\n        return mod_time, plain\n\n    def reset(self):\n        tmp_table = \"_tmp_\"\n        self.cursor.execute('DROP TABLE IF EXISTS %s' % tmp_table)\n        self.cursor.execute('CREATE TABLE %s (title TEXT PRIMARY KEY, time LONG, content BLOB)' % tmp_table)\n        keys = []\n        for title, mod_time in self.list():\n            content = self.view(title)\n            keys.append((title, mod_time, content))\n        self.init_master_password(table=tmp_table)\n        for title, mod_time, content in keys:\n            self.set(title, content, mod_time=mod_time, table=tmp_table)\n        self.cursor.execute(\"DROP TABLE %s\" % KeyBox.TABLE_NAME)\n        self.cursor.execute(\"ALTER TABLE %s RENAME TO %s\" % (tmp_table, KeyBox.TABLE_NAME))\n        self.conn.commit()\n\n\ndef input_content(title):\n    sys.stdout.write(\"Input content of '%s', enter an empty line to finish:\\n\" % title)\n    lines = []\n    while True:\n        line = raw_input()\n        if line:\n            lines.append(line)\n        else:\n            break\n    return '\\n'.join(lines)\n\n\ndef input_password(text):\n    password = getpass.getpass(text)\n    if password == \"\":\n        exit_with_error(\"Error: password not given\")\n    return password\n\n\ndef encrypt(plain, aes_key):\n    iv = ''.join(chr(random.randint(0, 0xFF)) for _ in range(Crypto.Cipher.AES.block_size))\n    cipher = Crypto.Cipher.AES.AESCipher(aes_key, Crypto.Cipher.AES.MODE_CFB, iv)\n    return iv + cipher.encrypt(plain)\n\n\ndef decrypt(encrypted, aes_key):\n    iv = encrypted[0:Crypto.Cipher.AES.block_size]\n    cipher = Crypto.Cipher.AES.AESCipher(aes_key, Crypto.Cipher.AES.MODE_CFB, iv)\n    return cipher.decrypt(encrypted[Crypto.Cipher.AES.block_size:])\n\n\ndef read_keys(a_file):\n    \"\"\"\n    Supported text file format is as follows:\n\n    KEY: key title1\n    MOD: 1540820240.0\n    key content line 11\n    key content line 12\n    ...\n\n    KEY: key title2\n    key content line 21\n    key content line 22\n    ...\n\n    \"\"\"\n    keys = []\n    with open(a_file, 'r') as fd:\n        title = ''\n        mod_time = -1\n        content_lines = []\n        for line in fd:\n            line = line.strip()\n            if line.startswith(\"KEY: \"):  # title line\n                if title != '' and content_lines != []:\n                    # remove the empty lines at the end\n                    while len(content_lines) > 0 and content_lines[-1] == \"\\n\":\n                        content_lines = content_lines[:-1]\n                    # add to keys for return\n                    if mod_time < 0: mod_time = time.time()\n                    keys.append((title, mod_time, '\\n'.join([aLine for aLine in content_lines])))\n                # set next key title, and clear content\n                title = line[5:]\n                content_lines = []\n            elif line.startswith(\"MOD: \"):\n                mod_time = float(line[5:])\n            elif title != \"\":\n                content_lines.append(line)\n            else:\n                sys.stderr.write(\"Warn: line '%s' ignored: title missing\\n\" % line)\n    # process the last key\n    if title != '' and content_lines != []:\n        # remove the empty lines at the end\n        while len(content_lines) > 0 and content_lines[-1] == \"\\n\":\n            content_lines = content_lines[:-1]\n        # add to keys for return\n        if mod_time < 0: mod_time = time.time()\n        keys.append((title, mod_time, '\\n'.join([aLine for aLine in content_lines])))\n\n    return keys\n\n\ndef exit_with_error(err_msg, err_code=-1):\n    sys.stderr.write(err_msg + \"\\n\")\n    sys.exit(err_code)\n\n\ndef get_default_db_file():\n    keybox_file = \"%s/.keybox\" % os.environ['HOME']\n    if not os.path.exists(keybox_file):\n        return \"%s/%s.keybox\" % (os.environ['HOME'], os.environ['USER'])\n\n    with open(keybox_file, 'r') as fd:\n        for line in fd:\n            return line\n\n\ndef set_default_db_file(a_file):\n    keybox_file = \"%s/.keybox\" % os.environ['HOME']\n    with open(keybox_file, 'w') as fd:\n        fd.write(os.path.abspath(a_file))\n\n\ndef main():\n    # parse command line arguments\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument('-d', '--database',\n                        help=('the sqlite database file to store keys. ' +\n                              'Default: the previously used database file (see its location in %s/.keybox), ' +\n                              'or %s/%s.keybox') % (os.environ[\"HOME\"], os.environ[\"HOME\"], os.environ['USER']))\n    subparsers = parser.add_subparsers(title=\"sub_commands\", dest=\"action\",\n                                       metavar='help|list|view|add|mod|del|import|export|reset')\n    subparsers.add_parser(\"help\", help=\"show this help message and exit\")\n\n    subparsers.add_parser(\"list\", help=\"list all key titles (this is default)\")\n\n    sub_parser = subparsers.add_parser(\"add\", help=\"add a new key title and content\")\n    sub_parser.add_argument(\"title\", help=\"a key title\")\n\n    sub_parser = subparsers.add_parser(\"view\", help=\"view the content for the key title matching the given keywords\")\n    sub_parser.add_argument(\"keyword\", nargs=\"+\", help=\"a keyword\")\n    sub_parser = subparsers.add_parser(\"mod\", help=\"modify the content for the key title matching the given keywords\")\n    sub_parser.add_argument(\"keyword\", nargs=\"+\", help=\"a keyword\")\n    sub_parser = subparsers.add_parser(\"del\",\n                                       help=\"delete an existing key title matching the given keywords and the key \" +\n                                            \"content\")\n    sub_parser.add_argument(\"keyword\", nargs=\"+\", help=\"a keyword\")\n\n    sub_parser = subparsers.add_parser(\"import\", help=\"import all key titles and contents from a text file\")\n    sub_parser.add_argument(\"file\", help=\"a text file containing key titles and contents to import\")\n    sub_parser = subparsers.add_parser(\"export\", help=\"export all key titles and contents to stdout or a file\")\n    sub_parser.add_argument(\"file\", nargs='?', help=\"a text file to export the key titles and contents\")\n\n    subparsers.add_parser(\"reset\", help=\"reset the master password\")\n\n    # 'list' if no sub-command is given\n    if len(sys.argv) == 1:\n        sys.argv.append('list')\n\n    args = parser.parse_args()\n\n    if args.action == 'help':\n        parser.print_help()\n        sys.exit(0)\n\n    if args.database is None:\n        args.database = get_default_db_file()  # type: str\n    else:\n        set_default_db_file(args.database)\n    keybox = KeyBox(args.database)\n    if args.action == 'list':\n        title_time_array = keybox.list()\n        if len(title_time_array) == 0:\n            sys.stdout.write(\"No item found\\n\")\n        else:\n            for title, mod_time in title_time_array:\n                print time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(mod_time)) + \" - \" + title\n        sys.exit(0)\n\n    # check errors before init or verify master password\n    if args.action == 'add':\n        if keybox.exists(args.title):\n            exit_with_error(\"Error: '%s' exists, try to view it or add with another title\" % args.title)\n    if args.action in ['view', 'mod', 'del']:\n        matches = keybox.search(args.keyword)\n        if len(matches) == 0:\n            exit_with_error(\n                \"Error: no title matching the given keywords, try to list all titles or change to another title\")\n        else:\n            sys.stdout.write(\"Found the following titles:\\n\")\n            for index, (title, mod_time) in enumerate(matches):\n                mod_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(mod_time))\n                print \"[%d] %s - %s\" % (index, mod_str, title)\n\n            index = 0\n            if len(matches) > 1:\n                index = -1\n                while index < 0 or index >= len(matches):\n                    index = raw_input(\"Select: [0] \").strip()\n                    if len(index) == 0:\n                        index = 0\n                        break\n                    else:\n                        try:\n                            index = int(index)\n                        except ValueError:\n                            pass\n\n            args.title = matches[index][0]\n\n    elif args.action == \"import\":\n        if not os.path.exists(args.file):\n            exit_with_error(\"Error: file '%s' not found.\" % args.file)\n    elif args.action == \"export\":\n        fd = sys.stdout\n        if args.file is not None:\n            if os.path.exists(args.file):\n                exit_with_error(\"Error: file exists, please choose a different file to export\")\n            else:\n                fd = open(args.file, 'w')\n    elif args.action == \"reset\":\n        if not keybox.exists(KeyBox.MASTER_KEY_TITLE):\n            exit_with_error(\"Error: master password is not set yet\")\n\n    if not keybox.exists(KeyBox.MASTER_KEY_TITLE):\n        keybox.init_master_password()\n    else:\n        keybox.verify_master_password()\n\n    if args.action == 'add':\n        plain = input_content(args.title)\n        keybox.set(args.title, plain)\n    elif args.action == \"view\":\n        mod_time, plain = keybox.view(args.title)\n        mod_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(mod_time))\n        sys.stdout.write(\"---\\nKEY: %s\\nMOD: %s\\n%s---\\n\" % (args.title, mod_str, plain))\n    elif args.action == \"mod\":\n        sys.stdout.write(\"---\\n%s---\\n\" % keybox.view(args.title)[1])\n        plain = input_content(args.title)\n        keybox.set(args.title, plain)\n    elif args.action == \"del\":\n        mod_time, plain = keybox.view(args.title)\n        mod_str = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(mod_time))\n        sys.stdout.write(\"---\\nKEY: %s:\\nMOD: %s\\n%s---\\n\" % (args.title, mod_str, plain))\n        confirm = raw_input(\"Confirm to delete key '%s' [yes/no]? \" % args.title)\n        while confirm not in ['yes', 'no']:\n            confirm = raw_input(\"Confirm to delete key '%s' [yes/no]? \" % args.title)\n        if confirm == 'yes':\n            keybox.delete(args.title)\n            sys.stdout.write(\"Deleted.\\n\")\n    elif args.action == \"import\":\n        for title, mod_time, content in read_keys(args.file):\n            if keybox.exists(title):\n                sys.stdout.write(\"skipped %s: exists in database\\n\" % title)\n            else:\n                keybox.set(title, content, mod_time=mod_time)\n                sys.stdout.write(\"imported %s\\n\" % title)\n    elif args.action == \"export\":\n        if fd == sys.stdout: fd.write(\"---\\n\")\n        for title, mod_time in keybox.list():\n            fd.write(\"KEY: %s\\n\" % title)\n            fd.write(\"MOD: %s\\n\" % mod_time)\n            fd.write(\"%s\" % keybox.view(title)[1])\n            if fd == sys.stdout:\n                fd.write(\"---\\n\")\n            else:\n                fd.write(\"\\n\")\n        if fd != sys.stdout:\n            sys.stdout.write(\"Exported to file %s\\n\" % args.file)\n    elif args.action == \"reset\":\n        keybox.reset()\n\n\nif __name__ == '__main__':\n    try:\n        main()\n    except KeyboardInterrupt:\n        sys.stdout.write(\"\\nUser aborted.\\n\")", "instruction_en": "This code is a mini key/password manager written in Python using the AES encryption algorithm. To use it, create an instance of the KeyBox class with a file path to store the keys. You can then list, search, add, view, modify, or delete keys. To import keys from a text file, use the \"import\" sub-command followed by the file path. To export keys to a file, use the \"export\" sub-command followed by an optional file path. To reset the master password, use the \"reset\" sub-command.", "original_line_number": 11840, "instruction_length": 567}
{"instruction": "Buat widget tersuai dengan tableview dan bar alat yang memaparkan semua `DataFramemodels` dan membolehkan penyuntingan ini (edit data, menambah/mengeluarkan, baris/lajur). Widget mempunyai lima butang: EditButton, AddColumnButton, AddrowButton, RemoveColumnButton, dan RemoverowButton. Widget ini juga mempunyai kelas dragtable yang mewarisi dari `qtgui.qtableview` dan membolehkan memulakan operasi seret dengan` pandascellpayload` pada indeks yang ditetapkan. Kelas `DataTableWidget` mempunyai beberapa kaedah yang juga slot, termasuk` Enableediting`, `UncheckButton`,` AddColumn`, `showaddColumndialog`,` addrow`, `removerow` `Model`,` ViewModel`, `View`,` UpdateDelegate`, `UpdateLegates`, dan` PemilihanModel`.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom pandasqt.compat import QtCore, QtGui, Qt, Slot, Signal\n\nfrom pandasqt.models.DataFrameModel import DataFrameModel\nfrom pandasqt.views.EditDialogs import AddAttributesDialog, RemoveAttributesDialog\nfrom pandasqt.views.CustomDelegates import createDelegate\nfrom pandasqt.models.mime import PandasCellPayload, MimeData\nfrom pandasqt.models.SupportedDtypes import SupportedDtypes\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n    \nclass DragTable(QtGui.QTableView):\n    \n    def __init__(self, parent=None):\n        \"\"\"create a table view with the ability to start drag operations\"\"\"\n        super(DragTable, self).__init__(parent)\n        self.setDragEnabled(True)\n\n    def startDrag(self, index):\n        \"\"\"start a drag operation with a PandasCellPayload on defined index.\n        \n        Args:\n            index (QModelIndex): model index you want to start the drag operation.\n        \"\"\"\n\n        if not index.isValid():\n            return\n        \n        dataFrame = self.model().dataFrame()\n\n        # get all infos from dataFrame\n        dfindex = dataFrame.iloc[[index.row()]].index\n        columnName = dataFrame.columns[index.column()]\n        dtype = dataFrame[columnName].dtype\n        value = dataFrame[columnName][dfindex]\n\n        # create the mime data\n        mimePayload = PandasCellPayload(\n            dfindex,\n            columnName,\n            value,\n            dtype,\n            hex(id(self.model()))\n        )\n        mimeData = MimeData()\n        mimeData.setData(mimePayload)\n                \n        # create the drag icon and start drag operation\n        drag = QtGui.QDrag(self)\n        drag.setMimeData(mimeData)\n        pixmap = QtGui.QPixmap(\":/icons/insert-table.png\")\n        drag.setHotSpot(QtCore.QPoint(pixmap.width()/3, pixmap.height()/3))\n        drag.setPixmap(pixmap)\n        result = drag.start(Qt.MoveAction)\n\n    def mouseMoveEvent(self, event):\n        super(DragTable, self).mouseMoveEvent(event)\n        self.startDrag(self.indexAt(event.pos()))\n\nclass DataTableWidget(QtGui.QWidget):\n    \"\"\"A Custom widget with a TableView and a toolbar.\n\n    This widget shall display all `DataFrameModels` and\n    enable the editing of this (edit data, adding/removing,\n    rows/columns).\n\n    \"\"\"\n    def __init__(self, parent=None, iconSize=QtCore.QSize(36, 36)):\n        \"\"\"Constructs the object with the given parent.\n\n        Args:\n            parent (QObject, optional): Causes the objected to be owned\n                by `parent` instead of Qt. Defaults to `None`.\n            iconSize (QSize, optional): Size of edit buttons. Defaults to QSize(36, 36).\n\n        \"\"\"\n        super(DataTableWidget, self).__init__(parent)\n        self._iconSize = iconSize\n        self.initUi()\n\n\n    def initUi(self):\n        \"\"\"Initalizes the Uuser Interface with all sub widgets.\n\n        \"\"\"\n        self.gridLayout = QtGui.QGridLayout(self)\n        self.gridLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.buttonFrame = QtGui.QFrame(self)\n        #self.buttonFrame.setMinimumSize(QtCore.QSize(250, 50))\n        #self.buttonFrame.setMaximumSize(QtCore.QSize(250, 50))\n        self.buttonFrame.setFrameShape(QtGui.QFrame.NoFrame)\n        spacerItemButton = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)\n\n        self.buttonFrameLayout = QtGui.QGridLayout(self.buttonFrame)\n        self.buttonFrameLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.editButton = QtGui.QToolButton(self.buttonFrame)\n        self.editButton.setObjectName('editbutton')\n        self.editButton.setText(self.tr(u'edit'))\n        self.editButton.setToolTip(self.tr(u'toggle editing mode'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/document-edit.png')))\n\n        self.editButton.setIcon(icon)\n\n        self.addColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.addColumnButton.setObjectName('addcolumnbutton')\n        self.addColumnButton.setText(self.tr(u'+col'))\n        self.addColumnButton.setToolTip(self.tr(u'add new column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-column-right.png')))\n\n        self.addColumnButton.setIcon(icon)\n\n        self.addRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.addRowButton.setObjectName('addrowbutton')\n        self.addRowButton.setText(self.tr(u'+row'))\n        self.addRowButton.setToolTip(self.tr(u'add new row'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-row-below.png')))\n\n        self.addRowButton.setIcon(icon)\n\n        self.removeColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeColumnButton.setObjectName('removecolumnbutton')\n        self.removeColumnButton.setText(self.tr(u'-col'))\n        self.removeColumnButton.setToolTip(self.tr(u'remove a column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-column.png')))\n\n        self.removeColumnButton.setIcon(icon)\n\n        self.removeRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeRowButton.setObjectName('removerowbutton')\n        self.removeRowButton.setText(self.tr(u'-row'))\n        self.removeRowButton.setToolTip(self.tr(u'remove selected rows'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-row.png')))\n\n        self.removeRowButton.setIcon(icon)\n\n        self.buttons = [self.editButton, self.addColumnButton, self.addRowButton, self.removeColumnButton, self.removeRowButton]\n\n        for index, button in enumerate(self.buttons):\n            button.setMinimumSize(self._iconSize)\n            button.setMaximumSize(self._iconSize)\n            button.setIconSize(self._iconSize)\n            button.setCheckable(True)\n            self.buttonFrameLayout.addWidget(button, 0, index, 1, 1)\n        self.buttonFrameLayout.addItem(spacerItemButton, 0, index+1, 1, 1)\n\n        for button in self.buttons[1:]:\n            button.setEnabled(False)\n\n        #self.tableView = QtGui.QTableView(self)\n        self.tableView = DragTable(self)\n        self.tableView.setAlternatingRowColors(True)\n        self.tableView.setSortingEnabled(True)\n        \n        self.gridLayout.addWidget(self.buttonFrame, 0, 0, 1, 1)\n        self.gridLayout.addWidget(self.tableView, 1, 0, 1, 1)\n\n        self.editButton.toggled.connect(self.enableEditing)\n        self.addColumnButton.toggled.connect(self.showAddColumnDialog)\n        self.addRowButton.toggled.connect(self.addRow)\n        self.removeRowButton.toggled.connect(self.removeRow)\n        self.removeColumnButton.toggled.connect(self.showRemoveColumnDialog)\n\n    def setButtonsVisible(self, visible):\n        \"\"\"hide/show the edit buttons\"\"\"\n        self.buttonFrame.setVisible(visible)\n        \n    @Slot(bool)\n    def enableEditing(self, enabled):\n        \"\"\"Enable the editing buttons to add/remove rows/columns and to edit the data.\n\n        This method is also a slot.\n        In addition, the data of model will be made editable,\n        if the `enabled` parameter is true.\n\n        Args:\n            enabled (bool): This flag indicates, if the buttons\n                shall be activated.\n\n        \"\"\"\n        for button in self.buttons[1:]:\n            button.setEnabled(enabled)\n            if button.isChecked():\n                button.setChecked(False)\n\n        model = self.tableView.model()\n\n        if model is not None:\n            model.enableEditing(enabled)\n\n    @Slot()\n    def uncheckButton(self):\n        \"\"\"Removes the checked stated of all buttons in this widget.\n\n        This method is also a slot.\n\n        \"\"\"\n        #for button in self.buttons[1:]:\n        for button in self.buttons:\n            # supress editButtons toggled event\n            button.blockSignals(True)\n            if button.isChecked():\n                button.setChecked(False)\n            button.blockSignals(False)\n\n    @Slot(str, object, object)\n    def addColumn(self, columnName, dtype, defaultValue):\n        \"\"\"Adds a column with the given parameters to the underlying model\n\n        This method is also a slot.\n        If no model is set, nothing happens.\n\n        Args:\n            columnName (str): The name of the new column.\n            dtype (numpy.dtype): The datatype of the new column.\n            defaultValue (object): Fill the column with this value.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.addDataFrameColumn(columnName, dtype, defaultValue)\n\n        self.addColumnButton.setChecked(False)\n\n    @Slot(bool)\n    def showAddColumnDialog(self, triggered):\n        \"\"\"Display the dialog to add a column to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the dialog will be created and shown.\n\n        \"\"\"\n        if triggered:\n            dialog = AddAttributesDialog(self)\n            dialog.accepted.connect(self.addColumn)\n            dialog.rejected.connect(self.uncheckButton)\n            dialog.show()\n\n    @Slot(bool)\n    def addRow(self, triggered):\n        \"\"\"Adds a row to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the row will be appended to the end.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            model.addDataFrameRows()\n            self.sender().setChecked(False)\n\n\n    @Slot(bool)\n    def removeRow(self, triggered):\n        \"\"\"Removes a row to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the selected row will be removed\n                from the model.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            selection = self.tableView.selectedIndexes()\n\n            rows = [index.row() for index in selection]\n            model.removeDataFrameRows(set(rows))\n            self.sender().setChecked(False)\n\n    @Slot(list)\n    def removeColumns(self, columnNames):\n        \"\"\"Removes one or multiple columns from the model.\n\n        This method is also a slot.\n\n        Args:\n            columnNames (list): A list of columns, which shall\n                be removed from the model.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.removeDataFrameColumns(columnNames)\n\n        self.removeColumnButton.setChecked(False)\n\n    @Slot(bool)\n    def showRemoveColumnDialog(self, triggered):\n        \"\"\"Display the dialog to remove column(s) from the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the dialog will be created and shown.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            if model is not None:\n                columns = model.dataFrameColumns()\n                dialog = RemoveAttributesDialog(columns, self)\n                dialog.accepted.connect(self.removeColumns)\n                dialog.rejected.connect(self.uncheckButton)\n                dialog.show()\n\n    def setViewModel(self, model):\n        \"\"\"Sets the model for the enclosed TableView in this widget.\n\n        Args:\n            model (DataFrameModel): The model to be displayed by\n                the Table View.\n\n        \"\"\"\n        if isinstance(model, DataFrameModel):\n            self.enableEditing(False)\n            self.uncheckButton()\n            \n            selectionModel = self.tableView.selectionModel()\n            self.tableView.setModel(model)\n            model.dtypeChanged.connect(self.updateDelegate)\n            model.dataChanged.connect(self.updateDelegates)\n            del selectionModel\n            \n    def setModel(self, model):\n        \"\"\"Sets the model for the enclosed TableView in this widget.\n\n        Args:\n            model (DataFrameModel): The model to be displayed by\n                the Table View.\n\n        \"\"\"\n        self.setViewModel(model)\n\n    def model(self):\n        \"\"\"Gets the viewModel\"\"\"\n        return self.view().model()\n\n    def viewModel(self):\n        \"\"\"Gets the viewModel\"\"\"\n        return self.view().model()\n\n    def view(self):\n        \"\"\"Gets the enclosed TableView\n\n        Returns:\n            QtGui.QTableView: A Qt TableView object.\n\n        \"\"\"\n        return self.tableView\n\n    def updateDelegate(self, column, dtype):\n        \"\"\"update the delegates for a specific column\n        \n        Args:\n            column (int): column index.\n            dtype (str): data type of column.\n        \n        \"\"\"\n        # as documented in the setDelegatesFromDtype function\n        # we need to store all delegates, so going from\n        # type A -> type B -> type A\n        # would cause a segfault if not stored.\n        createDelegate(dtype, column, self.tableView)\n\n    def updateDelegates(self):\n        \"\"\"reset all delegates\"\"\"\n        for index, column in enumerate(self.tableView.model().dataFrame().columns):\n            dtype = self.tableView.model().dataFrame()[column].dtype\n            self.updateDelegate(index, dtype)\n            \n    def selectionModel(self):\n        \"\"\"return the table views selectionModel\"\"\"\n        return self.view().selectionModel()", "instruction_en": "Create a custom widget with a TableView and a toolbar that displays all `DataFrameModels` and enables the editing of this (edit data, adding/removing, rows/columns). The widget has five buttons: editButton, addColumnButton, addRowButton, removeColumnButton, and removeRowButton. The widget also has a DragTable class that inherits from `QtGui.QTableView` and allows starting a drag operation with a `PandasCellPayload` on a defined index. The `DataTableWidget` class has several methods that are also slots, including `enableEditing`, `uncheckButton`, `addColumn`, `showAddColumnDialog`, `addRow`, `removeRow`, `removeColumns`, `showRemoveColumnDialog`, `setViewModel`, `setModel`, `model`, `viewModel`, `view`, `updateDelegate`, `updateDelegates`, and `selectionModel`.", "original_line_number": 11856, "instruction_length": 715}
{"instruction": "Kod ini mentakrifkan kelas yang dipanggil PlugIndiagnose yang mewarisi dari QDialog dan UI_Dialog. Ia mempunyai beberapa kaedah seperti initui (), displayPage (), membatalkanButton_clicked (), nextButton_clicked (), dan patchLineEdit_TextChanged (). Untuk menggunakan kod ini, buat contoh kelas plugIngnose dan panggil kaedahnya seperti yang diperlukan. Kaedah initui () menetapkan antara muka pengguna, manakala kaedah paparan () mengemas kini halaman semasa yang dipaparkan. Kaedah CancelButton_Clicked () menutup dialog apabila butang Batal diklik, manakala kaedah NextButton_Clicked () cuba memasang plugin dan menutup dialog jika berjaya. Kaedah PATHLINEEDIT_TEXTCHANGED () mengemas kini laluan plugin apabila pengguna jenis di jalan.", "input": "", "output": "# -*- coding: utf-8 -*-\n#\n# (c) Copyright 2001-2011 Hewlett-Packard Development Company, L.P.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA\n#\n# Authors: Amarnath Chitumalla\n#\n\n\n# Local\nfrom base.g import *\nfrom base import device, utils, pkit\nfrom prnt import cups\nfrom base.codes import *\nfrom .ui_utils import *\nfrom installer import pluginhandler\nfrom base.sixext import  to_unicode\n\n# Qt\nfrom PyQt4.QtCore import *\nfrom PyQt4.QtGui import *\nimport signal\n\n# Ui\nfrom .plugindiagnose_base import Ui_Dialog\n\n\n\nclass PluginDiagnose(QDialog, Ui_Dialog):\n    def __init__(self, parent, install_mode=PLUGIN_NONE, plugin_reason=PLUGIN_REASON_NONE, upgrade=False):\n        QDialog.__init__(self, parent)\n        self.install_mode = install_mode\n        self.plugin_reason = plugin_reason\n        self.plugin_path = None\n        self.result = False\n        self.pluginObj = pluginhandler.PluginHandle()\n        self.setupUi(self, upgrade)\n\n        self.user_settings = UserSettings()\n        self.user_settings.load()\n        self.user_settings.debug()\n\n        self.initUi()\n\n\n\n\n    def initUi(self):\n        # connect signals/slots\n        self.connect(self.CancelButton, SIGNAL(\"clicked()\"), self.CancelButton_clicked)\n        self.connect(self.NextButton, SIGNAL(\"clicked()\"), self.NextButton_clicked)\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n        # Application icon\n        self.setWindowIcon(QIcon(load_pixmap('hp_logo', '128x128')))\n\n\n    def PathLineEdit_textChanged(self, t):\n        self.plugin_path = to_unicode(t)\n        self.setPathIndicators()\n\n\n    #\n    # Misc\n    #\n\n    def displayPage(self, page):\n        self.updateStepText(page)\n        self.StackedWidget.setCurrentIndex(page)\n\n    def CancelButton_clicked(self):\n        self.close()\n\n\n    def NextButton_clicked(self):\n        self.NextButton.setEnabled(False)\n        self.CancelButton.setEnabled(False)\n        try:\n            plugin = PLUGIN_REQUIRED \n            plugin_reason = PLUGIN_REASON_NONE\n            ok, sudo_ok = pkit.run_plugin_command(plugin == PLUGIN_REQUIRED, plugin_reason)\n\n            if not ok or self.pluginObj.getStatus() != pluginhandler.PLUGIN_INSTALLED:\n                FailureUI(self, self.__tr(\"Failed to install Plug-in.\\nEither you have chosen to skip the Plug-in installation  or entered incorrect Password.\"))\n\n        finally:\n            endWaitCursor()\n        self.result = True\n        self.close()\n\n\n    def __tr(self,s,c = None):\n        return qApp.translate(\"PluginDialog\",s,c)", "instruction_en": "This code defines a class called PluginDiagnose that inherits from QDialog and Ui_Dialog. It has several methods such as initUi(), displayPage(), CancelButton_clicked(), NextButton_clicked(), and PathLineEdit_textChanged(). To use this code, create an instance of the PluginDiagnose class and call its methods as needed. The initUi() method sets up the user interface, while the displayPage() method updates the current page being displayed. The CancelButton_clicked() method closes the dialog when the Cancel button is clicked, while the NextButton_clicked() method attempts to install a plugin and closes the dialog if successful. The PathLineEdit_textChanged() method updates the plugin path when the user types in the path.", "original_line_number": 12404, "instruction_length": 739}
{"instruction": "Kod ini melakukan ujian prestasi berdasarkan ujian/highlevel/high-in-low6-memory.py. Overhead data adalah pada ~ 6.5 ms / mb. Overhead muat naik pangkalan data adalah kira -kira (7 ms / mb) yang sama dengan backend flatfile. Muat turun pangkalan data hampir percuma. Operasi auth sel berstruktur adalah kira -kira 10 ms. Penilaian makro senarai peta adalah murah, 5 ms per (selari) transformasi. Perjalanan semula adalah kira-kira 20 ms setiap transformasi (di atas makro). Penilaian ungkapan adalah kira -kira 10 ms + 0.5 ms / mb (input + output) setiap transformasi (hubungan spekulatif!). Penskalaan bukan linear diperhatikan antara 100 dan 1000 transformasi selari, kelembapan X4 diperhatikan untuk tiga overhed terakhir di atas. Penukaran bercampur-ke-str adalah mahal, jangan lakukannya!", "input": "", "output": "\"\"\"\nPerformance tests based on tests/highlevel/high-in-low6-memory.py\nSee also auth-*py.\n\n- Data overhead is now at ~6.5 ms / MB.\n  A lot of the data overhead comes from json.dumps. This is to build mixed cells.\n  The rest is from get_hash.\n  Pure Python version (with get_hash and dumps) is at 6.3 ms / MB,\n  so the rest of the data overhead is fine!\n\n- Database upload overhead is about the same (7 ms / MB) with a flatfile backend\n  Database download is almost free.\n\n- A structured cell auth operation is about 10 ms.\n\n- map-list macro evaluation is cheap, 5 ms per (parallel) transformation\n\n- re-translation is about 20 ms per transformation (on top of the macro)\n\n- expression evaluation is about 10 ms + 0.5 ms / MB (of input + output) per transformation\n  (speculative relationship!)\n\n- BUT: Non-linear scaling:\n  between 100 and 1000 parallel transformations, a x4 slowdown is observed for the last three overheads above.\n\n\nNOTE: Mixed-to-str conversion is expensive, don't do it!\n\n\n\n\"\"\"\n\nimport sys\nimport seamless\n\nimport seamless.core.execute\nseamless.core.execute.DIRECT_PRINT = True\n\nseamless.database_sink.connect()\nseamless.database_cache.connect()\n#seamless.set_ncores(2)\n#seamless.set_parallel_evaluations(5)\n\nseamless.set_ncores(8) ###\nseamless.set_parallel_evaluations(100)  ###\n\n# for the big testing, 20 evaluations\nseamless.set_parallel_evaluations(20)  ###\n\n\"\"\"\nimport logging\nlogging.basicConfig()\nlogging.getLogger(\"seamless\").setLevel(logging.DEBUG)\n\"\"\"\n\nfrom seamless.highlevel import Context, Cell, Macro\nfrom seamless.highlevel.library import LibraryContainer\n\nimport time\nimport cProfile\ncProfile.profiler = cProfile.Profile()\n\nmylib = LibraryContainer(\"mylib\")\nmylib.map_list_N = Context()\ndef constructor(ctx, libctx, context_graph, inp, result):\n    m = ctx.m = Macro()\n    m.graph = context_graph\n    m.pins.result = {\"io\": \"output\", \"celltype\": \"mixed\", \"hash_pattern\": {\"!\": \"#\"}}\n\n    ctx.inp = Context()\n    ctx.cs_inp = Context()\n    inp_prefix = \"INPUT_\"\n    m.inp_prefix = inp_prefix\n    for key in inp:\n        c = Cell()\n        ctx.inp[key] = c\n        c.hash_pattern = {\"!\": \"#\"}\n        inp[key].connect(c)\n        ctx.cs_inp[key] = Cell(\"checksum\")\n        ctx.cs_inp[key] = ctx.inp[key]\n        setattr(m, inp_prefix + key , ctx.cs_inp[key])\n\n    def map_list_N(ctx, inp_prefix, graph, **inp):\n        #print(\"INP\", inp)\n        first_k = list(inp.keys())[0]\n        length = len(inp[first_k])\n        first_k = first_k[len(inp_prefix):]\n        for k0 in inp:\n            k = k0[len(inp_prefix):]\n            if len(inp[k0]) != length:\n                err = \"all cells in inp must have the same length, but '{}' has length {} while '{}' has length {}\"\n                raise ValueError(err.format(k, len(inp[k0]), first_k, length))\n\n        print(\"LENGTH\", length)\n\n        from seamless.core import Cell as CoreCell\n        from seamless.core.unbound_context import UnboundContext\n        pseudo_connections = []\n        ctx.result = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n\n        ctx.sc_data = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n        ctx.sc_buffer = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n        ctx.sc = StructuredCell(\n            data=ctx.sc_data,\n            buffer=ctx.sc_buffer,\n            inchannels=[(n,) for n in range(length)],\n            outchannels=[()],\n            hash_pattern = {\"!\": \"#\"}\n        )\n\n        for n in range(length):\n            #print(\"MACRO\", n+1)\n            hc = HighLevelContext(graph)\n\n            subctx = \"subctx%d\" % (n+1)\n            setattr(ctx, subctx, hc)\n\n            if not hasattr(hc, \"inp\"):\n                raise TypeError(\"map_list_N context must have a subcontext called 'inp'\")\n            hci = hc.inp\n            if not isinstance(hci, UnboundContext):\n                raise TypeError(\"map_list_N context must have an attribute 'inp' that is a context, not a {}\".format(type(hci)))\n\n            for k0 in inp:\n                k = k0[len(inp_prefix):]\n                if not hasattr(hci, k):\n                    raise TypeError(\"map_list_N context must have a cell called inp.'{}'\".format(k))\n                if isinstance(hci[k], StructuredCell):\n                    raise TypeError(\"map_list_N context has a cell called inp.'{}', but its celltype must be mixed, not structured\".format(k))\n                if not isinstance(hci[k], CoreCell):\n                    raise TypeError(\"map_list_N context must have an attribute inp.'{}' that is a cell, not a {}\".format(k, type(hci[k])))\n                if hci[k].celltype != \"mixed\":\n                    raise TypeError(\"map_list_N context has a cell called inp.'{}', but its celltype must be mixed, not {}\".format(k, hci[k].celltype))\n\n                con = [\"..\" + k], [\"ctx\", subctx, \"inp\", k]\n                pseudo_connections.append(con)\n                cs = inp[k0][n]\n                hci[k].set_checksum(cs)\n\n            resultname = \"result%d\" % (n+1)\n            setattr(ctx, resultname, cell(\"mixed\"))\n            c = getattr(ctx, resultname)\n            hc.result.connect(c)\n            c.connect(ctx.sc.inchannels[(n,)])\n            con = [\"ctx\", subctx, \"result\"], [\"..result\"]\n            pseudo_connections.append(con)\n\n        ctx.sc.outchannels[()].connect(ctx.result)\n        ctx._pseudo_connections = pseudo_connections\n        print(\"/MACRO\")\n\n        \"\"\"\n        import logging\n        logging.basicConfig()\n        logging.getLogger(\"seamless\").setLevel(logging.DEBUG)\n        \"\"\"\n\n        import cProfile\n        cProfile.profiler.enable()\n\n    m.code = map_list_N\n    ctx.result = Cell()\n    ctx.result.hash_pattern = {\"!\": \"#\"}\n    ctx.result = m.result\n    result.connect_from(ctx.result)\n\n\nmylib.map_list_N.constructor = constructor\nmylib.map_list_N.params = {\n    \"context_graph\": \"context\",\n    \"inp\": {\n        \"type\": \"celldict\",\n        \"io\": \"input\"\n    },\n    \"result\": {\n        \"type\": \"cell\",\n        \"io\": \"output\"\n    },\n}\n\nctx = Context()\nctx.adder = Context()\nsctx = ctx.adder\nsctx.inp = Context()\nsctx.inp.a = Cell(\"mixed\")\nsctx.inp.b = Cell(\"mixed\")\nsctx.a = Cell(\"mixed\")\nsctx.b = Cell(\"mixed\")\nsctx.a = sctx.inp.a\nsctx.b = sctx.inp.b\ndef add(a,b):\n    print(\"ADD\", a[:10])\n    return a+b\nsctx.add = add\nsctx.add.a = sctx.a\nsctx.add.b = sctx.b\nsctx.result = sctx.add\nsctx.result.celltype = \"mixed\"\nctx.compute()\n\nctx.data_a = Cell()\nctx.data_a.hash_pattern = {\"!\": \"#\"}\nctx.data_b = Cell()\nctx.data_b.hash_pattern = {\"!\": \"#\"}\nctx.compute()\n\n# Next section is 14.5 secs (if the database is filled), but can be elided to ~0.5s by setting checksum directly (if in flatfile cache).\n# Not having a DB at all is also 13 secs, so DB request communication (without upload) doesn't cost much.\n\nrepeat = int(10e6)\n#repeat = int(5)\n#for n in range(1000): # 2x10 GB\n#for n in range(100): # 2x1 GB\nfor n in range(1000):\n    a = \"A:%d:\" % n + str(n%10) * repeat\n    b = \"B:%d:\" % n + str(n%10) * repeat\n    ctx.data_a[n] = a\n    ctx.data_b[n] = b\n    if n % 20 == 0:\n        ctx.compute()\n    print(n+1)\n\nctx.compute()\nprint(ctx.data_a.checksum)\nprint(ctx.data_b.checksum)\n\n\"\"\"\nctx.data_a.set_checksum(\"d07050610c50de8c810aa1d1e322786ed8932cf6eafa0fbe1f132b2c881af9c2\")\nctx.data_b.set_checksum(\"374c02504f89ed0a760b03c3e1fd2258988576b919d763254709b66fc7bfb22e\")\nctx.compute()\n\n\"\"\"\n\n#\n### For repeat=10 million, 1000 items\n### ctx.data_a.set_checksum(\"fa4e6aa7e7edaa6feb036fd5e8c28ffc48575cefc332187552c5be4bf0511af8\")\n### ctx.data_b.set_checksum(\"2988c44780790e4ffceb1f97391e475f165e316f27a656957282a2998aee9d4f\")\n\n### For repeat=10 million, 200 items\n### ctx.data_a.set_checksum(\"d07050610c50de8c810aa1d1e322786ed8932cf6eafa0fbe1f132b2c881af9c2\")\n### ctx.data_b.set_checksum(\"374c02504f89ed0a760b03c3e1fd2258988576b919d763254709b66fc7bfb22e\")\n\n### For repeat=10 million\n### ctx.data_a.set_checksum(\"983730afb7ab41d524b72f1097daaf4a3c15b98943291f96e523730849cabe8c\")\n### ctx.data_b.set_checksum(\"46dabc02b59be44064a9e06dd50bc6841833578c2b6339fbc43f090cc17831fa\")\n\n### For repeat=5\n### ctx.data_a.set_checksum(\"9b4a551a6c1c5830d6070b9c22ae1788b9743e9637be47d56103bcda019a897c\")\n### ctx.data_b.set_checksum(\"9820f1ab795db7b0d195f21966ecb071d76e9ce2fd3a90845974a3905584eb3e\")\nctx.compute()\n\n\"\"\"\nIf there is no database (100 x repeat 10e6):\n- 13 secs up to here (6.5 ms per MB)\n- 0.5 secs to evaluate the macro\n- 2.3 secs (2.8 - 0.5) for re-translation (23 ms per transformer)\n- 32 secs total time, which leaves 32 - 13 - 0.5 = 18.5 secs for transformation and expression evaluation\n  Since 13 secs is required for calculate checksum and decoding, that means ~5.5 secs (55 ms per transformer) overhead\n  This is a supplement of 32 ms over just re-translation\n\nIf there is no database (100 x repeat 5):\n- 2.3 secs up to here (12 ms per auth operation)\n- Still 0.5 secs to evaluate the macro\n- Still 2.3 secs (2.8 - 0.5) for re-translation (23 ms per transformer, independent of data size!)\n- 6.2 secs total time, which leaves 6.2 - 2.3 - 0.5 = 3.5 secs for transformation and expression evaluation\n  This is an overhead of 35 ms per transformer, a supplement of just 12 ms over re-translation\n  The 20 ms reduction compared to above comes from not handling 2x10 MB of input and 20 MB of output,\n  so that's 0.5 ms/MB.\n\nIf there is no database (1000 x repeat 5):\n- 11.7 secs up to here (12 ms per auth operation). So scales linearly.\n- 6.5 secs to evaluate the macro, so scales about linearly\n- 98 secs (104.5 - 6.5) for re-translation, which is 4x slower than above  (98 ms)\n- 145 secs total time, which leaves 145 - 11.7 - 6.5 = 127 secs for transformation and expression evaluation\n  This is an overhead of 127 ms per transformer, which is 4x slower than above (127 ms).\n  => So in principle, 90 sconds slower than might be\n    - Some 45 secs is await-upon-connection-tasks, this could be optimized?\n    - 12 seconds from isinstance is probably unavoidable\n    - 9 seconds comes from validate deep structure, that may be unavoidable\n    - 5 seconds each from taskmanager.add_task (61k tasks) and asyncio.Task.done (119 million tasks). Avoidable?\n  => do maplist-inside-maplist\n\nIf the database has not been filled:\n- 27.5 secs up to here\nIf the database has been filled:\n- 14 secs up to here: to synthesize the data, and to verify that all is present\n  So the raw upload is 13.5 seconds (27.5 - 14); and communication with the DB delays only 1 sec.\n- 1.5 secs up to here, with the above elision.\n\nWith the database:\n- 1.5 secs to evaluate the macro (DB slows down!)\n- 5.5 secs for re-translation\n- 45.7 secs total time, which leaves 45.7 - 5.5 - 1.5 = 38.5 secs for transformation and expression evaluation\n  Compare this to the 18.5 secs w/o database, this is a loss of 20 secs.\n  But we have to count the download of the inputs and upload of the results.\n  When removing the tfr entries from the database, transformations will be repeated, but no buffers will be uploaded,\n  as the sink knows them already.\n  This brings total time down to 32 secs, the same as no database!\n  So all of the extra overhead is from upload, and download is almost free. (This could be hard disk caching, though)\n- 5.5 secs total time with pulling transformation results out of the DB. Again, download is almost free.\n\n\nBig test with the database (1000 x repeat 10e6):\n- Total time 940 secs. Data upload overhead should be ~120 secs, and Seamless data overhead should be ~140 secs.\n- 142 secs for re-translation + macro evaluation (142 ms / transformation), a factor 6 slowdown\n- 940 - 142 - 120 - 140 = ~540 secs for evaluation\n   I.e. 540 ms per transformation. If the same slowdown applies, it would have been 90.\n   But we would have expected 30. So a larger slowdown (fewer parallel expressions may have been a cause too)\n\"\"\"\n\nctx.result = Cell()\nctx.result.hash_pattern = {\"!\": \"#\"}\nctx.compute()\n\nctx.include(mylib.map_list_N)\nctx.inst = ctx.lib.map_list_N(\n    context_graph = ctx.adder,\n    inp = {\"a\": ctx.data_a, \"b\": ctx.data_b},\n    result = ctx.result\n)\nctx.compute()\n\nprint(\"Exception:\", ctx.inst.ctx.m.exception)\nprint(ctx.result.data)\n\nimport pstats\nsortby = 'tottime'\nps = pstats.Stats(cProfile.profiler).sort_stats(sortby)\nps.print_stats(40)\n\nt0 = time.time()\n\"\"\"\nprint(\"Re-set\")\ngraph = ctx.get_graph()\nctx_dummy = Context()\ndummy_graph = ctx_dummy.get_graph()\nctx.set_graph(dummy_graph)\nctx.translate(force=True)\nctx.compute()\nprint(time.time()-t0)\nprint(\"Re-eval\")\nctx.set_graph(graph)\n\"\"\"\n\"\"\"\nctx.translate(force=True)\nctx.compute()\nprint(time.time()-t0)\n\"\"\"", "instruction_en": "This code performs performance tests based on tests/highlevel/high-in-low6-memory.py. The data overhead is at ~6.5 ms / MB. The database upload overhead is about the same (7 ms / MB) with a flatfile backend. Database download is almost free. A structured cell auth operation is about 10 ms. Map-list macro evaluation is cheap, 5 ms per (parallel) transformation. Re-translation is about 20 ms per transformation (on top of the macro). Expression evaluation is about 10 ms + 0.5 ms / MB (of input + output) per transformation (speculative relationship!). Non-linear scaling is observed between 100 and 1000 parallel transformations, a x4 slowdown is observed for the last three overheads above. Mixed-to-str conversion is expensive, don't do it!", "original_line_number": 12610, "instruction_length": 793}
{"instruction": "Kod ini mengandungi kelas yang dipanggil Multiincludethroughput dengan beberapa kaedah yang berinteraksi dengan Elasticsearch untuk mengimport, memilih, mengubah suai, dan memadam data. Ia juga termasuk kaedah untuk menambah, mengedit, dan memadam anotasi untuk data dan garis masa. Untuk menggunakan kod ini, import modul yang diperlukan dan buat contoh kelas multiinCludethroughput. Kemudian, hubungi kaedah yang dikehendaki dengan parameter yang sesuai untuk melakukan tindakan yang dikehendaki pada data Elasticsearch.", "input": "", "output": "#  Copyright (C) 2016  Jamie Acosta, Jennifer Weand, Juan Soto, Mark Eby, Mark Smith, Andres Olivas\n#\n# This file is part of DssVisualizer.\n#\n# DssVisualizer is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# DssVisualizer is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with DssVisualizer.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom plugins.datasource.elasticsearch.annotations import Annotations\nfrom plugins.datasource.elasticsearch.common import Common\nfrom elasticsearch import Elasticsearch\nfrom plugins.datasource.elasticsearch.selecting import Selecting\n\n\nclass MultiIncludeThroughput:\n\n    def __init__(self):\n        self.esIndex = Common().getIndexName()\n        self.multiIncludeThroughputDocType = \"multiincludethroughput\"\n        self.resultSize = Common().getSizeToReturn()\n\n    def importMultiIncludeThroughputData(self, jsonObjects):\n        es = Elasticsearch()\n        es.indices.create(index=self.esIndex, ignore=400)\n        insertedCount = 0\n        for json in jsonObjects:\n            result = es.index(index=self.esIndex, doc_type=self.multiIncludeThroughputDocType, body=json)\n            insertedCount += result[\"_shards\"][\"successful\"]\n        return insertedCount\n\n    # select data by date range of the 'start' column\n    def selectMultiIncludeThroughputData(self, startDate, endDate, techNames, eventNames, eventTechNames):\n        select = Selecting().generateSelectQuery(startDate, endDate, techNames, eventNames, eventTechNames, False, True)\n        data = Elasticsearch().search(index=self.esIndex, doc_type=self.multiIncludeThroughputDocType, size=self.resultSize, body=select)\n        return Selecting().fixAllTheData(data)\n\n    # select single data point\n    def selectMultiIncludeThroughputDataById(self, dataId):\n        data = Elasticsearch().get(index=self.esIndex, doc_type=self.multiIncludeThroughputDocType, id=dataId)\n        return Selecting().fixOneData(data)\n\n    # add or edits a fixedData record to this data point\n    def modifyFixedMultiIncludeThroughputData(self, dataId, traffic_xy_id, className, x, y, isDeleted):\n        updateFixed = {\"doc\": {\"fixedData\": {\"traffic_xy_id\": traffic_xy_id, \"className\": className, \"x\": x, \"y\": y, \"isDeleted\": isDeleted}}}\n        result = Elasticsearch().update(index=self.esIndex, doc_type=self.multiIncludeThroughputDocType, body=updateFixed, id = dataId)\n        return Common().getModfiedCount(result)\n\n    # delete the fixedData\n    def deleteFixedMultiIncludeThroughputData(self, dataId):\n        deleteFixed = {\"script\" : \"ctx._source.remove(\\\"fixedData\\\")\"}\n        result = Elasticsearch().update(index=self.esIndex, doc_type=self.multiIncludeThroughputDocType, body=deleteFixed, id = dataId)\n        return Common().getModfiedCount(result)\n\n    # add or edit an annotation to the object.  This will add a single 'annotation' attribute to the object.\n    def modifyAnnotationMultiIncludeThroughput(self, dataId, annotationText):\n        return Annotations().modifyAnnotation(self.multiIncludeThroughputDocType, dataId, annotationText)\n\n    # add an annotation to an array of annotations for the dataId\n    def addAnnotationToArrayMultiIncludeThroughput(self, dataId, annotationText):\n        return Annotations().addAnnotationToArray(self.multiIncludeThroughputDocType, dataId, annotationText)\n\n    # edit an annotation in the array of annotations.\n    def editAnnotationInArrayMultiIncludeThroughput(self, dataId, oldAnnotationText, newAnnotationText):\n        return Annotations().editAnnotationInArray(self.multiIncludeThroughputDocType, dataId, oldAnnotationText, newAnnotationText)\n\n    # delete an annotation from array for the dataId\n    def deleteAnnotationFromArrayMultiIncludeThroughput(self, dataId, annotationText):\n        return Annotations().deleteAnnotationFromArray(self.multiIncludeThroughputDocType, dataId, annotationText)\n\n    # deletes all annotations for the dataId\n    def deleteAllAnnotationsForMultiIncludeThroughput(self, dataId):\n        return Annotations().deleteAllAnnotationsForData(self.multiIncludeThroughputDocType, dataId)\n\n    # add an annotation to the timeline, not a datapoint\n    def addAnnotationToMultiIncludeThroughputTimeline(self, multiIncludeThroughput, annotationText):\n        return Annotations().addAnnotationToTimeline(self.multiIncludeThroughputDocType, multiIncludeThroughput, annotationText)\n\n    # def getDistinctTechNamesForEvents(self, eventNames):\n    #     collection = self.getMultiIncludeThroughputCollection()\n    #     return TechAndEventNames().getDistinctTechNamesForEvents(collection, eventNames)\n    #\n    # def getDistinctEventNames(self):\n    #     collection = self.getMultiIncludeThroughputCollection()\n    #     return TechAndEventNames().getDistinctEventNames(collection)\n    #\n    # def getDistinctTechAndEventNames(self):\n    #     collection = self.getMultiIncludeThroughputCollection()\n    #     return TechAndEventNames().getDistinctTechAndEventNames(collection)", "instruction_en": "This code contains a class called MultiIncludeThroughput with several methods that interact with Elasticsearch to import, select, modify, and delete data. It also includes methods to add, edit, and delete annotations for the data and timeline. To use this code, import the necessary modules and create an instance of the MultiIncludeThroughput class. Then, call the desired methods with the appropriate parameters to perform the desired actions on the Elasticsearch data.", "original_line_number": 12764, "instruction_length": 522}
{"instruction": "Buat kelas yang dipanggil EpgSelection yang mewarisi dari EpgSelectionChannel dan Epgservicezap. Kelas ini harus mempunyai kaedah __init__ yang mengambil sesi, perkhidmatan, zapfunc, eventid, bouquetchangecb, servicechangecb, epgtype, startbouquet, startref, dan bunga. Jika EPGTYPE tidak \"serupa\" atau \"tunggal\", cetak mesej amaran dan teruskan dalam mod EPG tunggal. Kelas ini juga harus mempunyai kaedah untuk pelbagai menekan butang, seperti redbuttonpressed, greenbuttonpressed, yellowbuttonpressed, dan bluebuttonpressed. Di samping itu, kelas harus mempunyai kaedah untuk menyegarkan senarai, membuka senarai pemasa, dan menambah pemasa dari acara dengan senyap. Akhirnya, kelas harus mempunyai kaedah untuk membuka pandangan acara.", "input": "", "output": "from __future__ import print_function\nfrom Screens.InfoBar import InfoBar\nfrom enigma import eServiceReference\nfrom Components.ActionMap import HelpableActionMap\nfrom Screens.EpgSelectionChannel import EPGSelectionChannel\nfrom Screens.EpgSelectionBase import EPGServiceZap\nfrom Screens.TimerEntry import addTimerFromEventSilent\n\n\n# Keep for backwards compatibility with plugins, including the parameter naming.\n# This class assumes that EPGSelection is only used in the SingleEPG sense.\nclass EPGSelection(EPGSelectionChannel, EPGServiceZap):\n\tdef __init__(self, session, service=None, zapFunc=None, eventid=None, bouquetChangeCB=None, serviceChangeCB=None, EPGtype=\"similar\", StartBouquet=None, StartRef=None, bouquets=None):\n\t\tif EPGtype not in (\"similar\", \"single\"):\n\t\t\tprint(\"[EPGSelection] Warning: EPGSelection does not support type '%s'\" % EPGtype)\n\t\t\tprint(\"               Attempting to continue in single EPG mode\")\n\t\tEPGSelectionChannel.__init__(self, session, eServiceReference(service))\n\t\tEPGServiceZap.__init__(self, zapFunc or InfoBar.instance.zapToService)\n\n\t\t# Rewrite the EPG actions to invoke the compatibility functions.\n\t\thelpDescription = _(\"EPG Commands\")\n\t\tself[\"epgactions\"] = HelpableActionMap(self, \"EPGSelectActions\", {\n\t\t\t\"info\": (self.Info, _(\"Show detailed event info\")),\n\t\t\t\"epg\": (self.epgButtonPressed, _(\"Show detailed event info\")),\n\t\t\t\"menu\": (self.createSetup, _(\"Setup menu\"))\n\t\t}, prio=-1, description=helpDescription)\n\t\tself[\"colouractions\"] = HelpableActionMap(self, \"ColorActions\", {\n\t\t\t\"red\": (self.redButtonPressed, _(\"IMDB search for current event\")),\n\t\t\t\"redlong\": (self.redButtonPressedLong, _(\"Sort EPG list\")),\n\t\t\t\"green\": (self.greenButtonPressed, _(\"Add/Remove timer for current event\")),\n\t\t\t\"greenlong\": (self.greenButtonPressedLong, _(\"Show timer list\")),\n\t\t\t\"yellow\": (self.yellowButtonPressed, _(\"Search for similar events\")),\n\t\t\t\"blue\": (self.blueButtonPressed, _(\"Add an autotimer for current event\")),\n\t\t\t\"bluelong\": (self.blueButtonPressedLong, _(\"Show autotimer list\"))\n\t\t}, prio=-1, description=helpDescription)\n\n\t# EPGSearch bypasses base class initialisation\n\t# try to limit the scope of its quirkyness by providing a limited\n\t# initialisation path\n\tdef EPGSearch_init(self, session):\n\t\tEPGServiceZap.__init__(self, InfoBar.instance.zapToService)\n\n\t# Backwards compatibility properties for plugins.\n\t@property\n\tdef ChoiceBoxDialog(self):\n\t\treturn self.choiceBoxDialog\n\n\t@ChoiceBoxDialog.setter\n\tdef ChoiceBoxDialog(self, value):\n\t\tself.choiceBoxDialog = value\n\n\t# Backwards compatibility functions for plugins.\n\t# Button names.\n\tdef redButtonPressed(self):\n\t\tself.openIMDb()\n\n\tdef redButtonPressedLong(self):\n\t\tself.sortEpg()\n\n\tdef greenButtonPressed(self):\n\t\tself.addEditTimer()\n\n\tdef greenButtonPressedLong(self):\n\t\tself.showTimerList()\n\n\tdef yellowButtonPressed(self):\n\t\tself.openEPGSearch()\n\n\tdef blueButtonPressed(self):\n\t\tself.addAutoTimer()\n\n\tdef blueButtonPressedLong(self):\n\t\tself.showAutoTimerList()\n\n\tdef Info(self):\n\t\tself.infoKeyPressed()\n\n\tdef InfoLong(self):\n\t\tself.OpenSingleEPG()\n\n\tdef infoKeyPressed(self):\n\t\tself.openEventView()\n\n\tdef eventSelected(self): # used by EPG Search plugin\n\t\tself.openEventView()\n\n\tdef epgButtonPressed(self):\n\t\tself.openEventView()\n\n\t# Actions\n\tdef showTimerList(self):\n\t\tself.openTimerList()\n\n\tdef showAutoTimerList(self):\n\t\tself.openAutoTimerList()\n\n\tdef OpenSingleEPG(self):\n\t\tself.openSingleEPG()\n\n\tdef sortEpg(self):\n\t\tself.sortEPG(self)\n\n\tdef timerAdd(self):\n\t\tself.addEditTimerMenu()\n\n\tdef doRecordTimer(self):\n\t\tself.doInstantTimer(0)\n\n\tdef doZapTimer(self):\n\t\tself.doInstantTimer(1)\n\n\tdef RecordTimerQuestion(self, manual=False):\n\t\tif manual:\n\t\t\tself.addEditTimer()\n\t\telse:\n\t\t\tself.addEditTimerMenu()\n\n\tdef doInstantTimer(self, zap=0):\n\t\tevent, service = self[\"list\"].getCurrent()[:2]\n\t\taddTimerFromEventSilent(self.session, self.refreshTimerActionButton, event, service, zap)\n\n\t# Things that need to be able to be overridden.\n\tdef refreshList(self):\n\t\ttry:\n\t\t\t# Allow plugins to override using the old all lowercase method name.\n\t\t\tself.refreshlist()\n\t\texcept AttributeError:\n\t\t\tEPGSelectionChannel.refreshList(self)", "instruction_en": "Create a class called EPGSelection that inherits from EPGSelectionChannel and EPGServiceZap. The class should have an __init__ method that takes in a session, service, zapFunc, eventid, bouquetChangeCB, serviceChangeCB, EPGtype, StartBouquet, StartRef, and bouquets. If EPGtype is not \"similar\" or \"single\", print a warning message and continue in single EPG mode. The class should also have methods for various button presses, such as redButtonPressed, greenButtonPressed, yellowButtonPressed, and blueButtonPressed. Additionally, the class should have methods for refreshing the list, opening the timer list, and adding a timer from an event silently. Finally, the class should have a method for opening the event view.", "original_line_number": 12804, "instruction_length": 739}
{"instruction": "Kod ini mentakrifkan formaldehid mm-line fitter berdasarkan model RADEX. Untuk menggunakan fitter, import modul yang diperlukan dan hubungi fungsi `H2CO_MM_RADEX` dengan parameter yang sesuai. Fungsi `formaldehyde_mm` menghasilkan spektrum formaldehid model berdasarkan parameter Gaussian yang mudah. Kelas `formaldehyde_mm_model` adalah model spektrum untuk spektrum formaldehid. `Formaldehyde_mm_fitter` dan` formaldehyde_mm_vheight_fitter` adalah fitters untuk spektrum formaldehid. Kelas `pmfformaldehydemodel` dan` pmfformaldehydemodelvtau` adalah model fungsi untuk spektrum formaldehid.", "input": "", "output": "\"\"\"\n===========================\nFormaldehyde mm-line fitter\n===========================\n\nThis is a formaldehyde 3_03-2_02 / 3_22-221 and 3_03-2_02/3_21-2_20 fitter.  \nIt is based entirely on RADEX models.\n\nThis is the EWR fork of the fitter in pyspeckit.  \n\"\"\"\nimport numpy as np\nimport pyspeckit.spectrum.models.hyperfine as hyperfine\nfrom pyspeckit.spectrum.models import fitter,model#,modelgrid\ntry: # for model grid reading\n    import astropy.io.fits as pyfits\nexcept ImportError:\n    import pyfits\ntry:\n    import scipy.interpolate\n    import scipy.ndimage\n    scipyOK = True\nexcept ImportError:\n    scipyOK=False\n\nline_names = ['threeohthree','threetwotwo','threetwoone']\n\n# http://adsabs.harvard.edu/abs/1971ApJ...169..429T has the most accurate freqs\n# http://adsabs.harvard.edu/abs/1972ApJ...174..463T [twotwo]\ncentral_freq_dict = { \n        'threeohthree': 218.222192e9,\n        'threetwotwo': 218.475632e9,\n        'threetwoone': 218.760066e9,\n    }\nline_strength_dict={\n        'threeohthree': 1.,\n        'threetwotwo': 1.,\n        'threetwoone': 1.,\n        }\nrelative_strength_total_degeneracy={\n        'threeohthree': 1.,\n        'threetwotwo': 1.,\n        'threetwoone': 1.,\n        }\nfreq_dict = central_freq_dict \naval_dict = {\n        'threeohthree': 2.818e-4,\n        'threetwotwo': 1.571e-4,\n        'threetwoone': 1.577e-4,\n    }\n\nvoff_lines_dict = {\n        'threeohthree': 0.,\n        'threetwotwo': 0.,\n        'threetwoone': 0.,\n        }\n\n\nformaldehyde_mm_vtau = hyperfine.hyperfinemodel(line_names, voff_lines_dict,\n        freq_dict, line_strength_dict, relative_strength_total_degeneracy)\nformaldehyde_mm_vtau_fitter = formaldehyde_mm_vtau.fitter\nformaldehyde_mm_vtau_vheight_fitter = formaldehyde_mm_vtau.vheight_fitter\n\ndef h2co_mm_radex(xarr, \n        Temperature=25,\n        logColumn=13,\n        logDensity=4,\n        xoff_v=0.0,\n        width=1.0, \n        grid_vwidth=1.0,\n        gridbundle = None,\n        debug=False,\n        verbose=False,\n        **kwargs):\n\n    \"\"\"\n    Use a grid of RADEX-computed models to make a model line spectrum\n\n    The RADEX models have to be available somewhere.\n    OR they can be passed as arrays.  If as arrays, the form should be:\n    texgrid = ((minfreq1,maxfreq1,texgrid1),(minfreq2,maxfreq2,texgrid2))\n\n    xarr must be a SpectroscopicAxis instance\n    xoff_v, width are both in km/s\n\n\n    Parameters\n    ----------\n    grid_vwidth : float\n        the velocity assumed when computing the grid in km/s\n        this is important because tau = modeltau / width (see, e.g., \n        Draine 2011 textbook pgs 219-230)\n    density : float\n        Density!\n    \"\"\"\n\n    # Convert X-units to frequency in GHz\n    xarr = xarr.as_unit('Hz', quiet=True)\n    Tex303,Tex322,Tex321,tau303,tau322,tau321 = gridbundle\n\n\n    # if this gets too far different from 1, we are gonna have a Bad Time.\n    scalefac = grid_vwidth/width\n\n    tex = (Tex303(logColumn,logDensity,Temperature),\n           Tex322(logColumn,logDensity,Temperature),\n           Tex321(logColumn,logDensity,Temperature))\n    tau =  (tau303(logColumn,logDensity,Temperature)*scalefac,\n            tau322(logColumn,logDensity,Temperature)*scalefac,\n            tau321(logColumn,logDensity,Temperature)*scalefac)\n    if np.any(np.isnan(tex)) or np.any(np.isnan(tau)):\n        raise ValueError(\"Invalid column/density\")\n\n    if verbose:\n        for ta,tk in zip(tau,tex):\n            print \"density %20.12g temperature %20.12g column %20.12g: tau %20.12g tex %20.12g\" % (logDensity, Temperature, logColumn, ta, tk)\n\n    if debug:\n        import pdb; pdb.set_trace()\n\n# here there be physics\n    ckms = 2.99792458e5\n    freq_dict = { \n        '303': 218.222192e9,\n        '322': 218.475632e9,\n        '321': 218.760066e9,\n        }\n    Tbg = 2.73 #because it totally is\n\n\n    nu0 = np.array([ 218.222192e9, 218.475632e9,218.760066e9])\n    nuwidth = [width/ckms*nu for nu in nu0]\n    nuoff = [xoff_v/ckms*nu for nu in nu0]\n    minfreq = nu0/1e9 - 0.25\n    maxfreq = nu0/1e9 + 0.25\n#    spec2 = np.zeros(len(xarr))\n#    for ii in range(len(nu0)):\n#        taunu = tau[ii]*np.exp(-(xarr+nuoff[ii]-nu0[ii])**2/(2.0*nuwidth[ii]**2))\n#        spec2 = spec2 + (1-np.exp(-taunu))*tex[ii] + Tbg*(np.exp(-taunu)-1)  #second term assumes an ON-OFF \n\n    spec = np.sum([\n            (formaldehyde_mm_vtau(xarr, Tex=float(tex[ii]), tau=float(tau[ii]),\n                                  xoff_v=xoff_v, width=width, **kwargs)\n             * (xarr.as_unit('GHz')>minfreq[ii]) * (xarr.as_unit('GHz')<maxfreq[ii])) for ii in xrange(len(tex))],\n                  axis=0)\n#    import pdb \n#    pdb.set_trace()\n        \n\n    return spec\n\n\ndef formaldehyde_mm(xarr, amp=1.0, xoff_v=0.0, width=1.0, \n        return_components=False ):\n    \"\"\"\n    Generate a model Formaldehyde spectrum based on simple gaussian parameters\n\n    the \"amplitude\" is an essentially arbitrary parameter; we therefore define it to be Tex given tau=0.01 when\n    passing to the fitter\n    The final spectrum is then rescaled to that value\n\n    The components are independent, but with offsets set by frequency... in principle.\n\n    \"\"\"\n\n    mdl = formaldehyde_vtau(xarr, Tex=amp*0.01, tau=0.01, xoff_v=xoff_v,\n            width=width,\n            return_components=return_components)\n    if return_components:\n        mdlpeak = np.abs(mdl).squeeze().sum(axis=0).max()\n    else:\n        mdlpeak = np.abs(mdl).max()\n    if mdlpeak > 0:\n        mdl *= amp/mdlpeak\n\n    return mdl\n\n\nclass formaldehyde_mm_model(model.SpectralModel):\n    pass\n\nformaldehyde_mm_fitter = formaldehyde_mm_model(formaldehyde_mm, 3,\n        parnames=['amp','center','width'], \n        parlimited=[(False,False),(False,False), (True,False)], \n        parlimits=[(0,0), (0,0), (0,0)],\n        shortvarnames=(\"A\",\"v\",\"\\\\sigma\"), # specify the parameter names (TeX is OK)\n        fitunits='Hz' )\n\nformaldehyde_mm_vheight_fitter = formaldehyde_mm_model(fitter.vheightmodel(formaldehyde_mm), 4,\n        parnames=['height','amp','center','width'], \n        parlimited=[(False,False),(False,False),(False,False), (True,False)], \n        parlimits=[(0,0), (0,0), (0,0), (0,0)],\n        shortvarnames=(\"H\",\"A\",\"v\",\"\\\\sigma\"), # specify the parameter names (TeX is OK)\n        fitunits='Hz' )\n\n\ntry:\n    import pymodelfit\n\n    class pmfFormaldehydeModel(pymodelfit.FunctionModel1DAuto):\n        def f(self, x, amp0=1.0, xoff_v0=0.0,width0=1.0):\n            return formaldehyde(x,\n                    amp=amp0,\n                    xoff_v=xoff_v0,width=width0)\n\n    class pmfFormaldehydeModelVtau(pymodelfit.FunctionModel1DAuto):\n        def f(self, x, Tex0=1.0, tau0=0.01, xoff_v0=0.0, width0=1.0):\n            return formaldehyde_vtau(x,\n                    Tex=Tex0, tau=tau0,\n                    xoff_v=xoff_v0,width=width0)\nexcept ImportError:\n    pass", "instruction_en": "This code defines a formaldehyde mm-line fitter based on RADEX models. To use the fitter, import the necessary modules and call the `h2co_mm_radex` function with the appropriate parameters. The `formaldehyde_mm` function generates a model Formaldehyde spectrum based on simple Gaussian parameters. The `formaldehyde_mm_model` class is a spectral model for the Formaldehyde spectrum. The `formaldehyde_mm_fitter` and `formaldehyde_mm_vheight_fitter` are fitters for the Formaldehyde spectrum. The `pmfFormaldehydeModel` and `pmfFormaldehydeModelVtau` classes are function models for the Formaldehyde spectrum.", "original_line_number": 13216, "instruction_length": 593}
{"instruction": "Kod ini menguji garpu bip di Python 2. Untuk menjalankan ujian, ikuti langkah -langkah berikut: \n\n1. Mulakan nod tunggal.\n2. Gunakan RegTest Lock-in dengan isyarat blok 108/144.\n3. Aktifkan selepas 144 blok lagi.\n4. lombong 2 blok dan simpan koin untuk kegunaan kemudian.\n5. Tambang 141 blok ke peralihan dari ditakrifkan hingga bermula.\n6. Tambang 100 blok memberi isyarat kesediaan dan 44 tidak agar gagal mengubah keadaan tempoh ini.\n7. lombong 108 blok memberi isyarat kesediaan dan 36 blok tidak memberi isyarat kesediaan (bermula-> terkunci_in).\n8. lombong lebih lanjut 143 blok (terkunci_in).\n9. Uji bahawa penguatkuasaan tidak mencetuskan (yang mencetuskan aktif).\n10. Uji bahawa penguatkuasaan telah mencetuskan.", "input": "", "output": "#!/usr/bin/env python2\n# Copyright (c) 2015 The Deuscoin Core developers\n# Distributed under the MIT/X11 software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n#\n\nfrom test_framework.test_framework import ComparisonTestFramework\nfrom test_framework.util import *\nfrom test_framework.mininode import CTransaction, NetworkThread\nfrom test_framework.blocktools import create_coinbase, create_block\nfrom test_framework.comptool import TestInstance, TestManager\nfrom test_framework.script import CScript, OP_1NEGATE, OP_NOP3, OP_DROP\nfrom binascii import hexlify, unhexlify\nimport cStringIO\nimport time\nimport itertools\n\n'''\nThis test is meant to exercise BIP forks\nConnect to a single node.\nregtest lock-in with 108/144 block signalling\nactivation after a further 144 blocks\nmine 2 block and save coinbases for later use\nmine 141 blocks to transition from DEFINED to STARTED\nmine 100 blocks signalling readiness and 44 not in order to fail to change state this period\nmine 108 blocks signalling readiness and 36 blocks not signalling readiness (STARTED->LOCKED_IN)\nmine a further 143 blocks (LOCKED_IN)\ntest that enforcement has not triggered (which triggers ACTIVE)\ntest that enforcement has triggered\n'''\n\n\n\nclass BIP9SoftForksTest(ComparisonTestFramework):\n\n    def __init__(self):\n        self.num_nodes = 1\n\n    def setup_network(self):\n        self.nodes = start_nodes(1, self.options.tmpdir,\n                                 extra_args=[['-debug', '-whitelist=127.0.0.1']],\n                                 binary=[self.options.testbinary])\n\n    def run_test(self):\n        self.test = TestManager(self, self.options.tmpdir)\n        self.test.add_all_connections(self.nodes)\n        NetworkThread().start() # Start up network handling in another thread\n        self.test.run()\n\n    def create_transaction(self, node, coinbase, to_address, amount):\n        from_txid = node.getblock(coinbase)['tx'][0]\n        inputs = [{ \"txid\" : from_txid, \"vout\" : 0}]\n        outputs = { to_address : amount }\n        rawtx = node.createrawtransaction(inputs, outputs)\n        tx = CTransaction()\n        f = cStringIO.StringIO(unhexlify(rawtx))\n        tx.deserialize(f)\n        tx.nVersion = 2\n        return tx\n\n    def sign_transaction(self, node, tx):\n        signresult = node.signrawtransaction(hexlify(tx.serialize()))\n        tx = CTransaction()\n        f = cStringIO.StringIO(unhexlify(signresult['hex']))\n        tx.deserialize(f)\n        return tx\n\n    def generate_blocks(self, number, version, test_blocks = []):\n        for i in xrange(number):\n            block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n            block.nVersion = version\n            block.rehash()\n            block.solve()\n            test_blocks.append([block, True])\n            self.last_block_time += 1\n            self.tip = block.sha256\n            self.height += 1\n        return test_blocks\n\n    def get_bip9_status(self, key):\n        info = self.nodes[0].getblockchaininfo()\n        for row in info['bip9_softforks']:\n            if row['id'] == key:\n                return row\n        raise IndexError ('key:\"%s\" not found' % key)\n\n\n    def test_BIP(self, bipName, activated_version, invalidate, invalidatePostSignature):\n        # generate some coins for later\n        self.coinbase_blocks = self.nodes[0].generate(2)\n        self.height = 3  # height of the next block to build\n        self.tip = int (\"0x\" + self.nodes[0].getbestblockhash() + \"L\", 0)\n        self.nodeaddress = self.nodes[0].getnewaddress()\n        self.last_block_time = int(time.time())\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'defined')\n\n        # Test 1\n        # Advance from DEFINED to STARTED\n        test_blocks = self.generate_blocks(141, 4)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'started')\n\n        # Test 2\n        # Fail to achieve LOCKED_IN 100 out of 144 signal bit 1\n        # using a variety of bits to simulate multiple parallel softforks\n        test_blocks = self.generate_blocks(50, activated_version) # 0x20000001 (signalling ready)\n        test_blocks = self.generate_blocks(20, 4, test_blocks) # 0x00000004 (signalling not)\n        test_blocks = self.generate_blocks(50, activated_version, test_blocks) # 0x20000101 (signalling ready)\n        test_blocks = self.generate_blocks(24, 4, test_blocks) # 0x20010000 (signalling not)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'started')\n\n        # Test 3\n        # 108 out of 144 signal bit 1 to achieve LOCKED_IN\n        # using a variety of bits to simulate multiple parallel softforks\n        test_blocks = self.generate_blocks(58, activated_version) # 0x20000001 (signalling ready)\n        test_blocks = self.generate_blocks(26, 4, test_blocks) # 0x00000004 (signalling not)\n        test_blocks = self.generate_blocks(50, activated_version, test_blocks) # 0x20000101 (signalling ready)\n        test_blocks = self.generate_blocks(10, 4, test_blocks) # 0x20010000 (signalling not)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'locked_in')\n\n        # Test 4\n        # 143 more version 536870913 blocks (waiting period-1)\n        test_blocks = self.generate_blocks(143, 4)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'locked_in')\n\n        # Test 5\n        # Check that the new rule is enforced\n        spendtx = self.create_transaction(self.nodes[0],\n                self.coinbase_blocks[0], self.nodeaddress, 1.0)\n        invalidate(spendtx)\n        spendtx = self.sign_transaction(self.nodes[0], spendtx)\n        spendtx.rehash()\n        invalidatePostSignature(spendtx)\n        spendtx.rehash()\n        block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n        block.nVersion = activated_version\n        block.vtx.append(spendtx)\n        block.hashMerkleRoot = block.calc_merkle_root()\n        block.rehash()\n        block.solve()\n\n        self.last_block_time += 1\n        self.tip = block.sha256\n        self.height += 1\n        yield TestInstance([[block, True]])\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'active')\n\n        # Test 6\n        # Check that the new sequence lock rules are enforced\n        spendtx = self.create_transaction(self.nodes[0],\n                self.coinbase_blocks[1], self.nodeaddress, 1.0)\n        invalidate(spendtx)\n        spendtx = self.sign_transaction(self.nodes[0], spendtx)\n        spendtx.rehash()\n        invalidatePostSignature(spendtx)\n        spendtx.rehash()\n\n        block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n        block.nVersion = 5\n        block.vtx.append(spendtx)\n        block.hashMerkleRoot = block.calc_merkle_root()\n        block.rehash()\n        block.solve()\n        self.last_block_time += 1\n        yield TestInstance([[block, False]])\n\n        # Restart all\n        stop_nodes(self.nodes)\n        wait_deuscoinds()\n        shutil.rmtree(self.options.tmpdir)\n        self.setup_chain()\n        self.setup_network()\n        self.test.clear_all_connections()\n        self.test.add_all_connections(self.nodes)\n        NetworkThread().start() # Start up network handling in another thread\n\n\n\n    def get_tests(self):\n        for test in itertools.chain(\n                self.test_BIP('csv', 536870913, self.sequence_lock_invalidate, self.donothing),\n                self.test_BIP('csv', 536870913, self.mtp_invalidate, self.donothing),\n                self.test_BIP('csv', 536870913, self.donothing, self.csv_invalidate)\n        ):\n            yield test\n\n    def donothing(self, tx):\n        return\n\n    def csv_invalidate(self, tx):\n        '''Modify the signature in vin 0 of the tx to fail CSV\n        Prepends -1 CSV DROP in the scriptSig itself.\n        '''\n        tx.vin[0].scriptSig = CScript([OP_1NEGATE, OP_NOP3, OP_DROP] +\n                                      list(CScript(tx.vin[0].scriptSig)))\n\n    def sequence_lock_invalidate(self, tx):\n        '''Modify the nSequence to make it fails once sequence lock rule is activated (high timespan)\n        '''\n        tx.vin[0].nSequence = 0x00FFFFFF\n        tx.nLockTime = 0\n\n    def mtp_invalidate(self, tx):\n        '''Modify the nLockTime to make it fails once MTP rule is activated\n        '''\n        # Disable Sequence lock, Activate nLockTime\n        tx.vin[0].nSequence = 0x90FFFFFF\n        tx.nLockTime = self.last_block_time\n\nif __name__ == '__main__':\n    BIP9SoftForksTest().main()", "instruction_en": "This code tests BIP forks in Python 2. To run the test, follow these steps: \n\n1. Start a single node.\n2. Use regtest lock-in with 108/144 block signalling.\n3. Activate after a further 144 blocks.\n4. Mine 2 blocks and save coinbases for later use.\n5. Mine 141 blocks to transition from DEFINED to STARTED.\n6. Mine 100 blocks signalling readiness and 44 not in order to fail to change state this period.\n7. Mine 108 blocks signalling readiness and 36 blocks not signalling readiness (STARTED->LOCKED_IN).\n8. Mine a further 143 blocks (LOCKED_IN).\n9. Test that enforcement has not triggered (which triggers ACTIVE).\n10. Test that enforcement has triggered.", "original_line_number": 14044, "instruction_length": 721}
{"instruction": "Buat skrip Python yang merangkumi fungsi berikut:\n- Mean_Accuracy (GroundTruth, Ramalan): Mengira ketepatan min antara tanah dan ramalan menggunakan matriks kekeliruan dari sklearn.metrics.\n- memastikan_dir (alamat): cek jika direktori di alamat yang diberikan wujud, dan menciptanya jika tidak.\n- Draw_BBox (IMG, BBOX, COLOR = 100, WIDTH = 2): Lukis segi empat tepat dengan warna dan lebar yang diberikan di sekitar kotak sempadan (bbox) pada imej yang diberikan (IMG).\n- get_rect (img, rect_info): Mengembalikan kawasan segi empat tepat imej (IMG) yang ditakrifkan oleh koordinat (xmin, xmax, ymin, ymax) dalam rect_info.\n- get_rect_from_bbox (img, bbox): Mengembalikan kawasan segi empat tepat imej (IMG) yang ditakrifkan oleh kotak terikat (bbox).\n- l2_feat_norm (feat): Menormalkan vektor ciri (feat) menggunakan normalisasi L2.", "input": "", "output": "import numpy\nimport sklearn.metrics\nimport os\nimport cv2\nimport numpy as np\n\n\ndef mean_accuracy(groundtruth, predictions):\n    groundtruth_cm = sklearn.metrics.confusion_matrix(groundtruth, groundtruth).astype(numpy.float32)\n    predictions_cm = sklearn.metrics.confusion_matrix(predictions, groundtruth).astype(numpy.float32)\n    return numpy.mean(numpy.diag(predictions_cm) / numpy.diag(groundtruth_cm))\n\n\ndef ensure_dir(address):\n    if not os.path.exists(address):\n        os.makedirs(address)\n\n\ndef draw_bbox(img, bbox, color=100, width=2):\n    try:\n        bx, by, bw, bh = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n    except:\n        bx, by, bw, bh = bbox\n    new_img = img.copy()\n    cv2.rectangle(new_img, (bx, by), (bx+bw, by+bh), color, width)\n    return new_img\n\n\ndef get_rect(img, rect_info):\n    xmin, xmax, ymin, ymax = rect_info\n    return img[xmin:xmax, ymin:ymax]\n\n\ndef get_rect_from_bbox(img, bbox):\n    by, bx, bw, bh = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n\n    return img[bx:bx+bh, by:by+bw]\n\n\ndef l2_feat_norm(feat):\n    row_norms = np.linalg.norm(feat, axis=1)\n    new_feat = feat / row_norms[:, np.newaxis]\n    return new_feat", "instruction_en": "Create a Python script that includes the following functions:\n- mean_accuracy(groundtruth, predictions): calculates the mean accuracy between the groundtruth and predictions using the confusion matrix from sklearn.metrics.\n- ensure_dir(address): checks if the directory at the given address exists, and creates it if it doesn't.\n- draw_bbox(img, bbox, color=100, width=2): draws a rectangle with the given color and width around the bounding box (bbox) on the given image (img).\n- get_rect(img, rect_info): returns the rectangular region of the image (img) defined by the coordinates (xmin, xmax, ymin, ymax) in rect_info.\n- get_rect_from_bbox(img, bbox): returns the rectangular region of the image (img) defined by the bounding box (bbox).\n- l2_feat_norm(feat): normalizes the feature vector (feat) using L2 normalization.", "original_line_number": 14205, "instruction_length": 833}
{"instruction": "Kod yang diberikan adalah kelas python yang dipanggil `isomeasurer` yang mewarisi dari kelas lain yang dipanggil` pengukur`. Ia mengandungi kaedah untuk membuat pengukuran isophotal pada imej astronomi, termasuk mendapatkan laluan fail untuk kontur isophotes, membuat pengukuran pada peta dan menulisnya ke fail CSV, membuat panel visual untuk memaparkan imej komposit dan pengukuran isophotal, dan mendapatkan kontur dari imej. Kelas ini juga mengandungi beberapa kaedah penolong untuk menukar antara unit yang berbeza dan untuk membaca dan menulis fail acar python. Untuk menggunakan kod ini, buat contoh kelas `isomeasurer` dan panggil kaedahnya dengan parameter yang sesuai.", "input": "", "output": "# isomeasurer.py \n# ALS 2017/06/01\nimport os\nimport astropy.units as u\nfrom astropy.io import fits\nimport numpy as np\nimport astropy.table as at\nimport pickle\nimport scipy.ndimage as simg\n\n\nfrom ..measurer import Measurer\nfrom ... import tabtools\nfrom . import polytools\nfrom . import plottools\n\nclass isoMeasurer(Measurer):\n\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\tchild of Measurer\n\t\tdo isophotal measurements\n\t\t\"\"\"\n\t\tsuper(isoMeasurer, self).__init__(**kwargs)\n\n\t\tself.msrtype = 'iso'\n\n\n\tdef get_fp_contours(self, imgtag='OIII5008_I', onlycenter=False, suffix=''):\n\t\t\"\"\" e.g., msr_iso-OIII5008_I{suffix}_contours.pkl \n\t\t\t\\or   msr_iso-OIII5008_I{suffix}_contours-ctr.pkl \n\t\t\"\"\"\n\t\tif onlycenter:\n\t\t\tctrtag = '-ctr'\n\t\telse:\n\t\t\tctrtag = ''\n\n\t\tfp_root = self.get_fp_msrtagroot(imgtag=imgtag, suffix=suffix)\n\t\treturn fp_root+'_contours{ctrtag}.pkl'.format(ctrtag=ctrtag)\n\n\n\tdef make_measurements(self, imgtag='OIII5008_I', isocut=3.e-15*u.Unit('erg / (arcsec2 cm2 s)'), minarea=5, onlycenter=True, centerradius=5.*u.arcsec, plotsuffix='', savecontours=False, plotmsr=False, msrsuffix='', overwrite=False, append=False):\n\t\t\"\"\"\n\t\tmake measurements on a map and write to msr_iso.csv. \n\t\t\tif imgtag='OIII5008_I' then measure 'stamp-OIII5008_I.fits'\n\n\t\tParams\n\t\t------\n\t\tself\n\t\timgtag='OIII5008_I'\n\t\toverwrite = False (bool)\n\n\t\tisocut=1.e-15*u.Unit('erg / (arcsec2 cm2 s)'):\n\t\t\tisophote cut\n\t\tminarea=0:\n\t\t\tconnected contour area (# pix) above the area is counted as part of the isophote measurement\n\t\tonlycenter=False:\n\t\t\twhether to consider only the center contours\n\t\tcenterradius=2.*u.arcsec\n\t\tplotsuffix = '':\n\t\t\tplotsuffix label to be attach to the end of the plot or contour file names. \n\t\tsavecontours=False\n\t\tplotmsr=False\n\t\tmsrsuffix=''\n\t\t\tplotsuffix label in the end of the measurement csv file: msr_iso_{msrsuffix}.csv.\n\t\toverwrite=False\n\t\tappend=False\n\n\t\tReturn\n\t\t------\n\t\tstatus (bool)\n\n\t\tWrite Output \n\t\t------------\n\t\te.g., msr_iso.csv\n\t\t\"\"\"\n\t\tfn = self.get_fp_msr(msrsuffix=msrsuffix)\n\n\t\tcondi = {'imgtag': imgtag, 'isocut': isocut, 'minarea': minarea, 'onlycenter': onlycenter, 'centerradius': centerradius}\n\n\t\tif append or overwrite or (not tabtools.fn_has_row(fn, condi)):\n\t\t\tprint(\"[isomeasurer] making measurement\")\n\n\t\t\timg = self.get_stamp_img(imgtag=imgtag, wunit=True)\n\t\t\txc, yc = self._get_xc_yc(img)\n\n\t\t\t# calc\n\t\t\tif np.all(~np.isnan(img)): \n\t\t\t\tcontours = self._get_contours_from_img(img=img, isocut=isocut, xc=xc, yc=yc, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius)\n\t\t\t\ttab_msr = self._get_tab_measurements_from_contours(contours=contours, xc=xc, yc=yc)\n\t\t\telse: \n\t\t\t\tcontours = []\n\t\t\t\ttab_msr = self._get_tab_measurements_nan()\n\n\t\t\ttab_params = self._get_tab_params(imgtag=imgtag, isocut=isocut, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius)\n\t\t\ttabout = at.hstack([tab_params, tab_msr])\n\n\t\t\t# output\n\t\t\ttabtools.write_row(fn=fn, row=tabout, condi=condi, overwrite=overwrite, append=append)\n\n\t\t\t# optional output\n\t\t\tif savecontours:\n\t\t\t\tfn_contours = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=plotsuffix)\n\t\t\t\twrite_pickle(contours, fn_contours, overwrite=overwrite)\n\n\t\t\tif plotmsr:\n\t\t\t\tfn_plot = self.get_fp_msrplot(imgtag=imgtag, suffix=plotsuffix)\n\t\t\t\tplottools.make_plot_img_w_contours(fn_plot=fn_plot, img=img, contours=contours)\n\n\t\telse:\n\t\t\tprint(\"[isomeasurer] skip making measurement as files exist\")\n\n\t\treturn os.path.isfile(fn)\n\n\n\tdef make_visualpanel(self, fn=None, compo_bands ='gri', imgtag='OIII5008_I', onlycenter=True, minarea=5, centerradius=5.*u.arcsec, tocolorbar=True, totitle=True, fontsize=12, overwrite=False):\n\t\t\"\"\" \n\t\tmake panel figure to visualize the composit and the iso measurements\n\t\tsaved to e.g., 'msr_iso-OIII5008_I_panel.pdf'\n\n\t\tParams\n\t\t------\n\t\tfn = None: default: msr_iso_{imgtag}_panel.pdf\n\t\tcompo_bands ='gri', imgtag='OIII5008_I', overwrite=False\n\n\t\tReturn\n\t\t------\n\t\tstatus\n\t\t\"\"\"\n\t\tif fn is None:\n\t\t\tfn = self.get_fp_msrplot(imgtag=imgtag, suffix='_panel')\n\t\telse: \n\t\t\tfn = self.dir_obj+fn\n\n\t\tif not os.path.isfile(fn) or overwrite:\n\t\t\tprint(\"[isomeasurer] making visual panel\")\n\n\t\t\t# get files ready \n\t\t\tself.make_colorimg(bands=compo_bands, img_type='stamp', overwrite=False)\n\n\t\t\t# access data\n\t\t\timg_compo = simg.imread(self.dir_obj+'color_stamp-{}.png'.format(compo_bands))\n\t\t\timg_map = self.get_stamp_img(imgtag=imgtag, wunit=False)\n\n\t\t\tsuffix = '_3e-15'\n\t\t\tisocut = 3.e-15*u.Unit('erg / (arcsec2 cm2 s)')\n\t\t\tfn_contours3 = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=suffix)\n\t\t\tif not os.path.isfile(fn_contours3):\n\t\t\t\tprint(\"[isomeasurer] re-doing measurements to make contours required for visual panel plots\")\n\t\t\t\tself.make_measurements(imgtag=imgtag, isocut=isocut, plotsuffix=suffix, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius, overwrite=True, savecontours=True, plotmsr=False), \n\n\t\t\tcontours3 = read_pickle(fn_contours3)\n\n\t\t\tsuffix = '_1e-15'\n\t\t\tisocut = 1.e-15*u.Unit('erg / (arcsec2 cm2 s)')\n\t\t\tfn_contours1 = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=suffix)\n\t\t\tif not os.path.isfile(fn_contours1):\n\t\t\t\tprint(\"[isomeasurer] re-doing measurements to make contours required for visual panel plots\")\n\t\t\t\tself.make_measurements(imgtag=imgtag, isocut=isocut, plotsuffix=suffix, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius, overwrite=True, savecontours=True, plotmsr=False), \n\n\t\t\tcontours1 = read_pickle(fn_contours1)\n\n\t\t\tz = self.z\n\t\t\tpixsize = self.pixsize.to_value(u.arcsec)\n\t\t\tlegend_suffix = ' at 3'\n\t\t\tname = self.obj.name[4:]\n\n\t\t\ttitle_compo = '${}~{}~{}~$'.format(compo_bands[0], compo_bands[1], compo_bands[2])+'$\\mathrm{Composite}$'\n\t\t\ttitle_map = '$\\mathrm{[OIII]\\lambda 5007~Intensity}$'\n\t\t\tlabel_cbar = '$I~[10^{-15}~\\mathrm{erg~s^{-1}~cm^{-2}~arcsec^{-2}}]$'\n\n\t\t\tplottools.make_iso_visual_panel(fn, img_compo, img_map, contours1, contours3, z, pixsize, legend_suffix, name, title_compo, title_map, label_cbar, tocolorbar=tocolorbar, totitle=totitle, fontsize=fontsize)\n\n\t\telse:\n\t\t\tprint(\"[isomeasurer] skip making visual panel as files exist\")\n\n\t\treturn os.path.isfile(fn)\n\n\t\t\n\tdef _get_tab_params(self, imgtag, isocut, minarea, onlycenter, centerradius):\n\t\t\"\"\"\n\t\treturn a one row table of the measurement params \n\t\t\"\"\"\n\t\ttab = at.Table([[imgtag], [str(isocut)], [minarea], [onlycenter], [str(centerradius)], ], names=['imgtag', 'isocut', 'minarea', 'onlycenter', 'centerradius', ])\n\t\treturn tab\n\n\n\tdef _get_tab_measurements_from_contours(self, contours, xc, yc):\n\t\t\"\"\" \n\t\tcalculate iso measurements from contours, return a table like: \n\t\t\"\"\"\n\n\t\ttab = polytools.ShapeParamsTab_from_contours(contours, xc, yc)\n\n\t\t# unit conversion\n\t\tarea_ars = tab['area_pix'][0]*(self.pixsize/u.arcsec)**2\n\t\tdmax_ars = self._pix_to_theta(tab['dmax_pix'][0], wunit=False)\n\t\trmax_ars = self._pix_to_theta(tab['rmax_pix'][0], wunit=False)\n\t\tdper_ars = self._pix_to_theta(tab['dper_pix'][0], wunit=False)\n\n\t\tkpc_per_arcsec = np.array(self._get_kpc_proper_per_arcsec())\n\n\t\tarea_kpc = area_ars * kpc_per_arcsec**2\n\t\tdmax_kpc = dmax_ars * kpc_per_arcsec\n\t\trmax_kpc = rmax_ars * kpc_per_arcsec\n\t\tdper_kpc = dper_ars * kpc_per_arcsec\n\n\t\ttab_converted = at.Table(names=['area_kpc', 'dmax_kpc', 'rmax_kpc', 'dper_kpc', 'area_ars', 'dmax_ars', 'rmax_ars', 'dper_ars', ])\n\t\ttab_converted.add_row([area_kpc, dmax_kpc, rmax_kpc, dper_kpc, area_ars, dmax_ars, rmax_ars, dper_ars, ])\n\n\t\ttabout = at.hstack([tab_converted, tab])\n\n\t\treturn tabout\n\n\n\tdef _get_tab_measurements_nan(self):\n\t\t\"\"\" \n\t\treturn a tab measurement just like _get_tab_measurements_from_contours() but with entries all nan. \n\t\t\"\"\"\n\t\tnames = ['area_kpc', 'dmax_kpc', 'rmax_kpc', 'dper_kpc', 'area_ars', 'dmax_ars', 'rmax_ars', 'dper_ars', 'area_pix', 'dmax_pix', 'rmax_pix', 'dper_pix', 'theta_dmax', 'theta_rmax', 'theta_dper', 'aspectr']\n\n\t\ttabout = at.Table(names=names)\n\t\ttabout.add_row([np.nan for i in range(len(names))])\n\n\t\treturn tabout\n\n\n\tdef _get_contours_from_img(self, img, isocut, xc, yc, minarea=0., onlycenter=False, centerradius=2.*u.arcsec):\n\t\t\"\"\"\n\t\tmake contour at isocut of image as python pickle file (fn_contours)\n\t\talways overwrite\n\n\t\tParams\n\t\t------\n\t\tself\n\t\timg (array)\n\t\tisocut (float or quantity):\n\t\t\thas to be of the same type of unit as image\n\t\tminarea (float):\n\t\t\tminimum area (pix) to be considered as contour patch\n\t\tonlycenter (bool):\n\t\t\twhether to take only center patches as patches (they all have to pass minarea test as well)\n\t\tcenterradius (angular quantity):\n\t\t\tif onlycenter = True, then it sets the radius of the center area. only patches overlapping with that area will be considered. \n\n\t\t\"\"\"\n\n\t\t# prep\n\t\ttry: \n\t\t\timg.unit\n\t\texcept:\n\t\t\timg_nparr = img/isocut\n\t\telse:\n\t\t\timg_nparr = np.array((img/isocut).to(u.dimensionless_unscaled))\n\n\t\t# find contours -- satisfy minarea\n\t\tcontours = polytools.find_largecontours(img=img_nparr, threshold=1., minarea=minarea)\n\n\t\tif onlycenter:  # select only those at the center\n\t\t\tcenterradius_pix = self._theta_to_pix(centerradius)\n\t\t\tcontours = polytools.select_center_contours(contours, xc, yc, radius=centerradius_pix)\n\n\t\treturn contours\n\n\n\ndef read_pickle(fn):\n\twith open(fn, 'rb') as handle:\n\t\tresult = pickle.load(handle)\n\treturn result\n\n\ndef write_pickle(result, fn, overwrite=False):\n\n\tif not os.path.isfile(fn) or overwrite:\n\t\twith open(fn, 'wb') as handle:\n\t\t\tpickle.dump(result, handle)", "instruction_en": "The given code is a Python class called `isoMeasurer` that inherits from another class called `Measurer`. It contains methods for making isophotal measurements on astronomical images, including getting the file path for the contours of the isophotes, making measurements on a map and writing them to a CSV file, making a visual panel to display the composite image and the isophotal measurements, and getting the contours from an image. The class also contains some helper methods for converting between different units and for reading and writing Python pickle files. To use this code, create an instance of the `isoMeasurer` class and call its methods with appropriate parameters.", "original_line_number": 14310, "instruction_length": 678}
{"instruction": "Kod ini mengkonfigurasi fail MML dengan tetapan DataSource. Untuk menggunakannya, anda perlu menjalankan skrip dengan hujah -hujah berikut: \n\n-HOST: Nama Hos (lalai adalah 'localhost')\n--Port: Nombor port (lalai adalah '5432')\n--dbname: Nama pangkalan data (lalai adalah 'abs')\n--User: Nama pengguna (lalai adalah 'abs')\n--password: kata laluan (lalai adalah 'abs')\n--SRS: Sistem rujukan spatial (lalai adalah ' +proj = merc +a = 6378137 +b = 6378137 +lat_ts = 0.0 +lon_0 = 0.0 +x_0 = 0.0 +y_0 = 0.0 +k = 1.0 +unit = m +nadgrids +\n--Shapedir: Direktori di mana shapefiles terletak (lalai adalah './layers/')\n-Extent: Kotak Terikat untuk Mengehadkan Pertanyaan (lalai adalah '12570320.00, -5403474.50,17711958.00, -1636391.88')\n--MML: Nama fail MML untuk dikonfigurasi (diperlukan)\n\nSkrip akan mengubah suai fail MML dengan mengemas kini tetapan DataSource untuk lapisan yang menggunakan DataSource PostGIS dan menambah tetapan DataSource untuk lapisan yang menggunakan shapefiles. Fail MML yang diubah suai akan disimpan di lokasi yang sama dengan fail asal.", "input": "", "output": "#!/usr/bin/env python\n\n# Copyright (c) 2011, Development Seed, Inc.\n#               2011, Andrew Harvey <andrew.harvey4@gmail.com>\n#               All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in\n#       the documentation and/or other materials provided with the\n#       distribution.\n#     * Neither the name of the Development Seed, Inc. nor the names of\n#       its contributors may be used to endorse or promote products\n#       derived from this software without specific prior written\n#       permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n# IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n# TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n# PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER\n# OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport json\nfrom sys import path\nfrom os.path import join\nimport argparse\n\n#################################\n## argparse\n\nparser = argparse.ArgumentParser(description='Configure an MML file with datasource settings')\nparser.add_argument('--host', default='localhost')\nparser.add_argument('--port', default='5432')\nparser.add_argument('--dbname', default='abs')\nparser.add_argument('--user', default='abs')\nparser.add_argument('--password', default='abs')\nparser.add_argument('--srs', default='+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0.0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over')\n\nparser.add_argument('--shapedir', default='./layers/')\n\n# Increase performance if you are only rendering a particular area by\n# specifying a bounding box to restrict queries. Format is \"XMIN,YMIN,XMAX,YMAX\" in the\n# same units as the database (probably spherical mercator meters). The\n# whole world is \"-20037508.34,-20037508.34,20037508.34,20037508.34\".\n# Leave blank to let Mapnik estimate.\nparser.add_argument('--extent', default='12570320.00,-5403474.50,17711958.00,-1636391.88')\n\nparser.add_argument('--mml', required=True)\n\nargs = parser.parse_args()\n\n#################################\n## configure mml\n\nmml = join(path[0], args.mml + '/' + args.mml + '.mml')\n\nshoreline_300 = args.shapedir.rstrip('/') + '/shoreline_300.shp'\nprocessed_p = args.shapedir.rstrip('/') + '/processed_p.shp'\n\nwith open(mml, 'r') as f:\n  newf = json.loads(f.read())\nf.closed\n\nwith open(mml, 'w') as f:\n  for layer in newf[\"Layer\"]:\n    if \"Datasource\" in layer:\n      ds_type = layer[\"Datasource\"].get(\"type\")\n      if ds_type and ds_type == \"postgis\":\n        layer[\"Datasource\"][\"host\"] = args.host\n        layer[\"Datasource\"][\"port\"] = args.port\n        layer[\"Datasource\"][\"dbname\"] = args.dbname\n        layer[\"Datasource\"][\"user\"] = args.user\n        layer[\"Datasource\"][\"password\"] = args.password\n        layer[\"Datasource\"][\"extent\"] = args.extent\n        layer[\"srs\"] = args.srs\n    else:\n      if layer[\"id\"] == \"shoreline_300\":\n        layer[\"Datasource\"] = dict();\n        layer[\"Datasource\"][\"file\"] = shoreline_300\n        layer[\"Datasource\"][\"type\"] = 'shape'\n        layer[\"geometry\"] = 'polygon'\n        layer[\"srs\"] = '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over'\n      elif layer[\"id\"] == \"processed_p\":\n        layer[\"Datasource\"] = dict();\n        layer[\"Datasource\"][\"file\"] = processed_p\n        layer[\"Datasource\"][\"type\"] = 'shape'\n        layer[\"geometry\"] = 'polygon'\n        layer[\"srs\"] = '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over'\n  f.write(json.dumps(newf, indent=2))\nf.closed", "instruction_en": "This code configures an MML file with datasource settings. To use it, you need to run the script with the following arguments: \n\n--host: the host name (default is 'localhost')\n--port: the port number (default is '5432')\n--dbname: the database name (default is 'abs')\n--user: the user name (default is 'abs')\n--password: the password (default is 'abs')\n--srs: the spatial reference system (default is '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0.0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over')\n--shapedir: the directory where the shapefiles are located (default is './layers/')\n--extent: the bounding box to restrict queries (default is '12570320.00,-5403474.50,17711958.00,-1636391.88')\n--mml: the name of the MML file to be configured (required)\n\nThe script will modify the MML file by updating the datasource settings for layers that use a postgis datasource and adding datasource settings for layers that use shapefiles. The modified MML file will be saved in the same location as the original file.", "original_line_number": 14318, "instruction_length": 1058}
{"instruction": "Kod ini mentakrifkan fungsi yang dipanggil `parse_source` yang mengambil rentetan sebagai input dan mengembalikan tuple bernama` saluran`. Tuple `Channel` mempunyai tiga bidang:` type`, `kata kunci`, dan` berjenama`. Bidang `jenis` adalah integer yang mewakili jenis saluran (1 untuk langsung, 2 untuk PPC, dan 3 untuk SEO). Medan `Kata Kunci` adalah rentetan yang mewakili kata kunci yang berkaitan dengan saluran. Bidang `berjenama` adalah boolean yang menunjukkan sama ada kata kunci berjenama atau tidak. Fungsi pertama memeriksa jika rentetan input bermula dengan \"PPC\" atau \"SEO\" dan menetapkan medan `jenis` dengan sewajarnya. Jika rentetan input tidak bermula dengan \"PPC\" atau \"SEO\", medan `Type` ditetapkan kepada 1 (langsung). Fungsi itu kemudian memanggil sama ada `get_ppc_keywords` atau` get_seo_keywords` untuk mengekstrak kata kunci dari rentetan input, bergantung kepada nilai medan `type`. Akhirnya, fungsi memanggil `is_branded` untuk menentukan sama ada kata kunci berjenama atau tidak dan mengembalikan tuple` saluran`.", "input": "", "output": "'''\n * Author: Lukasz Jachym\n * Date: 9/14/13\n * Time: 5:40 PM\n *\n * This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.\n * To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/.\n'''\n\nfrom collections import namedtuple\nfrom branding import is_branded\n\nDIRECT = 1\nPPC = 2\nSEO = 3\n\nChannel = namedtuple('Channel', ['type', 'keywords', 'branded'])\n\ndef get_ppc_keywords(ppc_str):\n    \"\"\"\n    :param ppc_str:\n    :return: keywords\n    \"\"\"\n\n    try:\n        keywords = ppc_str.split('_')[2]\n\n    except KeyError:\n        raise ValueError\n\n    return keywords\n\ndef get_seo_keywords(seo_query):\n    \"\"\"\n    :param ppc_str:\n    :return: keywords\n    \"\"\"\n\n    try:\n        keywords = seo_query.split(' :: ')[2]\n\n    except KeyError:\n        raise ValueError\n\n    return keywords\n\ndef parse_source(source_str):\n\n    if (source_str[0:3] == 'ppc'):\n        channel_type = PPC\n        keywords = get_ppc_keywords(source_str)\n        channel_keywords = keywords\n        channel_branded = is_branded(keywords)\n        channel = Channel(channel_type, channel_keywords, channel_branded)\n    else:\n        if source_str[0:3] == 'seo':\n            channel_type = SEO\n            keywords = get_seo_keywords(source_str)\n            channel_keywords = keywords\n            channel_branded = is_branded(keywords)\n            channel = Channel(channel_type, channel_keywords, channel_branded)\n        else:\n            channel_type = DIRECT\n            channel_keywords = None\n            channel_branded = False\n            channel = Channel(channel_type, channel_keywords, channel_branded)\n\n    return channel", "instruction_en": "This code defines a function called `parse_source` that takes a string as input and returns a `Channel` named tuple. The `Channel` tuple has three fields: `type`, `keywords`, and `branded`. The `type` field is an integer that represents the type of channel (1 for DIRECT, 2 for PPC, and 3 for SEO). The `keywords` field is a string that represents the keywords associated with the channel. The `branded` field is a boolean that indicates whether the keywords are branded or not. The function first checks if the input string starts with \"ppc\" or \"seo\" and sets the `type` field accordingly. If the input string does not start with \"ppc\" or \"seo\", the `type` field is set to 1 (DIRECT). The function then calls either `get_ppc_keywords` or `get_seo_keywords` to extract the keywords from the input string, depending on the value of the `type` field. Finally, the function calls `is_branded` to determine whether the keywords are branded or not and returns the `Channel` tuple.", "original_line_number": 14586, "instruction_length": 1040}
{"instruction": "Kod ini mentakrifkan kelas papan kekunci yang boleh mensimulasikan peristiwa papan kekunci pada Mac. Untuk menggunakannya, buat contoh kelas papan kekunci dan panggil kaedah Press_Key dan Release_key dengan nama kunci yang anda mahu simulasi. Kaedah Special_Key_Assignment menetapkan beberapa kunci khas yang biasa untuk menjadikannya lebih mudah digunakan. Kelas KeyboardEvent menetapkan pendengar untuk acara papan kekunci dan memanggil kaedah yang sesuai pada kelas papan kekunci apabila peristiwa berlaku. Untuk menggunakannya, buat contoh kelas keyboardEvent dan panggil kaedah lariannya untuk mula mendengar acara.", "input": "", "output": "#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport time\nfrom Quartz import *\nfrom AppKit import NSEvent\nfrom .base import KeyboardMeta, KeyboardEventMeta\n\n# Taken from events.h\n# /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/Headers/Events.h\ncharacter_translate_table = {\n    'a': 0x00,\n    's': 0x01,\n    'd': 0x02,\n    'f': 0x03,\n    'h': 0x04,\n    'g': 0x05,\n    'z': 0x06,\n    'x': 0x07,\n    'c': 0x08,\n    'v': 0x09,\n    'b': 0x0b,\n    'q': 0x0c,\n    'w': 0x0d,\n    'e': 0x0e,\n    'r': 0x0f,\n    'y': 0x10,\n    't': 0x11,\n    '1': 0x12,\n    '2': 0x13,\n    '3': 0x14,\n    '4': 0x15,\n    '6': 0x16,\n    '5': 0x17,\n    '=': 0x18,\n    '9': 0x19,\n    '7': 0x1a,\n    '-': 0x1b,\n    '8': 0x1c,\n    '0': 0x1d,\n    ']': 0x1e,\n    'o': 0x1f,\n    'u': 0x20,\n    '[': 0x21,\n    'i': 0x22,\n    'p': 0x23,\n    'l': 0x25,\n    'j': 0x26,\n    '\\'': 0x27,\n    'k': 0x28,\n    ';': 0x29,\n    '\\\\': 0x2a,\n    ',': 0x2b,\n    '/': 0x2c,\n    'n': 0x2d,\n    'm': 0x2e,\n    '.': 0x2f,\n    '`': 0x32,\n    ' ': 0x31,\n    '\\r': 0x24,\n    '\\t': 0x30,\n    'shift': 0x38\n}\n\n# Taken from ev_keymap.h\n# http://www.opensource.apple.com/source/IOHIDFamily/IOHIDFamily-86.1/IOHIDSystem/IOKit/hidsystem/ev_keymap.h\nspecial_key_translate_table = {\n    'KEYTYPE_SOUND_UP': 0,\n    'KEYTYPE_SOUND_DOWN': 1,\n    'KEYTYPE_BRIGHTNESS_UP': 2,\n    'KEYTYPE_BRIGHTNESS_DOWN': 3,\n    'KEYTYPE_CAPS_LOCK': 4,\n    'KEYTYPE_HELP': 5,\n    'POWER_KEY': 6,\n    'KEYTYPE_MUTE': 7,\n    'UP_ARROW_KEY': 8,\n    'DOWN_ARROW_KEY': 9,\n    'KEYTYPE_NUM_LOCK': 10,\n    'KEYTYPE_CONTRAST_UP': 11,\n    'KEYTYPE_CONTRAST_DOWN': 12,\n    'KEYTYPE_LAUNCH_PANEL': 13,\n    'KEYTYPE_EJECT': 14,\n    'KEYTYPE_VIDMIRROR': 15,\n    'KEYTYPE_PLAY': 16,\n    'KEYTYPE_NEXT': 17,\n    'KEYTYPE_PREVIOUS': 18,\n    'KEYTYPE_FAST': 19,\n    'KEYTYPE_REWIND': 20,\n    'KEYTYPE_ILLUMINATION_UP': 21,\n    'KEYTYPE_ILLUMINATION_DOWN': 22,\n    'KEYTYPE_ILLUMINATION_TOGGLE': 23\n}\n\nclass Keyboard(KeyboardMeta):\n    def press_key(self, key):\n        if key in special_key_translate_table:\n            self._press_special_key(key, True)\n        else:\n            self._press_normal_key(key, True)\n\n    def release_key(self, key):\n        if key in special_key_translate_table:\n            self._press_special_key(key, False)\n        else:\n            self._press_normal_key(key, False)\n\n    def special_key_assignment(self):\n        self.volume_mute_key = 'KEYTYPE_MUTE'\n        self.volume_down_key = 'KEYTYPE_SOUND_DOWN'\n        self.volume_up_key = 'KEYTYPE_SOUND_UP'\n        self.media_play_pause_key = 'KEYTYPE_PLAY'\n\n        # Doesn't work :(\n        # self.media_next_track_key = 'KEYTYPE_NEXT'\n        # self.media_prev_track_key = 'KEYTYPE_PREVIOUS'\n\n    def _press_normal_key(self, key, down):\n        try:\n            if self.is_char_shifted(key):\n                key_code = character_translate_table[key.lower()]\n\n                event = CGEventCreateKeyboardEvent(None, \n                            character_translate_table['shift'], down)\n                CGEventPost(kCGHIDEventTap, event)\n                # Tiny sleep to let OS X catch up on us pressing shift\n                time.sleep(.01)\n\n            else:\n                key_code = character_translate_table[key]\n\n\n            event = CGEventCreateKeyboardEvent(None, key_code, down)\n            CGEventPost(kCGHIDEventTap, event)\n\n\n        except KeyError:\n            raise RuntimeError(\"Key {} not implemented.\".format(key))\n\n    def _press_special_key(self, key, down):\n        \"\"\" Helper method for special keys. \n\n        Source: http://stackoverflow.com/questions/11045814/emulate-media-key-press-on-mac\n        \"\"\"\n        key_code = special_key_translate_table[key]\n\n        ev = NSEvent.otherEventWithType_location_modifierFlags_timestamp_windowNumber_context_subtype_data1_data2_(\n                NSSystemDefined, # type\n                (0,0), # location\n                0xa00 if down else 0xb00, # flags\n                0, # timestamp\n                0, # window\n                0, # ctx\n                8, # subtype\n                (key_code << 16) | ((0xa if down else 0xb) << 8), # data1\n                -1 # data2\n            )\n\n        CGEventPost(0, ev.CGEvent())\n\nclass KeyboardEvent(KeyboardEventMeta):\n    def run(self):\n        tap = CGEventTapCreate(\n            kCGSessionEventTap,\n            kCGHeadInsertEventTap,\n            kCGEventTapOptionDefault,\n            CGEventMaskBit(kCGEventKeyDown) |\n            CGEventMaskBit(kCGEventKeyUp),\n            self.handler,\n            None)\n\n        loopsource = CFMachPortCreateRunLoopSource(None, tap, 0)\n        loop = CFRunLoopGetCurrent()\n        CFRunLoopAddSource(loop, loopsource, kCFRunLoopDefaultMode)\n        CGEventTapEnable(tap, True)\n\n        while self.state:\n            CFRunLoopRunInMode(kCFRunLoopDefaultMode, 5, False)\n\n    def handler(self, proxy, type, event, refcon):\n        key = CGEventGetIntegerValueField(event, kCGKeyboardEventKeycode)\n        if type == kCGEventKeyDown:\n            self.key_press(key)\n        elif type == kCGEventKeyUp:\n            self.key_release(key)\n\n        if self.capture:\n            CGEventSetType(event, kCGEventNull)\n\n        return event", "instruction_en": "This code defines a Keyboard class that can simulate keyboard events on a Mac. To use it, create an instance of the Keyboard class and call its press_key and release_key methods with the name of the key you want to simulate. The special_key_assignment method sets up some common special keys to make them easier to use. The KeyboardEvent class sets up a listener for keyboard events and calls the appropriate methods on the Keyboard class when events occur. To use it, create an instance of the KeyboardEvent class and call its run method to start listening for events.", "original_line_number": 14922, "instruction_length": 620}
{"instruction": "Kod ini menjana model penyambung menggunakan cadquery. Untuk menjalankan skrip, pastikan Freecad (v1.5 dan v1.6 telah diuji) dan plugin Freecad CadQuery (v0.3.0 dan v0.2.0 telah diuji) dipasang. Untuk menjana fail VRML/langkah, gunakan skrip Export_Conn_JST_XH dari direktori induk. Skrip ini menghasilkan model untuk penyambung siri 4Ucon 17809. Untuk menjana bahagian tertentu, tetapkan pembolehubah part_to_build ke kekunci bahagian yang dikehendaki. Fungsi Generate_part menghasilkan pin, badan, dan kenalan untuk kekunci bahagian yang ditentukan. Pin, badan, dan kenalan boleh dipaparkan menggunakan fungsi pertunjukan.", "input": "", "output": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# CadQuery script to generate connector models\n\n## requirements\n## freecad (v1.5 and v1.6 have been tested)\n## cadquery FreeCAD plugin (v0.3.0 and v0.2.0 have been tested)\n##   https://github.com/jmwright/cadquery-freecad-module\n\n## This script can be run from within the cadquery module of freecad.\n## To generate VRML/ STEP files for, use export_conn_jst_xh\n## script of the parent directory.\n\n#* This is a cadquery script for the generation of MCAD Models.             *\n#*                                                                          *\n#*   Copyright (c) 2016                                                     *\n#* Rene Poeschl https://github.com/poeschlr                                 *\n#* All trademarks within this guide belong to their legitimate owners.      *\n#*                                                                          *\n#*   This program is free software; you can redistribute it and/or modify   *\n#*   it under the terms of the GNU General Public License (GPL)             *\n#*   as published by the Free Software Foundation; either version 2 of      *\n#*   the License, or (at your option) any later version.                    *\n#*   for detail see the LICENCE text file.                                  *\n#*                                                                          *\n#*   This program is distributed in the hope that it will be useful,        *\n#*   but WITHOUT ANY WARRANTY; without even the implied warranty of         *\n#*   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the          *\n#*   GNU Library General Public License for more details.                   *\n#*                                                                          *\n#*   You should have received a copy of the GNU Library General Public      *\n#*   License along with this program; if not, write to the Free Software    *\n#*   Foundation, Inc.,                                                      *\n#*   51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA           *\n#*                                                                          *\n#* The models generated with this script add the following exception:       *\n#*   As a special exception, if you create a design which uses this symbol, *\n#*   and embed this symbol or unaltered portions of this symbol into the    *\n#*   design, this symbol does not by itself cause the resulting design to   *\n#*   be covered by the GNU General Public License. This exception does not  *\n#*   however invalidate any other reasons why the design itself might be    *\n#*   covered by the GNU General Public License. If you modify this symbol,  *\n#*   you may extend this exception to your version of the symbol, but you   *\n#*   are not obligated to do so. If you do not wish to do so, delete this   *\n#*   exception statement from your version.                                 *\n#****************************************************************************\n\n__title__ = \"model description for 4UCON 17809 series connectors\"\n__author__ = \"hackscribble\"\n__Comment__ = 'model description for 4UCON 17809 series connectors using cadquery'\n\n___ver___ = \"0.3 18/06/2020\"\n\n\nimport cadquery as cq\nfrom Helpers import show\nfrom collections import namedtuple\nimport FreeCAD\nfrom conn_4ucon_17809_params import *\n\nfrom ribbon import Ribbon\n\n\ndef generate_straight_pin(params, pin_1_side):\n    foot_height = seriesParams.foot_height\n    pin_width=seriesParams.pin_width\n    pin_depth=seriesParams.pin_depth\n    pin_height=seriesParams.pin_height\n    pin_inside_distance=seriesParams.pin_inside_distance\n    pin_thickness = seriesParams.pin_thickness\n    chamfer_long = seriesParams.pin_chamfer_long\n    chamfer_short = seriesParams.pin_chamfer_short\n    sign = 1 if pin_1_side else -1\n    pin=cq.Workplane(\"YZ\").workplane(offset=-pin_width/2.0)\\\n        .moveTo(0, foot_height)\\\n        .line(sign*pin_thickness/2,0)\\\n        .line(sign*1.27,-foot_height)\\\n        .line(0, -2.54)\\\n        .line(sign*-pin_thickness,0)\\\n        .line(0, 2.54)\\\n        .line(sign*-1.27, foot_height)\\\n        .line(0,1)\\\n        .close()\\\n        .extrude(pin_width).edges(\"|X\").fillet(0.07)\n    return pin\n\n\ndef generate_2_pin_group(params, pin_1_side):\n    pin_pitch=params.pin_pitch\n    pin_y_pitch=params.pin_y_pitch\n    num_pins=params.num_pins\n    pin_a = generate_straight_pin(params, pin_1_side).translate((0, -pin_y_pitch/2, 0))\n    pin_b = pin_a.translate((0, -2 * pin_y_pitch, 0))\n    pin_group = pin_a.union(pin_b)\n    return pin_group\n\n\ndef generate_pins(params):\n    pin_pitch=params.pin_pitch\n    num_pins=params.num_pins\n    pins = generate_2_pin_group(params, pin_1_side=True)\n    for i in range(1, num_pins // 2):\n        pins = pins.union(generate_2_pin_group(params, i % 2 == 0).translate((i*pin_pitch,0,0)))\n    return pins\n\n\ndef generate_2_contact_group(params):\n    pin_y_pitch=params.pin_y_pitch\n    foot_height = seriesParams.foot_height\n    pin_thickness = seriesParams.pin_thickness\n    pin_width=seriesParams.pin_width\n    y_offset = -(2*pin_y_pitch)\n    c_list = [\n        ('start', {'position': (pin_y_pitch, foot_height), 'direction': 90.0, 'width':pin_thickness}),\n        ('line', {'length': 4.5}),\n        ('arc', {'radius': 0.2, 'angle': 35.0}),\n        ('line', {'length': 3}),\n        ('arc', {'radius': 2.0, 'angle': -70.0}),\n        ('line', {'length': 2}),\n        ('arc', {'radius': 0.2, 'angle': 35.0}),\n        ('line', {'length': 2.8}),\n    ]\n    ribbon = Ribbon(cq.Workplane(\"YZ\").workplane(offset=-pin_width/2.0), c_list)\n    contact1 = ribbon.drawRibbon().extrude(pin_width)\n    contact2 = contact1.mirror(\"XZ\")\n    contact1 = contact1.union(contact2).translate((0,-3*pin_y_pitch/2.0,0))\n    return contact1\n\n\ndef generate_contacts(params):\n    num_pins=params.num_pins\n    pin_pitch=params.pin_pitch\n    pair = generate_2_contact_group(params)\n    contacts = pair\n    for i in range(0, num_pins // 2):\n        contacts = contacts.union(pair.translate((i*pin_pitch,0,0)))\n    return contacts\n\n\ndef generate_body(params, calc_dim):\n    pin_inside_distance = seriesParams.pin_inside_distance\n    pin_width = seriesParams.pin_width\n    num_pins = params.num_pins\n    pin_pitch = params.pin_pitch\n    pin_y_pitch=params.pin_y_pitch\n\n    body_length = calc_dim.length\n    body_width = seriesParams.body_width\n    body_height = seriesParams.body_height\n    body_fillet_radius = seriesParams.body_fillet_radius\n\n    marker_x_inside = seriesParams.marker_x_inside\n    marker_y_inside = seriesParams.marker_y_inside\n    marker_size = seriesParams.marker_size\n    marker_depth = seriesParams.marker_depth\n\n    foot_height = seriesParams.foot_height\n    foot_width = seriesParams.foot_width\n    foot_length = seriesParams.foot_length\n    foot_inside_distance = seriesParams.foot_inside_distance\n\n    slot_length = calc_dim.slot_length\n    slot_outside_pin = seriesParams.slot_outside_pin\n    slot_width = seriesParams.slot_width\n    slot_depth = seriesParams.slot_depth\n    slot_chamfer = seriesParams.slot_chamfer\n\n    hole_width = seriesParams.hole_width\n    hole_length = seriesParams.hole_length\n    hole_offset = seriesParams.hole_offset\n    hole_depth = seriesParams.hole_depth\n\n    top_void_depth = seriesParams.top_void_depth\n    top_void_width = seriesParams.top_void_width\n    bottom_void_width = calc_dim.bottom_void_width\n\n    recess_depth = seriesParams.recess_depth\n    recess_large_width = seriesParams.recess_large_width\n    recess_small_width = seriesParams.recess_small_width\n    recess_height = seriesParams.recess_height\n\n    x_offset = (((num_pins // 2) - 1)*pin_pitch)/2.0\n    y_offset = -(1.5*pin_y_pitch)\n\n    # body\n    body = cq.Workplane(\"XY\").workplane(offset=foot_height).moveTo(x_offset, y_offset)\\\n        .rect(body_length, body_width).extrude(body_height)\\\n        .edges(\"|Z\").fillet(body_fillet_radius).edges(\">Z\").fillet(body_fillet_radius)\n\n    # pin 1 marker\n    body = body.faces(\">Z\").workplane().moveTo(-(body_length/2)+marker_x_inside, (body_width/2)-marker_y_inside)\\\n        .line(-marker_size,-marker_size/2).line(0, marker_size).close().cutBlind(-marker_depth)\n\n    # foot\n    foot = cq.Workplane(\"YZ\").workplane(offset=(body_length/2)-foot_inside_distance)\\\n        .moveTo(y_offset - foot_length/2, 0)\\\n        .line(foot_length*0.2,0)\\\n        .line(0,foot_height/2)\\\n        .line(foot_length*0.6,0)\\\n        .line(0,-foot_height/2)\\\n        .line(foot_length*0.2,0)\\\n        .line(0,foot_height)\\\n        .line(-foot_length,0)\\\n        .close()\\\n        .extrude(-foot_width)\n\n    foot_mirror = foot.mirror(\"YZ\")\n\n    foot = foot.union(foot_mirror).translate((x_offset, 0, 0))\n\n    body = body.union(foot)\n\n    # slot\n    body = body.faces(\">Z\").workplane().rect(slot_length, slot_width).cutBlind(-slot_depth)\n\n    chamfer = cq.Workplane(\"XY\").workplane(offset=foot_height+body_height).moveTo(x_offset, y_offset) \\\n    .rect(slot_length+2*slot_chamfer, slot_width+2*slot_chamfer) \\\n    .workplane(offset=-slot_chamfer).rect(slot_length, slot_width) \\\n    .loft(combine=True)\n\n    body = body.cut(chamfer)\n\n    # contact holes\n    body = body.faces(\">Z\").workplane().center(0, hole_offset)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, hole_length)\\\n        .center(0, -2*hole_offset)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, hole_length)\\\n        .cutBlind(-2)\n\n    # internal void\n    body = body.faces(\">Z\").workplane(offset=-hole_depth)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, top_void_width)\\\n        .cutBlind(-(top_void_depth-hole_depth))\n\n    body = body.faces(\">Z\").workplane(offset=-top_void_depth)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, bottom_void_width)\\\n        .cutBlind(-(body_height-top_void_depth))\n\n    # body end recesses\n    body = body.faces(\">Z\").workplane().center(body_length/2.0-recess_depth/2.0, 0)\\\n        .rect(recess_depth, recess_small_width).cutBlind(-recess_height)\n\n    recess = cq.Workplane(\"XY\").workplane(offset=foot_height+body_height).center(x_offset-body_length/2.0+recess_depth/2.0, y_offset)\\\n        .rect(recess_depth, recess_large_width).extrude(-recess_height).edges(\">X\").edges(\"|Z\").fillet(0.3)\n\n    body = body.cut(recess)\n\n    return body\n\n\ndef generate_part(part_key):\n    params = all_params[part_key]\n    calc_dim = dimensions(params)\n    pins = generate_pins(params)\n    body = generate_body(params, calc_dim)\n    contacts = generate_contacts(params)\n    return (pins, body, contacts)\n\n\n# opened from within freecad\nif \"module\" in __name__:\n    part_to_build = 'ucon_17809_02x10_1.27mm'\n\n    FreeCAD.Console.PrintMessage(\"Started from CadQuery: building \" +\n                                 part_to_build + \"\\n\")\n    (pins, body, contacts) = generate_part(part_to_build)\n\n    show(pins)\n    show(body)\n    show(contacts)", "instruction_en": "This code generates connector models using CadQuery. To run the script, ensure that freecad (v1.5 and v1.6 have been tested) and cadquery FreeCAD plugin (v0.3.0 and v0.2.0 have been tested) are installed. To generate VRML/STEP files, use the export_conn_jst_xh script of the parent directory. The script generates models for the 4UCON 17809 series connectors. To generate a specific part, set the part_to_build variable to the desired part key. The generate_part function generates the pins, body, and contacts for the specified part key. The pins, body, and contacts can be displayed using the show function.", "original_line_number": 15085, "instruction_length": 624}
{"instruction": "Buat kelas blockstatistics yang menghasilkan raster output downsampled dengan mengira ukuran statistik ke atas blok persegi piksel yang tidak bertindih dalam raster input. Kelas ini harus mempunyai kaedah getParameterInfo yang mengembalikan maklumat mengenai raster input, saiz blok, ukuran statistik, dan faktor downsampling. Kelas ini juga harus mempunyai kaedah getConfiguration yang menetapkan faktor pensampelan, mewarisi sifat, membatalkan sifat, menetapkan topeng input, dan menetapkan padding. Kelas ini harus mempunyai kaedah UpdaterasterInfo yang mengemas kini saiz sel raster output, jenis piksel, statistik, dan histogram berdasarkan faktor downsampling dan ukuran statistik. Akhirnya, kelas harus mempunyai kaedah UpdatePixels yang mengira ukuran statistik di atas setiap blok piksel dalam raster input dan menghasilkan raster output dan topeng. Kelas ini juga harus mempunyai kaedah UpdateKeymetadata yang menetapkan datatype output raster untuk \"diproses\".", "input": "", "output": "import numpy as np\r\nfrom skimage.transform import resize\r\nfrom skimage.util import view_as_blocks\r\n\r\n\r\nclass BlockStatistics():\r\n\r\n    def __init__(self):\r\n        self.name = \"Block Statistics Function\"\r\n        self.description = (\"Generates a downsampled output raster by computing a statistical \"\r\n                            \"measure over non-overlapping square blocks of pixels in the input raster.\")\r\n        self.func = np.mean\r\n        self.padding = 0\r\n\r\n    def getParameterInfo(self):\r\n        return [\r\n            {\r\n                'name': 'raster',\r\n                'dataType': 'raster',\r\n                'value': None,\r\n                'required': True,\r\n                'displayName': \"Input Raster\",\r\n                'description': \"The primary input raster over which block statistics is computed.\"\r\n            },\r\n            {\r\n                'name': 'size',\r\n                'dataType': 'numeric',\r\n                'value': 1,\r\n                'required': False,\r\n                'displayName': \"Block Size\",\r\n                'description': (\"The number of pixels along each side of the square \"\r\n                                \"non-overlapping block.\")\r\n            },\r\n            {\r\n                'name': 'measure',\r\n                'dataType': 'string',\r\n                'value': 'Mean',\r\n                'required': False,\r\n                'displayName': \"Measure\",\r\n                'domain': ('Minimum', 'Maximum', 'Mean', 'Median', 'Sum', 'Nearest'),\r\n                'description': (\"The statistical measure computed over each \"\r\n                                \"block of pixels in the input raster.\")\r\n            },\r\n            {\r\n                'name': 'factor',\r\n                'dataType': 'numeric',\r\n                'value': 1,\r\n                'required': False,\r\n                'displayName': \"Downsampling Factor\",\r\n                'description': (\"The integer factor by which the output raster is \"\r\n                                \"downsampled relative to the input raster.\")\r\n            },\r\n        ]\r\n\r\n    def getConfiguration(self, **scalars):\r\n        s = scalars.get('size', None)\r\n        s = 3 if s is None else s\r\n        self.padding = int(s / 2)\r\n        return {\r\n            'samplingFactor': scalars.get('size', 1.0),\r\n            'inheritProperties': 4 | 8,             # inherit everything but the pixel type (1) and NoData (2)\r\n            'invalidateProperties': 2 | 4 | 8,      # invalidate histogram, statistics, and key metadata\r\n            'inputMask': True,\r\n            'resampling': False,\r\n            'padding': self.padding,\r\n        }\r\n\r\n    def updateRasterInfo(self, **kwargs):\r\n        f = kwargs.get('factor', 1.0)\r\n        kwargs['output_info']['cellSize'] = tuple(np.multiply(kwargs['raster_info']['cellSize'], f))\r\n        kwargs['output_info']['pixelType'] = 'f4'   # output pixels values are floating-point\r\n        kwargs['output_info']['statistics'] = ()\r\n        kwargs['output_info']['histogram'] = ()\r\n\r\n        m = kwargs.get('measure')\r\n        m = m.lower() if m is not None and len(m) else 'mean'\r\n\r\n        if m == 'minimum':\r\n            self.func = np.min\r\n        elif m == 'maximum':\r\n            self.func = np.max\r\n        elif m == 'mean':\r\n            self.func = np.mean\r\n        elif m == 'median':\r\n            self.func = np.median\r\n        elif m == 'sum':\r\n            self.func = np.sum\r\n        elif m == 'nearest':\r\n            self.func = None\r\n\r\n        return kwargs\r\n\r\n    def updatePixels(self, tlc, shape, props, **pixelBlocks):\r\n        p = pixelBlocks['raster_pixels']\r\n        m = pixelBlocks['raster_mask']\r\n\r\n        if self.func is None:\r\n            b = resize(p, shape, order=0, preserve_range=True)\r\n        else:\r\n            blockSizes = tuple(np.divide(p.shape, shape))\r\n            b = np.ma.masked_array(view_as_blocks(p, blockSizes),\r\n                                   view_as_blocks(~m.astype('b1'), blockSizes))\r\n            for i in range(len(b.shape) // 2):\r\n                b = self.func(b, axis=-1)\r\n            b = b.data\r\n\r\n        d = self.padding\r\n        pixelBlocks['output_pixels'] = b.astype(props['pixelType'], copy=False)\r\n        pixelBlocks['output_mask'] = resize(m, shape, order=0, preserve_range=True).astype('u1', copy=False)\r\n        return pixelBlocks\r\n\r\n    def updateKeyMetadata(self, names, bandIndex, **keyMetadata):\r\n        if bandIndex == -1:\r\n            keyMetadata['datatype'] = 'Processed'\r\n        return keyMetadata", "instruction_en": "Create a BlockStatistics class that generates a downsampled output raster by computing a statistical measure over non-overlapping square blocks of pixels in the input raster. The class should have a getParameterInfo method that returns information about the input raster, block size, statistical measure, and downsampling factor. The class should also have a getConfiguration method that sets the sampling factor, inherits properties, invalidates properties, sets the input mask, and sets the padding. The class should have an updateRasterInfo method that updates the output raster's cell size, pixel type, statistics, and histogram based on the downsampling factor and statistical measure. Finally, the class should have an updatePixels method that computes the statistical measure over each block of pixels in the input raster and generates the output raster and mask. The class should also have an updateKeyMetadata method that sets the datatype of the output raster to \"Processed\".", "original_line_number": 15705, "instruction_length": 971}
{"instruction": "Kod yang diberikan adalah kelas Python bernama FileSystem yang menyediakan antara muka untuk operasi sistem fail. Untuk menggunakan kelas ini, buat contohnya dan panggil kaedahnya. The methods include abspath, realpath, path_to_module, expanduser, basename, chdir, copyfile, dirname, exists, dirs_under, files_under, getcwd, glob, isabs, isfile, getsize, isdir, join, listdir, mkdtemp, maybe_make_directory, move, mtime, normpath, open_binary_tempfile, open_binary_file_for_reading, read_binary_file, write_binary_file, open_text_file_for_reading, open_text_file_for_writing, open_stdin, read_text_file, write_text_file, sha1, relasi", "input": "", "output": "# Copyright (C) 2010 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"Wrapper object for the file system / source tree.\"\"\"\n\nimport codecs\nimport errno\nimport exceptions\nimport filecmp\nimport glob\nimport hashlib\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport time\n\n\nclass FileSystem(object):\n    \"\"\"FileSystem interface for webkitpy.\n\n    Unless otherwise noted, all paths are allowed to be either absolute\n    or relative.\"\"\"\n    sep = os.sep\n    pardir = os.pardir\n\n    def abspath(self, path):\n        # FIXME: This gross hack is needed while we transition from Cygwin to native Windows, because we\n        # have some mixing of file conventions from different tools:\n        if sys.platform == 'cygwin':\n            path = os.path.normpath(path)\n            path_components = path.split(os.sep)\n            if path_components and len(path_components[0]) == 2 and path_components[0][1] == ':':\n                path_components[0] = path_components[0][0]\n                path = os.path.join('/', 'cygdrive', *path_components)\n\n        return os.path.abspath(path)\n\n    def realpath(self, path):\n        return os.path.realpath(path)\n\n    def path_to_module(self, module_name):\n        \"\"\"A wrapper for all calls to __file__ to allow easy unit testing.\"\"\"\n        # FIXME: This is the only use of sys in this file. It's possible this function should move elsewhere.\n        return sys.modules[module_name].__file__  # __file__ is always an absolute path.\n\n    def expanduser(self, path):\n        return os.path.expanduser(path)\n\n    def basename(self, path):\n        return os.path.basename(path)\n\n    def chdir(self, path):\n        return os.chdir(path)\n\n    def copyfile(self, source, destination):\n        shutil.copyfile(source, destination)\n\n    def dirname(self, path):\n        return os.path.dirname(path)\n\n    def exists(self, path):\n        return os.path.exists(path)\n\n    def dirs_under(self, path, dir_filter=None):\n        \"\"\"Return the list of all directories under the given path in topdown order.\n\n        Args:\n            dir_filter: if not None, the filter will be invoked\n                with the filesystem object and the path of each dirfound.\n                The dir is included in the result if the callback returns True.\n        \"\"\"\n        def filter_all(fs, dirpath):\n            return True\n        dir_filter = dir_filter or filter_all\n\n        dirs = []\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            if dir_filter(self, dirpath):\n                dirs.append(dirpath)\n        return dirs\n\n    def files_under(self, path, dirs_to_skip=[], file_filter=None):\n        \"\"\"Return the list of all files under the given path in topdown order.\n\n        Args:\n            dirs_to_skip: a list of directories to skip over during the\n                traversal (e.g., .svn, resources, etc.)\n            file_filter: if not None, the filter will be invoked\n                with the filesystem object and the dirname and basename of\n                each file found. The file is included in the result if the\n                callback returns True.\n        \"\"\"\n        def filter_all(fs, dirpath, basename):\n            return True\n\n        file_filter = file_filter or filter_all\n        files = []\n        if self.isfile(path):\n            if file_filter(self, self.dirname(path), self.basename(path)):\n                files.append(path)\n            return files\n\n        if self.basename(path) in dirs_to_skip:\n            return []\n\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            for d in dirs_to_skip:\n                if d in dirnames:\n                    dirnames.remove(d)\n\n            for filename in filenames:\n                if file_filter(self, dirpath, filename):\n                    files.append(self.join(dirpath, filename))\n        return files\n\n    def getcwd(self):\n        return os.getcwd()\n\n    def glob(self, path):\n        return glob.glob(path)\n\n    def isabs(self, path):\n        return os.path.isabs(path)\n\n    def isfile(self, path):\n        return os.path.isfile(path)\n\n    def getsize(self, path):\n        return os.path.getsize(path)\n\n    def isdir(self, path):\n        return os.path.isdir(path)\n\n    def join(self, *comps):\n        return os.path.join(*comps)\n\n    def listdir(self, path):\n        return os.listdir(path)\n\n    def mkdtemp(self, **kwargs):\n        \"\"\"Create and return a uniquely named directory.\n\n        This is like tempfile.mkdtemp, but if used in a with statement\n        the directory will self-delete at the end of the block (if the\n        directory is empty; non-empty directories raise errors). The\n        directory can be safely deleted inside the block as well, if so\n        desired.\n\n        Note that the object returned is not a string and does not support all of the string\n        methods. If you need a string, coerce the object to a string and go from there.\n        \"\"\"\n        class TemporaryDirectory(object):\n            def __init__(self, **kwargs):\n                self._kwargs = kwargs\n                self._directory_path = tempfile.mkdtemp(**self._kwargs)\n\n            def __str__(self):\n                return self._directory_path\n\n            def __enter__(self):\n                return self._directory_path\n\n            def __exit__(self, type, value, traceback):\n                # Only self-delete if necessary.\n\n                # FIXME: Should we delete non-empty directories?\n                if os.path.exists(self._directory_path):\n                    os.rmdir(self._directory_path)\n\n        return TemporaryDirectory(**kwargs)\n\n    def maybe_make_directory(self, *path):\n        \"\"\"Create the specified directory if it doesn't already exist.\"\"\"\n        try:\n            os.makedirs(self.join(*path))\n        except OSError, e:\n            if e.errno != errno.EEXIST:\n                raise\n\n    def move(self, source, destination):\n        shutil.move(source, destination)\n\n    def mtime(self, path):\n        return os.stat(path).st_mtime\n\n    def normpath(self, path):\n        return os.path.normpath(path)\n\n    def open_binary_tempfile(self, suffix):\n        \"\"\"Create, open, and return a binary temp file. Returns a tuple of the file and the name.\"\"\"\n        temp_fd, temp_name = tempfile.mkstemp(suffix)\n        f = os.fdopen(temp_fd, 'wb')\n        return f, temp_name\n\n    def open_binary_file_for_reading(self, path):\n        return codecs.open(path, 'rb')\n\n    def read_binary_file(self, path):\n        \"\"\"Return the contents of the file at the given path as a byte string.\"\"\"\n        with file(path, 'rb') as f:\n            return f.read()\n\n    def write_binary_file(self, path, contents):\n        with file(path, 'wb') as f:\n            f.write(contents)\n\n    def open_text_file_for_reading(self, path, errors='strict'):\n        # Note: There appears to be an issue with the returned file objects\n        # not being seekable. See http://stackoverflow.com/questions/1510188/can-seek-and-tell-work-with-utf-8-encoded-documents-in-python .\n        return codecs.open(path, 'r', 'utf8', errors)\n\n    def open_text_file_for_writing(self, path):\n        return codecs.open(path, 'w', 'utf8')\n\n    def open_stdin(self):\n        return codecs.StreamReaderWriter(sys.stdin,\n                                             codecs.getreader('utf8'),\n                                             codecs.getwriter('utf8'),\n                                             'replace')\n\n    def read_text_file(self, path):\n        \"\"\"Return the contents of the file at the given path as a Unicode string.\n\n        The file is read assuming it is a UTF-8 encoded file with no BOM.\"\"\"\n        with codecs.open(path, 'r', 'utf8') as f:\n            return f.read()\n\n    def write_text_file(self, path, contents):\n        \"\"\"Write the contents to the file at the given location.\n\n        The file is written encoded as UTF-8 with no BOM.\"\"\"\n        with codecs.open(path, 'w', 'utf-8') as f:\n            f.write(contents.decode('utf-8') if type(contents) == str else contents)\n\n    def sha1(self, path):\n        contents = self.read_binary_file(path)\n        return hashlib.sha1(contents).hexdigest()\n\n    def relpath(self, path, start='.'):\n        return os.path.relpath(path, start)\n\n    class _WindowsError(exceptions.OSError):\n        \"\"\"Fake exception for Linux and Mac.\"\"\"\n        pass\n\n    def remove(self, path, osremove=os.remove):\n        \"\"\"On Windows, if a process was recently killed and it held on to a\n        file, the OS will hold on to the file for a short while.  This makes\n        attempts to delete the file fail.  To work around that, this method\n        will retry for a few seconds until Windows is done with the file.\"\"\"\n        try:\n            exceptions.WindowsError\n        except AttributeError:\n            exceptions.WindowsError = FileSystem._WindowsError\n\n        retry_timeout_sec = 3.0\n        sleep_interval = 0.1\n        while True:\n            try:\n                osremove(path)\n                return True\n            except exceptions.WindowsError, e:\n                time.sleep(sleep_interval)\n                retry_timeout_sec -= sleep_interval\n                if retry_timeout_sec < 0:\n                    raise e\n\n    def rmtree(self, path):\n        \"\"\"Delete the directory rooted at path, whether empty or not.\"\"\"\n        shutil.rmtree(path, ignore_errors=True)\n\n    def copytree(self, source, destination):\n        shutil.copytree(source, destination)\n\n    def split(self, path):\n        \"\"\"Return (dirname, basename + '.' + ext)\"\"\"\n        return os.path.split(path)\n\n    def splitext(self, path):\n        \"\"\"Return (dirname + os.sep + basename, '.' + ext)\"\"\"\n        return os.path.splitext(path)\n\n    def compare(self, path1, path2):\n        return filecmp.cmp(path1, path2)", "instruction_en": "The given code is a Python class named FileSystem that provides an interface for file system operations. To use this class, create an instance of it and call its methods. The methods include abspath, realpath, path_to_module, expanduser, basename, chdir, copyfile, dirname, exists, dirs_under, files_under, getcwd, glob, isabs, isfile, getsize, isdir, join, listdir, mkdtemp, maybe_make_directory, move, mtime, normpath, open_binary_tempfile, open_binary_file_for_reading, read_binary_file, write_binary_file, open_text_file_for_reading, open_text_file_for_writing, open_stdin, read_text_file, write_text_file, sha1, relpath, remove, rmtree, copytree, split, splitext, and compare.", "original_line_number": 15892, "instruction_length": 633}
{"instruction": "Buat borang untuk pihak ketiga ke pihak pertama OAuth 2.0 akses token pertukaran dengan bidang dan kaedah yang diberikan. Bentuknya harus mewarisi dari OAuthform dan Scopemixin. Ia sepatutnya mempunyai Charfield untuk Access_Token, Client_ID, dan ScopeChoiceField untuk skop. Borang ini harus mengesahkan medan akses_token dan klien_id dan menaikkan ralat oauthvalidationError yang sesuai jika mereka hilang. Ia juga harus memeriksa sama ada klien_id adalah pelanggan awam yang sah dan jika backend adalah penyedia yang disokong. Akhirnya, ia harus mengesahkan pengguna menggunakan Access_Token dan Allow_inactive_user = benar dan menaikkan ralat OAuthValidationError jika akses_token tidak sah.", "input": "", "output": "\"\"\"\nForms to support third-party to first-party OAuth 2.0 access token exchange\n\"\"\"\nimport provider.constants\nfrom django.contrib.auth.models import User\nfrom django.forms import CharField\nfrom edx_oauth2_provider.constants import SCOPE_NAMES\nfrom oauth2_provider.models import Application\nfrom provider.forms import OAuthForm, OAuthValidationError\nfrom provider.oauth2.forms import ScopeChoiceField, ScopeMixin\nfrom provider.oauth2.models import Client\nfrom requests import HTTPError\nfrom social.backends import oauth as social_oauth\nfrom social.exceptions import AuthException\n\nfrom third_party_auth import pipeline\n\n\nclass AccessTokenExchangeForm(ScopeMixin, OAuthForm):\n    \"\"\"Form for access token exchange endpoint\"\"\"\n    access_token = CharField(required=False)\n    scope = ScopeChoiceField(choices=SCOPE_NAMES, required=False)\n    client_id = CharField(required=False)\n\n    def __init__(self, request, oauth2_adapter, *args, **kwargs):\n        super(AccessTokenExchangeForm, self).__init__(*args, **kwargs)\n        self.request = request\n        self.oauth2_adapter = oauth2_adapter\n\n    def _require_oauth_field(self, field_name):\n        \"\"\"\n        Raise an appropriate OAuthValidationError error if the field is missing\n        \"\"\"\n        field_val = self.cleaned_data.get(field_name)\n        if not field_val:\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_request\",\n                    \"error_description\": \"{} is required\".format(field_name),\n                }\n            )\n        return field_val\n\n    def clean_access_token(self):\n        \"\"\"\n        Validates and returns the \"access_token\" field.\n        \"\"\"\n        return self._require_oauth_field(\"access_token\")\n\n    def clean_client_id(self):\n        \"\"\"\n        Validates and returns the \"client_id\" field.\n        \"\"\"\n        return self._require_oauth_field(\"client_id\")\n\n    def clean(self):\n        if self._errors:\n            return {}\n\n        backend = self.request.backend\n        if not isinstance(backend, social_oauth.BaseOAuth2):\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_request\",\n                    \"error_description\": \"{} is not a supported provider\".format(backend.name),\n                }\n            )\n\n        self.request.session[pipeline.AUTH_ENTRY_KEY] = pipeline.AUTH_ENTRY_LOGIN_API\n\n        client_id = self.cleaned_data[\"client_id\"]\n        try:\n            client = self.oauth2_adapter.get_client(client_id=client_id)\n        except (Client.DoesNotExist, Application.DoesNotExist):\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_client\",\n                    \"error_description\": \"{} is not a valid client_id\".format(client_id),\n                }\n            )\n        if client.client_type not in [provider.constants.PUBLIC, Application.CLIENT_PUBLIC]:\n            raise OAuthValidationError(\n                {\n                    # invalid_client isn't really the right code, but this mirrors\n                    # https://github.com/edx/django-oauth2-provider/blob/edx/provider/oauth2/forms.py#L331\n                    \"error\": \"invalid_client\",\n                    \"error_description\": \"{} is not a public client\".format(client_id),\n                }\n            )\n        self.cleaned_data[\"client\"] = client\n\n        user = None\n        try:\n            user = backend.do_auth(self.cleaned_data.get(\"access_token\"), allow_inactive_user=True)\n        except (HTTPError, AuthException):\n            pass\n        if user and isinstance(user, User):\n            self.cleaned_data[\"user\"] = user\n        else:\n            # Ensure user does not re-enter the pipeline\n            self.request.social_strategy.clean_partial_pipeline()\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_grant\",\n                    \"error_description\": \"access_token is not valid\",\n                }\n            )\n\n        return self.cleaned_data", "instruction_en": "Create a form for third-party to first-party OAuth 2.0 access token exchange with the given fields and methods. The form should inherit from OAuthForm and ScopeMixin. It should have CharField for access_token, client_id, and ScopeChoiceField for scope. The form should validate the access_token and client_id fields and raise an appropriate OAuthValidationError error if they are missing. It should also check if the client_id is a valid public client and if the backend is a supported provider. Finally, it should authenticate the user using the access_token and allow_inactive_user=True and raise an OAuthValidationError error if the access_token is not valid.", "original_line_number": 16393, "instruction_length": 695}
{"instruction": "Buat aplikasi kuiz menggunakan Python's Tkinter Library. Aplikasi ini harus mempunyai halaman permulaan dengan dua butang untuk memilih subjek (geografi atau matematik). Selepas memilih subjek, aplikasi itu harus memaparkan satu siri soalan rawak dengan pelbagai jawapan pilihan. Pengguna harus dapat memilih jawapan dan mengesahkannya. Selepas menjawab semua soalan, aplikasi itu harus memaparkan skor pengguna dan mesej berdasarkan prestasi mereka. Aplikasi ini juga harus memainkan kesan bunyi apabila pengguna memilih jawapan atau mengesahkan pilihan mereka.", "input": "", "output": "from tkinter import *\nfrom tkinter import ttk\nimport random\n\nbutton_width = 17\nnumber_of_characters_per_row = 56\ndiff_for_answers = 8\ncolor = '#%02x%02x%02x' % (231, 231, 231)\n\nimport subprocess  # poskusile 5 razlicnih modulov: pyglet, mp3play, sound in se dva pa noben ni delal\n\n\n# pygame se nama zdi prevelika knjiznica za dodati za samo nekaj zvokov\ndef play_button_click():  # dela samo na OS X!\n    subprocess.call([\"afplay\", \"Sounds/button_click.mp3\"])\n    # dela prepoasi!! - ko to dela, ne dela ni drugo!\n\n\n# subprocess.call([\"afplay\", \"music.mp3\"]) # ce to igram, potem nic drugo ne dela dokler se glasba ne konca!\n\nimport gettext\n\n_ = gettext.gettext\n\n# noinspection PyBroadException\ntry:\n    en = gettext.translation('main', localedir='locale', languages=['en'])\n    en.install()\nexcept:\n    print(_(\"Prevedba v angleski jezik ni bila mogoca.\"))\n\n\nclass Quiz(Tk):\n    frames = {}\n    number_of_questions = 5\n    question_count = 0\n    number_of_all_questions = 20  # per subject in SUBJECTdata.txt\n    points = 0  # number of points user gets for answering the question correctly\n\n    def __init__(self, *args, **kwargs):\n        Tk.__init__(self, *args, **kwargs)\n        Tk.wm_title(self, _(\"Maturitetni kviz\"))\n\n        self.initialize_container_frame()\n        self.initialize_start_page()\n\n        self.set_images()\n\n    def initialize_container_frame(self):\n        self.container = ttk.Frame(self)  # to je frame, ki nima na sebi ni, na njega zlagama nove\n        self.container.pack_propagate(0)\n        self.container.pack(pady=10, padx=10)\n\n        self.container.grid_rowconfigure(0, weight=1)\n        # default weight je 0, kar pomeni da bo ta imel najvecji prostor ko spremenimo velikost - zaenkrat nima veze ker je sam\n        self.container.grid_columnconfigure(0, weight=1)\n\n    def initialize_start_page(self):\n        start_page = StartPage(self.container, self)\n        start_page.grid(row=0, column=0, sticky=\"nsew\")\n        self.frames[0] = start_page\n        self.show_frame()\n\n    def show_frame(self):\n        if self.question_count <= self.number_of_questions:\n            frame = self.frames.get(self.question_count, None)  # da slucajno ne pride do zrusitve programa\n            if frame is not None:\n                frame.tkraise()  # naloi nov frame - vpraanje\n            else:\n                print(_(\"Nekaj se je zalomilo. Vprasanja ni bilo mogoe naloiti\"))\n            self.question_count += 1\n        else:\n            self.show_result_frame()\n\n    def set_subject(self, subject):\n        self.create_random_questions(subject)\n        self.show_frame()\n        play_button_click()\n\n    def create_random_questions(self, subject):\n        random_question_numbers = []\n        table_of_possible_question_numbers = list(\n            range(1, self.number_of_all_questions + 1))  # iti more od 1 do vkljucno stevila\n\n        # tu samo dolocimo random stevilke vprasanj, stevilka pomeni vrstica v dokumentu:\n        while len(random_question_numbers) < self.number_of_questions:\n            rand_number = random.choice(table_of_possible_question_numbers)\n            random_question_numbers.append(rand_number)\n\n            if rand_number in table_of_possible_question_numbers:\n                table_of_possible_question_numbers.remove(rand_number)\n            else:\n                print(_(\"Pri doloanju tvojih vpraanj se je zalomilo.\"))  # spet da slucajno ne pride do zrusitve\n\n        # nalozimo dejanska vprasanja, prikazemo zaenkrat se nobenega:\n        question_count = 1  # to ni lastnost metode self.question_count, ampak nova spremenljivka\n        for number in random_question_numbers:\n            question = Question(self.container, self, subject, number)\n            self.frames[question_count] = question\n            question_count += 1\n            question.grid(row=0, column=0, sticky=\"nsew\")\n\n    def show_result_frame(self):\n        result_page = ResultPage(self.container, self)\n        result_page.grid(row=0, column=0, sticky=\"nsew\")\n        result_page.tkraise()\n\n        # ponastavimo rezultate, ce bo slucajno igral ponovno:\n        self.question_count = 0\n        self.points = 0\n        self.destroy_previous_frames()  # da se nam spomin ne zabase\n\n    def destroy_previous_frames(self):\n        for frame in self.frames.values():\n            frame.destroy()\n        self.frames = {}\n\n    def increase_points(self):\n        self.points += 1\n\n    def set_images(self):\n        correct_photo = PhotoImage(file=\"Images/correct.gif\")\n        Label(self, image=correct_photo)\n        self.correct_photo = correct_photo\n        wrong_photo = wrong_photo = PhotoImage(file=\"Images/wrong.gif\")\n        Label(self, image=wrong_photo)\n        self.wrong_photo = wrong_photo\n\n\nclass StartPage(ttk.Frame):  # podeduje metode in lastnosti razreda\n    def __init__(self, parent, quiz_reference):  # self je container - vse se bo nalagalo na container\n        ttk.Frame.__init__(self, parent)\n        self.quiz_reference = quiz_reference\n\n        self.show_frame()\n\n    def show_frame(self):\n        text = _('''Pozdravljen bodoi maturant!\\nPred tabo je kratek kviz iz maturitetnih predmetov\\n''')\n        ttk.Label(self, text=text, justify=\"center\").pack(padx=10)\n\n        self.show_image()\n\n        ttk.Label(self, text=_(\"Izberi podroje:\")).pack(pady=10, padx=10)\n\n        button_geo = ttk.Button(self, text=_(\"Geografija\"),\n                               command=lambda: self.quiz_reference.set_subject(\"GEO\"),\n                               width=button_width)\n        button_geo.pack(side=\"bottom\")\n        button_mat = ttk.Button(self, text=_(\"Matematika\"),\n                               command=lambda: self.quiz_reference.set_subject(\"MAT\"),\n                               width=button_width)\n        button_mat.pack(side=\"bottom\")\n        # lambda uporabimo, da lahko podamo parameter in ob tem ne sproimo klica funkcije\n\n    def show_image(self):\n        photo = PhotoImage(file=\"Images/slika.gif\")\n        label = ttk.Label(self, image=photo)\n\n        self.start_page_image = photo  # treba je imeti se eno povezavo, zato da je avtomatsko ne izbrise\n        label.pack()\n\n\nclass Question(ttk.Frame):\n    question = \"\"\n    correct_answer = 0\n    possible_answers = {}\n    chosen_answer = \"\"\n    is_confirm_button_showing = False\n    radio_buttons = []\n\n    def __init__(self, parent, quiz_reference, subject, number):  # ko imama stevilko, poiscema vprasanje, odgovor in mozne odgovore iz datoteke\n        ttk.Frame.__init__(self, parent)\n\n        self.quiz_reference = quiz_reference\n        self.subject = subject\n        self.number = number\n        self.get_data()\n\n        self.show_frame_widgets()\n\n    def show_frame_widgets(self):\n        self.show_the_question()\n        self.show_possible_answers()\n\n    def show_the_question(self):\n        '''prikaze vprasanje na label widgetu'''\n        edited_text = self.check_if_text_too_long(self.question, number_of_characters_per_row)\n\n        ttk.Label(self, text=edited_text).pack(pady=15, padx=10, side=\"top\")\n\n    def check_if_text_too_long(self, unedited_text, allowed_number_of_chars):\n        '''vrne primerno preurejen text z novimi vrsticami, ce je trenutno predolg'''\n        if len(unedited_text) <= number_of_characters_per_row: return unedited_text  # je ze ok\n\n        text = ''''''  # vecvrsticni string\n        num_of_chars = 0  # in current row\n\n        for word in unedited_text.split(\" \"):\n            num_of_chars += len(word)\n            if num_of_chars < allowed_number_of_chars:\n                text += word + \" \"\n            else:\n                text = text + word + \"\\n\"\n                num_of_chars = 0\n        return text.strip(\"\\n\")\n\n    def show_possible_answers(self):\n        self.radio_buttons = {}\n        self.var = StringVar()\n        for possible_answer in self.possible_answers:\n            possible_answer = self.check_if_text_too_long(possible_answer,\n                                                          number_of_characters_per_row - diff_for_answers)\n            R = ttk.Radiobutton(self,\n                                compound=\"left\",\n                                text=possible_answer,\n                                variable=self.var,\n                                value=possible_answer,\n                                command=self.set_chosen_answer)\n            # Ko uporabnik izbere odgovor, se mu prikaze gumb za potrditev, ko stisne nanj se preveri pravilnost izbire\n            self.radio_buttons[possible_answer] = R\n            R.pack(anchor='w')\n\n    def set_chosen_answer(self):\n        if not self.is_confirm_button_showing: self.show_confirm_button()\n\n    def show_confirm_button(self):\n        self.confirm_button = ttk.Button(self, text=_(\"Potrdi izbiro\"),\n                                         command=self.check_the_answer,\n                                         width=button_width)\n        self.confirm_button.pack(pady=8, side=\"bottom\")\n        self.is_confirm_button_showing = True\n\n    def change_text_on_confirm_button(self):\n        self.confirm_button.destroy()\n        self.next_q_button = ttk.Button(self, text=_(\"Naprej\"),\n                                        command=self.confirm_button_pressed,\n                                        width=button_width)\n        self.next_q_button.pack(pady=8, side=\"bottom\")\n\n        # prepreci da stisne na gumbe:\n        for text, radio_button in self.radio_buttons.items():\n            radio_button.configure(state=DISABLED)\n            #if radio_button.text == self.chosen_answer: print(self.chosen_answer) # to ne dela! zato je narejeno z slovarjem\n            if text == self.chosen_answer:\n                appropriate_image = self.quiz_reference.correct_photo if self.chosen_answer == self.correct_answer \\\n                    else self.quiz_reference.wrong_photo\n                #print(appropriate_image.name)\n                #radio_button.configure(image=appropriate_image) # TU BI SE MORALA PRIKAZATI ZRAVEN PRIMERNA SLIKA\n\n    def confirm_button_pressed(self):\n        play_button_click()\n        self.quiz_reference.show_frame()\n\n    def check_the_answer(self):\n        self.chosen_answer = self.var.get()\n        if self.chosen_answer == self.correct_answer: self.quiz_reference.increase_points()\n        self.change_text_on_confirm_button()\n        play_button_click()\n\n    def get_data(self):\n        data = self.subject + \"data.txt\"\n\n        with open(data, \"r\") as file:\n            lines = [line.strip() for line in file]\n            currentLine = lines[self.number]\n            # zapisano v obliki Vprasanje;odg1:odg2:odg3;odgovorPravilen\n            data = currentLine.split(\";\")\n            self.question = data[0]\n            self.correct_answer = data[2]\n            self.possible_answers = data[1].split(\":\")\n\n\nclass ResultPage(ttk.Frame):\n    def __init__(self, parent, quiz_reference):  # ko imama stevilko, poiscema vprasanje, odgovor in mozne odgovore iz datoteke\n        ttk.Frame.__init__(self, parent)\n        self.quiz_reference = quiz_reference\n        self.show_frame_widgets()\n\n    def show_frame_widgets(self):\n        points = self.quiz_reference.points\n        all_points = self.quiz_reference.number_of_questions\n        ttk.Label(self, text=\"Tvoj rezultat je: {} od {} tok!\".\n                  format(points, all_points)).pack(pady=10, padx=10)\n\n        text_message = self.appropriate_message(points)\n        ttk.Label(self, text=text_message).pack(pady=10, padx=10)\n\n        appropriate_image = \"Images/failure.gif\" if points <= all_points // 2 else \"Images/bravo.gif\"\n\n        photo = PhotoImage(file=appropriate_image)\n        label = ttk.Label(self, image=photo)\n\n        self.congratulation_photo = photo\n        label.pack(pady=15)\n\n        ttk.Button(self, text=\"Igraj ponovno!\",\n                   command=self.quiz_reference.initialize_start_page,\n                   width=button_width).pack(side=\"bottom\")\n\n    def appropriate_message(self, user_points):\n        \"\"\"Prikaze sporocilo glede na rezultat\"\"\"\n        all_points = self.quiz_reference.number_of_questions\n        if user_points in range(all_points // 2 + 1):\n            message = \"Tvoje znanje je nezadostno!\"\n        elif user_points in range(all_points // 2 + 1, all_points // 4):\n            message = \"Tvoje znanje je zadovoljivo.\"\n        elif user_points in range(all_points // 4, all_points):\n            message = \"estitam, dosegel si skoraj vse toke!\"\n        else:\n            message = \"Bravo, tvoje znanje je izjemno!!!\"  # dosegel je vse toke\n        return message\n\n\napp = Quiz()\napp.geometry(\"500x250\")\napp.configure(bg=color)  # sicer bi bil rob beli\n\n# velikost okna - to ni resitev, hocem nastavit velikost vseh framov, ne samo okna, ker se zdaj udno poravnava\napp.resizable(0, 0)  # v nobeno smer ni resizable\napp.mainloop()", "instruction_en": "Create a quiz application using Python's tkinter library. The application should have a start page with two buttons for selecting a subject (geography or mathematics). After selecting a subject, the application should display a series of random questions with multiple choice answers. The user should be able to select an answer and confirm it. After answering all questions, the application should display the user's score and a message based on their performance. The application should also play a sound effect when the user selects an answer or confirms their choice.", "original_line_number": 16509, "instruction_length": 562}
{"instruction": "Buat kelas FileInfoPanel yang mewarisi dari QTabWidget Pyqt4. Kelas harus mempunyai kaedah __init__ yang mengambil dalam job_list, dispatcher_list, config_info, dan induk sebagai argumen. Kelas ini juga harus mempunyai kaedah update_ui yang membersihkan tab semasa dan menambah tab baru berdasarkan perisian kerja dan penerima kerja semasa. Sekiranya perisian itu adalah Maya, tambahkan dua tab untuk Mayapanel dan jika penerima mental adalah mental, tambahkan tab MentalRayPanel. Jika perisian itu Nuke, tambahkan tab NukePanel.", "input": "", "output": "from PyQt4 import QtGui, QtCore\nimport gui.submit.fileinfo.common\nimport gui.submit.fileinfo.maya1\nimport gui.submit.fileinfo.maya2\nimport gui.submit.fileinfo.maya_mentalray\nimport gui.submit.fileinfo.nuke\n\n\nclass FileInfoPanel(QtGui.QTabWidget):\n\n    def __init__(self, job_list, dispatcher_list, config_info, parent=None):\n        QtGui.QTabWidget.__init__(self)\n        self._parent = parent\n        self._joblist = job_list\n        self._dipatcherList = dispatcher_list\n        self._configInfo = config_info\n\n        self.update_ui()\n\n    def update_ui(self):\n        self.clear()\n\n        job_common_panel = gui.submit.fileinfo.common.CommonPanel(self._joblist, self._parent)\n        self.addTab(job_common_panel, \"fileinfo\")\n\n        if \"Maya\" in self._joblist.get_current_job().getvalue(\"[*].*.software\")[0]:\n            maya_panel1 = gui.submit.fileinfo.maya1.MayaPanel(self._joblist, self._parent)\n            self.addTab(maya_panel1, \"Maya1\")\n            maya_panel2 = gui.submit.fileinfo.maya2.MayaPanel(self._joblist, self._parent)\n            self.addTab(maya_panel2, \"Maya2\")\n\n            if \"mentalray\" in self._joblist.get_current_job().getvalue(\"[*].*.renderer\")[0]:\n                mentalray_panel = gui.submit.fileinfo.maya_mentalray.MentalrayPanel(self._joblist, self._parent)\n                self.addTab(mentalray_panel, \"Mentalray\")\n\n        elif \"Nuke\" in self._joblist.get_current_job().getvalue(\"[*].*.software\")[0]:\n            nuke_panel = gui.submit.fileinfo.nuke.NukePanel(self._joblist, self._parent)\n            self.addTab(nuke_panel, \"Nuke\")", "instruction_en": "Create a FileInfoPanel class that inherits from PyQt4's QTabWidget. The class should have an __init__ method that takes in job_list, dispatcher_list, config_info, and parent as arguments. The class should also have an update_ui method that clears the current tabs and adds new tabs based on the current job's software and renderer. If the software is Maya, add two tabs for MayaPanel and if the renderer is mentalray, add a MentalrayPanel tab. If the software is Nuke, add a NukePanel tab.", "original_line_number": 16608, "instruction_length": 529}
{"instruction": "Untuk mengkonfigurasi penghalaan URL untuk projek Django, buat senarai yang dipanggil `urlpatterns` dalam fail yang dipanggil` urls.py`. Di dalam senarai ini, gunakan `laluan ()` dan `url ()` fungsi untuk memetakan URL ke pandangan. Anda juga boleh memasukkan konfigurasi URL lain menggunakan fungsi `termasuk ()`. Akhirnya, gunakan fungsi `format_suffix_patterns ()` untuk menambah sokongan untuk format tindak balas yang berbeza. Secara pilihan, gunakan fungsi `statik ()` untuk menyampaikan fail media semasa pembangunan.", "input": "", "output": "\"\"\"tumuli URL Configuration\n\nThe `urlpatterns` list routes URLs to views. For more information please see:\n    https://docs.djangoproject.com/en/2.0/topics/http/urls/\nExamples:\nFunction views\n    1. Add an import:  from my_app import views\n    2. Add a URL to urlpatterns:  path('', views.home, name='home')\nClass-based views\n    1. Add an import:  from other_app.views import Home\n    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')\nIncluding another URLconf\n    1. Import the include() function: from django.urls import include, path\n    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))\n\"\"\"\n\nfrom django.core.exceptions import ImproperlyConfigured\nfrom django.conf import settings\nfrom django.conf.urls import url, include\nfrom django.conf.urls.static import static\nfrom django.contrib import admin\nfrom django.urls import include, path\nfrom rest_framework.urlpatterns import format_suffix_patterns\nfrom biography import views\n\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    # path('api/', include(biography.urls)),\n    url('api/bio/(?P<username>.+)/', views.BiographyByUserList.as_view()), # get user's Bio by username\n    url('^api/periods/(?P<username>.+)/$', views.PeriodByUserList.as_view()), # get user's Periods by username\n    url('^api/memoirs/(?P<username>.+)/$', views.MemoirsByUserList.as_view()),\n    # get user's Memoirs by username\n    url('^api/contentatom/(?P<username>.+)/$', views.ContentAtomByUserList.as_view()),\n     # get user's Content Atoms by username\n]\n\nurlpatterns = format_suffix_patterns(urlpatterns)\n\ntry:\n    urlpatterns += static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\nexcept ImproperlyConfigured:\n    # it's on S3, nothing for us to do\n    pass", "instruction_en": "To configure URL routing for a Django project, create a list called `urlpatterns` in a file called `urls.py`. Inside this list, use the `path()` and `url()` functions to map URLs to views. You can also include other URL configurations using the `include()` function. Finally, use the `format_suffix_patterns()` function to add support for different response formats. Optionally, use the `static()` function to serve media files during development.", "original_line_number": 16925, "instruction_length": 524}
{"instruction": "Kod ini menyediakan beberapa fungsi untuk analisis data. Untuk menggunakan fungsi ini, import pakej yang diperlukan (Numpy, NetworkX, Scipy.Spatial, dan Scipy.Stats) dan panggil fungsi yang dikehendaki dengan input yang sesuai. Fungsi yang ada ialah:\n\n- find_peaks (th, gc): Cari puncak dalam dataset tertentu (GC) di ambang yang sepadan (TH).\n- Nodedistance (Affine, VP, N1, N2): Mengira jarak antara dua nod dalam graf, memandangkan indeks mereka (N1, N2) dan matriks transformasi afin (affine) dan kedudukan voxel (VP).\n- power_law_fit (x, y): Sesuai dengan fungsi undang -undang kuasa kepada data yang diberikan (x, y) dan mengembalikan eksponen skala (a) dan nilai yang dipasang (y_fit).\n- exp_fit (x, y): Sesuai dengan fungsi eksponen ke data yang diberikan (x, y) dan mengembalikan amplitud (a), skala panjang ciri (x0), dan nilai yang dipasang (y_fit).\n- gaussian_fit (x, y): Sesuai dengan fungsi Gaussian kepada data yang diberikan (x, y) dan mengembalikan amplitud (a), kedudukan pusat (x0), sisihan piawai, dan nilai yang dipasang (Y_FIT).\n- window_correlation (x, y, w): Mengira pekali korelasi Pearson antara dua vektor (x, y) menggunakan tetingkap gelongsor saiz w.\n- find_th_jumps (bn, max_clus = 2): Cari ambang di mana lompatan berlaku dalam graf tertentu (bn), yang ditakrifkan sebagai gabungan kluster terbesar dengan sehingga max_clus kluster tambahan.\n- Nodelist2Volumen (BN, Nodelist, Element): Mewujudkan perwakilan volum 3D set nod (nodel) dalam graf (BN), dengan nilai elemen yang ditentukan (elemen).", "input": "", "output": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed May 15 20:11:59 2013\n\n@author: andres\n\"\"\"\nimport numpy\nimport networkx\nfrom scipy import spatial\nfrom scipy import stats\n\ndef find_peaks(th,gc):\n  peaks=[]\n  for i in range(1,len(th)-1):\n    if gc[1][i-1]<gc[1][i] and gc[1][i]>gc[1][i+1]:\n      peaks.append((th[i],gc[1][i]))\n  return peaks\n\ndef nodedistance(affine,vP,n1,n2):\n  \"\"\"\n  node distance in cm. (en general)\n  \"\"\"\n  ind1=vP[n1]\n  ind2=vP[n2]\n  if len(ind1)==3:\n    ind1.append(1)\n  if len(ind2)==3:\n    ind2.append(1)\n  v1=numpy.dot(affine, numpy.transpose(ind1))[0:3]\n  v2=numpy.dot(affine, numpy.transpose(ind2))[0:3]\n  d=spatial.distance.euclidean(v1,v2)\n  return d\n\ndef power_law_fit(x,y):\n  pl = lambda A, d, x: A*x**d \n  a, b, r_value, p_value, std_err = stats.linregress(numpy.log(x),numpy.log(y))\n  y_fit=pl(numpy.exp(b),a,x)\n  return (a,y_fit)\n\ndef exp_fit(x,y):\n  exp_fun = lambda A, x0, x: A*numpy.exp(x/x0) \n  a, b, r_value, p_value, std_err = stats.linregress(x,numpy.log(y))\n  A=numpy.exp(b)  \n  x0=1.0/a  \n  y_fit=exp_fun(A,x0,x)\n  return (A,x0,y_fit)\n\ndef gaussian_fit(x,y):\n  pl = lambda A, x0, s, x: A*numpy.exp(((x-x0)**2)/s) \n  p = numpy.polyfit(x,numpy.log(y),2)\n  s=1./p[0]\n  x0=-p[1]/(2*p[0])\n  A=numpy.exp(p[2]+(p[1]**2)/(4*p[0]))\n  y_fit=pl(A,x0,s,x)\n  return ((A,x0,s),y_fit)\n\ndef window_correlation(x,y,w):\n  if len(x)!=len(y):\n    print 'vector x and y must be of the same size'\n    print 'len(x)='+str(len(x))\n    print 'len(y)='+str(len(y))\n    return\n  if len(x)<w:\n    print 'window mus be smaller than len(x)'\n    print 'len(x)='+str(len(x))+' w='+str(w)\n  N=len(x)-w\n  return [stats.pearsonr(x[i:i+w],y[i:i+w])[0] for i in range(N)]\n\ndef find_th_jumps(bn,max_clus=2):\n  \"\"\"\n  Returns the thresholds where a jump occurs. A jump is defined as the \n  join of the biggest cluster with, up to, the max_clus cluster.\n  \"\"\"\n  NON=bn.get_non()\n  node_list=[node for node,dat in NON.nodes(data=True) if dat['order']==0]\n  subNON=networkx.Graph()\n  for n1,n2 in NON.edges_iter(nbunch=node_list):\n    subNON.add_edge(n1,n2)\n  node_list=networkx.connected_components(subNON)[0]\n  subNON=NON.subgraph(node_list)\n  max_th=max([dat['th'] for n,dat in subNON.nodes(data=True)])\n  N=bn.number_of_nodes()\n  jumps=[]\n  first_cluster=(0,[])\n  for node,data in NON.nodes(data=True):\n    if NON.degree(node)>=3 and NON.node[node]['order']==0:\n      for node2 in NON.neighbors(node):\n        if 0<NON.node[node2]['order']<=max_clus:\n          if 20*len(NON.node[node2]['cc'])>len(NON.node[node]['cc']) or 200*len(NON.node[node2]['cc'])>N:\n            if NON.node[node2]['th']<max_th:\n              jumps.append((NON.node[node2]['th'],NON.node[node2]['cc']))\n              if NON.node[node2]['th']>first_cluster[0]:\n                for node3 in NON.neighbors(node):\n                  if NON.node[node3]['order']==0 and NON.node[node3]['th']==NON.node[node2]['th']:\n                    first_cluster=((NON.node[node3]['th'],NON.node[node3]['cc']))\n  jumps.append(first_cluster)\n  jumps=sorted(jumps,key=lambda x: x[0],reverse=True)\n  return jumps\n    \ndef nodelist2volumen(bn,nodelist,element):\n  node2voxel=bn.node2voxel\n  B=numpy.zeros(bn.volume_shape)   \n  for node in nodelist:\n    (i,j,k)=node2voxel[str(node)]\n    B[i,j,k]=element\n  return B", "instruction_en": "This code provides several functions for data analysis. To use these functions, import the necessary packages (numpy, networkx, scipy.spatial, and scipy.stats) and call the desired function with the appropriate inputs. The available functions are:\n\n- find_peaks(th,gc): finds peaks in a given dataset (gc) at the corresponding thresholds (th).\n- nodedistance(affine,vP,n1,n2): calculates the distance between two nodes in a graph, given their indices (n1, n2) and the affine transformation matrix (affine) and voxel positions (vP).\n- power_law_fit(x,y): fits a power law function to the given data (x, y) and returns the scaling exponent (a) and the fitted values (y_fit).\n- exp_fit(x,y): fits an exponential function to the given data (x, y) and returns the amplitude (A), characteristic length scale (x0), and fitted values (y_fit).\n- gaussian_fit(x,y): fits a Gaussian function to the given data (x, y) and returns the amplitude (A), center position (x0), standard deviation (s), and fitted values (y_fit).\n- window_correlation(x,y,w): calculates the Pearson correlation coefficient between two vectors (x, y) using a sliding window of size w.\n- find_th_jumps(bn,max_clus=2): finds the thresholds where a jump occurs in a given graph (bn), defined as the join of the biggest cluster with up to max_clus additional clusters.\n- nodelist2volumen(bn,nodelist,element): creates a 3D volume representation of a given set of nodes (nodelist) in a graph (bn), with the specified element value (element).", "original_line_number": 17099, "instruction_length": 1526}
{"instruction": "Kod ini mengandungi satu set tugas untuk membersihkan, membina, dan menggunakan pakej Python yang dipanggil Marvin. Ia juga termasuk tugas untuk mengemas kini versi lalai dan fail modul, mengemas kini pakej Git, dan menyediakan pakej di Utah. Untuk menggunakan tugas -tugas ini, import fungsi `koleksi` dan` tugas` dari modul `invoke` dan membuat objek` koleksi` baru. Kemudian, tambahkan tugas yang dikehendaki ke koleksi menggunakan penghias `tugas`. Akhirnya, jalankan tugas menggunakan kaedah `run` objek` konteks`.", "input": "", "output": "# !usr/bin/env python2\n# -*- coding: utf-8 -*-\n#\n# Licensed under a 3-clause BSD license.\n#\n# @Author: Brian Cherinka\n# @Date:   2017-06-10 16:46:40\n# @Last modified by: Jos Snchez-Gallego (gallegoj@uw.edu)\n# @Last modified time: 2018-11-14 19:37:22\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nfrom invoke import Collection, task\n\n\nDIRPATH = '/home/manga/software/git/manga/marvin'\nMODULEPATH = '/home/manga/software/git/modulefiles'\n\n\n@task\ndef clean_docs(ctx):\n    ''' Cleans up the docs '''\n    print('Cleaning the docs')\n    ctx.run(\"rm -rf docs/sphinx/_build\")\n\n\n@task\ndef build_docs(ctx, clean=False):\n    ''' Builds the Sphinx docs '''\n\n    if clean:\n        print('Cleaning the docs')\n        ctx.run(\"rm -rf docs/sphinx/_build\")\n\n    print('Building the docs')\n    os.chdir('docs/sphinx')\n    ctx.run(\"make html\", pty=True)\n\n\n@task\ndef show_docs(ctx):\n    \"\"\"Shows the Sphinx docs\"\"\"\n    print('Showing the docs')\n    os.chdir('docs/sphinx/_build/html')\n    ctx.run('open ./index.html')\n\n\n@task\ndef clean(ctx):\n    ''' Cleans up the crap '''\n    print('Cleaning')\n    # ctx.run(\"rm -rf docs/sphinx/_build\")\n    ctx.run(\"rm -rf htmlcov\")\n    ctx.run(\"rm -rf build\")\n    ctx.run(\"rm -rf dist\")\n\n\n@task(clean)\ndef deploy(ctx, repo=None):\n    ''' Deploy to pypi '''\n    print('Deploying to Pypi!')\n    rstr = ''\n    if repo:\n        rstr = '-r {0}'.format(repo)\n    ctx.run(\"python setup.py sdist bdist_wheel --universal\")\n    ctx.run(\"twine upload {0} dist/*\".format(rstr))\n\n\n\n@task\ndef update_default(ctx, path=None, version=None):\n    ''' Updates the default version module file'''\n\n    assert version is not None, 'A version is required to update the default version!'\n    assert path is not None, 'A path must be specified!'\n\n    # update default version\n    f = open('.version', 'r+')\n    data = f.readlines()\n    data[1] = 'set ModulesVersion \"{0}\"\\n'.format(version)\n    f.seek(0, 0)\n    f.writelines(data)\n    f.close()\n\n\n@task\ndef update_module(ctx, path=None, wrap=None, version=None):\n    ''' Update a module file '''\n\n    assert version is not None, 'A version is required to update the module file!'\n    assert path is not None, 'A path must be specified!'\n    print('Setting up module files!')\n    os.chdir(path)\n    newfile = 'mangawork.marvin_{0}'.format(version) if wrap else version\n    oldfile = 'mangawork.marvin_2.1.3' if wrap else 'master'\n    searchline = 'marvin' if wrap else 'version'\n    ctx.run('cp {0} {1}'.format(oldfile, newfile))\n    f = open('{0}'.format(newfile), 'r+')\n    data = f.readlines()\n    index, line = [(i, line) for i, line in enumerate(data)\n                   if 'set {0}'.format(searchline) in line][0]\n    data[index] = 'set {0} {1}\\n'.format(searchline, version)\n    f.seek(0, 0)\n    f.writelines(data)\n    f.close()\n\n    # update the default version\n    update_default(ctx, path=path, version=newfile)\n\n\n@task\ndef update_git(ctx, version=None):\n    ''' Update the git package at Utah '''\n    assert version is not None, 'A version is required to checkout a new git repo!'\n    print('Checking out git tag {0}'.format(version))\n    verpath = os.path.join(DIRPATH, version)\n    # checkout and setup new git tag\n    os.chdir(DIRPATH)\n    ctx.run('git clone https://github.com/sdss/marvin.git {0}'.format(version))\n    os.chdir(verpath)\n    ctx.run('git checkout {0}'.format(version))\n    ctx.run('git submodule update --init --recursive')\n    # ctx.run('python -c \"from get_version import generate_version_py; '\n    #         'generate_version_py(\\'sdss-marvin\\', {0}, False)'.format(version))\n\n\n@task\ndef update_current(ctx, version=None):\n    ''' Update the current symlink '''\n    assert version is not None, 'A version is required to update the current symlink!'\n    # reset the current symlink\n    os.chdir(DIRPATH)\n    ctx.run('rm current')\n    ctx.run('ln -s {0} current'.format(version))\n\n\n@task\ndef switch_module(ctx, version=None):\n    ''' Switch to the marvin module of the specified version and start it '''\n    assert version is not None, 'A version is required to setup Marvin at Utah!'\n    ctx.run('uwsgi --stop /home/www/sas.sdss.org/mangawork/marvin/pid/uwsgi_marvin.pid')\n    ctx.run('module unload wrapmarvin')\n    ctx.run('module load wrapmarvin/mangawork.marvin_{0}'.format(version))\n    ctx.run('uwsgi /home/manga/software/git/manga/marvin/{0}/python/marvin/web/uwsgi_conf_files/uwsgi_marvin_mangawork.ini'.format(version))\n\n\n@task\ndef update_uwsgi(ctx, version=None):\n    ''' Reset the uwsgi symlink to the new version and touch the file to Emperor reload Marvin '''\n    assert version is not None, 'A version is required to setup Marvin at Utah!'\n    os.chdir('/etc/uwsgi/vassals')\n    new_path = '/home/manga/software/git/manga/marvin/{0}/python/marvin/web/uwsgi_conf_files/uwsgi_marvin_mangawork.ini'.format(version)\n    ctx.run('rm uwsgi_marvin_mangawork.ini')\n    ctx.run('ln -s {0} uwsgi_marvin_mangawork.ini'.format(new_path))\n    ctx.run('touch uwsgi_marvin_mangawork.ini')\n\n\n@task\ndef setup_utah(ctx, version=None):\n    ''' Setup the package at Utah and update the release '''\n    assert version is not None, 'A version is required to setup Marvin at Utah!'\n\n    # update git\n    update_git(ctx, version=version)\n\n    # update_current\n    update_current(ctx, version=version)\n\n    # update modules\n    marvin = os.path.join(MODULEPATH, 'marvin')\n    wrap = os.path.join(MODULEPATH, 'wrapmarvin')\n    update_module(ctx, path=marvin, version=version)\n    update_module(ctx, path=wrap, wrap=True, version=version)\n\n    # restart the new marvin\n    # switch_module(ctx, version=version)\n    update_uwsgi(ctx, version=version)\n    print('Marvin version {0} is set up!\\n'.format(version))\n    print('Check for the new Marvin version at the bottom of the Marvin Web main page!')\n    # print('Please run ...\\n stopmarvin \\n module switch wrapmarvin '\n    #       'wrapmarvin/mangawork.marvin_{0} \\n startmarvin \\n'.format(version))\n\n\nos.chdir(os.path.dirname(__file__))\n\nns = Collection(clean, deploy, setup_utah)\ndocs = Collection('docs')\ndocs.add_task(build_docs, 'build')\ndocs.add_task(clean_docs, 'clean')\ndocs.add_task(show_docs, 'show')\nns.add_collection(docs)\nupdates = Collection('update')\nupdates.add_task(update_git, 'git')\nupdates.add_task(update_current, 'current')\nupdates.add_task(update_module, 'module')\nupdates.add_task(update_default, 'default')\nns.add_collection(updates)", "instruction_en": "This code contains a set of tasks to clean, build, and deploy a Python package called Marvin. It also includes tasks to update the default version and module files, update the git package, and set up the package at Utah. To use these tasks, import the `Collection` and `task` functions from the `invoke` module and create a new `Collection` object. Then, add the desired tasks to the collection using the `task` decorator. Finally, run the tasks using the `run` method of the `Context` object.", "original_line_number": 17724, "instruction_length": 519}
{"instruction": "Buat kelas yang dipanggil \"gethpuxdata\" yang mengambil maklumat sistem dari mesin HP-UX. Kelas harus mempunyai permulaan yang mengambil alamat IP, port ssh, waktu tamat, nama pengguna, kata laluan, use_key_file, key_file, get_serial_info, get_hardware_info, get_os_details, get_cpu_info, get_memory_info, obcreed_domain Kelas harus mempunyai kaedah utama yang menghubungkan ke mesin, memeriksa jika pengguna adalah root, dan mengambil maklumat sistem seperti memori, perkakasan, UUID, nombor siri, nama, versi OS, kuasa CPU, alamat MAC, alamat IP, kiraan CPU, dan maklumat cakera. Kelas harus memformat data dan mengembalikannya sebagai senarai.", "input": "", "output": "import paramiko\r\nimport math\r\nimport json\r\n\r\n\r\nclass GetHPUXData:\r\n    def __init__(self, ip, ssh_port, timeout, usr, pwd, use_key_file, key_file,\r\n                 get_serial_info, get_hardware_info, get_os_details,\r\n                 get_cpu_info, get_memory_info, ignore_domain, upload_ipv6, debug):\r\n        self.machine_name = ip\r\n        self.port = int(ssh_port)\r\n        self.timeout = timeout\r\n        self.username = usr\r\n        self.password = pwd\r\n        self.ssh = paramiko.SSHClient()\r\n        self.use_key_file = use_key_file\r\n        self.key_file = key_file\r\n        self.get_serial_info = get_serial_info\r\n        self.get_hardware_info = get_hardware_info\r\n        self.get_os_details = get_os_details\r\n        self.get_cpu_info = get_cpu_info\r\n        self.get_memory_info = get_memory_info\r\n        self.ignore_domain = ignore_domain\r\n        self.upload_ipv6 = upload_ipv6\r\n        self.debug = debug\r\n        self.ssh = paramiko.SSHClient()\r\n        self.conn = None\r\n        self.root = False\r\n        self.sysdata = {}\r\n        self.nic_data = {'nic_parts': {}}\r\n        self.ip_data = []\r\n        self.disk_data = {'hdd_parts':[]}\r\n        self.name = None\r\n        self.paths = {}\r\n        self.alldata = []\r\n        self.name = None\r\n\r\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r\n\r\n    def main(self):\r\n\r\n        self.connect()\r\n        self.are_u_root()\r\n        self.get_sys_1()\r\n        self.get_sys_2()\r\n        self.get_macs()\r\n        self.get_ips()\r\n\r\n\r\n        self.get_cpu_num()\r\n        self.get_disks()\r\n        self.format_data()\r\n        return self.alldata\r\n\r\n    def connect(self):\r\n        try:\r\n            if not self.use_key_file:\r\n                self.ssh.connect(str(self.machine_name), port=self.port,\r\n                                 username=self.username, password=self.password, timeout=self.timeout)\r\n            else:\r\n                self.ssh.connect(str(self.machine_name), port=self.port,\r\n                                 username=self.username, key_filename=self.key_file, timeout=self.timeout)\r\n        except paramiko.AuthenticationException:\r\n            print str(self.machine_name) + ': authentication failed'\r\n            return None\r\n        except Exception as err:\r\n            print str(self.machine_name) + ': ' + str(err)\r\n            return None\r\n\r\n    def execute(self, cmd, need_sudo=False):\r\n        if need_sudo and not self.root: # not working currently, maybe in the future\r\n            cmd_sudo = \"sudo -S -p '' %s\" % cmd\r\n            stdin, stdout, stderr = self.ssh.exec_command(cmd_sudo, timeout=30)\r\n            stdin.write('%s\\n' % self.password)\r\n            stdin.flush()\r\n        else:\r\n            stdin, stdout, stderr = self.ssh.exec_command(cmd, timeout=30)\r\n\r\n        data_err = stderr.readlines()\r\n        data_out = stdout.readlines()\r\n        return data_out, data_err\r\n\r\n    def are_u_root(self):\r\n        cmd = 'id -u'\r\n        data, err = self.execute(cmd)\r\n        if data[0].strip() == '0':\r\n            self.root = True\r\n        else:\r\n            self.root = False\r\n        if not self.root:\r\n            print '[!] You must be root to run HP-UX discovery!'\r\n            return\r\n\r\n    def format_data(self):\r\n        self.alldata.append(self.sysdata)\r\n        self.alldata.append(self.nic_data)\r\n        self.alldata.append(self.disk_data)\r\n\r\n\r\n    def get_sys_1(self):\r\n        cmd = '/usr/contrib/bin/machinfo'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec  in raw:\r\n                if rec.startswith('memory:'):\r\n                    ram = int(math.ceil(float(rec.split()[1])))\r\n                    self.sysdata.update({'memory':ram})\r\n                if rec.startswith('model:'):\r\n                    model = rec.split(':')[1].strip().strip('\"')\r\n                    self.sysdata.update({'hardware': model})\r\n                if rec.startswith('machine id number:'):\r\n                    uuid = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'uuid': uuid})\r\n                if rec.startswith('machine serial number'):\r\n                    serial = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'serial_no': serial})\r\n                if rec.startswith('nodename:'):\r\n                    name = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'name': name})\r\n                    self.name = name\r\n                if rec.startswith('release:'):\r\n                    os_version = rec.split(':')[1].strip()\r\n                    osver = ' '.join(os_version.split()[1:]).strip()\r\n                    self.sysdata.update({'os': 'hp-ux'})\r\n                    self.sysdata.update({'osver': osver if osver else 'D42_NULL'})\r\n        else:\r\n            print '[!] Error in get_sys_1(). Message was: %s' % data_err\r\n\r\n    def get_sys_2(self):\r\n        cmd = '/opt/ignite/bin/print_manifest'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec in raw:\r\n\r\n                if rec.startswith('model:'):\r\n                    model = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'hardware': model})\r\n                if rec.startswith('main memory:'):\r\n                    m = rec.split(':')[1].split()[0]\r\n                    ram = int(math.ceil(float(m.strip())))\r\n                    self.sysdata.update({'memory': ram})\r\n                if 'speed:' in rec and 'mhz' in rec:\r\n                    cpu_speed= rec.split(':')[1].strip('mhz').strip()\r\n                    self.sysdata.update({'cpupower': cpu_speed})\r\n                if rec.startswith('hostname'):\r\n                    name = rec.split(':')[1].strip()\r\n                    self.name = name\r\n                    self.sysdata.update({'name': name})\r\n        else:\r\n            print '[!] Error in get_sys_2(). Message was: %s' % data_err\r\n\r\n\r\n    def get_macs(self):\r\n        cmd = 'lanscan'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec  in raw:\r\n                if rec.split()[3] == 'up':\r\n                    words = rec.split()\r\n                    nic_mac = words[1]\r\n                    nic_name = words[4]\r\n                    mac = ''.join(nic_mac.split('0x')[1:])\r\n                    n=2\r\n                    raw = [mac[i:i + n] for i in range(0, len(mac), n)]\r\n                    macaddress = ':'.join(raw)\r\n                    self.nic_data['nic_parts'].update({nic_name:{'serial_no':macaddress}})\r\n        else:\r\n            print '[!] Error in get_macs(). Message was: %s' % data_err\r\n\r\n    def get_ips(self):\r\n        ip_data = {}\r\n        mac_data = {}\r\n\r\n        for nic in self.nic_data['nic_parts']:\r\n            mac = self.nic_data['nic_parts'][nic]['serial_no']\r\n            ip_data.update({'device':self.name})\r\n            ip_data.update({'tag': nic})\r\n            mac_data.update({'device': self.name})\r\n            mac_data.update({'port_name': nic})\r\n            mac_data.update({'macaddress': mac})\r\n\r\n            ip_data.update({'macaddress': mac})\r\n            cmd = 'ifconfig %s | grep inet' % nic\r\n            data_out, data_err = self.execute(cmd, False)\r\n            if not data_err:\r\n                raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n                for rec in raw:\r\n                    ip = rec.split()[1].strip()\r\n                    self.nic_data['nic_parts'][nic].update({'ipaddress':ip})\r\n                    ip_data.update({'ipaddress': ip})\r\n            else:\r\n                print '[!] Error in get_ips(). Message was: %s' % data_err\r\n            self.alldata.append(ip_data)\r\n            self.alldata.append(mac_data)\r\n\r\n    def get_cpu_num(self):\r\n        cmd = 'ioscan -fnk|grep proc | wc -l'\r\n        data_out, data_err = self.execute(cmd, False)\r\n\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            if raw:\r\n                cpu_num = raw[0]\r\n                self.sysdata.update({'cpucount': cpu_num})\r\n        else:\r\n            print '[!] Error in get_cpu_num(). Message was: %s' % data_err\r\n\r\n    def get_disks(self):\r\n        cmd = 'ls /dev/rdisk/'\r\n        data_out, data_err = self.execute(cmd, False)\r\n\r\n        if not data_err:\r\n            disks = list(set([x.strip().split('_')[0] for x in data_out if x]))\r\n            for disk in disks:\r\n                cmd = 'diskinfo /dev/rdisk/%s' % disk\r\n                data_out, data_err = self.execute(cmd, False)\r\n                if not data_err:\r\n                    raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n                    disk = {}\r\n                    for rec in raw:\r\n                        if 'describe of ' in rec:  # another disk\r\n                            if not len(disk) == 0:\r\n                                self.disk_data['hdd_parts'].append(disk)\r\n                                disk = {}\r\n                        else:\r\n                            if rec.startswith('product id'):\r\n                                product = rec.split(':')[1].strip()\r\n                                disk.update({'product': product})\r\n                            if rec.startswith('size'):\r\n                                size = int(math.ceil(float(rec.split(':')[1].split()[0].strip()) / 1024 / 1024))\r\n                                disk.update({'hdd_size': size})\r\n                                disk.update({'assignment': 'device'})\r\n                                if self.name:\r\n                                    disk.update({'device': self.name})\r\n                    self.disk_data['hdd_parts'].append(disk)\r\n\r\n        else:\r\n            print '[!] Error in get_disks(). Message was: %s' % data_err", "instruction_en": "Create a class called \"GetHPUXData\" that retrieves system information from an HP-UX machine. The class should have an initializer that takes in the IP address, SSH port, timeout, username, password, use_key_file, key_file, get_serial_info, get_hardware_info, get_os_details, get_cpu_info, get_memory_info, ignore_domain, upload_ipv6, and debug. The class should have a main method that connects to the machine, checks if the user is root, and retrieves system information such as memory, hardware, UUID, serial number, name, OS version, CPU power, MAC addresses, IP addresses, CPU count, and disk information. The class should format the data and return it as a list.", "original_line_number": 17812, "instruction_length": 645}
{"instruction": "Buat kelas postprocessor yang dipanggil PrefixSuffixReplacer yang menggantikan awalan dan akhiran dalam objek kotoran. Kelas harus mengambil rentetan awalan dan akhiran pilihan, memungkiri kepada '{' dan '}}' masing -masing. Kaedah process_filth harus melangkah melalui urutan objek kotoran dan menggantikan atribut penggantian_string dengan atribut jenis huruf besar yang dikelilingi oleh rentetan awalan dan akhiran. Jika sama ada awalan atau akhiran tidak disediakan, ia hanya perlu menambah yang disediakan kepada pengganti_string.", "input": "", "output": "from typing import Optional, Sequence\n\nfrom ...filth import Filth\nfrom ..base import PostProcessor\n\n\nclass PrefixSuffixReplacer(PostProcessor):\n    name = 'prefix_suffix_replacer'  # type: str\n\n    def __init__(self, prefix: Optional[str] = '{{', suffix: Optional[str] = '}}', name: Optional[str] = None):\n        super(PrefixSuffixReplacer, self).__init__(name=name)\n\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def process_filth(self, filth_list: Sequence[Filth]) -> Sequence[Filth]:\n        for filth_item in filth_list:\n            if filth_item.replacement_string is None:\n                filth_item.replacement_string = filth_item.type.upper()\n\n            if self.prefix is not None and self.suffix is not None:\n                filth_item.replacement_string = self.prefix + filth_item.replacement_string + self.suffix\n            elif self.prefix is not None:\n                filth_item.replacement_string = self.prefix + filth_item.replacement_string\n            elif self.suffix is not None:\n                filth_item.replacement_string = filth_item.replacement_string + self.suffix\n\n        return filth_list\n\n\n__all__ = ['PrefixSuffixReplacer']", "instruction_en": "Create a PostProcessor class called PrefixSuffixReplacer that replaces prefixes and suffixes in Filth objects. The class should take in optional prefix and suffix strings, defaulting to '{{' and '}}' respectively. The process_filth method should iterate through a sequence of Filth objects and replace the replacement_string attribute with the uppercase type attribute surrounded by the prefix and suffix strings. If either the prefix or suffix is not provided, it should only add the provided one to the replacement_string.", "original_line_number": 17946, "instruction_length": 535}
{"instruction": "Kod ini menyediakan fungsi untuk mengesan dan mendokumentasikan model tensorflow dan kaedah mereka. Untuk menggunakannya, import perpustakaan yang diperlukan dan hubungi fungsi `detect_model_documentation`, lulus dalam model TensorFlow sebagai hujah. Ini akan mengembalikan objek `modeldocumentation` yang mengandungi maklumat mengenai model, termasuk namanya, hyperparameter, gambaran keseluruhan, kaedah, dan rujukan. Untuk menulis maklumat ini ke cakera, hubungi fungsi `tulis`, lulus di jalan ke direktori di mana dokumentasi harus disimpan dan objek` modeldocumentation`. Untuk membaca dokumentasi dari cakera, hubungi fungsi `baca`, lulus di jalan ke direktori di mana dokumentasi disimpan dan kamus definisi tandatangan model.", "input": "", "output": "import tfi.json\nimport tensorflow as tf\nimport os.path\n\nimport tfi.data\nimport tfi.doc\n\nfrom google.protobuf.json_format import ParseDict\nfrom tfi.parse.docstring import GoogleDocstring\n\ndef _detect_method_documentation(*, bibliographer, model, method_name, method, signature_def):\n    # NOTE(adamb) Since we don't want to be parsing rst here, we'll just rewrite\n    #     it to include detected citations. Expect that this rst will be parsed\n    #     for real when rendering HTML.\n    docstr = GoogleDocstring(obj=method).result()\n    docstr_sections = docstr['sections']\n    text_sections = [v for k, v in docstr_sections if k == 'text']\n    overview = \"\\n\".join([l for t in text_sections for l in t])\n\n    docstr['args'] = _enrich_docs_with_tensor_info(docstr['args'], signature_def.inputs)\n    docstr['returns'] = _enrich_docs_with_tensor_info(docstr['returns'], signature_def.outputs)\n\n    return tfi.doc.MethodDocumentation(\n        name=method_name,\n        overview=bibliographer.rewrite(overview),\n        inputs=docstr['args'],\n        outputs=docstr['returns'],\n        examples=[\n            tfi.doc.MethodDataDocumentation.generate(\n                method=getattr(model, method_name),\n                inputs={\n                    input_name: eval(\"\\n\".join(input_val_lines), {}, {'m': model, 'tfi': tfi})\n                    for input_name, _, input_val_lines in docstr['example args']\n                },\n            ),\n        ],\n    )\n\ndef detect_model_documentation(model):\n        source = tfi.doc.ModelSource.detect(model)\n\n        bibliographer = tfi.doc.Bibliographer()\n\n        def maybeattr(o, attr, default=None):\n            return getattr(o, attr) if o and hasattr(o, attr) else default\n\n        # NOTE(adamb) Since we don't want to be parsing rst here, we'll just rewrite\n        #     it to include detected citations. Expect that this rst will be parsed\n        #     for real when rendering HTML.\n        model_docstr = GoogleDocstring(obj=model).result()\n        model_docstr_sections = model_docstr['sections']\n        text_sections = [v for k, v in model_docstr_sections if k == 'text']\n        overview = \"\\n\".join([l for t in text_sections for l in t])\n\n        return tfi.doc.ModelDocumentation(\n            name=maybeattr(model, '__name__', type(model).__name__),\n            hyperparameters=maybeattr(model, '__tfi_hyperparameters__', []),\n            overview=bibliographer.rewrite(overview),\n            implementation_notes=[],\n            authors=[\n                *[\n                    {\n                        \"name\": author['name'],\n                        \"url\": author['url'],\n                        \"role_noun\": \"Commits\",\n                        \"role_url\": author['commits_url'],\n                    }\n                    for author in maybeattr(source, 'authors', [])\n                ],\n            ],\n            source=source,\n            facets_overview_proto=maybeattr(model, '__tfi_facets_overview_proto__'),\n            methods=[\n                _detect_method_documentation(\n                    model=model,\n                    bibliographer=bibliographer,\n                    method_name=method_name,\n                    method=getattr(model, method_name),\n                    signature_def=signature_def,\n                )\n                for method_name, signature_def in maybeattr(model, '__tfi_signature_defs__').items()\n            ],\n            references=bibliographer.references(),\n        )\n\ndef _tensor_info_str(tensor):\n    if tensor.shape.ndims is None:\n        return '%s ?' % tensor.dtype.name\n\n    return '%s <%s>' % (\n        tensor.dtype.name,\n        ', '.join(['?' if n is None else str(n) for n in tensor.shape.as_list()]),\n    )\n\ndef _enrich_docs_with_tensor_info(doc_fields, tensor_dict):\n    existing = {k: v for k, _, v in doc_fields}\n    return [\n        (name, _tensor_info_str(tensor), existing.get(name, ''))\n        for name, tensor in tensor_dict.items()\n    ]\n\nclass MethodDocumentationLayout(object):\n    def __init__(self, base_path, assets_extra_path):\n        self.assets_extra_path = assets_extra_path\n        self.metadata_path = os.path.join(base_path, 'metadata.json')\n        self._base_path = base_path\n\n    def file(self, subpath):\n        return os.path.join(self._base_path, subpath)\n\nclass ModelDocumentationLayout(object):\n    def __init__(self, model_dir):\n        self.basename = os.path.basename(model_dir)\n        self.assets_extra_path = os.path.join(model_dir, 'assets.extra')\n        self.doc_path = os.path.join(self.assets_extra_path, 'doc')\n        self.metadata_path = os.path.join(self.doc_path, 'metadata.json')\n        self.methods_path = os.path.join(self.doc_path, 'methods')\n\n    def method(self, method_name):\n        return MethodDocumentationLayout(\n            os.path.join(self.methods_path, method_name),\n            self.assets_extra_path,\n        )\n\ndef _read_json_else(path, default):\n    if not os.path.exists(path):\n      return default\n\n    with open(path) as f:\n      return tfi.json.load(f)\n\ndef _write_json(path, obj):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, 'w') as f:\n        tfi.json.dump(obj, f)\n\nclass MethodDataDocumentationCodec(object):\n    def __init__(self, method_layout):\n        self._layout = method_layout\n\n    def write(self, method_example):\n        _write_json(\n            self._layout.file('inputs.json'),\n            {\n                name: value\n                for name, value in method_example.inputs().items()\n            }\n        )\n\n        if method_example.outputs() is not None:\n            _write_json(\n                self._layout.file('outputs.json'),\n                {\n                    name: value\n                    for name, value in method_example.outputs().items()\n                }\n            )\n\n    def read(self, signature_def):\n        return tfi.doc.MethodDataDocumentation(\n            inputs=self._detect(\n                lambda: self._read_json_tf_example_from(\n                    signature_def.inputs,\n                    'inputs.pb.json',\n                ),\n                lambda: self._read_json_example_from(\n                    signature_def.inputs,\n                    'inputs.json',\n                ),\n            ),\n            outputs=self._detect(\n                lambda: self._read_json_tf_example_from(\n                    signature_def.outputs,\n                    'outputs.pb.json',\n                ),\n                lambda: self._read_json_example_from(\n                    signature_def.outputs,\n                    'outputs.json',\n                ),\n            ),\n            # 'example result': _detect(\n            #     lambda: _read_json_tf_example_from(\n            #         signature_def.outputs,\n            #         'outputs.pb.json',\n            #     ),\n            #     lambda: _read_json_example_from(\n            #         signature_def.outputs,\n            #         'outputs.json',\n            #     ),\n            # ),\n        )\n\n    def _detect(self, *fns):\n        for fn in fns:\n            result = fn()\n            if result is not None:\n                return result\n\n    def _feature_for_tensor_info(self, tensor_info):\n        tensor_shape = tensor_info.tensor_shape.dim[1:]\n        dtype = tf.DType(tensor_info.dtype)\n        if tensor_shape[-1].size != -1:\n            return tf.FixedLenFeature(dtype=dtype, shape=[dim.size for dim in tensor_shape])\n        return tf.VarLenFeature(dtype=dtype)\n\n    def _read_json_tf_example_from(self, tensor_infos, subpath):\n        path = self._layout.file(subpath)\n        if not os.path.exists(path):\n            return None\n\n        with open(path) as f:\n            example_dict = tfi.json.load(f)\n\n        with tf.Session(graph=tf.Graph()) as session:\n            example_features = {\n                name: self._feature_for_tensor_info(tensor_info)\n                for name, tensor_info in tensor_infos.items()\n            }\n            return session.run(\n                tf.parse_single_example(\n                    ParseDict(example_dict, tf.train.Example()).SerializeToString(),\n                    features=example_features))\n\n    def _read_json_example_from(self, tensor_infos, subpath):\n        path = self._layout.file(subpath)\n        if not os.path.exists(path):\n            return None\n\n        with open(path) as f:\n            return tfi.data.json(\n                f.read(),\n                assets_extra_root=self._layout.assets_extra_path)\n\nclass MethodDocumentationCodec(object):\n    def __init__(self, method_name, method_layout):\n        self._name = method_name\n        self._layout = method_layout\n\n    def write(self, method_doc):\n        metadata = {\n            'documentation': {\n                'inputs': {\n                    name: doc\n                    for name, tensor_info, doc in method_doc.inputs()\n                },\n                'outputs': {\n                    name: doc\n                    for name, tensor_info, doc in method_doc.outputs()\n                },\n            },\n        }\n        MethodDataDocumentationCodec(self._layout).write(method_doc.examples()[0]),\n        _write_json(self._layout.metadata_path, metadata)\n        \n    def read(self, signature_def):\n        metadata = _read_json_else(self._layout.metadata_path, {})\n        doc = metadata.get('documentation', {})\n        doc_inputs = doc.get('inputs', {})\n        doc_outputs = doc.get('outputs', {})\n\n        return tfi.doc.MethodDocumentation(\n            name=self._name,\n            overview=metadata.get('overview', None),\n            inputs=[\n                (name, self._tensor_info_str(ti), doc_inputs.get(name, ''))\n                for name, ti in signature_def.inputs.items()\n            ],\n            outputs=[\n                (name, self._tensor_info_str(ti), doc_outputs.get(name, ''))\n                for name, ti in signature_def.outputs.items()                    \n            ],\n            examples=[\n              MethodDataDocumentationCodec(self._layout).read(signature_def),\n            ],\n        )\n\n    def _tensor_info_str(self, tensor_info):\n        if tensor_info.tensor_shape.unknown_rank:\n            return '%s ?' % tf.as_dtype(tensor_info.dtype).name\n\n        return '%s <%s>' % (\n            tf.as_dtype(tensor_info.dtype).name,\n            ', '.join([\n                '?' if dim.size == -1 else str(dim.size)\n                for dim in tensor_info.tensor_shape.dim\n            ]),\n        )\n\nclass ModelDocumentationCodec(object):\n    def __init__(self, path):\n        self._layout = ModelDocumentationLayout(path)\n\n    def _method_codecs(self, method_names):\n        return [\n            (\n                method_name,\n                MethodDocumentationCodec(\n                    method_name,\n                    self._layout.method(method_name),\n                )\n            )\n            for method_name in method_names\n        ]\n\n    def write(self, model_doc):\n        metadata = {\n            'name': model_doc.name(),\n            'overview': model_doc.overview(),\n            'hyperparameters': [\n                (name, str(val_type), val, docs)\n                for name, val_type, val, docs in model_doc.hyperparameters()\n            ],\n            'authors': model_doc.authors(),\n            'references': model_doc.references(),\n            'implementation_notes': model_doc.implementation_notes(),\n            'source': model_doc.source(),\n            'facets_overview_proto': None, # model_doc.facets_overview_proto(),\n        }\n\n        methods = model_doc.methods()\n        for method_name, method_codec in self._method_codecs(methods.keys()):\n            method_codec.write(methods[method_name])\n\n        _write_json(self._layout.metadata_path, metadata)\n\n    def read(self, signature_defs):\n        metadata = _read_json_else(self._layout.metadata_path, {})\n        return tfi.doc.ModelDocumentation(\n            # TODO(adamb) Should be transformed to the below structure, with val_type_str -> val_type\n            # (name, val_type, val, docs)\n            hyperparameters=metadata.get('hyperparameters', []),\n            name=metadata.get('name', self._layout.basename),\n            overview=metadata.get('overview', None),\n            methods=[\n                method_codec.read(signature_defs[method_name])\n                for method_name, method_codec in self._method_codecs(signature_defs.keys())\n            ],\n            authors=metadata.get('authors', []),\n            references=metadata.get('references', {}),\n            implementation_notes=metadata.get('implementation_notes', []),\n            source=metadata.get('source', []),\n            facets_overview_proto=None,\n        )\n\ndef read(path, signature_defs):\n    return ModelDocumentationCodec(path).read(signature_defs)\n\ndef write(path, model_doc):\n    return ModelDocumentationCodec(path).write(model_doc)", "instruction_en": "This code provides functions for detecting and documenting TensorFlow models and their methods. To use it, import the necessary libraries and call the `detect_model_documentation` function, passing in the TensorFlow model as an argument. This will return a `ModelDocumentation` object containing information about the model, including its name, hyperparameters, overview, methods, and references. To write this information to disk, call the `write` function, passing in the path to the directory where the documentation should be saved and the `ModelDocumentation` object. To read the documentation from disk, call the `read` function, passing in the path to the directory where the documentation is saved and a dictionary of the model's signature definitions.", "original_line_number": 18025, "instruction_length": 733}
{"instruction": "Muat turun Pakej NLTK yang diperlukan dan Perpustakaan Fuzzywuzzy. Tentukan kamus singkatan negeri AS dan nama penuh mereka. Tentukan fungsi untuk menukar tag TreeBank Penn ke tag WordNet, menukar kata -kata yang ditandakan ke sinset, menetapkan singkatan keadaan dalam token, token bersih dengan mengeluarkan kata -kata berhenti dan tanda baca, dan mengira persamaan ayat menggunakan WordNet. Tentukan fungsi utama yang membandingkan persamaan ayat fokus kepada senarai ayat menggunakan fungsi Senten_Similarity. Fungsi utama juga mempunyai pilihan untuk mengabaikan bilangan bulat dalam perbandingan.", "input": "", "output": "'''\nnltk.download(['wordnet', 'stopwords', 'punkt']) if not already downloaded.\nShould add to wordnet if you want more words to compare as reference to.\n'''\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom fuzzywuzzy import fuzz, process\nstop_words = stopwords.words('english')\n\n\nstates = {\n    'ak': 'alaska',\n    'al': 'alabama',\n    'ar': 'arkansas',\n    'as': 'american samoa',\n    'az': 'arizona',\n    'ca': 'california',\n    'co': 'colorado',\n    'ct': 'connecticut',\n    'dc': 'district of columbia',\n    'de': 'delaware',\n    'fl': 'florida',\n    'ga': 'georgia',\n    'gu': 'guam',\n    'hi': 'hawaii',\n    'ia': 'iowa',\n    'id': 'idaho',\n    'il': 'illinois',\n    'in': 'indiana',\n    'ks': 'kansas',\n    'ky': 'kentucky',\n    'la': 'louisiana',\n    'ma': 'massachusetts',\n    'md': 'maryland',\n    'me': 'maine',\n    'mi': 'michigan',\n    'mn': 'minnesota',\n    'mo': 'missouri',\n    'mp': 'northern mariana islands',\n    'ms': 'mississippi',\n    'mt': 'montana',\n    'na': 'national',\n    'nc': 'north carolina',\n    'nd': 'north dakota',\n    'ne': 'nebraska',\n    'nh': 'new hampshire',\n    'nj': 'new jersey',\n    'nm': 'new mexico',\n    'nv': 'nevada',\n    'ny': 'new york',\n    'oh': 'ohio',\n    'ok': 'oklahoma',\n    'or': 'oregon',\n    'pa': 'pennsylvania',\n    'pr': 'puerto rico',\n    'ri': 'rhode island',\n    'sc': 'south carolina',\n    'sd': 'south dakota',\n    'tn': 'tennessee',\n    'tx': 'texas',\n    'ut': 'utah',\n    'va': 'virginia',\n    'vi': 'virgin islands',\n    'vt': 'vermont',\n    'wa': 'washington',\n    'wi': 'wisconsin',\n    'wv': 'west virginia',\n    'wy': 'wyoming'\n}\n\ndef penn_to_wn(tag):\n    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n    if tag.startswith('N'):\n        return 'n'\n\n    if tag.startswith('V'):\n        return 'v'\n\n    if tag.startswith('J'):\n        return 'a'\n\n    if tag.startswith('R'):\n        return 'r'\n\n    return None\n\ndef tagged_to_synset(word, tag):\n    wn_tag = penn_to_wn(tag)\n    # wn_tag is None if no definition is found\n    if wn_tag is None:\n        return word\n    # try:\n        # most probable english word\n    return wn.synsets(word, wn_tag)[0]\n    # except:\n    #     return word\n\ndef fix_state_abbrev(tokens):\n    token = [\n        states[token] if states.get(token) else token\n        for token in tokens\n    ]\n    return token\n\ndef clean_tokens(tokens, ignore_integers=False):\n    punctuations = ['(',')',';',':','[',']',',','.','/']\n    keywords = [\n        word for word in tokens\n        if not word in stop_words and not word in punctuations\n    ]\n    keywords = fix_state_abbrev(keywords)\n    if ignore_integers:\n        keywords = [word for word in keywords if not is_possible_integer(word)]\n    return keywords\n\ndef clean(word):\n    word = str(word).lower().strip()\n    punctuations = ['(',')',';',':','[',']',',','.','/']\n    for punctuation in punctuations:\n        word = word.replace(punctuation, '')\n    return word\n\ndef is_possible_integer(word):\n    try:\n        int(word)\n        return True\n    except:\n        return False\n\ndef sentence_similarity(sentence1, sentence2, ignore_integers=False):\n    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n    # Tokenize and tag\n    sentence1 = ' '.join([clean(word) for word in sentence1.split()])\n    sentence2 = ' '.join([clean(word) for word in sentence2.split()])\n    tokens1 = word_tokenize(sentence1)\n    tokens2 = word_tokenize(sentence2)\n    tokens1 = clean_tokens(tokens1, ignore_integers)\n    tokens2 = clean_tokens(tokens2, ignore_integers)\n\n    # tag\n    sentence1 = pos_tag(tokens1)\n    sentence2 = pos_tag(tokens2)\n\n    # Get the synsets for the tagged words\n    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n    print(synsets1)\n    print(synsets2)\n    # Filter out the Nones\n    synsets1 = [ss for ss in synsets1 if ss]\n    synsets2 = [ss for ss in synsets2 if ss]\n\n    score, count = 0.0, 0.0\n\n    # For each word in the first sentence\n    for synset1 in synsets1:\n        # Get the similarity value of the most similar word in the other sentence\n        best_score=[\n            wn.path_similarity(synset1, synset2)\n            if not isinstance(synset1, str) and not isinstance(synset2, str)\n            # just in case there are scientific words wordnet does not have\n            else fuzz.ratio(str(synset1), str(synset2)) / 100\n            for synset2 in synsets2\n        ]\n        best_score=[s if s else 0 for s in best_score]\n        # print(synsets1, synsets2)\n        # Check that the similarity could have been computed\n        if best_score:\n            score += max(best_score)\n            count += 1\n\n    # Average the values\n    if count > 0:\n        score /= count\n    else:\n        score = 0\n\n    return score\n\ndef get_tokenized_sentence(sentence):\n    # Tokenize and tag\n    sentence = pos_tag(word_tokenize(sentence))\n    # Get the synsets for the tagged words\n    synsets = []\n    for tagged_word in sentence:\n        synset = tagged_to_synset(*tagged_word)\n        if synset:\n            synsets.append(synset)\n        else:\n            synsets.append(tagged_word[0])\n    return synsets # str(sorted(synsets))\n\ndef main():\n    sentences = [\n        \"life is good in pa 92092\",\n        \"life is good in pa\",\n        \"life is good within pa 92092/2\",\n        \"life is good pa 92092/2\",\n        \"life is good in pa 92092/2\",\n        \"testing for difference\"\n    ]\n    focus_sentence = \"life is good in pennsylvania\"\n\n    for sentence in sentences:\n        # print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n        print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence, ignore_integers=True)))\n\n    # print(sentence_similarity(focus_sentence, sentences[2], ignore_integers=True))\n\nif __name__ == '__main__':\n    main()", "instruction_en": "Download necessary NLTK packages and fuzzywuzzy library. Define a dictionary of US state abbreviations and their full names. Define functions to convert Penn Treebank tags to Wordnet tags, convert tagged words to synsets, fix state abbreviations in tokens, clean tokens by removing stop words and punctuations, and compute sentence similarity using Wordnet. Define a main function that compares the similarity of a focus sentence to a list of sentences using the sentence_similarity function. The main function also has an option to ignore integers in the comparison.", "original_line_number": 18231, "instruction_length": 602}
{"instruction": "Kod ini membaca fail input dan melakukan pengiraan pada nilai input berdasarkan persamaan yang disediakan dalam fail. Untuk menggunakan kod, berikan laluan fail input dan laluan fail output sebagai argumen baris arahan. Fail input hendaklah mengandungi persamaan yang akan dinilai, dengan setiap persamaan pada baris berasingan. Persamaan boleh bentuk y = x + z, y = x * z, atau y = x. Fail input juga harus menentukan nilai awal pembolehubah dalam set x. Fail output akan mengandungi nilai akhir pembolehubah dalam set y.", "input": "", "output": "#!/usr/bin/python\nimport sys\nimport re\nimport string\n\nvariables = {}\nouts = {}\nmonomial = \"([a-zA-z]+\\d+)\"\nmn = re.compile(monomial)\n\ndef extractValues(strin):\n        xAddr1 = strin[2].strip()\n        xAddr2 = strin[4].strip()\n        if xAddr1 in variables:\n            x1 = variables[xAddr1]\n        else:\n            raise Exception(\"equationReader: variable \" + xAddr1 + \" not found\")\n        if mn.match(xAddr2):\n            if xAddr2 in variables:\n                x2 = variables[xAddr2]\n            else:\n                raise Exception(\"equationReader: variable \" + xAddr2 + \" not found\")\n        else:\n            x2 = bool(int(xAddr2))\n        return {'x1':x1, 'x2':x2}\n\nif len(sys.argv) != 3:\n    raise Exception(\"Usage: equationReader <input file> <output file>\")\n\nfin = open(sys.argv[1], \"r\")\n\n\nlines = fin.readlines()\ninputs = re.compile(\"\\d+ inputs\")\noutputs = re.compile(\"\\d+ outputs\")\nfor index, line in enumerate(lines):\n    if inputs.match(line):\n        #Creation of the x set\n        xLine = lines[index + 1]\n        xValueLine = lines[index + 2]\n        ins = string.split(xLine)\n        insValues = string.split(xValueLine)\n        if len(ins) != len(insValues):\n            print(line + xLine + xValueLine)\n            raise Exception(\"equationReader: you need to provide a starting value for each x inserted\")\n        for i in range(len(ins)):\n            x = ins[i].strip()\n            variables[x] = bool(int(insValues[i]))\n    else:\n        if outputs.match(line):\n            #Creation of the y set\n            yLine = lines[index + 1]\n            ins = string.split(yLine, \" \")\n            for y in ins:\n                y.strip()\n                outs[y] = None\n            else:\n                if line == \"begin\":\n                    #When the equations start we get to the next cicle which performs the calculations\n                    break\n\n#y = x + z\nequation_XOR = re.compile(monomial + \" = \" + monomial + \" \\+ (\" + monomial + \"|(0|1))\")\n#y = x * z\nequation_AND = re.compile(monomial + \" = \" + monomial + \" \\* (\" + monomial + \"|(0|1))\")\n#y = x\nequation_ASSIGNEMENT = re.compile(monomial + \" = (\" + monomial + \"|(0|1))\")\n\nfor index, line in enumerate(lines):\n    tmp = string.split(line, \" \")\n    print(line)\n    if equation_XOR.match(line):\n        xdict = extractValues(tmp)\n        yAddr = tmp[0]\n        y = xdict['x1'] ^ xdict['x2']\n        variables[yAddr] = y\n        if yAddr in outs:\n            outs[yAddr] = y\n    else:\n        if equation_AND.match(line):\n            xdict = extractValues(tmp)\n            yAddr = tmp[0]\n            y = xdict['x1'] & xdict['x2']\n            variables[yAddr] = y\n            if yAddr in outs:\n                outs[yAddr] = y\n        else:\n            if equation_ASSIGNEMENT.match(line):\n                yAddr = tmp[0].strip()\n                xAddr = tmp[2].strip()\n                if mn.match(xAddr):\n                    if xAddr in variables:\n                        x = variables[xAddr]\n                    else:\n                        raise Exception(\"equationReader: variable \" + xAddr + \" not found\")\n                else:\n                    x = bool(xAddr)\n                y = x\n                variables[yAddr] = y\n                if yAddr in outs:\n                    outs[yAddr] = y\n            else:\n                print(\"Skipping malformed equation:\" + line)\n\n#Printing out the results\nfin.close()\nfout = open(sys.argv[2], \"w\")\nfor key, value in outs.items():\n    fout.write(key + \" = {}\\n\".format(int(value)))\nfout.close()", "instruction_en": "The code reads an input file and performs calculations on the input values based on equations provided in the file. To use the code, provide the input file path and output file path as command line arguments. The input file should contain the equations to be evaluated, with each equation on a separate line. The equations can be of the form y = x + z, y = x * z, or y = x. The input file should also specify the initial values of the variables in the x set. The output file will contain the final values of the variables in the y set.", "original_line_number": 18350, "instruction_length": 522}
{"instruction": "Buat aplikasi web Django yang membolehkan pengguna membuat dan mengurus Mungerbuilders, yang merupakan objek yang menentukan cara mengubah data CSV. Permohonan harus mempunyai ciri -ciri berikut:\n\n1. Import Perpustakaan yang diperlukan: OS, RE, CSV, JSON, Time, Django.Shortcuts, Django.http, Django.Contrib, Django.conf, Guardian.Shortcuts, dan model dari direktori semasa.\n\n2. Tentukan fungsi pandangan yang dipanggil \"Munger_builder_index\" yang mengambil objek permintaan dan mengembalikan templat HTML yang diberikan. Templat harus memaparkan senarai mungerbuilders bahawa pengguna semasa mempunyai kebenaran untuk diubah suai. Sekiranya pengguna tidak mempunyai Mungerbuilders, fungsi itu harus membuat sampel Mungerbuilder dan mengembalikan senarai.\n\n3. Tentukan fungsi yang dipanggil \"get_user_or_create_anon\" yang mengambil objek permintaan dan mengembalikan pengguna semasa atau membuat pengguna tanpa nama jika tidak ada pengguna semasa. Fungsi ini harus memberikan kebenaran kepada pengguna tanpa nama dan menambahkannya kepada kumpulan \"sampel global\".\n\n4. Tentukan fungsi yang dipanggil \"add_sample_munger\" yang mengambil objek pengguna dan mencipta sampel Mungerbuilder untuk pengguna itu. Mungerbuilder harus mempunyai nama yang termasuk nama pengguna pengguna dan harus mempunyai beberapa datafields dengan pivotfields yang berbeza. Fungsi ini harus mengembalikan senarai mungerbuilders yang pengguna mempunyai kebenaran untuk diubah suai.\n\n5. Tentukan fungsi paparan yang dipanggil \"New_Munger_Builder\" yang mengambil objek permintaan dan mencipta Mungerbuilder baru untuk pengguna semasa. Fungsi ini harus diarahkan ke fungsi pandangan yang dipanggil \"pivot_builder\" dengan ID Mungerbuilder sebagai parameter.\n\n6. Tentukan fungsi pandangan yang dipanggil \"pivot_builder\" yang mengambil objek permintaan dan ID Mungerbuilder dan mengembalikan templat HTML yang diberikan. Templat harus memaparkan borang untuk membuat dan mengubahsuai Datafields dan Pivotfields untuk Mungerbuilder.\n\n7. Tentukan fungsi pandangan yang dipanggil \"muat turun_munger\" yang mengambil objek permintaan dan ID Mungerbuilder dan mengembalikan objek respons yang memuat turun fail CSV dari data yang diubah oleh Mungerbuilder.\n\n8. Tentukan fungsi paparan yang dipanggil \"muat turun_test_data\" yang mengambil objek permintaan dan ID Mungerbuilder dan mengembalikan objek respons yang memuat turun fail CSV data ujian untuk Mungerbuilder.\n\n9. Tentukan fungsi pandangan yang dipanggil \"Poll_For_Download\" yang mengambil objek permintaan dan mengembalikan objek tindak balas yang mengundi tugas tak segerak untuk muat turun fail. Fungsi ini harus memeriksa sama ada tugas itu siap dan mengembalikan nama fail fail yang dimuat turun.\n\n10. Tentukan beberapa fungsi penolong yang menghuraikan medan teks, periksa sama ada pengguna semasa tidak dikenali, dan memaparkan mesej amaran jika perlu.", "input": "", "output": "import os\nimport re\nimport csv\nimport json\nimport time\n\nfrom django.shortcuts import render, render_to_response\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.contrib import messages\nfrom django.contrib.auth.models import User, Group\nfrom django.contrib.auth import authenticate, login\nfrom django.conf import settings\n\nfrom guardian.shortcuts import assign_perm, get_objects_for_user\n\nfrom .models import DataField, FieldType, CSVDocument, MungerBuilder, PivotField\nfrom .tasks import download_munger_async, download_test_data_async\n\nINDEX_REDIRECT = HttpResponseRedirect('/script_builder/munger_builder_index')\n\ndef munger_builder_index(request):\n\n    user = get_user_or_create_anon(request)\n\n    anon_check(request)\n\n    munger_builder_list = get_objects_for_user(user, 'script_builder.change_mungerbuilder')\n    if len(munger_builder_list) == 0:\n        munger_builder_list = add_sample_munger(user)\n\n    context = {'munger_builder_list': munger_builder_list}\n    return render(request, 'script_builder/munger_builder_index.html', context)\n\ndef get_user_or_create_anon(request):\n    if not request.user.id:\n        timestamp = int(time.time())\n        credentials = {\n            'username': 'anon_{0}'.format(timestamp),\n            'password': timestamp,\n        }\n        user = User.objects.create_user(**credentials)\n        user.save()\n        assign_perm('script_builder.add_mungerbuilder', user)\n        assign_perm('script_builder.add_fieldtype', user)\n        assign_perm('script_builder.add_datafield', user)\n        assign_perm('script_builder.add_pivotfield', user)\n        group = Group.objects.get(name='Global Sample')\n        user.groups.add(group)\n        user.save()\n        anon_user = authenticate(**credentials)\n        login(request, anon_user)\n    else:\n        user = request.user\n    return user\n\ndef add_sample_munger(user):\n\n    mb = MungerBuilder.objects.create(\n        munger_name='Sample for {0}'.format(user.username),\n        input_path='test_data.csv',\n        is_sample=True,\n    )\n    mb.save()\n    mb.assign_perms(user)\n\n    sample_field_dict = {\n        'order_num': ['count'],\n        'product': None,\n        'sales_name': ['index'],\n        'region': ['column'],\n        'revenue': ['mean', 'sum'],\n        'shipping': ['median'],\n    }\n\n    for field_name, field_types in sample_field_dict.items():\n        data_field = DataField.objects.create(munger_builder=mb, current_name=field_name)\n        data_field.save()\n        data_field.assign_perms(user)\n        if field_types:\n            for type_name in field_types:\n                field_type = FieldType.objects.get(type_name=type_name)\n                PivotField.objects.create(data_field=data_field, field_type=field_type).save()\n\n    return get_objects_for_user(user, 'script_builder.change_mungerbuilder')\n\ndef new_munger_builder(request):\n    user = get_user_or_create_anon(request)\n    mb = MungerBuilder.objects.create(munger_name='New Munger - {0}'.format(user.username))\n    mb.save()\n    mb.assign_perms(user)\n    return HttpResponseRedirect('/script_builder/pivot_builder/{0}'.format(mb.id))\n\ndef pivot_builder(request, munger_builder_id):\n\n    anon_check(request)\n\n    mb = MungerBuilder.objects.get(pk=munger_builder_id)\n\n    if not mb.user_is_authorized():\n        return INDEX_REDIRECT\n\n    return render(request, 'script_builder/pivot_builder_react.html', context={'mb': mb})\n\ndef download_munger(request, munger_builder_id):\n    task = download_munger_async.delay(munger_builder_id)\n    return render_to_response('script_builder/poll_for_download.html',\n                              {'task_id': task.id, 'mb_id': munger_builder_id})\n\ndef download_test_data(request, munger_builder_id):\n    task = download_test_data_async.delay(munger_builder_id)\n    return render_to_response('script_builder/poll_for_download.html',\n                              {'task_id': task.id, 'mb_id': munger_builder_id})\n\ndef poll_for_download(request):\n\n    task_id = request.GET.get(\"task_id\")\n    filename = request.GET.get(\"filename\")\n\n    if filename == 'test_data.csv':\n        async_func = download_test_data_async\n        file_path = os.path.join(settings.STATIC_ROOT, filename)\n    else:\n        async_func = download_munger_async\n        file_path = os.path.join(settings.MEDIA_ROOT, 'user_munger_scripts', '{0}'.format(filename))\n\n    if request.is_ajax():\n        result = async_func.AsyncResult(task_id)\n        if result.ready():\n            return HttpResponse(json.dumps({\"filename\": result.get()}))\n        return HttpResponse(json.dumps({\"filename\": None}))\n\n    with open(file_path, 'r') as f:\n        response = HttpResponse(f, content_type='application/octet-stream')\n        response['Content-Disposition'] = 'attachment; filename={0}'.format(filename)\n        return response\n\n# Helper Functions\n\ndef parse_text_fields(form, request, input_type):\n    if input_type == 'text':\n        return re.split('[,\\t\\n]', form.cleaned_data['fields_paste'])\n\n    if input_type == 'csv':\n        new_csv = CSVDocument(csv_file=request.FILES['csv_file'])\n        new_csv.save()\n        reader = csv.DictReader(request.FILES['csv_file'])\n        return reader.fieldnames\n\ndef anon_check(request):\n    if 'anon_' in request.user.username:\n        anon_message = \"\"\"You are logged in as an anonymous user.\n                       You may not be able to transfer any mungers to a permanent account in the future.\n                       Register to save mungers.\"\"\"\n        messages.warning(request, anon_message)", "instruction_en": "Create a Django web application that allows users to create and manage MungerBuilders, which are objects that define how to transform CSV data. The application should have the following features:\n\n1. Import necessary libraries: os, re, csv, json, time, django.shortcuts, django.http, django.contrib, django.conf, guardian.shortcuts, and models from the current directory.\n\n2. Define a view function called \"munger_builder_index\" that takes a request object and returns a rendered HTML template. The template should display a list of MungerBuilders that the current user has permission to modify. If the user has no MungerBuilders, the function should create a sample MungerBuilder and return the list.\n\n3. Define a function called \"get_user_or_create_anon\" that takes a request object and returns the current user or creates an anonymous user if there is no current user. The function should assign permissions to the anonymous user and add them to the \"Global Sample\" group.\n\n4. Define a function called \"add_sample_munger\" that takes a user object and creates a sample MungerBuilder for that user. The MungerBuilder should have a name that includes the user's username and should have several DataFields with different PivotFields. The function should return a list of MungerBuilders that the user has permission to modify.\n\n5. Define a view function called \"new_munger_builder\" that takes a request object and creates a new MungerBuilder for the current user. The function should redirect to a view function called \"pivot_builder\" with the MungerBuilder's ID as a parameter.\n\n6. Define a view function called \"pivot_builder\" that takes a request object and a MungerBuilder ID and returns a rendered HTML template. The template should display a form for creating and modifying DataFields and PivotFields for the MungerBuilder.\n\n7. Define a view function called \"download_munger\" that takes a request object and a MungerBuilder ID and returns a response object that downloads a CSV file of the MungerBuilder's transformed data.\n\n8. Define a view function called \"download_test_data\" that takes a request object and a MungerBuilder ID and returns a response object that downloads a CSV file of test data for the MungerBuilder.\n\n9. Define a view function called \"poll_for_download\" that takes a request object and returns a response object that polls an asynchronous task for a file download. The function should check if the task is ready and return the filename of the downloaded file.\n\n10. Define several helper functions that parse text fields, check if the current user is anonymous, and display warning messages if necessary.", "original_line_number": 18613, "instruction_length": 2878}
{"instruction": "Kod yang diberikan adalah modul untuk komunikasi antara peranti. Untuk menggunakannya, ikuti arahan ini:\n\n1. Import modul yang diperlukan: `traceback`,` threading`, `multiprocessing`,` giliran`, `tidur`, dan` logger` dari `kivy.logger`.\n2. Tentukan `stay_alive_timeout` dan` command_close` dan `command_keep_alive` pemalar.\n3. Tentukan fungsi `connection_process_message_reader ()` yang membaca mesej dari sambungan dan meletakkannya dalam giliran terima.\n4. Tentukan fungsi `Connection_Process_Message_Writer ()` yang menulis mesej dari giliran penghantaran ke sambungan.\n5. Tentukan fungsi `connection_message_process ()` yang menetapkan sambungan, memulakan pembaca dan penulis benang, dan menunggu arahan untuk menutup sambungan.\n6. Tentukan kelas `comms ()` dengan kaedah untuk memulakan proses sambungan, dapatkan peranti yang tersedia, periksa sama ada sambungan dibuka, buka sambungan, simpan sambungan hidup, menutup sambungan, membaca mesej, dan menulis mesej.\n7. Untuk menggunakan kelas `comms ()`, buat contoh dengan peranti dan parameter sambungan, hubungi kaedah `terbuka ()` untuk memulakan proses sambungan, dan kemudian gunakan kaedah `read_message ()` dan `write_message ()` untuk berkomunikasi dengan peranti. Panggil kaedah `Keep_alive ()` secara berkala untuk memastikan sambungan hidup, dan panggil kaedah `Close ()` untuk menutup sambungan apabila selesai.", "input": "", "output": "import traceback\nimport threading\nimport multiprocessing\nfrom Queue import Empty\nfrom time import sleep\nfrom kivy.logger import Logger\nfrom autosportlabs.comms.commscommon import PortNotOpenException\n\nSTAY_ALIVE_TIMEOUT = 4\nCOMMAND_CLOSE = 'CLOSE'\nCOMMAND_KEEP_ALIVE = 'PING'\n\ndef connection_process_message_reader(rx_queue, connection, should_run):\n    Logger.debug('Comms: connection process message reader started')\n\n    while should_run.is_set():\n        try:\n            msg = connection.read_line()\n            if msg:\n                rx_queue.put(msg)\n        except:\n            Logger.error('Comms: Exception in connection_process_message_reader')\n            Logger.debug(traceback.format_exc())\n            should_run.clear()\n            sleep(0.5)\n    Logger.debug('Comms: connection process message reader exited')\n\ndef connection_process_message_writer(tx_queue, connection, should_run):\n    Logger.debug('Comms: connection process message writer started')\n    while should_run.is_set():\n        try:\n            message = tx_queue.get(True, 1.0)\n            if message:\n                connection.write(message)\n        except Empty:\n            pass\n        except Exception as e:\n            Logger.error('Comms: Exception in connection_process_message_writer ' + str(e))\n            Logger.debug(traceback.format_exc())\n            should_run.clear()\n            sleep(0.5)\n    Logger.debug('Comms: connection process message writer exited')\n\ndef connection_message_process(connection, device, rx_queue, tx_queue, command_queue):\n    Logger.debug('Comms: connection process starting')\n\n    try:\n        connection.open(device)\n        connection.flushInput()\n        connection.flushOutput()\n\n        reader_writer_should_run = threading.Event()\n        reader_writer_should_run.set()\n\n        reader_thread = threading.Thread(target=connection_process_message_reader, args=(rx_queue, connection, reader_writer_should_run))\n        reader_thread.start()\n\n        writer_thread = threading.Thread(target=connection_process_message_writer, args=(tx_queue, connection, reader_writer_should_run))\n        writer_thread.start()\n\n\n        while reader_writer_should_run.is_set():\n            try:\n                command = command_queue.get(True, STAY_ALIVE_TIMEOUT)\n                if command == COMMAND_CLOSE:\n                    Logger.debug('Comms: connection process: got close command')\n                    reader_writer_should_run.clear()\n            except Empty:\n                Logger.debug('Comms: keep alive timeout')\n                reader_writer_should_run.clear()\n        Logger.debug('Comms: connection worker exiting')\n\n        reader_thread.join()\n        writer_thread.join()\n\n        try:\n            connection.close()\n        except:\n            Logger.debug('Comms: Exception closing connection worker connection')\n            Logger.debug(traceback.format_exc())\n    except Exception as e:\n        Logger.debug('Comms: Exception setting up connection process: ' + str(type(e)) + str(e))\n        Logger.trace(traceback.format_exc())\n\n    Logger.debug('Comms: connection worker exited')\n\n\nclass Comms():\n    CONNECT_TIMEOUT = 1.0\n    DEFAULT_TIMEOUT = 1.0\n    QUEUE_FULL_TIMEOUT = 1.0\n    _timeout = DEFAULT_TIMEOUT\n    device = None\n    _connection = None\n    _connection_process = None\n    _rx_queue = None\n    _tx_queue = None\n    _command_queue = None\n\n    def __init__(self, device, connection):\n        self.device = device\n        self._connection = connection\n        self.supports_streaming = False\n\n    def start_connection_process(self):\n        rx_queue = multiprocessing.Queue()\n        tx_queue = multiprocessing.Queue(5)\n        command_queue = multiprocessing.Queue()\n        connection_process = multiprocessing.Process(target=connection_message_process, args=(self._connection, self.device, rx_queue, tx_queue, command_queue))\n        connection_process.start()\n        self._rx_queue = rx_queue\n        self._tx_queue = tx_queue\n        self._command_queue = command_queue\n        self._connection_process = connection_process\n\n    def get_available_devices(self):\n        return self._connection.get_available_devices()\n\n    def isOpen(self):\n        return self._connection_process != None and self._connection_process.is_alive()\n\n    def open(self):\n        connection = self._connection\n        Logger.debug('Comms: Opening connection ' + str(self.device))\n        self.start_connection_process()\n\n    def keep_alive(self):\n        try:\n            self._command_queue.put_nowait(COMMAND_KEEP_ALIVE)\n        except:\n            pass\n\n    def close(self):\n        Logger.debug('Comms: comms.close()')\n        if self.isOpen():\n            try:\n                Logger.debug('Comms: closing connection process')\n                self._command_queue.put_nowait(COMMAND_CLOSE)\n                self._connection_process.join(self._timeout * 2)\n                Logger.debug('Comms: connection process joined')\n            except:\n                Logger.error('Comms: Timeout joining connection process')\n\n    def read_message(self):\n        if not self.isOpen():\n            raise PortNotOpenException('Port Closed')\n        try:\n            return self._rx_queue.get(True, self._timeout)\n        except:  # returns Empty object if timeout is hit\n            return None\n\n    def write_message(self, message):\n        if not self.isOpen(): raise PortNotOpenException('Port Closed')\n        self._tx_queue.put(message, True, Comms.QUEUE_FULL_TIMEOUT)", "instruction_en": "The given code is a module for communication between devices. To use it, follow these instructions:\n\n1. Import the necessary modules: `traceback`, `threading`, `multiprocessing`, `Queue`, `sleep`, and `Logger` from `kivy.logger`.\n2. Define the `STAY_ALIVE_TIMEOUT` and `COMMAND_CLOSE` and `COMMAND_KEEP_ALIVE` constants.\n3. Define the `connection_process_message_reader()` function that reads messages from the connection and puts them in the receive queue.\n4. Define the `connection_process_message_writer()` function that writes messages from the transmit queue to the connection.\n5. Define the `connection_message_process()` function that sets up the connection, starts the reader and writer threads, and waits for commands to close the connection.\n6. Define the `Comms()` class with methods to start the connection process, get available devices, check if the connection is open, open the connection, keep the connection alive, close the connection, read messages, and write messages.\n7. To use the `Comms()` class, create an instance with the device and connection parameters, call the `open()` method to start the connection process, and then use the `read_message()` and `write_message()` methods to communicate with the device. Call the `keep_alive()` method periodically to keep the connection alive, and call the `close()` method to close the connection when finished.", "original_line_number": 19068, "instruction_length": 1379}
{"instruction": "Kod ini mengandungi fungsi untuk menghasilkan soalan dan jawapan gred untuk latihan teori muzik. Fungsi \"ijazah\" meminta tahap nota yang diberikan dalam skala yang diberikan, fungsi \"gred_degree\" menghasilkan kunci rawak dan meminta pengguna untuk mengenal pasti tahap nota dalam kekunci dan skala itu, fungsi \"triad\" meminta nota dalam triad yang dibina pada skala yang diberikan, Dan Kunci dan skala itu. Untuk menggunakan fungsi ini, hubungi fungsi yang sesuai dengan argumen yang diperlukan dan masukkan maklumat yang diminta apabila diminta.", "input": "", "output": "'''\nQuestions generation functions\n'''\n\nimport random\n\n\ndef degree(note, scale, degree):\n    '''\n    What is the <Number> of <Note> <Scale>?\n    '''\n    try:\n        answer = raw_input('What is the {} of {} {}: '.format(str(degree + 1), note, scale.capitalize()))\n        return answer, degree\n    except KeyboardInterrupt:\n        print '\\nQUITTER!'\n        raise SystemExit\n\n\ndef grade_degree(key, note, scale):\n    deg = random.randint(0, 6)\n    answer = key[deg]\n    correct = False\n    while not correct:\n        my_answer, my_degree = degree(note, scale, deg)\n        if my_answer == answer:\n            print 'You Done got it Right!'\n            correct = True\n        else:\n            continue\n\n\ndef triad(note, scale):\n    '''\n    What are the notes in a <NOTE> <Scale> triad?\n    '''\n    try:\n        answer = raw_input('What notes are in a {} {} triad: '.format(note, scale.capitalize()))\n        return answer\n    except KeyboardInterrupt:\n        print '\\nQUITTER!'\n        raise SystemExit\n\n\ndef grade_triad(key, note, scale):\n    correct = False\n    answer_triad = [key[0], key[2], key[4]]\n    my_triad = []\n    while not correct:\n        answer = triad(note, scale)\n        if ',' in answer:\n            my_triad = answer.split(', ')\n            print my_triad\n            if len(my_triad) != 3:\n                my_triad = answer.split(',')\n        else:\n            my_triad = answer.split(' ')\n        if len(my_triad) != 3:\n            print 'Answer with commas or spaces between notes'\n            raise SystemExit\n        validation = [i for i, x in zip(answer_triad, my_triad) if i == x]\n        if len(validation) == 3:\n            print 'You Done got it Right!  '\n            correct = True\n        else:\n            continue", "instruction_en": "This code contains functions to generate questions and grade answers for music theory exercises. The \"degree\" function asks for the degree of a given note in a given scale, the \"grade_degree\" function generates a random key and asks the user to identify the degree of a note in that key and scale, the \"triad\" function asks for the notes in a triad built on a given note in a given scale, and the \"grade_triad\" function generates a random key and asks the user to identify the notes in a triad built on a given note in that key and scale. To use these functions, call the appropriate function with the necessary arguments and input the requested information when prompted.", "original_line_number": 19487, "instruction_length": 546}
{"instruction": "Untuk menggunakan pakej Django-MPTT, ikuti langkah-langkah ini:\n\n1. Import os, install_schemes, dan persediaan dari distutils.command.install.\n2. Tentukan fungsi yang dipanggil Fullsplit yang memisahkan nama laluan ke dalam komponen dalam cara neutral platform.\n3. Ubah suai pemasangan_schemes untuk meletakkan data_files dalam lokasi pemasangan khusus platform.\n4. Menyusun senarai pakej yang tersedia dan fail data.\n5. Panggil fungsi persediaan dengan parameter yang sesuai, termasuk nama, keterangan, versi, pengarang, pengarang_email, URL, pakej, data_files, dan pengelas.", "input": "", "output": "\"\"\"\nBased entirely on Django's own ``setup.py``.\n\"\"\"\nimport os\nfrom distutils.command.install import INSTALL_SCHEMES\nfrom distutils.core import setup\n\ndef fullsplit(path, result=None):\n    \"\"\"\n    Split a pathname into components (the opposite of os.path.join) in a\n    platform-neutral way.\n    \"\"\"\n    if result is None:\n        result = []\n    head, tail = os.path.split(path)\n    if head == '':\n        return [tail] + result\n    if head == path:\n        return result\n    return fullsplit(head, [tail] + result)\n\n# Tell distutils to put the data_files in platform-specific installation\n# locations. See here for an explanation:\n# http://groups.google.com/group/comp.lang.python/browse_thread/thread/35ec7b2fed36eaec/2105ee4d9e8042cb\nfor scheme in INSTALL_SCHEMES.values():\n    scheme['data'] = scheme['purelib']\n\n# Compile the list of packages available, because distutils doesn't have\n# an easy way to do this.\npackages, data_files = [], []\nroot_dir = os.path.dirname(__file__)\nmptt_dir = os.path.join(root_dir, 'mptt')\npieces = fullsplit(root_dir)\nif pieces[-1] == '':\n    len_root_dir = len(pieces) - 1\nelse:\n    len_root_dir = len(pieces)\n\nfor dirpath, dirnames, filenames in os.walk(mptt_dir):\n    # Ignore dirnames that start with '.'\n    for i, dirname in enumerate(dirnames):\n        if dirname.startswith('.'): del dirnames[i]\n    if '__init__.py' in filenames:\n        packages.append('.'.join(fullsplit(dirpath)[len_root_dir:]))\n    elif filenames:\n        data_files.append([dirpath, [os.path.join(dirpath, f) for f in filenames]])\n\nsetup(\n    name = 'django-mptt',\n    description = 'Utilities for implementing Modified Preorder Tree Traversal with your Django Models and working with trees of Model instances',\n    version = '0.3_pre',\n    author = 'Jonathan Buchanan',\n    author_email = 'jonathan.buchanan@gmail.com',\n    url = 'http://code.google.com/p/django-mptt/',\n    packages = packages,\n    data_files = data_files,\n    classifiers = ['Development Status :: 4 - Beta',\n                   'Environment :: Web Environment',\n                   'Framework :: Django',\n                   'Intended Audience :: Developers',\n                   'License :: OSI Approved :: BSD License',\n                   'Operating System :: OS Independent',\n                   'Programming Language :: Python',\n                   'Topic :: Utilities'],\n)", "instruction_en": "To use the Django-mptt package, follow these steps:\n\n1. Import os, INSTALL_SCHEMES, and setup from distutils.command.install.\n2. Define a function called fullsplit that splits a pathname into components in a platform-neutral way.\n3. Modify the INSTALL_SCHEMES to put the data_files in platform-specific installation locations.\n4. Compile the list of packages available and the data files.\n5. Call the setup function with the appropriate parameters, including the name, description, version, author, author_email, url, packages, data_files, and classifiers.", "original_line_number": 19866, "instruction_length": 576}
{"instruction": "Buat dua labah-labah bernama \"CW-All-Candidates\" dan \"CW-Trends\" menggunakan scrapy. Labah-labah \"CW-All-Candidates\" harus membuat permintaan untuk mengikis hasil pilihan raya dari sama ada URL \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/constituencywises03%S.Htm? \"http://eciresults.nic.in/constituencywises05%S.htm?ac=%s\" untuk setiap 40 kawasan pilihan raya, bergantung kepada nilai pemboleh ubah \"endpoint\". Labah -labah itu harus menghuraikan tindak balas untuk mengekstrak negara, kawasan, calon, parti, undi, dan status dan menghasilkan hasilnya. Labah-labah \"CW-Trends\" harus membuat permintaan untuk mengikis trend pilihan raya dari sama ada url \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/statewises03%s.htm \"http://eciresults.nic.in/statewises05%s.htm\" untuk setiap 3 negeri, bergantung kepada nilai pembolehubah \"endpoint\". Labah -labah itu kemudiannya menghuraikan tindak balas untuk mengekstrak kawasan, nombor kawasan, calon terkemuka, parti terkemuka, calon trailing, parti trailing, margin, dan status dan menghasilkan hasilnya.", "input": "", "output": "import scrapy\nfrom scrapy import Request\n\nclass CWACResultsSpider(scrapy.Spider):\n    name = \"cw-all-candidates\"\n    def start_requests(self):\n        for i in range(40):\n            if self.endpoint == 'archive':            \n                yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/ConstituencywiseS03%s.htm?ac=%s' % (i+1,i+1), callback=self.parse)\n            else:\n                yield Request('http://eciresults.nic.in/ConstituencywiseS05%s.htm?ac=%s' % (i+1,i+1), callback=self.parse)              \n    def parse(self, response):\n\tresults = response.css('#div1 > table > tr')\n        for result in results[3:len(results)-1]:\n            yield {\n                'state': results[0].css('td::text').extract_first().split(' - ')[0],\n\t\t'constituency': results[0].css('td::text').extract_first().split(' - ')[1],\n\t\t'candidate': result.css('td::text')[0].extract(),\n                'party': result.css('td::text')[1].extract(),\n                'votes': result.css('td::text')[2].extract(),\n\t\t'status': results[1].css('td::text').extract_first(),\n            }\n\nclass CWTrendsSpider(scrapy.Spider):\n    name = \"cw-trends\"\n    def start_requests(self):\n        if self.endpoint == 'archive':\n            yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03.htm', callback=self.parse)\n        else:\n            yield Request('http://eciresults.nic.in/StatewiseS05.htm', callback=self.parse)\n        for i in range(3):\n            if self.endpoint == 'archive':\n                yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03%s.htm' % (i+1), callback=self.parse)\n            else:  \n                yield Request('http://eciresults.nic.in/StatewiseS05%s.htm' % (i+1), callback=self.parse)\n    \n    def parse(self, response):\n        results = response.css('#divACList > table > tr')\n        for result in results[4:len(results)-1]:\n            yield {\n                'constituency': result.css('td::text')[0].extract(),\n                'const. no.': result.css('td::text')[1].extract(),\n                'leading candidate': result.css('td::text')[2].extract(),\n                'leading party': result.css('td::text')[3].extract(),\n\t\t'trailing candidate': result.css('td::text')[4].extract(),\n\t\t'trailing party': result.css('td::text')[5].extract(),\n\t\t'margin': result.css('td::text')[6].extract(),\n\t\t'status': result.css('td::text')[7].extract()\n            }", "instruction_en": "Create two spiders named \"cw-all-candidates\" and \"cw-trends\" using Scrapy. The \"cw-all-candidates\" spider should make requests to scrape election results from either the URL \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/ConstituencywiseS03%s.htm?ac=%s\" or \"http://eciresults.nic.in/ConstituencywiseS05%s.htm?ac=%s\" for each of the 40 constituencies, depending on the value of the \"endpoint\" variable. The spider should then parse the response to extract the state, constituency, candidate, party, votes, and status and yield the results. The \"cw-trends\" spider should make requests to scrape election trends from either the URL \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03%s.htm\" or \"http://eciresults.nic.in/StatewiseS05%s.htm\" for each of the 3 states, depending on the value of the \"endpoint\" variable. The spider should then parse the response to extract the constituency, constituency number, leading candidate, leading party, trailing candidate, trailing party, margin, and status and yield the results.", "original_line_number": 20129, "instruction_length": 1095}
{"instruction": "Kod ini adalah skrip untuk menyaring isoforms dari fail peptida dalam format FASTA ENSEMBL atau NCBI. Ia juga boleh menambah lebih mudah untuk membaca label spesies untuk setiap urutan dalam fail. Untuk menggunakan skrip ini, anda perlu menentukan fail input, jenis fail input, dan fail output. Untuk fail ensembl, gunakan arahan berikut: python isoform_filter.py -i [input_fasta_file] -t ens -l [species_label] -o [output_filename]. Untuk fail NCBI, gunakan arahan berikut: python isoform_filter.py -i [input_fasta_file] -t ncbi -g [toplevel_gff_file] -l [species_label] -o [output_filename]. Untuk hanya melakukan relabel, set -f 0. Anda tidak perlu fail GFF untuk fail NCBI dalam kes ini. Untuk NCBI Relabeling, ID Gen juga dipindahkan ke hadapan baris tajuk.", "input": "", "output": "#!/usr/bin/python\n########################################################################################\n#Script to filter out isoforms from peptide files in FASTA ENSEMBL or NCBI format. This\n#script can also add an easier to read species label to each sequence within the file.\n#\n#Sample ENSEMBL usage: python isoform_filter.py -i [input_fasta_file] -t ens -l [species_label] -o [output_filename]\n#\n#Sample NCBI usage: python isoform_filter.py -i [input_fasta_file] -t ncbi -g [toplevel_gff_file] -l [species_label] -o [output_filename]\n#\n#To just do the relabeling, set -f 0. You shouldn't need a gff file for the NCBI file in\n#this case. For NCBI relabeling, the gene ID is also moved to the front of the title line.\n#\n#Written by: Gregg Thomas, Summer 2014\n#\n#NCBI filter command kindly provided by the folks at NCBI.\n#\n########################################################################################\n\nimport sys, re, os, argparse\nsys.path.append(os.path.dirname(os.path.realpath(__file__)) + \"/corelib/\")\nimport core\n\n############################################\n#Function definitions.\n\ndef optParse():\n#This function handles the command line options.\n\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\"-i\", dest=\"input_file\", help=\"An input file containing peptides from a species in FASTA format\");\n\tparser.add_argument(\"-t\", dest=\"file_type\", help=\"Currently supported file types are ENSEMBL and NCBI peptide files. Enter as 'ens' or 'ncbi' here. Note: If file type is NCBI you will also need to specify the top level gff file with -g\")\n\tparser.add_argument(\"-g\", dest=\"gff_file\", help=\"If file type is NCBI, the top level gff file is also needed and should be specified here.\");\n\tparser.add_argument(\"-l\", dest=\"spec_label\", help=\"A species label to add to the gene ID of each sequence.\", default=\"\");\n\tparser.add_argument(\"-o\", dest=\"output_file\", help=\"The desired name of the output file. If none is specified the default is [input_filename]_isofiltered.fa or [input_filename]_isofiltered_relabel.fa\");\n\n\targs = parser.parse_args();\n\n\tif None in [args.input_file, args.file_type, args.output_file]:\n\t\tsys.exit(core.errorOut(1, \"An input file (-i), input file type (-t), and an output file (-o) must all be specified\"));\n\n\tif args.file_type not in ['ens', 'ncbi']:\n\t\tsys.exit(core.errorOut(2, \"File type (-t) must be one of either 'ens' (Ensembl) or 'ncbi'\"));\n\n\tif args.file_type == \"ens\" and args.gff_file != None:\n\t\tsys.exit(core.errorOut(3, \"A gff file (-g) should not be specified with file type ens\"));\n\n\tif args.file_type == \"ncbi\" and args.gff_file == None:\n\t\tsys.exit(core.errorOut(4, \"A gff file (-g) must be specified with file type ncbi\"));\n\n\treturn args.input_file, args.file_type, args.gff_file, args.spec_label, args.output_file;\n\n############################################\ndef ensFilter(inseqs, spec_label, outfilename):\n\n\tprint \"Indexing\", len(inseqs), \"sequences to be filtered.\";\n\tprint \"Parsing identifiers...\";\n\n\tfor title in inseqs:\n\t\tgeneid = title[title.index(\"gene:\") + 5:title.index(\"gene:\") + 23];\n\n\t\tif geneid in identDict:\n\t\t\tidentDict[geneid].append((title, inseqs[title]));\n\t\telse:\n\t\t\tidentDict[geneid] = [];\n\t\t\tidentDict[geneid].append((title, inseqs[title]));\n\tsys.stderr.write('\\b');\n\n\tprint \"Filtering and writing sequences...\";\n\tnumbars, donepercent, i = 0,[],0;\n\n\tfor key in identDict:\n\t\tnumbars, donepercent = core.loadingBar(i, len(identDict), donepercent, numbars);\n\n\t\tif len(identDict[key]) == 1:\n\t\t\tlong_title, long_seq = identDict[key][0];\n\n\t\telse:\n\t\t\ttitlelist = [];\n\t\t\tseqlist = [];\n\n\t\t\tfor tup in identDict[key]:\n\t\t\t\tcur_itle, cur_seq = tup;\n\t\t\t\ttitlelist.append(cur_itle);\n\t\t\t\tseqlist.append(cur_seq);\n\n\t\t\tlong_seq = max(seqlist, key=len)\n\t\t\tlong_title = titlelist[seqlist.index(long_seq)];\n\n\t\tnew_title = \">\" + spec_label + \"_\" + long_title[1:];\n\t\tcore.writeSeq(outfilename, long_seq, new_title);\n\t\ti += 1;\n\n\tpstring = \"100.0% complete.\";\n\tsys.stderr.write('\\b' * len(pstring) + pstring);\n\tprint \"\\nDone!\";\n\tprint i, \"sequences written.\";\n\tprint len(inseqs) - i, \"sequences filtered.\";\n\n############################################\ndef ncbiFilter(inseqs, gff_file, spec_label, outfilename):\n\n\tnumbars, donepercent, i = 0, [], 0;\n\n\n\tprint \"Obtaining longest isoforms from .gff file...\";\n\n\tcmd = \"zcat \" + gff_file + \" | awk \\'BEGIN{FS=\\\"\t\\\";OFS=\\\"|\\\"}$3==\\\"CDS\\\"{if($4<$5){print $5-$4+1,$9}else{print $4-$5+1,$9}}\\' | grep \\\"[NX]P[_]\\\" | sed \\'s/\\([0-9]*\\).*GeneID:\\([0-9]*\\).*\\([NX]P[_][0-9]*\\.[0-9]*\\).*/\\\\1|\\\\2|\\\\3/\\' | awk \\'BEGIN{FS=\\\"|\\\";OFS=\\\"\\t\\\";gene=\\\"\\\";acc=\\\"\\\";len=0}{if(acc!=$3){print gene,acc,len/3-1;gene=$2;acc=$3;len=$1}else{len=len+$1}}END{print gene,acc,len/3-1}\\' | sort -k1,1n -k3,3nr -k2,2 | awk \\'BEGIN{FS=\\\"\t\\\";OFS=\\\"\t\\\";gene=\\\"\\\";acc=\\\"\\\";len=0}{if(gene!=$1){print $1,$2,$3};gene=$1;acc=$2;len=$3}\\' > ncbi_isoform_filter_tmp11567.txt\"\n\tos.system(cmd);\n\n\ttmpFile = open(\"ncbi_isoform_filter_tmp11567.txt\", \"r\");\n\ttmpLines = tmpFile.readlines();\n\ttmpFile.close();\n\tos.system(\"rm ncbi_isoform_filter_tmp11567.txt\");\n\n\tlongest_isos = [];\n\n\tfor each in tmpLines:\n\t\tlongest_isos.append(each.split(\"\\t\")[1]);\n\tlongest_isos = filter(None, longest_isos);\n\n\tprint \"Writing longest isoforms to output file...\";\n\n\tcount = 0;\n\n\tfor title in inseqs:\n\t\tnumbars, donepercent = core.loadingBar(i, len(inseqs), donepercent, numbars);\n\t\ti += 1;\n\n\t\tfound = 0;\n\n\t\tfor gid in longest_isos:\n\t\t\tif gid in title:\n\t\t\t\tgid = title[title.index(\"P_\")-1:title.index(\"|\",title.index(\"P_\"))]\n\t\t\t\tnew_title = \">\" + spec_label + \"_\" + gid + \" |\" + title[1:title.index(\"P_\")-1] + title[title.index(\"|\",title.index(\"P_\"))+1:];\n\t\t\t\tcore.writeSeq(outfilename, inseqs[title], new_title);\n\t\t\t\tcount += 1;\n\t\t\t\tbreak;\n\n\tpstring = \"100.0% complete.\";\n\tsys.stderr.write('\\b' * len(pstring) + pstring);\n\tprint \"\\nDone!\";\n\tprint count, \"sequences written.\";\n\tprint len(inseqs) - count, \"sequences filtered.\";\n\n############################################\n#Main Block\n############################################\n\ninfilename, in_type, gff_file, label, outfilename = optParse();\n\npad = 50;\nprint \"=======================================================================\";\nprint \"\\t\\t\\t\" + core.getDateTime();\nprint core.spacedOut(\"Filtering isoforms from:\", pad), infilename;\nif in_type == \"ens\":\n\tprint core.spacedOut(\"File type:\", pad), \"Ensembl\";\nif in_type == \"ncbi\":\n\tprint core.spacedOut(\"File type:\", pad), \"NCBI\";\n\tprint core.spacedOut(\"Using GFF file:\", pad), gff_file;\nif in_type == \"crow\":\n\tprint core.spacedOut(\"File type:\", pad), \"Crow\";\nif label != \"\":\n\tprint core.spacedOut(\"Adding label to beginning of FASTA headers:\", pad), label;\nprint core.spacedOut(\"Writing output to:\", pad), outfilename;\ncore.filePrep(outfilename);\nprint \"--------------------------\";\n\nidentDict = {};\nins, skip_flag = core.fastaReader(infilename);\n\nif in_type == \"ens\":\n\tensFilter(ins, label, outfilename);\nelif in_type == \"ncbi\":\n\tncbiFilter(ins, gff_file, label, outfilename);\n\nprint \"=======================================================================\";\n\n\n\n\n\n\n\n\n\n## DEFUNCT FILTER FOR THE CROW FILES\n# elif in_type == \"crow\":\n# \tcrowFilter(ins, label, outfilename);\n\n# def crowFilter(inSeqs, filterflag, speclabel, outFilename):\n# \trotator = 0;\n# \tnumbars = 0;\n# \tdonepercent = [];\n# \ti = 0;\n\n# \tif filterflag == 1:\n# \t\tprint \"Indexing\", len(inSeqs), \"sequences to be filtered.\";\n# \t\tprint \"Parsing identifiers...\";\n\n# \t\tfor each in inSeqs:\n\n# \t\t\trotator = core.loadingRotator(i, rotator, 100)\n\n# \t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, each[0], each[1], each[2], each[3]);\n\n# \t\t\tif \"gene=\" not in curTitle:\n# \t\t\t\tprint curTitle;\n# \t\t\t\tcontinue;\n\n# \t\t\tgeneid = curTitle[curTitle.index(\"gene=\") + 5:].strip();\n\n# \t\t\tif geneid in identDict:\n# \t\t\t\tidentDict[geneid].append(each);\n\n# \t\t\telse:\n# \t\t\t\tidentDict[geneid] = [];\n# \t\t\t\tidentDict[geneid].append(each);\n\n# \t\t\ti = i + 1;\n\n# \t\tsys.stderr.write('\\b');\n\n# \t\tprint \"Filtering and writing sequences...\";\n\n# \t\ti = 0;\n# \t\tcount = 0;\n\n# \t\tfor key in identDict:\n\n# \t\t\tnumbars, donepercent = core.loadingBar(i, len(identDict), donepercent, numbars);\n\n# \t\t\tif len(identDict[key]) == 1:\n# \t\t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, identDict[key][0][0], identDict[key][0][1], identDict[key][0][2], identDict[key][0][3]);\n\n# \t\t\t\tif speclabel != \"\":\n# \t\t\t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n# \t\t\t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n# \t\t\t\telse:\n# \t\t\t\t\tcore.writeSeq(outFilename, curSeq, curTitle);\n\n# \t\t\t\tcount = count + 1;\n\n# \t\t\telse:\n# \t\t\t\ttitlelist = [];\n# \t\t\t\tseqlist = [];\n\n# \t\t\t\tfor inds in identDict[key]:\n# \t\t\t\t\taTitle, aSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\ttitlelist.append(aTitle);\n# \t\t\t\t\tseqlist.append(aSeq);\n\n# \t\t\t\tlongseq = max(seqlist, key=len)\n\n# \t\t\t\tfor inds in identDict[key]:\n# \t\t\t\t\taTitle, aSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\tif aSeq == longseq:\n# \t\t\t\t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\t\tif speclabel != \"\":\n# \t\t\t\t\t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n# \t\t\t\t\t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n# \t\t\t\t\t\telse:\n# \t\t\t\t\t\t\tcore.writeSeq(outFilename, curSeq, curTitle);\n\n# \t\t\t\t\t\tcount = count + 1;\n# \t\t\t\t\t\tbreak;\n\n# \t\t\ti = i + 1;\n\n# \t\tpstring = \"100.0% complete.\";\n# \t\tsys.stderr.write('\\b' * len(pstring) + pstring);\n# \t\tprint \"\\nDone!\";\n# \t\tprint count, \"out of\", len(identDict), \"identifiers written.\";\n# \t\tprint len(inSeqs) - count, \"sequences filtered.\";\n\n# \telse:\n# \t\tprint \"Relabeling...\";\n# \t\tfor seq in inSeqs:\n\t\n# \t\t\tnumbars, donepercent = core.loadingBar(i, len(inSeqs), donepercent, numbars);\n# \t\t\ti = i + 1;\n\n# \t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, seq[0], seq[1], seq[2], seq[3]);\n\n# \t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n\n# \t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n\n\n# \t\tpstring = \"100.0% complete.\";\n# \t\tsys.stderr.write('\\b' * len(pstring) + pstring);\n# \t\tprint \"\\nDone!\";", "instruction_en": "This code is a script to filter out isoforms from peptide files in FASTA ENSEMBL or NCBI format. It can also add an easier to read species label to each sequence within the file. To use this script, you need to specify the input file, input file type, and output file. For ENSEMBL files, use the following command: python isoform_filter.py -i [input_fasta_file] -t ens -l [species_label] -o [output_filename]. For NCBI files, use the following command: python isoform_filter.py -i [input_fasta_file] -t ncbi -g [toplevel_gff_file] -l [species_label] -o [output_filename]. To just do the relabeling, set -f 0. You shouldn't need a gff file for the NCBI file in this case. For NCBI relabeling, the gene ID is also moved to the front of the title line.", "original_line_number": 20290, "instruction_length": 762}
{"instruction": "Kod ini mentakrifkan fungsi python yang dipanggil `dd_hyperbolic_single` yang menjalankan analisis Bayesian mengenai tugas diskaun kelewatan menggunakan model hiperbolik. Fungsi ini mengambil data yang ditetapkan dalam bentuk data Pandas, laluan fail, atau \"contoh\" rentetan, dan mengembalikan contoh `hbayesdm.taskmodel` dengan pelbagai komponen, termasuk model yang dipasang dan data mentah. Fungsi ini mempunyai beberapa hujah pilihan yang membolehkan penyesuaian analisis, seperti bilangan lelaran, bilangan rantai, dan nilai awal. Untuk menggunakan fungsi ini, hubungi `dd_hyperbolic_single ()` dengan argumen yang dikehendaki.", "input": "", "output": "from typing import Sequence, Union, Any\nfrom collections import OrderedDict\n\nfrom numpy import Inf, exp\nimport pandas as pd\n\nfrom hbayesdm.base import TaskModel\nfrom hbayesdm.preprocess_funcs import dd_single_preprocess_func\n\n__all__ = ['dd_hyperbolic_single']\n\n\nclass DdHyperbolicSingle(TaskModel):\n    def __init__(self, **kwargs):\n        super().__init__(\n            task_name='dd',\n            model_name='hyperbolic',\n            model_type='single',\n            data_columns=(\n                'subjID',\n                'delay_later',\n                'amount_later',\n                'delay_sooner',\n                'amount_sooner',\n                'choice',\n            ),\n            parameters=OrderedDict([\n                ('k', (0, 0.1, 1)),\n                ('beta', (0, 1, 5)),\n            ]),\n            regressors=OrderedDict([\n                \n            ]),\n            postpreds=['y_pred'],\n            parameters_desc=OrderedDict([\n                ('k', 'discounting rate'),\n                ('beta', 'inverse temperature'),\n            ]),\n            additional_args_desc=OrderedDict([\n                \n            ]),\n            **kwargs,\n        )\n\n    _preprocess_func = dd_single_preprocess_func\n\n\ndef dd_hyperbolic_single(\n        data: Union[pd.DataFrame, str, None] = None,\n        niter: int = 4000,\n        nwarmup: int = 1000,\n        nchain: int = 4,\n        ncore: int = 1,\n        nthin: int = 1,\n        inits: Union[str, Sequence[float]] = 'vb',\n        ind_pars: str = 'mean',\n        model_regressor: bool = False,\n        vb: bool = False,\n        inc_postpred: bool = False,\n        adapt_delta: float = 0.95,\n        stepsize: float = 1,\n        max_treedepth: int = 10,\n        **additional_args: Any) -> TaskModel:\n    \"\"\"Delay Discounting Task - Hyperbolic Model\n\n    Individual Bayesian Modeling of the Delay Discounting Task \n    using Hyperbolic Model [Mazur1987]_ with the following parameters:\n    \"k\" (discounting rate), \"beta\" (inverse temperature).\n\n    \n\n    \n    .. [Mazur1987] Mazur, J. E. (1987). An adjustment procedure for studying delayed reinforcement.\n\n    \n\n    User data should contain the behavioral data-set of all subjects of interest for\n    the current analysis. When loading from a file, the datafile should be a\n    **tab-delimited** text file, whose rows represent trial-by-trial observations\n    and columns represent variables.\n\n    For the Delay Discounting Task, there should be 6 columns of data\n    with the labels \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\". It is not necessary for the columns to be\n    in this particular order; however, it is necessary that they be labeled\n    correctly and contain the information below:\n\n    - \"subjID\": A unique identifier for each subject in the data-set.\n    - \"delay_later\": An integer representing the delayed days for the later option (e.g. 1, 6, 28).\n    - \"amount_later\": A floating point number representing the amount for the later option (e.g. 10.5, 13.4, 30.9).\n    - \"delay_sooner\": An integer representing the delayed days for the sooner option (e.g. 0).\n    - \"amount_sooner\": A floating point number representing the amount for the sooner option (e.g. 10).\n    - \"choice\": If amount_later was selected, choice == 1; else if amount_sooner was selected, choice == 0.\n\n    .. note::\n        User data may contain other columns of data (e.g. ``ReactionTime``,\n        ``trial_number``, etc.), but only the data within the column names listed\n        above will be used during the modeling. As long as the necessary columns\n        mentioned above are present and labeled correctly, there is no need to\n        remove other miscellaneous data columns.\n\n    .. note::\n\n        ``adapt_delta``, ``stepsize``, and ``max_treedepth`` are advanced options that\n        give the user more control over Stan's MCMC sampler. It is recommended that\n        only advanced users change the default values, as alterations can profoundly\n        change the sampler's behavior. See [Hoffman2014]_ for more information on the\n        sampler control parameters. One can also refer to 'Section 34.2. HMC Algorithm\n        Parameters' of the `Stan User's Guide and Reference Manual`__.\n\n        .. [Hoffman2014]\n            Hoffman, M. D., & Gelman, A. (2014).\n            The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\n            Journal of Machine Learning Research, 15(1), 1593-1623.\n\n        __ https://mc-stan.org/users/documentation/\n\n    Parameters\n    ----------\n    data\n        Data to be modeled. It should be given as a Pandas DataFrame object,\n        a filepath for a data file, or ``\"example\"`` for example data.\n        Data columns should be labeled as: \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\".\n    niter\n        Number of iterations, including warm-up. Defaults to 4000.\n    nwarmup\n        Number of iterations used for warm-up only. Defaults to 1000.\n\n        ``nwarmup`` is a numerical value that specifies how many MCMC samples\n        should not be stored upon the beginning of each chain. For those\n        familiar with Bayesian methods, this is equivalent to burn-in samples.\n        Due to the nature of the MCMC algorithm, initial values (i.e., where the\n        sampling chains begin) can have a heavy influence on the generated\n        posterior distributions. The ``nwarmup`` argument can be set to a\n        higher number in order to curb the effects that initial values have on\n        the resulting posteriors.\n    nchain\n        Number of Markov chains to run. Defaults to 4.\n\n        ``nchain`` is a numerical value that specifies how many chains (i.e.,\n        independent sampling sequences) should be used to draw samples from\n        the posterior distribution. Since the posteriors are generated from a\n        sampling process, it is good practice to run multiple chains to ensure\n        that a reasonably representative posterior is attained. When the\n        sampling is complete, it is possible to check the multiple chains for\n        convergence by running the following line of code:\n\n        .. code:: python\n\n            output.plot(type='trace')\n    ncore\n        Number of CPUs to be used for running. Defaults to 1.\n    nthin\n        Every ``nthin``-th sample will be used to generate the posterior\n        distribution. Defaults to 1. A higher number can be used when\n        auto-correlation within the MCMC sampling is high.\n\n        ``nthin`` is a numerical value that specifies the \"skipping\" behavior\n        of the MCMC sampler. That is, only every ``nthin``-th sample is used to\n        generate posterior distributions. By default, ``nthin`` is equal to 1,\n        meaning that every sample is used to generate the posterior.\n    inits\n        String or list specifying how the initial values should be generated.\n        Options are ``'fixed'`` or ``'random'``, or your own initial values.\n    ind_pars\n        String specifying how to summarize the individual parameters.\n        Current options are: ``'mean'``, ``'median'``, or ``'mode'``.\n    model_regressor\n        Whether to export model-based regressors. Currently not available for this model.\n    vb\n        Whether to use variational inference to approximately draw from a\n        posterior distribution. Defaults to ``False``.\n    inc_postpred\n        Include trial-level posterior predictive simulations in\n        model output (may greatly increase file size). Defaults to ``False``.\n    adapt_delta\n        Floating point value representing the target acceptance probability of a new\n        sample in the MCMC chain. Must be between 0 and 1. See note below.\n    stepsize\n        Integer value specifying the size of each leapfrog step that the MCMC sampler\n        can take on each new iteration. See note below.\n    max_treedepth\n        Integer value specifying how many leapfrog steps the MCMC sampler can take\n        on each new iteration. See note below.\n    **additional_args\n        Not used for this model.\n\n    Returns\n    -------\n    model_data\n        An ``hbayesdm.TaskModel`` instance with the following components:\n\n        - ``model``: String value that is the name of the model ('dd_hyperbolic_single').\n        - ``all_ind_pars``: Pandas DataFrame containing the summarized parameter values\n          (as specified by ``ind_pars``) for each subject.\n        - ``par_vals``: OrderedDict holding the posterior samples over different parameters.\n        - ``fit``: A PyStan StanFit object that contains the fitted Stan model.\n        - ``raw_data``: Pandas DataFrame containing the raw data used to fit the model,\n          as specified by the user.\n        \n\n    Examples\n    --------\n\n    .. code:: python\n\n        from hbayesdm import rhat, print_fit\n        from hbayesdm.models import dd_hyperbolic_single\n\n        # Run the model and store results in \"output\"\n        output = dd_hyperbolic_single(data='example', niter=2000, nwarmup=1000, nchain=4, ncore=4)\n\n        # Visually check convergence of the sampling chains (should look like \"hairy caterpillars\")\n        output.plot(type='trace')\n\n        # Plot posterior distributions of the hyper-parameters (distributions should be unimodal)\n        output.plot()\n\n        # Check Rhat values (all Rhat values should be less than or equal to 1.1)\n        rhat(output, less=1.1)\n\n        # Show the LOOIC and WAIC model fit estimates\n        print_fit(output)\n    \"\"\"\n    return DdHyperbolicSingle(\n        data=data,\n        niter=niter,\n        nwarmup=nwarmup,\n        nchain=nchain,\n        ncore=ncore,\n        nthin=nthin,\n        inits=inits,\n        ind_pars=ind_pars,\n        model_regressor=model_regressor,\n        vb=vb,\n        inc_postpred=inc_postpred,\n        adapt_delta=adapt_delta,\n        stepsize=stepsize,\n        max_treedepth=max_treedepth,\n        **additional_args)", "instruction_en": "This code defines a Python function called `dd_hyperbolic_single` that runs a Bayesian analysis of the Delay Discounting Task using the Hyperbolic Model. The function takes in a data set in the form of a Pandas DataFrame, a file path, or the string \"example\", and returns an `hbayesdm.TaskModel` instance with various components, including the fitted model and raw data. The function has several optional arguments that allow for customization of the analysis, such as the number of iterations, number of chains, and initial values. To use the function, call `dd_hyperbolic_single()` with the desired arguments.", "original_line_number": 20340, "instruction_length": 632}
{"instruction": "Ini adalah kod python untuk membuat borang pendaftaran pengguna dengan medan tambahan. Untuk menggunakan kod ini, import modul yang diperlukan dan buat kelas UserForm, UserProfileForm, dan UsErregistrationForm. Kelas UserRegistrationForm memperluaskan kelas UserCreationForm dan menambah medan E -mel2, TimeZone2, dan Skype. Kaedah Clean_email2 memeriksa jika e -mel sudah ada dalam pangkalan data dan menimbulkan ralat pengesahan jika ia berlaku. Kaedah SAVE menjimatkan medan e -mel, zon waktu, dan skype pengguna ke model UserProfile.", "input": "", "output": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (C) 2011 Adriano Monteiro Marques\n#\n# Author: Amit Pal <amix.pal@gmail.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom django import forms\nfrom django.contrib.auth.forms import UserCreationForm\nfrom django.contrib.auth.forms import AdminPasswordChangeForm\nfrom django.contrib.auth.models import User\nfrom django.utils.translation import ugettext as _\nfrom datetime import datetime\nimport pytz\nfrom pytz import timezone\nimport pdb\n\nfrom netadmin.users.models import UserProfile\n\n\nclass UserForm(forms.ModelForm):\n    class Meta:\n        model = User\n        fields = ('first_name', 'last_name', 'email')\n\nclass UserProfileForm(forms.ModelForm):\n    class Meta:\n        model = UserProfile\n        fields = ('is_public', 'in_search')\n        \nclass UserRegistrationForm(UserCreationForm):\n    email2 = forms.EmailField(label=_(\"E-mail\"))\n    timezone2 = forms.ChoiceField(choices=[(x, x) for x in pytz.common_timezones], label = _(\"TimeZone\"))\n    skype = forms.CharField(max_length=20)\n    \n    def clean_email2(self):\n        email2 = self.cleaned_data['email2']\n        try:\n            user = User.objects.get(email=email2)\n        except User.DoesNotExist:\n            return email2\n        raise forms.ValidationError(_(\"Account with this e-mail address already exists.\"))\n    \n    def save(self, commit=True):\n        user = super(UserRegistrationForm, self).save(commit=False)\n        user.email = self.cleaned_data[\"email2\"]\n        user.is_active = False\n        if commit:\n            user.save()\n            user_profile = user.get_profile()\n            user_profile.timezone = self.cleaned_data[\"timezone2\"]\n            user_profile.skype = self.cleaned_data[\"skype\"]\n            user_profile.save()\n        return user", "instruction_en": "This is a Python code for creating user registration form with additional fields. To use this code, import the necessary modules and create a UserForm, UserProfileForm, and UserRegistrationForm classes. The UserRegistrationForm class extends the UserCreationForm class and adds email2, timezone2, and skype fields. The clean_email2 method checks if the email already exists in the database and raises a validation error if it does. The save method saves the user's email, timezone, and skype fields to the UserProfile model.", "original_line_number": 20432, "instruction_length": 537}
{"instruction": "Kod ini adalah untuk pengeluar giliran tak segerak. Untuk menggunakannya, terlebih dahulu mengimport perpustakaan AIOAMQP. Kemudian, buat contoh kelas Asyncqueueproducer dan lulus dalam objek App. Hubungi kaedah Connect untuk mewujudkan sambungan ke pelayan RabbitMQ. Gunakan kaedah create_queue untuk membuat giliran baru, lulus dalam nama giliran jika dikehendaki. Gunakan kaedah delete_queue untuk memadam barisan. Untuk menambah mesej ke barisan, gunakan kaedah add_msg_to_queue, lulus dalam nama dan mesej barisan. Untuk mendapatkan respons dari mesej, gunakan kaedah add_msg_to_queue_with_response, lulus dalam nama dan mesej giliran.", "input": "", "output": "# coding: utf-8\n\nimport aioamqp\nimport uuid\nimport logging\nfrom core.utils import async_wait_for\n\n\nlog = logging.getLogger(__name__)\n\n\nclass AsyncQueueProducer(object):\n    messages = {}\n    connection = None\n    channel = None\n    consumer_tag = None\n    responses_queue = None\n    commands_queue = None\n\n    def __init__(self, app):\n        self.app = app\n\n    async def connect(self):\n        params = {\n            'loop': self.app.loop,\n            'login': self.app.cfg.RABBITMQ_USER,\n            'password': self.app.cfg.RABBITMQ_PASSWORD,\n            'host': self.app.cfg.RABBITMQ_HOST,\n            'port': self.app.cfg.RABBITMQ_PORT\n        }\n        self.connection = await self.make_connection(params)\n        self.channel = await self.connection.channel()\n        self.responses_queue, self.consumer_tag = await self.create_queue_and_consume()\n        self.commands_queue = await self.create_queue(self.app.cfg.RABBITMQ_COMMAND_QUEUE)\n\n    async def create_queue(self, queue_name=None):\n        if not queue_name:\n            result = await self.channel.queue_declare(exclusive=True)\n        else:\n            result = await self.channel.queue_declare(queue_name=queue_name)\n        queue, messages, consumers = result.get('queue'), result.get('message_count'), result.get('consumer_count')\n        log.info(\"Queue %s was declared(messages: %s, consumers: %s)\" % (queue, messages, consumers))\n        return queue\n\n    async def delete_queue(self, queue_name):\n        await self.channel.queue_delete(queue_name)\n        log.info('Queue %s was deleted' % queue_name)\n\n    async def create_queue_and_consume(self, queue_name=None):\n        if not queue_name:\n            queue_name = await self.create_queue()\n        else:\n            await self.create_queue(queue_name)\n        consumer_tag = await self.queue_consume(queue_name)\n        return queue_name, consumer_tag\n\n    @staticmethod\n    async def make_connection(params):\n        transport, connection = await aioamqp.connect(**params)\n        return connection\n\n    async def queue_consume(self, queue_name):\n        log.info(\"Start consuming for queue %s\" % queue_name)\n        await self.channel.basic_consume(\n            callback=self.on_message, queue_name=queue_name, no_ack=False\n        )\n\n    async def on_message(self, channel, body, envelope, properties):\n        log.debug(\"Got new message %s\" % body)\n        for correlation_id in list(self.messages.keys()):\n            if correlation_id == properties.correlation_id:\n                log.info(\"Response with corr_id %s from queue %s: %s\" % (correlation_id, self.responses_queue, body))\n                self.messages[correlation_id][\"response\"] = body\n                channel.basic_client_ack(delivery_tag=envelope.delivery_tag)\n\n    async def add_msg_to_queue(self, queue_name, msg):\n        correlation_id = str(uuid.uuid4())\n        await self.channel.basic_publish(\n            payload=str(msg),\n            exchange_name='',\n            routing_key=queue_name,\n            properties={\n                \"reply_to\": self.responses_queue,\n                \"correlation_id\": correlation_id\n            })\n        log.info(\"Message(id:%s body: %s) was published to %s\" % (correlation_id, msg, queue_name))\n        self.messages[correlation_id] = {\"request\": msg, \"response\": None}\n        return correlation_id\n\n    async def get_message_from_queue(self, correlation_id):\n        log.info(\"Waiting response for message with id: %s\" % correlation_id)\n        response = await async_wait_for(\n            lambda: self.messages.get(correlation_id).get(\"response\"),\n            self.app.loop,\n            timeout=self.app.cfg.BACKEND_REQUEST_TIMEOUT\n        )\n        del self.messages[correlation_id]\n        log.info(\"Got response %s for message with id: %s\" % (response, correlation_id))\n        return response\n\n    async def add_msg_to_queue_with_response(self, queue_name, msg):\n        correlation_id = await self.add_msg_to_queue(queue_name, msg)\n        response = await self.get_message_from_queue(correlation_id)\n        return response", "instruction_en": "This code is for an asynchronous queue producer. To use it, first import the aioamqp library. Then, create an instance of the AsyncQueueProducer class and pass in the app object. Call the connect method to establish a connection to the RabbitMQ server. Use the create_queue method to create a new queue, passing in the queue name if desired. Use the delete_queue method to delete a queue. To add a message to a queue, use the add_msg_to_queue method, passing in the queue name and message. To get a response from a message, use the add_msg_to_queue_with_response method, passing in the queue name and message.", "original_line_number": 20442, "instruction_length": 640}
{"instruction": "Kod ini mentakrifkan kelas python yang dipanggil PCAPLotVector yang mewarisi dari basecclass dan mempunyai beberapa kaedah untuk mengakses dan memanipulasi data. Untuk menggunakan kod ini, import modul yang diperlukan dari ert.cwrap, ert.enkf, dan ert.util. Kemudian, buat contoh kelas pcaplotvector dengan argumen yang diperlukan: komponen integer, objek matriks untuk matriks komponen utama, dan objek matriks untuk matriks komponen utama pemerhatian. Objek yang dihasilkan boleh digunakan untuk mengakses data yang disimpan dalam matriks, termasuk saiz, nilai individu, nilai pemerhatian, dan nilai tunggal. Apabila selesai menggunakan objek, hubungi kaedah percuma () untuk melepaskan memori yang diperuntukkan untuknya.", "input": "", "output": "from ert.cwrap import BaseCClass, CWrapper\nfrom ert.enkf import ENKF_LIB\nfrom ert.util import Matrix\n\n\nclass PcaPlotVector(BaseCClass):\n\n    def __init__(self, component, principal_component_matrix, observation_principal_component_matrix):\n        assert isinstance(component, int)\n        assert isinstance(principal_component_matrix, Matrix)\n        assert isinstance(observation_principal_component_matrix, Matrix)\n\n        c_pointer = PcaPlotVector.cNamespace().alloc(component, principal_component_matrix, observation_principal_component_matrix)\n        super(PcaPlotVector, self).__init__(c_pointer)\n\n\n    def __len__(self):\n        \"\"\" @rtype: int \"\"\"\n        return PcaPlotVector.cNamespace().size(self)\n\n\n    def __getitem__(self, index):\n        \"\"\"\n        @type index: int\n        @rtype: float \n        \"\"\"\n        assert isinstance(index, int)\n        return PcaPlotVector.cNamespace().get(self, index)\n\n    def __iter__(self):\n        cur = 0\n        while cur < len(self):\n            yield self[cur]\n            cur += 1\n\n    def getObservation(self):\n        \"\"\" @rtype: float \"\"\"\n        return PcaPlotVector.cNamespace().get_obs(self)\n\n    def getSingularValue(self):\n        \"\"\" @rtype: float \"\"\"\n        return PcaPlotVector.cNamespace().get_singular_value(self)\n        \n\n    def free(self):\n        PcaPlotVector.cNamespace().free(self)\n\n\n\ncwrapper = CWrapper(ENKF_LIB)\ncwrapper.registerType(\"pca_plot_vector\", PcaPlotVector)\ncwrapper.registerType(\"pca_plot_vector_obj\", PcaPlotVector.createPythonObject)\ncwrapper.registerType(\"pca_plot_vector_ref\", PcaPlotVector.createCReference)\n\nPcaPlotVector.cNamespace().alloc   = cwrapper.prototype(\"c_void_p pca_plot_vector_alloc(int, matrix, matrix)\")\nPcaPlotVector.cNamespace().free    = cwrapper.prototype(\"void pca_plot_vector_free(pca_plot_vector)\")\nPcaPlotVector.cNamespace().size    = cwrapper.prototype(\"int pca_plot_vector_get_size(pca_plot_vector)\")\nPcaPlotVector.cNamespace().get     = cwrapper.prototype(\"double pca_plot_vector_iget_sim_value(pca_plot_vector, int)\")\nPcaPlotVector.cNamespace().get_obs = cwrapper.prototype(\"double pca_plot_vector_get_obs_value(pca_plot_vector)\")\nPcaPlotVector.cNamespace().get_singular_value = cwrapper.prototype(\"double pca_plot_vector_get_singular_value(pca_plot_vector)\")", "instruction_en": "This code defines a Python class called PcaPlotVector that inherits from BaseCClass and has several methods for accessing and manipulating data. To use this code, import the necessary modules from ert.cwrap, ert.enkf, and ert.util. Then, create an instance of the PcaPlotVector class with the required arguments: an integer component, a Matrix object for the principal component matrix, and a Matrix object for the observation principal component matrix. The resulting object can be used to access the data stored in the matrices, including the size, individual values, observation value, and singular value. When finished using the object, call the free() method to release the memory allocated for it.", "original_line_number": 21442, "instruction_length": 724}
{"instruction": "Kod ini adalah pelaksanaan algoritma pemaksaan jangkaan untuk model campuran Gaussian. Untuk menggunakan kod tersebut, terlebih dahulu mengimport perpustakaan yang diperlukan: SFrame, numpy, matplotlib.pyplot, scipy.stats, salinan, pil, io, dan matplotlib.mlab. Kemudian, tentukan fungsi penjana_mog_data untuk menghasilkan data dari model campuran Gaussian dengan cara yang diberikan, kovarians, dan berat. Seterusnya, tentukan fungsi log_sum_exp untuk mengira log jumlah eksponen array. Kemudian, tentukan fungsi loglikeli untuk mengira loglikeli dataset yang diberikan untuk model campuran Gaussian dengan parameter yang diberikan. Seterusnya, tentukan fungsi EM untuk menjalankan algoritma pemaksaan jangkaan pada dataset yang diberikan dengan parameter awal yang diberikan. Akhirnya, gunakan fungsi plot_contours, plot_responsibilities_in_rb, get_top_images, dan save_images untuk memvisualisasikan hasil algoritma EM pada dataset tertentu.", "input": "", "output": "# Imports\nimport sframe\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport copy\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.mlab as mlab\nimport colorsys\n\n\ndef generate_MoG_data(num_data, means, covariances, weights):\n    data = []\n    for i in range(num_data):\n        #  Use np.random.choice and weights to pick a cluster id greater than or equal to 0 and less than num_clusters.\n        k = np.random.choice(len(weights), 1, p=weights)[0]\n        # Use np.random.multivariate_normal to create data from this cluster\n        x = np.random.multivariate_normal(means[k], covariances[k])\n        data.append(x)\n    return data\n\n\ndef log_sum_exp(Z):\n    \"\"\" Compute log(\\sum_i exp(Z_i)) for some array Z.\"\"\"\n    return np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))\n\n\ndef loglikelihood(data, weights, means, covs):\n    \"\"\" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. \"\"\"\n    num_clusters = len(means)\n    num_dim = len(data[0])\n    \n    ll = 0\n    for d in data:\n        \n        Z = np.zeros(num_clusters)\n        for k in range(num_clusters):\n            \n            # Compute (x-mu)^T * Sigma^{-1} * (x-mu)\n            delta = np.array(d) - means[k]\n            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))\n            \n            # Compute loglikelihood contribution for this data point and this cluster\n            Z[k] += np.log(weights[k])\n            Z[k] -= 1/2. * (num_dim * np.log(2*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)\n            \n        # Increment loglikelihood contribution of this data point across all clusters\n        ll += log_sum_exp(Z)\n        \n    return ll\n\n\ndef EM(data, init_means, init_covariances, init_weights, maxiter=1000, thresh=1e-4):\n    # Make copies of initial parameters, which we will update during each iteration\n    means = copy.deepcopy(init_means)\n    covariances = copy.deepcopy(init_covariances)\n    weights = copy.deepcopy(init_weights)\n    \n    # Infer dimensions of dataset and the number of clusters\n    num_data = len(data)\n    num_dim = len(data[0])\n    num_clusters = len(means)\n    \n    # Initialize some useful variables\n    resp = np.zeros((num_data, num_clusters))\n    ll = loglikelihood(data, weights, means, covariances)\n    ll_trace = [ll]\n    \n    for i in range(maxiter):\n        if i % 5 == 0:\n            print(\"Iteration %s\" % i)\n        \n        # E-step: compute responsibilities\n        # Update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j.\n        # Hint: To compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf.\n        for j in range(num_data):\n            for k in range(num_clusters):\n                # YOUR CODE HERE\n                resp[j, k] = weights[k] * multivariate_normal.pdf(x=data[j],\n                                                                  mean=means[k],\n                                                                  cov=covariances[k])\n        row_sums = resp.sum(axis=1)[:, np.newaxis]\n        resp = resp / row_sums # normalize over all possible cluster assignments\n\n        # M-step\n        # Compute the total responsibility assigned to each cluster, which will be useful when \n        # implementing M-steps below. In the lectures this is called N^{soft}\n        counts = np.sum(resp, axis=0)\n        \n        for k in range(num_clusters):\n            \n            # Update the weight for cluster k using the M-step update rule for the cluster weight, \\hat{\\pi}_k.\n            # YOUR CODE HERE\n            Nsoft_k = counts[k]\n            weights[k] = float(Nsoft_k)/float(num_data)\n            \n            # Update means for cluster k using the M-step update rule for the mean variables.\n            # This will assign the variable means[k] to be our estimate for \\hat{\\mu}_k.\n            weighted_sum = 0\n            for j in range(num_data):\n                # YOUR CODE HERE\n                weighted_sum += resp[j, k] * data[j]\n            # YOUR CODE HERE\n            means[k] = weighted_sum/Nsoft_k\n            \n            # Update covariances for cluster k using the M-step update rule for covariance variables.\n            # This will assign the variable covariances[k] to be the estimate for \\hat{Sigma}_k.\n            weighted_sum = np.zeros((num_dim, num_dim))\n            for j in range(num_data):\n                # YOUR CODE HERE (Hint: Use np.outer on the data[j] and this cluster's mean)\n                weighted_sum += resp[j, k] * np.outer(data[j] - means[k], data[j] - means[k])\n            # YOUR CODE HERE\n            covariances[k] = weighted_sum/Nsoft_k\n          \n        \n        # Compute the loglikelihood at this iteration\n        # YOUR CODE HERE\n        ll_latest = loglikelihood(data, weights, means, covariances)\n        ll_trace.append(ll_latest)\n        \n        # Check for convergence in log-likelihood and store\n        if (ll_latest - ll) < thresh and ll_latest > -np.inf:\n            break\n        ll = ll_latest\n    \n    if i % 5 != 0:\n        print(\"Iteration %s\" % i)\n    \n    out = {'weights': weights, 'means': means, 'covs': covariances, 'loglik': ll_trace, 'resp': resp}\n\n    return out\n\n\ndef plot_contours(data, means, covs, title):\n    plt.figure()\n    plt.plot([x[0] for x in data], [y[1] for y in data],'ko') # data\n\n    delta = 0.025\n    k = len(means)\n    x = np.arange(-2.0, 7.0, delta)\n    y = np.arange(-2.0, 7.0, delta)\n    X, Y = np.meshgrid(x, y)\n    col = ['green', 'red', 'indigo']\n    for i in range(k):\n        mean = means[i]\n        cov = covs[i]\n        sigmax = np.sqrt(cov[0][0])\n        sigmay = np.sqrt(cov[1][1])\n        sigmaxy = cov[0][1]/(sigmax*sigmay)\n        Z = mlab.bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n        plt.contour(X, Y, Z, colors = col[i])\n        plt.title(title)\n    plt.rcParams.update({'font.size':16})\n    plt.tight_layout()\n\n\ndef plot_responsibilities_in_RB(img, resp, title):\n    N, K = resp.shape\n    \n    HSV_tuples = [(x*1.0/K, 0.5, 0.9) for x in range(K)]\n    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n    \n    R = img['red']\n    B = img['blue']\n    resp_by_img_int = [[resp[n][k] for k in range(K)] for n in range(N)]\n    cols = [tuple(np.dot(resp_by_img_int[n], np.array(RGB_tuples))) for n in range(N)]\n\n    plt.figure()\n    for n in range(len(R)):\n        plt.plot(R[n], B[n], 'o', c=cols[n])\n    plt.title(title)\n    plt.xlabel('R value')\n    plt.ylabel('B value')\n    plt.rcParams.update({'font.size':16})\n    plt.tight_layout()\n\n\ndef get_top_images(assignments, cluster, k=5):\n    # YOUR CODE HERE\n    images_in_cluster = assignments[assignments['assignments']==cluster]\n    print images_in_cluster\n    top_images = images_in_cluster.topk('probs', k)\n    return top_images['image']\n\n\ndef save_images(images, prefix):\n    for i, image in enumerate(images):\n        Image.open(BytesIO(image._image_data)).save(prefix % i)\n\n\n# Model parameters\ninit_means = [\n    [5, 0], # mean of cluster 1\n    [1, 1], # mean of cluster 2\n    [0, 5]  # mean of cluster 3\n]\ninit_covariances = [\n    [[.5, 0.], [0, .5]], # covariance of cluster 1\n    [[.92, .38], [.38, .91]], # covariance of cluster 2\n    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n]\ninit_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n\n# Generate data\nnp.random.seed(4)\ndata = generate_MoG_data(100, init_means, init_covariances, init_weights)\n\n# Plot clusters\nplt.figure()\nd = np.vstack(data)\nplt.plot(d[:,0], d[:,1],'ko')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Test EM algorithm\nnp.random.seed(4)\n\n# Initialization of parameters\nchosen = np.random.choice(len(data), 3, replace=False)\ninitial_means = [data[x] for x in chosen]\ninitial_covs = [np.cov(data, rowvar=0)] * 3\ninitial_weights = [1/3.] * 3\n\n# Run EM \nresults = EM(data, initial_means, initial_covs, initial_weights)\n\n# Parameters after initialization\nplot_contours(data, initial_means, initial_covs, 'Initial clusters')\n\n# Parameters after 12 iterations\nresults = EM(data, initial_means, initial_covs, initial_weights, maxiter=12)\nplot_contours(data, results['means'], results['covs'], 'Clusters after 12 iterations')\n\n# Parameters after running EM to convergence\nresults = EM(data, initial_means, initial_covs, initial_weights)\nplot_contours(data, results['means'], results['covs'], 'Final clusters')\n\n# Log-likelihood plot\nloglikelihoods = results['loglik']\nplt.plot(range(len(loglikelihoods)), loglikelihoods, linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Load image data\nimages = sframe.SFrame('../data/Week04/images.sf/')\nimages['rgb'] = images.pack_columns(['red', 'green', 'blue'])['X4']\n\n# Run EM on image data\nnp.random.seed(1)\n\n# Initalize parameters\ninit_means = [images['rgb'][x] for x in np.random.choice(len(images), 4, replace=False)]\ncov = np.diag([images['red'].var(), images['green'].var(), images['blue'].var()])\ninit_covariances = [cov, cov, cov, cov]\ninit_weights = [1/4., 1/4., 1/4., 1/4.]\n\n# Convert rgb data to numpy arrays\nimg_data = [np.array(i) for i in images['rgb']]  \n\n# Run our EM algorithm on the image data using the above initializations. \n# This should converge in about 125 iterations\nout = EM(img_data, init_means, init_covariances, init_weights)\n\n# Log-likelihood plot\nll = out['loglik']\nplt.plot(range(len(ll)),ll,linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\nplt.figure()\nplt.plot(range(10,len(ll)),ll[10:],linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Visualize evolution of responsibility\nN, K = out['resp'].shape\nrandom_resp = np.random.dirichlet(np.ones(K), N)\nplot_responsibilities_in_RB(images, random_resp, 'Random responsibilities')\n\nout = EM(img_data, init_means, init_covariances, init_weights, maxiter=1)\nplot_responsibilities_in_RB(images, out['resp'], 'After 1 iteration')\n\nout = EM(img_data, init_means, init_covariances, init_weights, maxiter=20)\nplot_responsibilities_in_RB(images, out['resp'], 'After 20 iterations')\n\n# Interpreting clusters\nweights = out['weights']\nmeans = out['means']\ncovariances = out['covs']\nrgb = images['rgb']\nN = len(images) # number of images\nK = len(means) # number of clusters\n\nassignments = [0]*N\nprobs = [0]*N\n\nfor i in range(N):\n    # Compute the score of data point i under each Gaussian component:\n    p = np.zeros(K)\n    for k in range(K):\n        p[k] = weights[k]*multivariate_normal.pdf(rgb[i], mean=means[k], cov=covariances[k])\n        \n    # Compute assignments of each data point to a given cluster based on the above scores:\n    assignments[i] = np.argmax(p)\n    \n    # For data point i, store the corresponding score under this cluster assignment:\n    probs[i] = np.max(p)\n\nassignments = sframe.SFrame({'assignments':assignments, 'probs':probs, 'image': images['image']})\n\nfor idx in range(4):\n    get_top_images(assignments, idx)\n\nfor component_id in range(4):\n    print 'Component {0:d}'.format(component_id)\n    images = get_top_images(assignments, component_id)\n    save_images(images, 'component_{0:d}_%d.jpg'.format(component_id))\n    print '\\n'", "instruction_en": "The code is an implementation of the Expectation-Maximization algorithm for Gaussian Mixture Models. To use the code, first import the necessary libraries: sframe, numpy, matplotlib.pyplot, scipy.stats, copy, PIL, io, and matplotlib.mlab. Then, define the function generate_MoG_data to generate data from a Gaussian mixture model with given means, covariances, and weights. Next, define the function log_sum_exp to compute the log of the sum of exponentials of an array. Then, define the function loglikelihood to compute the loglikelihood of a given dataset for a Gaussian mixture model with given parameters. Next, define the function EM to run the Expectation-Maximization algorithm on a given dataset with given initial parameters. Finally, use the functions plot_contours, plot_responsibilities_in_RB, get_top_images, and save_images to visualize the results of the EM algorithm on a given dataset.", "original_line_number": 21452, "instruction_length": 945}
{"instruction": "Buat kelas xbmccontextui yang memanjangkan abstrakContextui dan mempunyai beberapa kaedah untuk membuat elemen UI dan berinteraksi dengan pengguna. Kelas ini mempunyai pembina yang mengambil XBMC_ADDON dan konteks. Kelas ini mempunyai kaedah untuk mewujudkan dialog kemajuan, menetapkan dan mendapatkan mod paparan, mendapatkan ID kulit, mengendalikan input papan kekunci, input angka, ya/tidak input, dan memilih dari senarai item. Kelas ini juga mempunyai kaedah untuk menunjukkan pemberitahuan, membuka tetapan, menyegarkan bekas, dan menetapkan, mendapatkan, dan membersihkan sifat -sifat tetingkap rumah.", "input": "", "output": "__author__ = 'bromix'\n\nfrom six import string_types\n\nimport xbmc\nimport xbmcgui\n\nfrom ..abstract_context_ui import AbstractContextUI\nfrom .xbmc_progress_dialog import XbmcProgressDialog\nfrom .xbmc_progress_dialog_bg import XbmcProgressDialogBG\nfrom ... import constants\nfrom ... import utils\n\n\nclass XbmcContextUI(AbstractContextUI):\n    def __init__(self, xbmc_addon, context):\n        AbstractContextUI.__init__(self)\n\n        self._xbmc_addon = xbmc_addon\n\n        self._context = context\n        self._view_mode = None\n\n    def create_progress_dialog(self, heading, text=None, background=False):\n        if background and self._context.get_system_version().get_version() > (12, 3):\n            return XbmcProgressDialogBG(heading, text)\n\n        return XbmcProgressDialog(heading, text)\n\n    def set_view_mode(self, view_mode):\n        if isinstance(view_mode, string_types):\n            view_mode = self._context.get_settings().get_int(constants.setting.VIEW_X % view_mode, 50)\n\n        self._view_mode = view_mode\n\n    def get_view_mode(self):\n        if self._view_mode is not None:\n            return self._view_mode\n\n        return self._context.get_settings().get_int(constants.setting.VIEW_DEFAULT, 50)\n\n    def get_skin_id(self):\n        return xbmc.getSkinDir()\n\n    def on_keyboard_input(self, title, default='', hidden=False):\n        # fallback for Frodo\n        if self._context.get_system_version().get_version() <= (12, 3):\n            keyboard = xbmc.Keyboard(default, title, hidden)\n            keyboard.doModal()\n            if keyboard.isConfirmed() and keyboard.getText():\n                text = utils.to_unicode(keyboard.getText())\n                return True, text\n            else:\n                return False, u''\n\n        # Starting with Gotham (13.X > ...)\n        dialog = xbmcgui.Dialog()\n        result = dialog.input(title, utils.to_unicode(default), type=xbmcgui.INPUT_ALPHANUM)\n        if result:\n            text = utils.to_unicode(result)\n            return True, text\n\n        return False, u''\n\n    def on_numeric_input(self, title, default=''):\n        dialog = xbmcgui.Dialog()\n        result = dialog.input(title, str(default), type=xbmcgui.INPUT_NUMERIC)\n        if result:\n            return True, int(result)\n\n        return False, None\n\n    def on_yes_no_input(self, title, text, nolabel='', yeslabel=''):\n        dialog = xbmcgui.Dialog()\n        return dialog.yesno(title, text, nolabel=nolabel, yeslabel=yeslabel)\n\n    def on_ok(self, title, text):\n        dialog = xbmcgui.Dialog()\n        return dialog.ok(title, text)\n\n    def on_remove_content(self, content_name):\n        text = self._context.localize(constants.localize.REMOVE_CONTENT) % utils.to_unicode(content_name)\n        return self.on_yes_no_input(self._context.localize(constants.localize.CONFIRM_REMOVE), text)\n\n    def on_delete_content(self, content_name):\n        text = self._context.localize(constants.localize.DELETE_CONTENT) % utils.to_unicode(content_name)\n        return self.on_yes_no_input(self._context.localize(constants.localize.CONFIRM_DELETE), text)\n\n    def on_select(self, title, items=[]):\n        _dict = {}\n        _items = []\n        i = 0\n        for item in items:\n            if isinstance(item, tuple):\n                _dict[i] = item[1]\n                _items.append(item[0])\n            else:\n                _dict[i] = i\n                _items.append(item)\n\n            i += 1\n\n        dialog = xbmcgui.Dialog()\n        result = dialog.select(title, _items)\n        return _dict.get(result, -1)\n\n    def show_notification(self, message, header='', image_uri='', time_milliseconds=5000):\n        _header = header\n        if not _header:\n            _header = self._context.get_name()\n        _header = utils.to_utf8(_header)\n\n        _image = image_uri\n        if not _image:\n            _image = self._context.get_icon()\n\n        _message = utils.to_utf8(message)\n        try:\n            _message = _message.replace(',', ' ')\n            _message = _message.replace('\\n', ' ')\n        except TypeError:\n            _message = _message.replace(b',', b' ')\n            _message = _message.replace(b'\\n', b' ')\n            _message = utils.to_unicode(_message)\n            _header = utils.to_unicode(_header)\n\n        xbmc.executebuiltin(\"Notification(%s, %s, %d, %s)\" % (_header, _message, time_milliseconds, _image))\n\n    def open_settings(self):\n        self._xbmc_addon.openSettings()\n\n    @staticmethod\n    def refresh_container():\n        xbmc.executebuiltin(\"Container.Refresh\")\n\n    @staticmethod\n    def set_home_window_property(property_id, value):\n        property_id = 'plugin.video.youtube-' + property_id\n        xbmcgui.Window(10000).setProperty(property_id, value)\n\n    @staticmethod\n    def get_home_window_property(property_id):\n        property_id = 'plugin.video.youtube-' + property_id\n        return xbmcgui.Window(10000).getProperty(property_id) or None\n\n    @staticmethod\n    def clear_home_window_property(property_id):\n        property_id = 'plugin.video.youtube-' + property_id\n        xbmcgui.Window(10000).clearProperty(property_id)", "instruction_en": "Create a class XbmcContextUI that extends AbstractContextUI and has several methods for creating UI elements and interacting with the user. The class has a constructor that takes in an xbmc_addon and a context. The class has methods for creating progress dialogs, setting and getting view modes, getting the skin ID, handling keyboard input, numeric input, yes/no input, and selecting from a list of items. The class also has methods for showing notifications, opening settings, refreshing the container, and setting, getting, and clearing properties of the home window.", "original_line_number": 21697, "instruction_length": 609}
{"instruction": "Kod ini mentakrifkan dua kelas, Mathquestions dan Triviaquestions, dan kelas kemahiran, Triviaskill, yang mewarisi dari Mycroftskill. Mathquestions menghasilkan soalan dan jawapan matematik, sementara triviaquestions menghasilkan soalan dan jawapan trivia. Triviaskill mengendalikan interaksi pengguna dan menggunakan dua kelas yang lain untuk menghasilkan soalan dan jawapan. Untuk menggunakan kod ini, buat contoh kelas TriviasKill dan panggil kaedahnya untuk mengendalikan input pengguna dan menghasilkan soalan dan jawapan.", "input": "", "output": "from adapt.intent import IntentBuilder\nfrom mycroft.skills.core import MycroftSkill\n\nimport random, math, os, sys\nfrom os.path import dirname\npath= dirname(dirname(__file__))\nsys.path.append(path)\n# import intent layers\nfrom service_intent_layer import IntentParser\n\n__author__ = 'jarbas'\n\n\nclass MathQuestions:\n    def __init__(self):\n        self.questions = []\n        self.init_questions()\n\n    def ft(self, text, randints, randdec, randsgnint):\n        return text.format(randints, randdec, randsgnint)\n\n    def init_questions(self):\n        # TODO more questions / equation types\n        self.questions.append([\"Convert {1[0]:0.2f} centimeters into meters.\", \"{1[0]}*0.01\"])\n        self.questions.append([\n            \"What is the length of the line segment with endpoints ({2[1]},{2[2]}) and ({2[3]},{2[4]})?\",\n            \"math.sqrt(({2[3]}-{2[1]})**2 + ({2[4]}-{2[2]})**2)\"])\n        self.questions.append([\"Solve for x in the equation {2[1]}x - {0[2]} = {2[7]}\", \"({2[7]}+{0[2]})*1./{2[1]}\"])\n\n    def ask_question(self):\n        question = random.choice(self.questions)\n        answer = question[1]\n        question = question[0]\n        question, answer = self.answer_question(question, answer)\n        return question, answer\n\n    def answer_question(self, question, answer):\n        randints = []\n        randdec = []\n        randsgnint = []\n\n        for a in range(1,\n                       11):  # Creates three arrays of whole numbers, random decimals, and random signed integers for use in questions.\n            randints.append(random.randint(1, 10))\n            randdec.append(math.sqrt(random.randint(1, 100)) * random.randint(1, 10))\n            randsgnint.append(random.randint(-10, 10))\n\n        question = self.ft(question, randints, randdec,\n                            randsgnint)  # The function replaces all symbols in the question with the correct number types\n        answer = eval(self.ft(answer, randints, randdec,\n                                   randsgnint))  # This stores the numerical answer based on the string provided with the answer.\n\n        return question, answer\n\n\nclass TriviaQuestions:\n    def __init__(self):\n        self.questions = {} #\"categorie\" : [[question, answer], [question, answer]]\n        self.categories = [\"general\", \"geography\", \"history\", \"literature\", \"movies\", \"music\", \"science\", \"sports\"]\n        self.load_questions()\n\n    def load_questions(self):\n        for cat in self.categories:\n            questions = []\n            answers = []\n            path = os.path.dirname(__file__) + '/' + cat + \".txt\"\n            with open(path) as f:\n                lines = f.readlines()\n                i = 1\n                for line in lines:\n                    if i % 2 == 0:\n                        answers.append(line)\n                    else:\n                        questions.append(line)\n                    i += 1\n\n            self.questions[cat] = []\n            for i in range(len(questions)):\n                self.questions[cat].append([questions[i], answers[i]])\n\n    def ask_question(self, categorie=\"general\"):\n        question = random.choice(self.questions[categorie])\n        answer = question[1]\n        question = question[0]\n        return question, answer\n\nclass TriviaSkill(MycroftSkill):\n\n    def __init__(self):\n        super(TriviaSkill, self).__init__(name=\"TriviaSkill\")\n        # initialize your variables\n        self.quizz = False\n        self.continuous = False\n        self.math = MathQuestions()\n        self.trivia = TriviaQuestions()\n        self.answer = None\n        self.categorie = \"all\"\n        self.categories = [\"math\", \"general\", \"geography\", \"history\", \"literature\", \"movies\", \"music\", \"science\", \"sports\"]\n\n    def initialize(self):\n        self.intent_parser = IntentParser(self.emitter)\n        # register intents\n        self.build_intents()\n\n    def build_intents(self):\n        # build\n        trivia_intent = IntentBuilder(\"TriviaGameIntent\") \\\n            .require(\"triviastart\").build()\n        cat_intent = IntentBuilder(\"TriviaCategorieIntent\") \\\n            .require(\"Categorie\").build()\n        geography_intent = IntentBuilder(\"GeographyQuestionIntent\") \\\n            .require(\"geography\").build()\n        history_intent = IntentBuilder(\"HistoryQuestionIntent\") \\\n            .require(\"history\").build()\n        literature_intent = IntentBuilder(\"LiteratureQuestionIntent\") \\\n            .require(\"literature\").build()\n        math_intent = IntentBuilder(\"MathQuestionIntent\") \\\n            .require(\"math\").build()\n        movie_intent = IntentBuilder(\"MovieQuestionIntent\") \\\n            .require(\"movie\").build()\n        music_intent = IntentBuilder(\"MusicQuestionIntent\") \\\n            .require(\"music\").build()\n        science_intent = IntentBuilder(\"ScienceQuestionIntent\") \\\n            .require(\"science\").build()\n        sports_intent = IntentBuilder(\"SportsQuestionIntent\") \\\n            .require(\"sports\").build()\n        general_intent = IntentBuilder(\"QuestionIntent\") \\\n            .require(\"question\").build()\n        stop_intent = IntentBuilder(\"StopTriviaIntent\") \\\n            .require(\"stoptrivia\").build()\n\n        # register\n        self.register_intent(trivia_intent,\n                             self.handle_trivia_game_start)\n        self.register_intent(geography_intent,\n                             self.handle_geography_question)\n        self.register_intent(history_intent,\n                             self.handle_history_question)\n        self.register_intent(literature_intent,\n                             self.handle_literature_question)\n        self.register_intent(math_intent,\n                             self.handle_math_question)\n        self.register_intent(movie_intent,\n                             self.handle_movies_question)\n        self.register_intent(music_intent,\n                             self.handle_music_question)\n        self.register_intent(science_intent,\n                             self.handle_science_question)\n        self.register_intent(sports_intent,\n                             self.handle_sports_question)\n        self.register_intent(general_intent,\n                             self.handle_general_question)\n        self.register_intent(cat_intent,\n                             self.handle_change_cat_intent)\n        self.register_intent(stop_intent,\n                             self.handle_stop_quizz)\n\n    def random_question(self):\n        if self.categorie == \"math\":\n            self.quizz = True\n            question, self.answer = self.math.ask_question()\n        elif self.categorie == \"all\":\n            self.quizz = True\n            cat = random.choice(self.categories)\n            if cat == \"math\":\n                question, self.answer = self.math.ask_question()\n            else:\n                question, self.answer = self.trivia.ask_question(cat)\n        else:\n            self.quizz = True\n            question, self.answer = self.trivia.ask_question(self.categorie)\n\n        return question\n\n    def handle_trivia_game_start(self, message):\n        if self.categorie == \"all\":\n            self.categorie = random.choice(self.categories)\n        self.speak_dialog(\"trivia\", {\"cat\": self.categorie, \"question\":self.random_question()})\n        self.continuous = True\n\n    def handle_change_cat_intent(self, message):\n        cat = message.data[\"Categorie\"].replace(\" \",\"\").replace('\"',\"\")\n        if cat in self.categories:\n            self.categorie = cat\n            self.speak_dialog(\"categorie\", {\"cat\": self.categorie})\n        else:\n            self.speak(cat + \" is an invalid categorie\")\n\n    def handle_math_question(self, message):\n        self.quizz = True\n        question, self.answer = self.math.ask_question()\n        self.speak(question, expect_response=True)\n\n    def handle_sports_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"sports\")\n        self.speak(question, expect_response=True)\n\n    def handle_movies_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"movies\")\n        self.speak(question, expect_response=True)\n\n    def handle_music_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"music\")\n        self.speak(question, expect_response=True)\n\n    def handle_literature_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"literature\")\n        self.speak(question, expect_response=True)\n\n    def handle_history_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"history\")\n        self.speak(question, expect_response=True)\n\n    def handle_geography_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"geography\")\n        self.speak(question, expect_response=True)\n\n    def handle_science_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question(\"science\")\n        self.speak(question, expect_response=True)\n\n    def handle_general_question(self, message):\n        self.quizz = True\n        question, self.answer = self.trivia.ask_question()\n        self.speak(question, expect_response=True)\n\n    def handle_stop_quizz(self, message):\n        self.stop()\n\n    def stop(self):\n        if self.quizz or self.continuous:\n            self.speak(\"Exiting Quizz mode\")\n            self.quizz = False\n            self.continuous = False\n            self.answer = None\n            self.categorie = \"all\"\n\n    def converse(self, transcript, lang=\"en-us\"):\n        # check if some of the intents will be handled\n        intent, id = self.intent_parser.determine_intent(transcript[0])\n        if id == self.skill_id:\n            # intent from this skill will be triggered\n            # only stop, change categorie, specific questions intents available\n            pass\n        elif self.continuous and self.answer is not None:\n            self.speak_dialog(\"trivianext\", {\"ans\" : str(self.answer), \"question\":self.random_question()}, expect_response=True)\n            return True\n        elif self.quizz and self.answer is not None:\n            self.speak(\"the correct answer is \" + str(self.answer), expect_response=True)\n            self.quizz = False\n            self.answer = None\n            return True\n\n        return False\n\n\ndef create_skill():\n    return TriviaSkill()", "instruction_en": "This code defines two classes, MathQuestions and TriviaQuestions, and a skill class, TriviaSkill, which inherits from MycroftSkill. MathQuestions generates math questions and answers, while TriviaQuestions generates trivia questions and answers. TriviaSkill handles user interactions and uses the other two classes to generate questions and answers. To use this code, create an instance of the TriviaSkill class and call its methods to handle user input and generate questions and answers.", "original_line_number": 21738, "instruction_length": 527}
{"instruction": "Gunakan kod ini untuk membuat loader templat caching yang membolehkan templat berasaskan cakera digunakan. Loader harus menyusun templat untuk prestasi untuk prestasi. Untuk menggunakan loader, buat contoh kelas loader dan lulus dalam senarai sumber dan parser. Sumber -sumbernya harus menjadi contoh sama ada kelas memori atau direktori sumber, dan parser harus menjadi contoh parser templat. Sebaik sahaja loader dimulakan, anda boleh menggunakan beban dan membuat kaedah untuk memuat dan membuat templat. Pada tingkap, gunakan slash ke hadapan sebagai pemisah jalan.", "input": "", "output": "\"\"\"A caching template loader that allows disk-based templates to be used.\"\"\"\n\n\nimport os, sys\nfrom abc import ABCMeta, abstractmethod\n\n\nclass TemplateDoesNotExist(Exception):\n    \n    \"\"\"A named template could not be found.\"\"\"\n\n\nclass Source(metaclass=ABCMeta):\n    \n    \"\"\"A source of template data.\"\"\"\n    \n    __slots__ = ()\n    \n    @abstractmethod\n    def load_source(self, template_name):\n        \"\"\"\n        Loads the template source code for the template of the given name.\n        \n        If no source code can be found, returns None.\n        \"\"\"\n        \n        \nclass MemorySource(Source):\n\n    \"\"\"A template loader that loads from memory.\"\"\"\n    \n    __slots__ = (\"templates\",)\n    \n    def __init__(self, templates):\n        \"\"\"Initializes the MemorySource from a dict of template source strings.\"\"\"\n        self.templates = templates\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from the memory template dict.\"\"\"\n        return self.templates.get(template_name)\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return \"<memory>\"\n        \n        \nclass DirectorySource(Source):\n    \n    \"\"\"A template loader that loads from a directory on disk.\"\"\"\n    \n    __slots__ = (\"dirname\")\n    \n    def __init__(self, dirname):\n        \"\"\"\n        Initializes the DirectorySource.\n        \n        On windows, the dirname should be specified using forward-slashes.\n        \"\"\"\n        self.dirname = dirname\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from disk.\"\"\"\n        template_path = os.path.normpath(os.path.join(self.dirname, template_name))\n        if os.path.exists(template_path):\n            with open(template_path, \"r\") as template_file:\n                return template_file.read()\n        return None\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return self.dirname\n\n\nclass DebugLoader:\n\n    \"\"\"\n    A template loader that doesn't cache compiled templates.\n    \n    Terrible performance, but great for debugging.\n    \"\"\"\n    \n    __slots__ = (\"_sources\", \"_parser\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"\n        Initializes the Loader.\n        \n        When specifying template_dirs on Windows,the forward slash '/' should be used as a path separator.\n        \"\"\"\n        self._sources = list(reversed(sources))\n        self._parser = parser\n    \n    def compile(self, template, name=\"__string__\", params=None, meta=None):\n        \"\"\"Compiles the given template source.\"\"\"\n        default_meta = {\n            \"__loader__\": self\n        }\n        default_meta.update(meta or {})\n        return self._parser.compile(template, name, params, default_meta)\n    \n    def _load_all(self, template_name):\n        \"\"\"Loads and returns all the named templates from the sources.\"\"\"\n        # Load from all the template sources.\n        templates = []\n        for source in self._sources:\n            template_src = source.load_source(template_name)\n            if template_src is not None:\n                meta = {\n                    \"__super__\": templates and templates[-1] or None,\n                }\n                templates.append(self.compile(template_src, template_name, {}, meta))\n        return templates\n    \n    def load(self, *template_names):        \n        \"\"\"\n        Loads and returns the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        if not template_names:\n            raise ValueError(\"You must specify at least one template name.\")\n        for template_name in template_names:\n            templates = self._load_all(template_name)\n            if templates:\n                return templates[-1]\n        # Raise an error.\n        template_name_str = \", \".join(repr(template_name) for template_name in template_names)\n        source_name_str = \", \".join(str(source) for source in self._sources)\n        raise TemplateDoesNotExist(\"Could not find a template named {} in any of {}.\".format(template_name_str, source_name_str))\n        \n    def render(self, *template_names, **params):\n        \"\"\"\n        Loads and renders the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        return self.load(*template_names).render(**params)\n\n\nclass Loader(DebugLoader):\n    \n    \"\"\"\n    A template loader.\n    \n    Compiled templates are cached for performance.\n    \"\"\"\n    \n    __slots__ = (\"_cache\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"Initializes the loader.\"\"\"\n        super(Loader, self).__init__(sources, parser)\n        self._cache = {}\n    \n    def clear_cache(self, ):\n        \"\"\"Clears the template cache.\"\"\"\n        self._cache.clear()\n        \n    def _load_all(self, template_name):\n        \"\"\"A caching version of the debug loader's load method.\"\"\"\n        if template_name in self._cache:\n            return self._cache[template_name]\n        template = super(Loader, self)._load_all(template_name)\n        self._cache[template_name] = template\n        return template", "instruction_en": "Use this code to create a caching template loader that allows disk-based templates to be used. The loader should cache compiled templates for performance. To use the loader, create an instance of the Loader class and pass in a list of sources and a parser. The sources should be instances of either the MemorySource or DirectorySource classes, and the parser should be an instance of a template parser. Once the loader is initialized, you can use its load and render methods to load and render templates. On Windows, use forward slashes as path separators.", "original_line_number": 21976, "instruction_length": 569}
{"instruction": "Kod ini mengekstrak miRNAs baru dari laporan dan boleh menghasilkan dump JSON. Untuk menggunakannya, buat skrip Python dan mengimport modul yang diperlukan (ArgParse, Json, OS, dan Tarikh). Tentukan fungsi \"extract_new_mirnas_from_report\" dengan input \"Report_tsv\" dan argumen pilihan \"jenis\" dengan nilai lalai \"baru\". Fungsi ini membaca fail laporan, memeriksa jika calon miRNA adalah keluarga baru, melangkau keluarga yang memerlukan semakan, dan menambah miRNA baru ke kamus. Tentukan fungsi \"extract_rfam_family_accessions\" dengan input \"Report_File\". Fungsi ini membaca fail laporan, memeriksa jika tumpang tindih kurang daripada atau sama dengan 100.0, dan menambah aksesi miRNA ke kamus. Tentukan fungsi \"parse_arguments\" tanpa input. Fungsi ini mewujudkan objek ArgumentParser dan menambah argumen yang diperlukan. Di blok utama, menghuraikan hujah -hujah yang menggunakan objek ArgumentParser dan panggil fungsi yang sesuai berdasarkan argumen. Jika argumen \"-membuat\" disediakan, menghasilkan dump JSON dengan nama fail yang sesuai.", "input": "", "output": "import argparse\nimport json\nimport os\nfrom datetime import date\n\n\n# -------------------------------------------------------------------------------\n\n\ndef extract_new_mirnas_from_report(report_tsv, type='new'):\n    \"\"\"\n    \"\"\"\n\n    new_mirnas = {}\n\n    fp = open(report_tsv, 'r')\n    count = 0\n    for line in fp:\n        line = line.strip().split('\\t')\n        # check if candidate mirna is a new family\n        if line[6].lower() == \"new family\":\n            # skip families requiring review\n            if line[1] != '?' and line[1] != '' and line[2] != \"1_SEED\":\n                if line[0] not in new_mirnas:\n                    print line\n                    new_mirnas[line[0]] = line[1]\n        elif line[6].lower() == 'done':\n            count += 1\n    fp.close()\n\n    return new_mirnas\n\n\n# -------------------------------------------------------------------------------\n\ndef extract_rfam_family_accessions(report_file):\n\n    fp = open(report_file, 'r')\n\n    accession_map = {}\n\n    for line in fp:\n        line = line.strip().split('\\t')\n\n        overlap = float(line[4])\n        if overlap <= 100.0:\n            # converts to upper to ensure labels match the constant\n            if line[6].strip().upper() == \"UPDATE SEED\":\n\n                rfam_acc = line[3].strip()\n\n                rfam_acc_list = []\n                if rfam_acc.find(',') == -1:\n                    rfam_acc_list = rfam_acc.split(',')\n                else:\n                    rfam_acc_list = [rfam_acc]\n\n                threshold = 0.0\n                if line[1] != '':\n                    threshold = float(line[1])\n\n                # trim any whitespace characters\n                mirbase_id = line[0].strip()\n\n\n                accession_map[mirbase_id] = {\"rfam_acc\": rfam_acc_list,\n                                              \"threshold\": threshold,\n                                              \"overlap\": float(line[4])}\n                \n    fp.close()\n\n    return accession_map\n\n\n# -------------------------------------------------------------------------------\n\ndef parse_arguments():\n    \"\"\"\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--report\", help=\"miRNA report in .tsv format\", action=\"store\")\n    parser.add_argument(\"--dest-dir\", help=\"Desctination directory\", action=\"store\", default=os.getcwd())\n    parser.add_argument(\"--old-rfam\", help=\"Fetches old Rfam miRNA accessions to be updated\",\n                        action=\"store_true\", default=False)\n    parser.add_argument(\"--create-dump\", help=\"Generates a JSON (.json) dump in destination directory\",\n                        action=\"store_true\", default=False)\n\n    return parser\n\n\n# -------------------------------------------------------------------------------\n\nif __name__ == '__main__':\n\n    parser = parse_arguments()\n    args = parser.parse_args()\n\n    accessions = None\n    if not args.old_rfam:\n        new_mirnas = extract_new_mirnas_from_report(args.report, type='new')\n        accessions = new_mirnas\n\n    else:\n        accessions = extract_rfam_family_accessions(args.report)\n\n    if args.create_dump:\n        filename = \"new_mirnas_\"\n\n        if args.old_rfam:\n            filename = \"mirna_families_to_update_\"\n        fp_out = open(os.path.join(args.dest_dir, filename + str(date.today()) + \".json\"), 'w')\n        json.dump(accessions, fp_out)\n        fp_out.close()", "instruction_en": "This code extracts new miRNAs from a report and can generate a JSON dump. To use it, create a Python script and import the necessary modules (argparse, json, os, and date). Define the function \"extract_new_mirnas_from_report\" with the input \"report_tsv\" and an optional argument \"type\" with a default value of \"new\". This function reads the report file, checks if the candidate miRNA is a new family, skips families requiring review, and adds the new miRNAs to a dictionary. Define the function \"extract_rfam_family_accessions\" with the input \"report_file\". This function reads the report file, checks if the overlap is less than or equal to 100.0, and adds the miRNA accessions to a dictionary. Define the function \"parse_arguments\" with no input. This function creates an ArgumentParser object and adds the necessary arguments. In the main block, parse the arguments using the ArgumentParser object and call the appropriate function based on the arguments. If the \"--create-dump\" argument is provided, generate a JSON dump with the appropriate filename.", "original_line_number": 22219, "instruction_length": 1043}
{"instruction": "Buat kelas yang dipanggil \"Watak\" yang mewakili watak pemain dengan statistik, peralatan, dan maklumat meratakan. Kelas harus mempunyai kaedah __init__ yang memulakan tahap watak, pengalaman_into_level, nama, peralatan_classes, statistik, dan leveling_formula. Ia juga harus mempunyai kaedah yang dipanggil \"LOAD_CHARACTER_FILE\" yang memuat fail JSON aksara dan menetapkan nama watak, kelas peralatan, dan statistik. Akhirnya, kelas harus mempunyai kaedah __Str__ yang mengembalikan perwakilan rentetan sifat -sifat watak. Import yang diperlukan adalah JSON, formula dari caladbolg.Agents, dan statistik dan peralatan dari caladbolg.agents.stats.", "input": "", "output": "import json\r\nfrom caladbolg.agents import formulas\r\nfrom caladbolg.agents.stats import Stats, EquipmentStats\r\n\r\nclass Character:\r\n    \"\"\"\r\n    Represents a player character.\r\n\r\n    Characters have stats, equipment, and leveling information.\r\n    \"\"\"\r\n\r\n    def __init__(self, character_file):\r\n        self.level = 1\r\n        self.experience_into_level = 0\r\n        self.name = None\r\n        self.equipment_classes = None\r\n        self.stats = None\r\n        self.leveling_formula = None\r\n        self.load_character_file(character_file)\r\n\r\n    def load_character_file(self, character_file):\r\n        \"\"\"\r\n        Loads a character JSON file, setting the name, equipment classes, and stats.\r\n\r\n        The ``name`` is naturally the character's name as it should appear in-game. While there's no hard limit, it\r\n        should be kept short to prevent it from being truncated or overlapping.\r\n\r\n        The ``equipment_classes`` is a set of string \"classes\" corresponding to which classes of equipment can\r\n        be equipped. Each piece of equipment has one or more classes. If there's at least one class in common, we can\r\n        equip the piece of equipment. The classes are not mentioned in-game, as the naming is intended to be internal.\r\n\r\n        The ``stats`` is the character's ``Stats`` instance. The equipment stats are not initialized yet as they are\r\n        not known at this time.\r\n\r\n        :param character_file: The file to load the character data from.\r\n        \"\"\"\r\n        character_json = json.load(character_file)\r\n        self.name = character_json['name']\r\n        self.equipment_classes = character_json['equipment_classes']\r\n        self.stats = Stats(character_json['base_stats']['health'],\r\n                           character_json['base_stats']['stamina'],\r\n                           character_json['base_stats']['strength'],\r\n                           character_json['base_stats']['magic'],\r\n                           character_json['base_stats']['endurance'],\r\n                           character_json['base_stats']['agility'])\r\n        try:\r\n            self.leveling_formula = formulas.leveling_formulas[character_json['leveling_formula']]\r\n        except KeyError:\r\n            raise ValueError('No leveling formula named \"{0}\" ({1})'.format(character_json['leveling_formula'],\r\n                             character_file.name)) from None\r\n\r\n    def __str__(self):\r\n        return str(self.__dict__)", "instruction_en": "Create a class called \"Character\" that represents a player character with stats, equipment, and leveling information. The class should have an __init__ method that initializes the character's level, experience_into_level, name, equipment_classes, stats, and leveling_formula. It should also have a method called \"load_character_file\" that loads a character JSON file and sets the character's name, equipment classes, and stats. Finally, the class should have a __str__ method that returns a string representation of the character's attributes. The necessary imports are json, formulas from caladbolg.agents, and Stats and EquipmentStats from caladbolg.agents.stats.", "original_line_number": 22256, "instruction_length": 646}
{"instruction": "Kod ini mentakrifkan kelas yang menjalankan skrip `kelimpahan_estimates_to_matrix.pl on pada gen atau isoforms mengira jadual yang dihasilkan oleh` Align_and_estimate_abundance.pl`. Untuk menggunakan kelas, lulus sama ada `genes.results` atau` isoforms.results` fail dalam slot berikut: `sample_data [<sample>] [\" genes.results \"]` atau `sample_data [<selsel>] [\" isoforms.results \"]`. Kelas mencipta fail berikut dalam slot berikut: `project.counts.matrix` dalam` self.sample_data [\"project_data\"] [\"counts.matrix\"] `,` project.not_cross_norm.fpkm.tmp` dalam `self.sample_data [\" project_data \"] [\" not_cross_norm.fpkm.tmp \"]`, `project.not_cross_norm.fpkm.tmp.tmm_info.txt` dalam `self.sample_data [\" project_data \"] [\" not_cross_norm.fpkm.tmp.tmm_info.txt \"]`, dan `project.tmm.fpkm.matrix` dalam` self.sample_data [\" Kelas memerlukan skrip `kelimpahan_estimates_to_matrix.pl` dan parameter` --gene_trans_map`.", "input": "", "output": "# -*- coding: UTF-8 -*-\n\"\"\" \n``trinity_statistics``\n-----------------------------------------------------------------\n:Authors: Menachem Sklarz\n:Affiliation: Bioinformatics core facility\n:Organization: National Institute of Biotechnology in the Negev, Ben Gurion University.\n\nA class that defines a module for running ``abundance_estimates_to_matrix.pl`` on genes or isoforms counts tables produced by ``align_and_estimate_abundance.pl``\n\nSee the script documentation `here <https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification#building-expression-matrices>`_.\n\nThis conversion makes sense at the project level - combining all sample matrices into a single, normalized, comparison table. However, for completeness, we included a sample scope option for running the script in each sample separately.\n\n.. Note:: ``scope`` is not defined for this module. It only makes sense to run ``abundance_estimates_to_matrix`` when comparing many samples against a single assembly\n\nRequires\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    \n* Either ``genes.results`` or ``isoforms.results`` files in the following slots:\n        \n        * ``sample_data[<sample>][\"genes.results\"]``\n        * ``sample_data[<sample>][\"isoforms.results\"]``\n\n    \nOutput:\n~~~~~~~~~~~~~\n\n* Creates the following files in the following slots:\n    \n        * ``<project>.counts.matrix``                        in ``self.sample_data[\"project_data\"][\"counts.matrix\"]``\n        * ``<project>.not_cross_norm.fpkm.tmp``              in ``self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp\"]``\n        * ``<project>.not_cross_norm.fpkm.tmp.TMM_info.txt`` in ``self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp.TMM_info.txt\"]``\n        * ``<project>.TMM.fpkm.matrix``                      in ``self.sample_data[\"project_data\"][\"TMM.fpkm.matrix\"]``\n\n        \nParameters that can be set        \n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. csv-table:: \n    :header: \"Parameter\", \"Values\", \"Comments\"\n\n    \"use_genes\", \"\", \"Use 'genes.results' matrix. If not passed, use 'isoforms.results'\"\n    \"redirects: --gene_trans_map\", \"path or 'none'\", \"If path, use path as gene_trans_map for all samples. If 'none', does not produce gene level estimates. **In order to use an internal gene_trans_map, do not pass this parameter!**\"\n    \n    \nLines for parameter file\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n\n    trin_map_stats:\n        module:             trinity_statistics\n        base:               trin_map1\n        script_path:        /path/to/abundance_estimates_to_matrix.pl\n        use_genes:       \n        redirects:\n            --est_method:   RSEM\n\nReferences\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGrabherr, M.G., Haas, B.J., Yassour, M., Levin, J.Z., Thompson, D.A., Amit, I., Adiconis, X., Fan, L., Raychowdhury, R., Zeng, Q. and Chen, Z., 2011. **Trinity: reconstructing a full-length transcriptome without a genome from RNA-Seq data**. *Nature biotechnology*, 29(7), p.644.\n\n\"\"\"\n\n\n\nimport os\nimport sys\nimport re\nfrom neatseq_flow.PLC_step import Step,AssertionExcept\n\n\n__author__ = \"Menachem Sklarz\"\n__version__ = \"1.6.0\"\n\n\nclass Step_trinity_statistics(Step):\n    \n    def step_specific_init(self):\n        self.shell = \"bash\"      # Can be set to \"bash\" by inheriting instances\n        self.file_tag = \"trin_stats\"\n        \n        if \"use_genes\" not in self.params:\n            self.write_warning(\"'use_genes' not passed. Using 'isoforms.results' matrix\")\n\n        \n    def step_sample_initiation(self):\n        \"\"\" A place to do initiation stages following setting of sample_data\n            Here you should do testing for dependency output. These will NOT exist at initiation of this instance. They are set only following sample_data updating\n        \"\"\"\n\n        # In new version, --gene_trans_map is compulsory! Adding\n        # If not passed:\n            # If one exists, use it. \n            # Otherwise, specify \"none\"\n        # If passed:\n            # If with value, use the value and set project \"gene_trans_map\" to value\n            # Otherwise, use existing\n        if \"--gene_trans_map\" not in self.params[\"redir_params\"]:\n            if \"gene_trans_map\" in self.sample_data[\"project_data\"]:\n                self.params[\"redir_params\"][\"--gene_trans_map\"] = self.sample_data[\"project_data\"][\"gene_trans_map\"]\n                self.use_gene_trans_map = True\n            else:\n                self.params[\"redir_params\"][\"--gene_trans_map\"] = \"none\"\n                self.use_gene_trans_map = False\n                    \n        else:  # --gene_trans_map is defined in redir_params\n            if self.params[\"redir_params\"][\"--gene_trans_map\"] == None:\n                raise AssertionExcept(\"You passed --gene_trans_map with no value. Please specify path or 'none'\")\n            elif self.params[\"redir_params\"][\"--gene_trans_map\"] == \"none\":\n                self.use_gene_trans_map = False\n            else:\n                self.sample_data[\"project_data\"][\"gene_trans_map\"] = self.params[\"redir_params\"][\"--gene_trans_map\"]\n                self.use_gene_trans_map = True\n            \n                \n            \n        \n        \n        \n    def create_spec_wrapping_up_script(self):\n        \"\"\" Add stuff to check and agglomerate the output data\n        \"\"\"\n        \n\n    def build_scripts(self):\n        \n\n        # Name of specific script:\n        self.spec_script_name = self.set_spec_script_name()\n\n        self.script = \"\"\n\n        # This line should be left before every new script. It sees to local issues.\n        # Use the dir it returns as the base_dir for this step.\n        use_dir = self.local_start(self.base_dir)\n\n        \n        prefix = self.sample_data[\"Title\"]\n\n        self.script += self.get_script_const()\n\n        self.script += \"--out_prefix %s \\\\\\n\\t\" % os.sep.join([use_dir, prefix])\n        # type2use is 'genes.results' or 'isoforms.results'. This is used to then select the correct slot from \"mapping\"\n        type2use = \"genes.results\" if \"use_genes\" in list(self.params.keys()) else \"isoforms.results\"\n        \n        for sample in self.sample_data[\"samples\"]:\n            try:\n                self.script += \"%s \\\\\\n\\t\" % self.sample_data[sample][type2use] \n            except:\n                raise AssertionExcept(\"file type %s does not exist for sample.\" % type2use, sample)\n        \n        self.script = self.script.rstrip(\"\\\\\\n\\t\")\n        self.script += \"\\n\\n\"\n        \n        \n        if not \"version\" in self.params or self.params[\"version\"].lower() == \"new\":\n\n        # Storing all output files even though probably not very useful downstream...\n            self.sample_data[\"project_data\"][\"isoform.raw_counts\"] = os.sep.join([self.base_dir,  \"%s.isoform.counts.matrix\" % prefix])\n            self.sample_data[\"project_data\"][\"isoform.norm_counts\"] = os.sep.join([self.base_dir, \"%s.isoform.TPM.not_cross_norm\" % prefix])\n            \n            self.stamp_file(self.sample_data[\"project_data\"][\"isoform.raw_counts\"] )\n            self.stamp_file(self.sample_data[\"project_data\"][\"isoform.norm_counts\"])\n\n            if(self.use_gene_trans_map):  # True when --gene_trans_map is not \"none\"\n                self.sample_data[\"project_data\"][\"gene.raw_counts\"] = os.sep.join([self.base_dir,  \"%s.gene.counts.matrix\" % prefix])\n                self.sample_data[\"project_data\"][\"gene.norm_counts\"] = os.sep.join([self.base_dir, \"%s.gene.TPM.not_cross_norm\" % prefix])\n                self.stamp_file(self.sample_data[\"project_data\"][\"gene.raw_counts\"] )\n                self.stamp_file(self.sample_data[\"project_data\"][\"gene.norm_counts\"])\n\n        else:\n            self.write_warning(\"Not storing output files for old version of trinity. \"\n                               \"If required, load the appropriate files with a 'manage_types' module\")\n\n       \n        # Move all files from temporary local dir to permanent base_dir\n        # Sees to copying local files to final destination (and other stuff)\n        self.local_finish(use_dir,self.base_dir)\n\n        self.create_low_level_script()", "instruction_en": "The code defines a class that runs the `abundance_estimates_to_matrix.pl` script on genes or isoforms counts tables produced by `align_and_estimate_abundance.pl`. To use the class, pass either `genes.results` or `isoforms.results` files in the following slots: `sample_data[<sample>][\"genes.results\"]` or `sample_data[<sample>][\"isoforms.results\"]`. The class creates the following files in the following slots: `project.counts.matrix` in `self.sample_data[\"project_data\"][\"counts.matrix\"]`, `project.not_cross_norm.fpkm.tmp` in `self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp\"]`, `project.not_cross_norm.fpkm.tmp.TMM_info.txt` in `self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp.TMM_info.txt\"]`, and `project.TMM.fpkm.matrix` in `self.sample_data[\"project_data\"][\"TMM.fpkm.matrix\"]`. The class requires the `abundance_estimates_to_matrix.pl` script and the `--gene_trans_map` parameter.", "original_line_number": 22274, "instruction_length": 913}
{"instruction": "Buat kelas TextInterface dengan kaedah untuk mendapatkan input pengguna dan memaparkan hasil carian. Kaedah GetUserInput mendorong pengguna untuk menentukan arah perjalanan dan stesen di mana mereka mahu mencari kereta api. Kaedah ShowApproachingTrains mengambil 2 argumen, stesen di mana pengguna mencari kereta api dan senarai kereta api yang kini menghampiri stesen itu. Kaedah ShowStationSearchResults mengambil 1 hujah, senarai hasil stesen yang mungkin. Kaedah AceonOrquit meminta pengguna sama ada mereka mahu melakukan carian baru atau berhenti program.", "input": "", "output": "import sys\n\nclass TextInterface:\n    \"\"\"Text-based interface that contains methods for getting input from the user and displaying search\n    results.\n    \"\"\"\n    \n    def __init__(self, trains_list):\n        \"\"\"Initializes the TextInterface object with a list of all train objects in the simulation.\n        \"\"\"\n        self.trains = trains_list\n    \n    def getUserInput(self):\n        \"\"\"This method prompts the user to specify a direction of travel and a station at which\n        they want to find trains. If a user types a direction other than n or s, the program will\n        ask them to provide a valid input before continuing.\n        \"\"\"\n        print \"\\nWelcome! This program lets you search for New York City subway trains running on the 1, 2, 3, 4, 5, 6, or S lines.\"\n        print \"Note that for S trains, northbound is eastbound in real life and southbound is westbound in real life.\"\n        print \"\\nFirst, choose a direction - northbound or southbound. Type n for northbound or s for southbound.\"\n        # valid_input will remain False until either n or s is typed\n        valid_input = False\n        while valid_input == False:\n            direction = raw_input()\n            if direction == \"n\":\n                valid_input = True\n                direction = 'northbound'\n            elif direction == 's':\n                valid_input = True\n                direction = 'southbound'\n            # If you really don't like our program, you can quit by typing q\n            elif direction == 'q':\n                sys.exit()\n            else:\n                print \"We didn't understand that. Please try again.\"\n        print \"\\nNow, search for the station you want trains from.\"\n        station = raw_input()\n        return direction, station\n    \n    def showApproachingTrains(self, station, list_of_trains):\n        \"\"\"Takes 2 arguments, the station at which the user is looking for trains and a list of\n        trains currently approaching that station, where each item in the list is formatted\n        [train_index, stop_number, arrival_time]. If the list is empty, it informs the user that\n        no trains are near the station. Otherwise, it looks up information about each train in\n        the list and displays it to the user.\n        \"\"\"\n        print \"...\"\n        if len(list_of_trains) == 0:\n            print \"Sorry, there aren't any trains currently approaching\", station\n        else:\n            print \"Here is a list of trains arriving at or departing from\", station, \"in the next 30 minutes:\\n\"\n            for train_list in list_of_trains:\n                train_number = train_list[0] # Used to look up train object in the master list of trains\n                stop_number = train_list[1]\n                if int(self.trains[train_number].getArrivalTime(stop_number)) <= 30:\n                    self.trains[train_number].showInfo(stop_number)\n        print \"\"\n    \n    def showStationSearchResults(self, results_list):\n        \"\"\"Takes 1 argument, a list of possible station results. If there is only one possible\n        result, this function will never be called, so it only has to handle list of length 0 or >1.\n        If the length of the list is 0, the program will ask the user whether they want to do\n        another search or quit. Otherwise, all possible results will be displayed next to a unique\n        integer, and the user will be asked to type in an integer to choose the station they want.\n        \"\"\"\n        print \"...\"\n        if len(results_list) == 0:\n            print \"Sorry, we couldn't find a station with that name.\\n\"\n            self.againOrQuit()\n        else:\n            print \"We found several stations with that name. Please choose one from the list below.\"\n            for i in range(len(results_list)):\n                print (i+1), ': ', results_list[i]\n            choice = int(raw_input(\"Type the number of the station you want: \"))\n            return results_list[choice-1]\n    \n    def againOrQuit(self):\n        \"\"\"Asks the user whether they want to perform a new search or quit the program.\n        \"\"\"\n        print \"Type n to do a new search or q to exit the program.\"\n        choice = raw_input()\n        if choice == \"n\":\n            return True\n        if choice == \"q\":\n            return False\n        else:\n            print \"We didn't understand that. Please try again.\"\n            return self.againOrQuit()", "instruction_en": "Create a TextInterface class with methods for getting user input and displaying search results. The getUserInput method prompts the user to specify a direction of travel and a station at which they want to find trains. The showApproachingTrains method takes 2 arguments, the station at which the user is looking for trains and a list of trains currently approaching that station. The showStationSearchResults method takes 1 argument, a list of possible station results. The againOrQuit method asks the user whether they want to perform a new search or quit the program.", "original_line_number": 22307, "instruction_length": 561}
{"instruction": "Kod ini adalah plugin untuk Flexget yang membolehkan URL menulis semula dan mencari di laman web Isohunt. Untuk menggunakannya, mengimport modul yang diperlukan dan mendaftarkan plugin menggunakan kod yang disediakan. Plugin ini menerima parameter kategori dan menyokong kategori berikut: Misc, Filem, Audio, TV, Permainan, Aplikasi, Pics, Anime, Komik, Buku, Video Muzik, Tidak Dikelaskan, dan Semua. Plugin boleh menulis semula URL dan mencari penyertaan berdasarkan kategori yang disediakan dan rentetan carian.", "input": "", "output": "from __future__ import unicode_literals, division, absolute_import\nfrom builtins import *  # pylint: disable=unused-import, redefined-builtin\nfrom future.moves.urllib.parse import quote\n\nimport logging\nimport re\n\nimport feedparser\n\nfrom flexget import plugin\nfrom flexget.entry import Entry\nfrom flexget.event import event\nfrom flexget.utils.search import torrent_availability, normalize_unicode\n\nlog = logging.getLogger('isohunt')\n\n\nclass UrlRewriteIsoHunt(object):\n    \"\"\"IsoHunt urlrewriter and search plugin.\n\n    should accept:\n    isohunt: <category>\n\n      categories:\n      empty or -1: All\n      0 : Misc.\n      1 : Video/Movies\n      2 : Audio\n      3 : TV\n      4 : Games\n      5 : Apps\n      6 : Pics\n      7 : Anime\n      8 : Comics\n      9 : Books\n      10: Music Video\n      11: Unclassified\n      12: ALL\n    \"\"\"\n\n    schema = {\n        'type': 'string',\n        'enum': ['misc', 'movies', 'audio', 'tv', 'games', 'apps', 'pics', 'anime', 'comics', 'books', 'music video',\n                 'unclassified', 'all']\n    }\n\n    def url_rewritable(self, task, entry):\n        url = entry['url']\n        # search is not supported\n        if url.startswith('http://isohunt.com/torrents/?ihq='):\n            return False\n        # not replaceable\n        if 'torrent_details' not in url:\n            return False\n        return url.startswith('http://isohunt.com') and url.find('download') == -1\n\n    def url_rewrite(self, task, entry):\n        entry['url'] = entry['url'].replace('torrent_details', 'download')\n\n    def search(self, task, entry, config):\n        # urllib.quote will crash if the unicode string has non ascii characters, so encode in utf-8 beforehand\n        optionlist = ['misc', 'movies', 'audio', 'tv', 'games', 'apps', 'pics', 'anime', 'comics', 'books',\n                      'music video', 'unclassified', 'all']\n        entries = set()\n        search_strings = [normalize_unicode(s) for s in entry.get('search_strings', [entry['title']])]\n        for search_string in search_strings:\n            url = 'http://isohunt.com/js/rss/%s?iht=%s&noSL' % (\n                quote(search_string.encode('utf-8')), optionlist.index(config))\n\n            log.debug('requesting: %s' % url)\n            rss = feedparser.parse(url)\n\n            status = rss.get('status', False)\n            if status != 200:\n                raise plugin.PluginWarning('Search result not 200 (OK), received %s' % status)\n\n            ex = rss.get('bozo_exception', False)\n            if ex:\n                raise plugin.PluginWarning('Got bozo_exception (bad feed)')\n\n            for item in rss.entries:\n                entry = Entry()\n                entry['title'] = item.title\n                entry['url'] = item.link\n\n                m = re.search(r'Size: ([\\d]+).*Seeds: (\\d+).*Leechers: (\\d+)', item.description, re.IGNORECASE)\n                if not m:\n                    log.debug('regexp did not find seeds / peer data')\n                    continue\n                else:\n                    log.debug('regexp found size(%s), Seeds(%s) and Leeches(%s)' % (m.group(1), m.group(2), m.group(3)))\n\n                    entry['content_size'] = int(m.group(1))\n                    entry['torrent_seeds'] = int(m.group(2))\n                    entry['torrent_leeches'] = int(m.group(3))\n                    entry['search_sort'] = torrent_availability(entry['torrent_seeds'], entry['torrent_leeches'])\n\n                entries.add(entry)\n\n        return entries\n\n\n@event('plugin.register')\ndef register_plugin():\n    plugin.register(UrlRewriteIsoHunt, 'isohunt', groups=['urlrewriter', 'search'], api_ver=2)", "instruction_en": "This code is a plugin for FlexGet that allows for URL rewriting and searching on the IsoHunt website. To use it, import the necessary modules and register the plugin using the provided code. The plugin accepts a category parameter and supports the following categories: misc, movies, audio, tv, games, apps, pics, anime, comics, books, music video, unclassified, and all. The plugin can rewrite URLs and search for entries based on the provided category and search string.", "original_line_number": 22362, "instruction_length": 514}
{"instruction": "Kod ini adalah pelaksanaan program percetakan imej berdasarkan haftoning. Untuk menggunakannya, terlebih dahulu mengimport perpustakaan yang diperlukan: sys, numpy, scipy, matplotlib.pyplot, dan dft dari proj04-01. Kemudian tentukan fungsi berikut: en_padding, de_padding, shift, sqdistance, lowpass_mask, dan highpass_mask. Akhirnya, hubungi fungsi utama untuk menjalankan program. Fungsi utama dibaca dalam fail imej, menggunakan padding dan beralih, melakukan DFT, menggunakan penapisan Highpass dengan frekuensi pemotongan yang berbeza, dan menjimatkan imej yang dihasilkan.", "input": "", "output": "#!/bin/python\n# *-* encoding=utf-8 *-*\n'''\nImage Priting Program Based on Haftoning\n'''\n\nimport sys\nimport numpy, scipy\nfrom scipy import ndimage\nfrom scipy import misc\nimport scipy.fftpack as fftpack\nimport matplotlib.pyplot as plt\n\nsys.path.append('../Proj04-01')\n\nfrom DFT import DFT_2D, IDFT_2D\n\ndef en_padding(img):\n    M, N = img.shape\n    P, Q = 2 * M, 2 * N\n    _img = numpy.zeros(P*Q).reshape((P, Q))\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y]\n    return _img\n\ndef de_padding(img):\n    P, Q = img.shape\n    M, N = P/2, Q/2\n    _img = numpy.zeros(M*N).reshape((M, N))\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y]\n    return _img\n\ndef shift(img):\n    M, N = img.shape\n    _img = img.copy()\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y] * numpy.power(-1.0, (x+y))\n    return _img\n\ndef sqdistance(p1, p2):\n    return ((p1[0]-p2[0])*(p1[0]-p2[0])) + \\\n           ((p1[1]-p2[1])*(p1[1]-p2[1]))\n\ndef lowpass_mask(P, Q, cuf_off_frequency):\n    center = (P/2.0, Q/2.0)\n    mask = numpy.zeros(P * Q).reshape(P, Q)\n    for u in range(P):\n        for v in range(Q):\n            mask[u][v] = numpy.exp(-1*sqdistance(center, (u, v)) / (2*(cuf_off_frequency*cuf_off_frequency)))\n    return mask\n\ndef highpass_mask(P, Q, cuf_off_frequency):\n    return 1.0 - lowpass_mask(P, Q, cuf_off_frequency)\n#    center = (P/2.0, Q/2.0)\n#    mask = numpy.zeros(P * Q).reshape(P, Q)\n #  for u in range(P):\n#        for v in range(Q):\n#            mask[u][v] = 1.0-numpy.exp(-1*sqdistance(center, (u, v)) / (2*(cuf_off_frequency*cuf_off_frequency)))\n#    return mask\n\ndef main():\n    img_file = 'Fig0441(a)(characters_test_pattern).tif'\n    img = misc.imread(img_file)\n\n    padding_img = en_padding(img)\n\n    padding_img = shift(padding_img)\n\n    dft_img = DFT_2D(padding_img)\n\n    for cut_off_frequency in [30, 60, 160]:\n        print cut_off_frequency\n        hp_mask = highpass_mask(dft_img.shape[0], dft_img.shape[1], cut_off_frequency)\n        \n        misc.imsave('%s_hpmask_%d.tif' % (img_file, cut_off_frequency), 255 * hp_mask)\n        hp_img  = numpy.multiply(dft_img, hp_mask)\n\n        misc.imsave('%s_fft_%d.tif' % (img_file, cut_off_frequency), numpy.log(1+numpy.abs(hp_img)))\n     \n        hp_idtft_img = shift(IDFT_2D(hp_img).real)\n        hp_idtft_img = de_padding(hp_idtft_img)\n        print hp_idtft_img.shape\n        misc.imsave('%s_hp_%d.tif' % (img_file, cut_off_frequency), hp_idtft_img)\n\nif __name__ == '__main__':\n    main()", "instruction_en": "This code is an implementation of an image printing program based on haftoning. To use it, first import the necessary libraries: sys, numpy, scipy, matplotlib.pyplot, and DFT from Proj04-01. Then define the following functions: en_padding, de_padding, shift, sqdistance, lowpass_mask, and highpass_mask. Finally, call the main function to run the program. The main function reads in an image file, applies padding and shifting, performs a DFT, applies highpass filtering with different cut-off frequencies, and saves the resulting images.", "original_line_number": 22419, "instruction_length": 578}
{"instruction": "Buat kelas Moviedataeditor yang mewarisi dari WX.Dialog. Kelas harus mempunyai kaedah untuk memulakan kawalan, menetapkan dan mendapatkan data, dan mengendalikan peristiwa. Kawalan hendaklah termasuk tetingkap tatal, kawalan teks, kawalan teks multiline, dan beberapa kawalan fielddatalist. Kelas juga harus mempunyai bar alat dengan butang untuk mendapatkan metadata dari IMDB. Apabila butang diklik, kelas harus memulakan benang baru untuk mendapatkan metadata dan mengemas kini kawalan dengan hasilnya. Kelas juga harus mempunyai butang simpan yang memeriksa jika medan tajuk kosong sebelum menutup dialog.", "input": "", "output": "#\n#\tportable-movie-organizer\n#\t\n#\tCopyright (c) 2010 Ali Aafee\n#\t\n#\tThis file is part of portable-movie-organizer.\n#\n#\tportable-movie-organizer is free software: you can redistribute it \n#\tand/or modify it under the terms of the GNU General Public License \n#\tas published by the Free Software Foundation, either version 3 of \n#\tthe License, or (at your option) any later version.\n#\n#\tportable-movie-organizer is distributed in the hope that it will \n#\tbe useful, but WITHOUT ANY WARRANTY; without even the implied \n#\twarranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  \n#\tSee the GNU General Public License for more details.\n#\n#\tYou should have received a copy of the GNU General Public License\n#\talong with portable-movie-organizer.  \n#\tIf not, see <http://www.gnu.org/licenses/>.\n\nimport wx\nimport FieldDataList\nimport ImdbAPI\nimport os.path\nimport thread\n\n\ndirName = os.path.dirname(os.path.abspath(__file__))\ndirName, fileName = os.path.split(dirName)\nresDir = os.path.join(dirName, 'res')\n\nclass MovieDataEditor(wx.Dialog):\n\tdef __init__(self, parent, postersPath, catchPath, title='Edit Movie Metadata'):\n\t\tself.title = title\n\t\tself.postersPath = postersPath\n\t\tself.catchPath = catchPath\n\t\t\n\t\tself._init_ctrls(parent)\n\t\t\n\t\n\tdef _init_ctrls(self, parent):\n\t\twx.Dialog.__init__(self, name='MovieEditor', parent=parent,\n\t\t\tstyle=wx.DEFAULT_DIALOG_STYLE | wx.RESIZE_BORDER,\n\t\t\ttitle=self.title, size=wx.Size(640,480))\n\t\t\t\n\t\tself.fieldWindow = wx.ScrolledWindow(self, size=wx.Size(200,200), style=wx.HSCROLL)\n\t\tself.fieldWindow.SetScrollbars(0,10,0,65)\n\t\t\n\t\tgridSizer = wx.FlexGridSizer(7,4,10,10)\n\t\tgridSizer.AddGrowableCol(1,1)\n\t\tgridSizer.AddGrowableCol(3,1)\n\t\t\n\t\tlabelWidth = -1\n\t\t\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\t\n\t\tself.lblTitle = wx.StaticText(self.fieldWindow, label='Title', size=wx.Size(labelWidth,-1))\n\t\tself.txtTitle = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblTitle, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtTitle, 1, wx.EXPAND)\n\t\t\n\t\tself.lblSort = wx.StaticText(self.fieldWindow, label='Sort', size=wx.Size(labelWidth,-1))\n\t\tself.txtSort = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblSort, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtSort, 1, wx.EXPAND)\n\t\t\n\t\tself.lblImage = wx.StaticText(self.fieldWindow, label='Poster', size=wx.Size(labelWidth,-1))\n\t\tself.txtImage = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblImage, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtImage, 1, wx.EXPAND)\n\t\t\t\n\t\tself.lblReleased = wx.StaticText(self.fieldWindow, label='Released', size=wx.Size(labelWidth,-1))\n\t\tself.txtReleased = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblReleased, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtReleased, 1, wx.EXPAND)\n\t\t\n\t\tself.lblRuntime = wx.StaticText(self.fieldWindow, label='Runtime', size=wx.Size(labelWidth,-1))\n\t\tself.txtRuntime = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblRuntime, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtRuntime, 1, wx.EXPAND)\n\t\t\n\t\tself.lblRated = wx.StaticText(self.fieldWindow, label='Rated', size=wx.Size(labelWidth,-1))\n\t\tself.txtRated = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblRated, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtRated, 1, wx.EXPAND)\n\t\t\n\t\tself.lblSummary = wx.StaticText(self.fieldWindow, label='Summary', size=wx.Size(labelWidth,-1))\n\t\tself.txtSummary = wx.TextCtrl(self.fieldWindow, style=wx.TE_MULTILINE, size=wx.Size(-1,80))\n\t\tgridSizer.Add(self.lblSummary, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtSummary, 1, wx.EXPAND)\n\t\t\n\t\tself.lblGenres = wx.StaticText(self.fieldWindow, label='Genres', size=wx.Size(labelWidth,-1))\n\t\tself.lstGenres = FieldDataList.FieldDataList(self.fieldWindow, size=wx.Size(-1,100))\n\t\tgridSizer.Add(self.lblGenres, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstGenres, 1, wx.EXPAND)\n\t\t\n\t\tself.lblActors = wx.StaticText(self.fieldWindow, label='Actors', size=wx.Size(labelWidth,-1))\n\t\tself.lstActors = FieldDataList.FieldDataList(self.fieldWindow, size=wx.Size(-1,100))\n\t\tgridSizer.Add(self.lblActors, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstActors, 1, wx.EXPAND)\n\t\t\n\t\tself.lblDirectors = wx.StaticText(self.fieldWindow, label='Directors', size=wx.Size(labelWidth,-1))\n\t\tself.lstDirectors = FieldDataList.FieldDataList(self.fieldWindow)\n\t\tgridSizer.Add(self.lblDirectors, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstDirectors, 1, wx.EXPAND)\n\t\t\n\t\tself.lblFiles = wx.StaticText(self.fieldWindow, label='Files', size=wx.Size(labelWidth,-1))\n\t\tself.lstFiles = FieldDataList.FieldDataList(self.fieldWindow)\n\t\tgridSizer.Add(self.lblFiles, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstFiles, 1, wx.EXPAND)\n\t\t\n\t\tgridSizer.Add(wx.StaticText(self.fieldWindow, label=''))\n\t\t\n\t\tself.fieldWindow.SetSizer(gridSizer)\n\t\tself.fieldWindow.Layout()\n\t\t\n\t\tself.btnSizer = self.CreateButtonSizer(wx.CANCEL)\n\t\t\n\t\tself.btnSave = wx.Button(self, label=\"Save\")\n\t\tself.btnSave.Bind(wx.EVT_BUTTON, self.OnSave)\n\t\tself.btnSizer.Add(self.btnSave)\n\t\t\n\t\tself.mainTb = self._create_main_tb(self)\n\t\t\n\t\tvbox = wx.BoxSizer(wx.VERTICAL)\n\t\tvbox.Add(self.mainTb, 0, wx.ALL | wx.ALIGN_LEFT | wx.EXPAND, 0 )\n\t\tvbox.Add(self.fieldWindow, 1, wx.EXPAND)\n\t\tvbox.Add(wx.StaticText(self,label=\"\"))\n\t\tvbox.Add(self.btnSizer, 0, wx.ALIGN_CENTER)\n\t\t\n\t\tself.SetSizer(vbox)\n\t\tself.Layout()\n\t\n\t\t\n\tdef _create_main_tb(self, parent):\n\t\ttb = wx.ToolBar(parent, style=wx.TB_TEXT|wx.TB_NODIVIDER|wx.TB_HORIZONTAL|wx.TB_FLAT)\n\t\ttb.SetToolBitmapSize((21, 21))\n\t\t\n\t\tself.tb_search = wx.NewId()\n\t\ttb.DoAddTool(\n\t\t\tbitmap=wx.Bitmap(os.path.join(resDir,'web.png'), wx.BITMAP_TYPE_PNG),\n\t\t\t#bitmap=wx.ArtProvider.GetBitmap(wx.ART_FIND),\n\t\t\tbmpDisabled=wx.NullBitmap, \n\t\t\tid=self.tb_search,\n\t\t\tkind=wx.ITEM_NORMAL, \n\t\t\tlabel='', \n\t\t\tlongHelp='', \n\t\t\tshortHelp='Get Metadata from IMDB')\n\t\tself.Bind(wx.EVT_TOOL, self.OnGetMetadata,\n\t\t\tid=self.tb_search)\n\t\t\t\n\t\tself.statusText = wx.StaticText(tb, label=\"\")\n\t\t\n\t\ttb.AddControl(self.statusText)\n\n\t\ttb.Realize()\n\t\t\n\t\treturn tb\n\t\t\n\t\t\n\tdef SetData(self, data):\n\t\tself.txtTitle.SetValue(data['title'])\n\t\tself.txtSort.SetValue(data['sort'])\n\t\tself.txtImage.SetValue(data['image'])\n\t\tself.txtReleased.SetValue(data['released'])\n\t\tself.txtRuntime.SetValue(data['runtime'])\n\t\tself.txtRated.SetValue(data['rated'])\n\t\tself.txtSummary.SetValue(data['summary'])\n\n\t\tself.lstGenres.DeleteAllItems()\n\t\tself.lstGenres.AddValues(data['genres'])\n\n\t\tself.lstActors.DeleteAllItems()\n\t\tself.lstActors.AddValues(data['actors'])\n\n\t\tself.lstDirectors.DeleteAllItems()\n\t\tself.lstDirectors.AddValues(data['directors'])\n\n\t\tself.lstFiles.DeleteAllItems()\n\t\tself.lstFiles.AddValues(data['files'])\n\t\t \n\t\t\n\t\t\n\tdef GetData(self):\n\t\tdata = {}\n\t\tdata['title'] = self.txtTitle.GetValue()\n\t\tdata['sort'] = self.txtSort.GetValue()\n\t\tdata['image'] = self.txtImage.GetValue()\n\t\tdata['released'] = self.txtReleased.GetValue()\n\t\tdata['runtime'] = self.txtRuntime.GetValue()\n\t\tdata['rated'] = self.txtRated.GetValue()\n\t\tdata['summary'] = self.txtSummary.GetValue()\n\t\t\n\t\tdata['genres'] = self.lstGenres.GetValues()\n\t\tdata['actors'] = self.lstActors.GetValues()\n\t\tdata['directors'] = self.lstDirectors.GetValues()\n\t\tdata['files'] = self.lstFiles.GetValues()\n\t\t\n\t\treturn data\n\t\t\n\t\t\n\tdef OnSave(self, event):\n\t\tif self.txtTitle.GetValue() == '':\n\t\t\tmsg = wx.MessageDialog(self, \n\t\t\t\t'Movie metadata cannot be saved without a Title. Cannot continue', \n\t\t\t\t'Movie Title Missing', wx.OK|wx.ICON_INFORMATION)\n\t\t\tmsg.ShowModal()\n\t\t\tmsg.Destroy()\n\t\telse:\n\t\t\tself.EndModal(wx.ID_OK)\n\t\t\n\t\t\n\tdef OnGetMetadata(self, event):\n\t\ttitle = self.txtTitle.GetValue()\n\t\tyear = self.txtReleased.GetValue()\n\t\t\n\t\tif title=='':\n\t\t\tdlg = wx.MessageDialog(self,\n\t\t\t\t\"Enter the title of the movie. Optionally enter the year(approximate).\", \n\t\t\t\t\"Get metadata from IMDB\",\n\t\t\t\twx.OK|wx.ICON_INFORMATION)\n\t\t\tresult = dlg.ShowModal()\n\t\t\tdlg.Destroy()\n\t\t\treturn\n\t\t\n\t\tself.mainTb.EnableTool(self.tb_search, False)\n\t\t\n\t\tself.statusText.SetLabel(\"Getting metadata from IMDB...\")\n\t\t\n\t\tthread.start_new_thread(self._get_metadata, (title, year, self.postersPath, self.catchPath))\n\t\t\n\t\n\tdef _get_metadata(self, title, year, postersPath, catchPath):\n\t\ttry:\n\t\t\tmetadata = ImdbAPI.GetMetadata(title, year, postersPath, catchPath)\n\t\t\n\t\t\twx.CallAfter(self._done_get_metadata, metadata)\n\t\texcept wx._core.PyDeadObjectError, e:\n\t\t\tprint \"dialog closed before thread could complete\"\n\t\t\n\t\n\tdef _done_get_metadata(self, metadata):\n\t\tself.statusText.SetLabel(\"\")\n\t\t\n\t\tif metadata != None:\n\t\t\tprint \"Success\"\n\t\t\t\n\t\t\tself.txtTitle.SetValue(metadata['title'])\n\t\t\tself.txtImage.SetValue(metadata['image'])\n\t\t\tself.txtReleased.SetValue(metadata['released'])\n\t\t\tself.txtRuntime.SetValue(metadata['runtime'])\n\t\t\tself.txtRated.SetValue(metadata['rated'])\n\t\t\tself.txtSummary.SetValue(metadata['summary'])\n\t\t\n\t\t\tprint \"Genres\"\n\t\t\tself.lstGenres.DeleteAllItems()\n\t\t\tself.lstGenres.AddValuesSimple(metadata['genres'])\n\n\t\t\tprint \"Actors\"\n\t\t\tself.lstActors.DeleteAllItems()\n\t\t\tself.lstActors.AddValuesSimple(metadata['actors'])\n\n\t\t\tprint \"Directors\"\n\t\t\tself.lstDirectors.DeleteAllItems()\n\t\t\tself.lstDirectors.AddValuesSimple(metadata['directors'])\n\t\telse:\n\t\t\tdlg = wx.MessageDialog(self,\n\t\t\t\t\"No results were found for the given title and year. (this may be due to a network error)\", \n\t\t\t\t\"Get metadata from IMDB\",\n\t\t\t\twx.OK|wx.ICON_INFORMATION)\n\t\t\tresult = dlg.ShowModal()\n\t\t\tdlg.Destroy()\n\t\t\n\t\tself.mainTb.EnableTool(self.tb_search, True)", "instruction_en": "Create a MovieDataEditor class that inherits from wx.Dialog. The class should have methods to initialize controls, set and get data, and handle events. The controls should include a scrolled window, text controls, a multiline text control, and several FieldDataList controls. The class should also have a toolbar with a button to get metadata from IMDB. When the button is clicked, the class should start a new thread to get the metadata and update the controls with the results. The class should also have a Save button that checks if the Title field is empty before closing the dialog.", "original_line_number": 22533, "instruction_length": 609}
{"instruction": "Kod ini mentakrifkan kelas enum yang dipanggil Rollstate dengan lima negeri yang mungkin, dan keadaan lalai NO_Roll. Ia juga mentakrifkan kamus penjelasan untuk setiap negeri, dan beberapa fungsi untuk bekerja dengan objek rollstate, termasuk memeriksa jika keadaan adalah keadaan roll terpaksa, menerangkan objek rollstate sebagai rentetan, dan menentukan peralihan rollstate yang dibenarkan berdasarkan keadaan semasa dan kedudukan harga. Untuk menggunakan kod ini, import modul yang diperlukan dan buat objek rollstate seperti yang diperlukan, kemudian hubungi fungsi yang disediakan untuk berfungsi dengan mereka.", "input": "", "output": "from enum import Enum\nfrom syscore.objects import named_object\n\nRollState = Enum(\"RollState\",(\n    \"No_Roll\",\n    \"Passive\",\n    \"Force\",\n    \"Force_Outright\",\n    \"Roll_Adjusted\"))\n\ndefault_state = RollState.No_Roll\nroll_adj_state = RollState.Roll_Adjusted\n\n\nroll_explanations = {\n    RollState.No_Roll:\"No rolling happens. Will only trade priced contract.\",\n    RollState.Passive:\"Allow the contract to roll naturally (closing trades in priced contract, opening trades in forward contract)\",\n    RollState.Force:\"Force the contract to roll ASAP using spread order\",\n    RollState.Force_Outright:\"Force the contract to roll ASAP using two outright orders\",\n    RollState.Roll_Adjusted:\"Roll adjusted prices from existing priced to new forward contract (after adjusted prices have been changed, will automatically move state to no roll\"}\n\ndef is_forced_roll_state(roll_state: RollState):\n    if roll_state == RollState.Force or roll_state == RollState.Force_Outright:\n        return True\n    else:\n        return False\n\ndef is_type_of_active_rolling_roll_state(roll_state: RollState):\n    if is_forced_roll_state(roll_state) or roll_state == RollState.Roll_Adjusted:\n        return True\n    else:\n        return False\n\n\ndef explain_roll_state_str(roll_state: RollState):\n    return roll_explanations[RollState[roll_state]]\n\ndef name_of_roll_state(roll_state: RollState):\n    return roll_state.name\n\ndef complete_roll_state(roll_state: RollState, priced_position):\n    if priced_position == 0:\n        flag_position_in_priced = 0\n    else:\n        flag_position_in_priced = 1\n\n    return \"%s%s\" % (name_of_roll_state(roll_state), flag_position_in_priced)\n\n\ndef allowable_roll_state_from_current_and_position(\n        current_roll_state: RollState, priced_position:int):\n    # Transition matrix: First option is recommended\n    # A 0 suffix indicates we have no position in the priced contract\n    # A 1 suffix indicates we do have a position in the priced contract\n    allowed_transition = dict(\n        No_Roll0=[\"Roll_Adjusted\", \"Passive\", \"No_Roll\"],\n        No_Roll1=[\"Passive\", \"Force\", \"Force_Outright\", \"No_Roll\"],\n        Passive0=[\"Roll_Adjusted\", \"Passive\", \"No_Roll\"],\n        Passive1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Force0=[\"Roll_Adjusted\", \"Passive\"],\n        Force1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Force_Outright0=[\"Roll_Adjusted\", \"Passive\"],\n        Force_Outright1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Roll_Adjusted0=[\"No_Roll\"],\n        Roll_Adjusted1=[\"Roll_Adjusted\"],\n    )\n\n    status_plus_position = complete_roll_state(\n        current_roll_state, priced_position)\n    try:\n        allowable_states = allowed_transition[status_plus_position]\n    except KeyError:\n        raise Exception(\n            \"State plus position %s not recognised\" %\n            status_plus_position)\n\n    return allowable_states", "instruction_en": "This code defines an Enum class called RollState with five possible states, and a default state of No_Roll. It also defines a dictionary of explanations for each state, and several functions for working with RollState objects, including checking if a state is a forced roll state, explaining a RollState object as a string, and determining the allowable RollState transitions based on the current state and a priced position. To use this code, import the necessary modules and create RollState objects as needed, then call the provided functions to work with them.", "original_line_number": 23495, "instruction_length": 617}
{"instruction": "Untuk menjalankan kod, ikuti langkah -langkah ini:\n1. Import the necessary libraries: `from __future__ import division`, `from __future__ import print_function`, `import os`, `import sys`, `import functools`, `import numpy as np`, `import pandas as pd`, `import multiprocessing`, `from pdb import set_trace`, `from Simulator import Simulate`, `dari Utils.Plotsutils Line Import, line2`,` dari Utils.Randomutil Import Random`, `dari Utils.misclutils import TimeUtil`.\n2. Kemas kini laluan: `root = os.path.join (os.getcwd (). Split ('proJ1') [0], 'proJ1')`, dan masukkannya ke laluan sistem jika belum ada: `Jika akar tidak di sys.path: sys.path.append (root)`.\n3. Tetapkan benih untuk penjana nombor rawak: `rand.set_seed (seed_val = 12458)`.\n4. Tentukan fungsi untuk mengira kadar kehilangan pelanggan: `def customer_loss_rate (pelanggan): ...`.\n5. Tentukan fungsi untuk merancang runtime: `def plot_runtime (x = none, y = none): ...`.\n6. Tentukan fungsi untuk merancang runtime vs purata: `def plot_runtime_vs_avg (x, y, y_1): ...`.\n7. Tentukan fungsi untuk melaksanakan tugas 5: `def task_5 (): ...`.\n8. Tentukan fungsi untuk merancang hasil tugas 5: `def task5_plot (): ...`.\n9. Tentukan fungsi untuk membandingkan hasil tugas 5 dengan nilai yang diharapkan: `def compare_plot (): ...`.\n10. Panggil fungsi dalam urutan berikut: `Task_5 ()`, `Task5_plot ()`, `compare_plot ()`.", "input": "", "output": "from __future__ import division\nfrom __future__ import print_function\nimport os\nimport sys\nimport functools\n\n# Update path\nroot = os.path.join(os.getcwd().split('proj1')[0], 'proj1')\nif root not in sys.path:\n    sys.path.append(root)\n\nimport numpy as np\nimport pandas as pd\nimport multiprocessing\nfrom pdb import set_trace\nfrom Simulator import simulate\nfrom Utils.PlotsUtils import line, line2\nfrom Utils.RandomUtil import Random\nfrom Utils.MisclUtils import TimeUtil\n\nrand = Random()\ntimer = TimeUtil()\n\n# Set seed\nrand.set_seed(seed_val=12458)\n\ndef customer_loss_rate(customers):\n    served = np.sum([customer.serviced for customer in customers])\n    total = len(customers)\n    return served / total\n\ndef plot_runtime(x=None, y=None):\n    line(x, y, x_label=r\"$\\rho$\", y_label=r\"Run Times\", the_title=r\"$\\mathrm{Run\\ Times\\ in\\ }\\mu\\mathrm{s\\ vs.\\ }\\rho$\")\n\n\ndef plot_runtime_vs_avg(x, y, y_1):\n    line2(x, y, x, y_1, label_1=\"Actual Runtimes\", label_2=\"Expected value of $\\rho$\", x_label=r\"$\\rho$\", y_label=r\"Run Times\", the_title=r\"$\\mathrm{Run\\ Times\\ in\\ }\\mu\\mathrm{s\\ vs.\\ }\\rho$\")\n\n\ndef task_5():\n    rho_list = np.arange(0.05, 1, 0.1)\n    C = 1e5\n    elapsed = []\n    for rho in rho_list:\n        start_time = timer.current_time()\n        serviced = simulate(l = rho, server_lim = 40, max_serviced=C, L=1, verbose=False)\n        end_time = timer.current_time()\n        elapsed.append(end_time-start_time)\n\n    data = pd.DataFrame([[a,b] for a, b in zip(rho_list, elapsed)], columns=[\"Rho\", \"Seconds\"])\n    data.to_csv(os.path.abspath(os.path.join(root,\"tasks/task5.csv\")))\n\ndef task5_plot():\n    data = pd.read_csv(os.path.abspath(\"tasks/task5.csv\"))\n    plot_runtime(data[\"Rho\"], data[\"Seconds\"])\n    set_trace()\n\n\ndef compare_plot():\n    rho_list = np.arange(0.05, 1, 0.1)\n    average_rho = [np.mean([rand.exponential(lam=p) for _ in xrange(10000)]) for p in rho_list]\n    data = pd.read_csv(os.path.abspath(\"tasks/task5.csv\"))\n    plot_runtime(data[\"Rho\"], average_rho)\n\n\nif __name__ == \"__main__\":\n    task_5()\n    task5_plot()\n    compare_plot()", "instruction_en": "To run the code, follow these steps:\n1. Import the necessary libraries: `from __future__ import division`, `from __future__ import print_function`, `import os`, `import sys`, `import functools`, `import numpy as np`, `import pandas as pd`, `import multiprocessing`, `from pdb import set_trace`, `from Simulator import simulate`, `from Utils.PlotsUtils import line, line2`, `from Utils.RandomUtil import Random`, `from Utils.MisclUtils import TimeUtil`.\n2. Update the path: `root = os.path.join(os.getcwd().split('proj1')[0], 'proj1')`, and append it to the system path if it's not already there: `if root not in sys.path: sys.path.append(root)`.\n3. Set the seed for the random number generator: `rand.set_seed(seed_val=12458)`.\n4. Define a function to calculate the customer loss rate: `def customer_loss_rate(customers): ...`.\n5. Define a function to plot the runtime: `def plot_runtime(x=None, y=None): ...`.\n6. Define a function to plot the runtime vs. average: `def plot_runtime_vs_avg(x, y, y_1): ...`.\n7. Define a function to perform task 5: `def task_5(): ...`.\n8. Define a function to plot the results of task 5: `def task5_plot(): ...`.\n9. Define a function to compare the results of task 5 with the expected values: `def compare_plot(): ...`.\n10. Call the functions in the following order: `task_5()`, `task5_plot()`, `compare_plot()`.", "original_line_number": 24829, "instruction_length": 1380}
{"instruction": "Untuk menjana kolaj imej dari Instagram, Tumblr, dan Google berdasarkan lokasi, ikuti langkah -langkah ini:\n1. Import perpustakaan yang diperlukan: permintaan, OS, masa, rawak, flask, permintaan, redirect, sesi, url_for, render_template, jsonify, tempat sampah, beban, oAuth2Session, json, urllib2, mekanis, indah, urlparse, dan membina.\n2. Tetapkan pembolehubah persekitaran 'OAUTHLIB_INSECURE_TRANSPORT' kepada '1'.\n3. Tentukan direktori templat sebagai direktori yang mengandungi templat HTML.\n4. Buat aplikasi Flask dengan nama 'aplikasi' dan tetapkan debug dan propagate_exceptions konfigurasi kepada benar.\n5. Tetapkan kunci rahsia untuk aplikasinya.\n6. Tetapkan ID Pelanggan Instagram, Rahsia Pelanggan, URL Pangkalan Kebenaran, URL Token, dan URL Carian Imej.\n7. Tentukan laluan untuk halaman kebenaran Instagram.\n8. Jika pengguna telah membenarkan aplikasinya, redirkan ke halaman utama.\n9. Buat objek OAuth2Session dengan ID Pelanggan Instagram dan Redirect URI.\n10. Dapatkan URL dan Nyatakan dari Objek OAuth2Session.\n11. Jimat Negeri dalam sesi.\n12. Galakan pengguna ke URL kebenaran.\n13. Tentukan laluan untuk halaman pengambilan token Instagram.\n14. Buat objek OAuth2Session dengan ID Pelanggan Instagram, Rahsia Pelanggan, dan Redirect URI, dan Negeri dari sesi.\n15. Ambil token menggunakan objek OAuth2Session dan kod dari hujah permintaan.\n16. Simpan token akses dalam sesi.\n17. Galakan pengguna ke halaman utama.\n18. Tentukan laluan untuk halaman rumah.\n19. Jika pengguna tidak membenarkan aplikasinya, mengalihkan ke halaman kebenaran Instagram.\n20. Dapatkan maklumat mengenai pengguna menggunakan token akses.\n21. Memberi template halaman rumah dengan data pengguna.\n22. Tentukan laluan untuk halaman kolaj.\n23. Jika kaedah permintaan adalah pos, dapatkan bilangan foto dan tempat dari data permintaan.\n24. Gunakan Google Geocoding untuk menukar tempat ke koordinat latitud dan longitud.\n25. Buat panggilan API ke Instagram untuk mendapatkan model.\n26. Pilih bilangan imej yang dikehendaki dari model.\n27. Dapatkan imej dari Tumblr dan Google menggunakan tempat itu sebagai tag atau pertanyaan.\n28. Dapatkan lapangan terbang, harga terendah, dan penjimatan purata untuk tempat menggunakan API JetBlue.\n29. Memberi templat halaman kolaj dengan imej, lapangan terbang, harga terendah, dan penjimatan purata.", "input": "", "output": "import requests\nimport os \nimport time\nimport random\nfrom flask import Flask, request, redirect, session, url_for, render_template\nfrom flask.json import jsonify, dumps, loads\nfrom requests_oauthlib import OAuth2Session\nimport requests\nimport json\nimport urllib2\nimport mechanize\nfrom bs4 import BeautifulSoup\nfrom urlparse import urlparse\nfrom apiclient.discovery import build\n\n\n\nos.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\ntmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')\n\napp = Flask(__name__, tmpl_dir)\napp.config['DEBUG'] = True\napp.config['PROPAGATE_EXCEPTIONS'] = True\napp.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'\n\ninstagram_client_id = \"115a6c0fd0a64bccbf213e4eafec554a\"\ninstagram_client_secret = \"72f3282930444d9e826e5f083ede32d1\"\ninstagram_authorization_base_url = \"https://api.instagram.com/oauth/authorize\"\ninstagram_token_url = \"https://api.instagram.com/oauth/access_token\"\ninstagram_image_search_url = \"https://api.instagram.com/v1/media/search\"\n\ngoogle_api_key = \"AIzaSyCLehiRvLWhFXbwkI6zojampXcICC0-rMU\"\ngoogle_geocoding_url = \"https://maps.googleapis.com/maps/api/geocode/json?address=%s\"\n\n\n@app.route('/')\ndef instagram_authorization():\n\tif(session.get(\"instagram_access_key\" != None)):\n\t\treturn redirect(\"/home\")\n\n\toauth = OAuth2Session(instagram_client_id, redirect_uri=\"http://127.0.0.1:5000/callback\")\n\tauthorization_url, state = oauth.authorization_url(instagram_authorization_base_url)\n\n\tsession['oauth_state'] = state\n\treturn redirect(authorization_url)\n\n@app.route('/callback', methods=[\"GET\"])\ndef instagram_token_retrieval():\n\toauth = OAuth2Session(instagram_client_id, redirect_uri=\"http://127.0.0.1:5000/callback\", state=session['oauth_state'])\n\t# When grabbing the token Instagram requires you give the code you got in the authorization step in the token step, along with client_id + secret -_-\n\t# This fetch token call might not be right for other APIs, it all demands on their requirements\n\tmy_token = oauth.fetch_token(instagram_token_url, code=request.args.get('code'), client_secret=instagram_client_secret, client_id=instagram_client_id, authorization_url=request.url)\n\tsession['instagram_access_key'] = my_token['access_token']\n\treturn redirect(\"/home\")\n\n'''\n\tRoute representing the home page\n'''\n@app.route('/home')\ndef home():\n\t#TODO: Put in Flickr APi for the home page. \n\tif(session.get('instagram_access_key') == None):\n\t\treturn redirect(\"/\")\n\t#Lets get info on myself the access_token holder\n\taccess_token = session['instagram_access_key']\n\tr = requests.request(\"GET\",'https://api.instagram.com/v1/users/self/?access_token=%s' % access_token)\n\treturn render_template('home.html', user_data=r.json())\n\n\n'''\n\tThe main route for the collage page\n'''\n#after user hits submit button. \n@app.route('/location/<place>', methods=[\"POST\", \"GET\"])\ndef get_collage(place):\n\t#headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n\t#payload = {'num_photos': 3, 'place': place}\n\turl = 'http://127.0.0.1:5000/location/instagram/'+place\n\t#import urlparse\n\t#url = 'http://127.0.0.1:5000/location/instagram/holder'\n\t#parts = urlparse.urlparse(url)\n\t#parts = parts._replace(path=)\n\t#parts.geturl()\n\t#print payload\n\tresponse = get_instagram_photos(place)\n\n\tresponse2= json.loads(get_google_images(place))\n\tresponse3= json.loads(get_google_images2(place))\n\tresponse4 = json.loads(get_tumblr_images(place)) \n \n\tprint \"RECIEVES\"\n\tprint response\n\tprint \"GOOGLE\"\n\tprint response2\n\tprint \"TUMBLR\" \n\tprint response4\n        place = place.replace(\"+\", \" \")\n        airport = get_airport(place)\n        price = \"Packages for Jetblue start as low as \" + str(get_lowest_price(place)) + \". \"\n        average_savings = \"And save up to \" + str(get_savings_percentage(place)) + \" compared to Expedia! Wow Jetblue is so awesome!\"\n    \treturn render_template('collage.html', place=place, photos_display=response, photos_google= response2, photos_tumblr= response4, photos_google2 = response3, lowest_price=price, average_savings=average_savings, airport=airport)\n\ndef get_airport(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[1]\n\n\ndef get_savings_percentage(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[5][:-1]\n\ndef get_lowest_price(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[4]\n        \n'''\n\tWill return a list of image URLs from instagram given the name of a location\n'''\n\ndef get_instagram_photos(place):\n\tprint \"hell\"\n\tprint place\n\tif(session.get('instagram_access_key') == None):\n\t\tprint \"REDIRECT\"\n\t\treturn redirect(\"/\")\n    #http://127.0.0.1:5000/location/instagram/Chicago/3\n    #place, num_photos, \n\t# Use Google Geocoding to convert place to lat and long coordinates\n\tnum_photos = 25;\n\tprint place\n\tlocation = requests.get(google_geocoding_url % place)\n\tlocation = location.json()\n\tprint location\n\tlat_coord = location.get(\"results\")[0].get(\"geometry\").get(\"location\").get(\"lat\")\n\tlong_coord = location.get(\"results\")[0].get(\"geometry\").get(\"location\").get(\"lng\")\n\tprint lat_coord\n\tprint long_coord\n\t# Make the API call to get the Models\n\tquerys = {\"lat\": lat_coord, \"lng\" : long_coord, \"min_timestamp\": \"1262304000\", \"max_timestamp\":\"1446940800\", \"distance\" : \"10000\" , \"access_token\": session.get('instagram_access_key')}\n\tinstagram_models = requests.get(instagram_image_search_url, params=querys)\n\tchosen_images = []\n\n\tjson_object = loads(instagram_models.text) \n\tprint json_object\n\tif len(json_object[\"data\"]) > num_photos:\n\t\tfor i in range(0, num_photos): \n\t\t\tchosen_images.append(json_object[\"data\"][i][\"images\"])\n\telse:\n\t\tfor i in range(0, len(json_object[\"data\"])):\n\t\t\tchosen_images.append(json_object[\"data\"][i][\"images\"])\n\t\tprint len(json_object[\"data\"])\n\t\tprint num_photos\n\n\tprint chosen_images\n\n\n\treturn chosen_images\t\n\ndef get_tumblr_images(place):\n\tprint \"GETTING TUMBLR\" \n\turl = ('https://api.tumblr.com/v2/tagged?tag='+urllib2.quote(place)+\"&api_key=YaGrzj5NUOlMDxQyTtkSBz1KEAnVyUYcCRKWT74VzNUJwRbtH4\")\n\tprint url \n\treq = urllib2.Request(url, headers={'accept': '*/*'})\n\tresponse = urllib2.urlopen(req)\n\tprint \"TUMBLR\"\n\tret = response.read() \n\tprint ret \n\treturn ret\n\ndef get_google_images(place):\n    print \"MOVING ON TO GOOGLE\"\n    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0&q='+ urllib2.quote(place) + \"&rsz=8\")\n    print url\n    req = urllib2.Request(url, headers={'accept': '*/*'})\n    response = urllib2.urlopen(req)\n    print \"GOOGLE RESPONSE\"\n    print type(response)\n    print \"TYPE OF RESPONSE.READ\"    \n    ret = response.read()\n    print len(ret)\n    print \"RET\"\n    print ret\n    return ret\n\n\n\ndef get_google_images2(place):\n    print \"MOVING ON TO GOOGLE\"\n    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0&q='+ urllib2.quote(place) +'&rsz=8&start=9')\n    print url\n    req = urllib2.Request(url, headers={'accept': '*/*'})\n    response = urllib2.urlopen(req)\n    print \"GOOGLE RESPONSE\"\n    print type(response)\n    print \"TYPE OF RESPONSE.READ\"    \n    ret = response.read()\n    print len(ret)\n    print \"RET\"\n    print ret\n    return ret\n\nif __name__ == '__main__':\n\tapp.run()", "instruction_en": "To generate a collage of images from Instagram, Tumblr, and Google based on a location, follow these steps:\n1. Import the necessary libraries: requests, os, time, random, Flask, request, redirect, session, url_for, render_template, jsonify, dumps, loads, OAuth2Session, json, urllib2, mechanize, BeautifulSoup, urlparse, and build.\n2. Set the environment variable 'OAUTHLIB_INSECURE_TRANSPORT' to '1'.\n3. Define the template directory as the directory containing the HTML templates.\n4. Create a Flask app with the name 'app' and set the DEBUG and PROPAGATE_EXCEPTIONS configurations to True.\n5. Set the secret key for the app.\n6. Set the Instagram client ID, client secret, authorization base URL, token URL, and image search URL.\n7. Define a route for the Instagram authorization page.\n8. If the user has already authorized the app, redirect to the home page.\n9. Create an OAuth2Session object with the Instagram client ID and redirect URI.\n10. Get the authorization URL and state from the OAuth2Session object.\n11. Save the state in the session.\n12. Redirect the user to the authorization URL.\n13. Define a route for the Instagram token retrieval page.\n14. Create an OAuth2Session object with the Instagram client ID, client secret, and redirect URI, and the state from the session.\n15. Fetch the token using the OAuth2Session object and the code from the request arguments.\n16. Save the access token in the session.\n17. Redirect the user to the home page.\n18. Define a route for the home page.\n19. If the user has not authorized the app, redirect to the Instagram authorization page.\n20. Get information about the user using the access token.\n21. Render the home page template with the user data.\n22. Define a route for the collage page.\n23. If the request method is POST, get the number of photos and the place from the request data.\n24. Use Google Geocoding to convert the place to latitude and longitude coordinates.\n25. Make an API call to Instagram to get the models.\n26. Choose the desired number of images from the models.\n27. Get images from Tumblr and Google using the place as a tag or query.\n28. Get the airport, lowest price, and average savings for the place using Jetblue API.\n29. Render the collage page template with the images, airport, lowest price, and average savings.", "original_line_number": 25010, "instruction_length": 2325}
{"instruction": "Kod ini mengambil status pelbagai peranti dari Verisure dan mengemas kini peranti yang sepadan di Domoticz. Untuk menggunakan kod ini, anda perlu mempunyai akaun Verisure dan pemasangan Domoticz. Pastikan perpustakaan yang diperlukan dipasang dan laluan ke fail konfigurasi adalah betul. Sebaik sahaja kod itu dilaksanakan, ia akan log masuk ke Verisure, mengambil status penggera, smartplugs, dan peranti iklim, dan mengemas kini peranti yang sepadan di Domoticz. Kod akan dijalankan setiap `frekuensi` saat, seperti yang dinyatakan dalam fail konfigurasi.", "input": "", "output": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport domoticz as d\nimport sys\nimport time\nsys.path.insert(0, '/opt/python-verisure/')\nimport verisure\nimport pickle\nimport pytz\nimport urllib3\nimport certifi\nfrom datetime import datetime\nfrom tzlocal import get_localzone\n\ndebug = False\n\ntry:\n    execfile(\"/etc/domoticz/scripts.conf\")\nexcept:\n    exec(open(\"/etc/domoticz/scripts.conf\").read())\n\nd.log(\"Getting status from Verisure...\")\n\nif int(time.time()) % frequency < 60 :\n\n\t#Login\n\ttry:\n\t\tf = open(mypagesSession, 'rb')\n\t\tmyPages = pickle.load(f)\n\t\tf.close()\n\texcept:\n\t\tmyPages = verisure.Session(email, verisurepass)\n\t\tmyPages.login()\n\t\tf = open(mypagesSession, 'wb')\n\t\tpickle.dump(myPages, f)\n\t\tf.close()\n\t\tif debug:\n\t\t\td.log(\"Loading file failed.\")\n\n\t#Get overview\n\ttry:\n\t\toverview = myPages.get_overview()\n\texcept:\n\t\tmyPages = verisure.Session(email, verisurepass)\n\t\tmyPages.login()\n\t\tf = open(mypagesSession, 'wb')\n\t\tpickle.dump(myPages, f)\n\t\tf.close()\n\t\toverview = myPages.get_overview()\n\t\tif debug:\n\t\t\td.log(\"Session was timed out\")\n\n\t#Alarm\n\tstatus = overview['armState']['statusType']\n\tif debug:\n\t\td.log(\"Verisure Alarm status: \", status )\n\tdevice = d.devices[atHome]\n\tif status == \"DISARMED\" or status == \"ARMED_HOME\":\n\t\tdevice.on()\n\telse:\n\t\tdevice.off()\n\n\t#Smartplugs\n\tfor i in overview['controlPlugs']:\n\t\tif debug:\n\t\t\td.log(\"Verisure Smartplug status for \" + i['area'].encode(\"utf-8\",\"ignore\") + \": \", i['currentState'] )\n\t\tdevice = d.devices[i['area'].encode(\"utf-8\",\"ignore\")]\n\t\tif i['currentState'] == \"ON\":\n\t\t\tdevice.on()\n\t\telse:\n\t\t\tdevice.off()\n\n\t#Climate\n\tfor i in overview['climateValues']:\n\t\tdevice = d.devices[i['deviceArea'].encode(\"utf-8\",\"ignore\")]\n\t\tdomlastupdate = datetime.strptime(device.last_update_string, '%Y-%m-%d %H:%M:%S')\n\t\tverilastupdate = datetime.strptime(i['time'][:-5], '%Y-%m-%dT%H:%M:%S')\n\t\tverilastupdate = verilastupdate.replace(tzinfo=pytz.UTC)\n\t\tverilastupdate = verilastupdate.astimezone(get_localzone())\n\t\tverilastupdate = verilastupdate.replace(tzinfo=None)\n\t\tif debug:\n\t\t\td.log(\"Domoticz last update of \" + device.name + \": \" + str(domlastupdate))\n\t\t\td.log(\"Verisure last update of \" + device.name + \": \" + str(verilastupdate))\n\n\t\tif verilastupdate > domlastupdate:\n\t\t\tif debug:\n\t\t\t\td.log(\"update domoticz climate device \" + device.name)\n\t\t\tif debug:\n\t\t\t\td.log(\"time: \" + i['time'] )\n\t\t\t\td.log(\"location: \" + i['deviceArea'].encode(\"utf-8\",\"ignore\") )\n\t\t\t\td.log(\"serial: \" + i['deviceLabel'] )\n\t\t\t\td.log(\"temperature: \" + str(i['temperature']))\n\t\t\tif 'humidity' in i:\n\t\t\t\tif debug:\n\t\t\t\t\td.log(\"humidity: \" + str(i['humidity']))\n\t\t\t\tif i['humidity'] < 20:\n\t\t\t\t\tcomf = 2\n\t\t\t\tif i['humidity'] >= 20 and i['humidity'] <= 35:\n\t\t\t\t\tcomf = 0\n\t\t\t\tif i['humidity'] > 35 and i['humidity'] <= 75:\n\t\t\t\t\tcomf = 1\n\t\t\t\tif i['humidity'] > 75:\n\t\t\t\t\tcomf = 3\n\t\t\t\turl = baseurl + \"type=command&param=udevice&idx=\" + climate[i['deviceArea'].encode(\"utf-8\",\"ignore\")] + \"&nvalue=0&svalue=\" + str(i['temperature']) + \";\" + str(i['humidity']) + \";\" + str(comf)\n\t\t\telse:\n\t\t\t\turl = baseurl + \"type=command&param=udevice&idx=\" + climate[i['deviceArea'].encode(\"utf-8\",\"ignore\")] + \"&nvalue=0&svalue=\" + str(i['temperature'])\n\n\t\t\tif debug:\n\t\t\t\td.log('URL: ' + url)\n\n\t\t\thttp = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n\t\t\tr = http.request('GET', url, timeout=2.5)\n\t\t\tif debug:\n\t\t\t\td.log(\"Status code: \" + str(r.status) + \"\\n\" + r.data)\n\t\t\tif r.status != 200:\n\t\t\t\td.log(\"Error updating temp in Domoticz. HTTP code: \" + str(r.status) + \" \" + r.data)\n\nelse:\n\tif debug:\n\t\td.log(\"Only runs every \" + str(frequency/60) + \" min.\")\n\nd.log(\"done getting status from Verisure\")", "instruction_en": "This code retrieves the status of various devices from Verisure and updates the corresponding devices in Domoticz. To use this code, you need to have a Verisure account and a Domoticz installation. Make sure that the necessary libraries are installed and that the paths to the configuration files are correct. Once the code is executed, it will log in to Verisure, retrieve the status of the alarm, smartplugs, and climate devices, and update the corresponding devices in Domoticz. The code will run every `frequency` seconds, as specified in the configuration file.", "original_line_number": 25028, "instruction_length": 557}
{"instruction": "Kod ini mentakrifkan model Django untuk mengendalikan kesilapan dalam projek. Untuk menggunakan kod ini, buat aplikasi Django dan tambahkan kod ini ke fail model.py. Kemudian, jalankan migrasi untuk membuat jadual pangkalan data yang diperlukan. Untuk membuat ralat baru, gunakan kaedah errormanager.create (), lulus dalam komputer, projek, dan perihalan sebagai argumen. Untuk mendapatkan pertanyaan semua kesilapan, gunakan kaedah DomainErrormAnager.Get_QuerySet (). Untuk mendapatkan pertanyaan kesilapan yang tidak terkawal, gunakan kaedah UncheckedManager.get_Queryset (). Untuk mendapatkan pertanyaan kesilapan yang ditimbulkan kepada pengguna tertentu, gunakan kaedah domainerrormanager.scope (), lulus pengguna sebagai hujah. Untuk mendapatkan kiraan kesilapan yang tidak terkawal, gunakan kaedah error.unchecked_count (). Untuk mendapatkan kamus kesilapan yang tidak terkawal yang dikelompokkan oleh projek dan platform, gunakan ralat.unchecked_by_project () kaedah, lulus pengguna sebagai hujah. Untuk mendapatkan kamus kesilapan yang dikelompokkan oleh status projek dan komputer, gunakan kaedah error.status_by_project (), lulus pengguna sebagai hujah. Untuk menandakan ralat seperti yang diperiksa, gunakan kaedah checked_ok () pada contoh ralat.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2015-2021 Jose Antonio Chavarra <jachavar@gmail.com>\n# Copyright (c) 2015-2021 Alberto Gacas <alberto@migasfree.org>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\nfrom django.db import models\nfrom django.db.models.aggregates import Count\nfrom django.utils.translation import gettext_lazy as _\n\nfrom ...core.models import Project\n\nfrom .computer import Computer\nfrom .event import Event\n\n\nclass DomainErrorManager(models.Manager):\n    def get_queryset(self):\n        return super().get_queryset().select_related(\n            'project',\n            'computer',\n            'computer__project',\n            'computer__sync_user',\n        )\n\n    def scope(self, user):\n        qs = self.get_queryset()\n        if not user.is_view_all():\n            qs = qs.filter(\n                project_id__in=user.get_projects(),\n                computer_id__in=user.get_computers()\n            )\n\n        return qs\n\n\nclass UncheckedManager(DomainErrorManager):\n    def get_queryset(self):\n        return super().get_queryset().filter(checked=0)\n\n    def scope(self, user):\n        return super().scope(user).filter(checked=0)\n\n\nclass ErrorManager(DomainErrorManager):\n    def create(self, computer, project, description):\n        obj = Error()\n        obj.computer = computer\n        obj.project = project\n        obj.description = description\n        obj.save()\n\n        return obj\n\n\nclass Error(Event):\n    description = models.TextField(\n        verbose_name=_(\"description\"),\n        null=True,\n        blank=True\n    )\n\n    checked = models.BooleanField(\n        verbose_name=_(\"checked\"),\n        default=False,\n    )\n\n    project = models.ForeignKey(\n        Project,\n        on_delete=models.CASCADE,\n        verbose_name=_(\"project\")\n    )\n\n    objects = ErrorManager()\n    unchecked = UncheckedManager()\n\n    @staticmethod\n    def unchecked_count(user=None):\n        if not user:\n            return Error.unchecked.count()\n\n        return Error.unchecked.scope(user).count()\n\n    @staticmethod\n    def unchecked_by_project(user):\n        total = Error.unchecked_count(user)\n\n        projects = list(Error.unchecked.scope(user).values(\n            'project__name',\n            'project__id',\n            'project__platform__id',\n        ).annotate(\n            count=Count('id')\n        ).order_by('project__platform__id', '-count'))\n\n        platforms = list(Error.unchecked.scope(user).values(\n            'project__platform__id',\n            'project__platform__name'\n        ).annotate(\n            count=Count('id')\n        ).order_by('project__platform__id', '-count'))\n\n        return {\n            'total': total,\n            'inner': platforms,\n            'outer': projects,\n        }\n\n    @staticmethod\n    def status_by_project(user):\n        total = Error.objects.scope(user).count()\n\n        projects = list(Error.objects.scope(user).values(\n            'computer__status',\n            'project__id',\n            'project__name',\n        ).annotate(\n            count=Count('id')\n        ).order_by('computer__status', '-count'))\n\n        status = list(Error.objects.scope(user).values(\n            'computer__status',\n        ).annotate(\n            count=Count('id')\n        ).order_by('computer__status', '-count'))\n\n        for item in status:\n            item['status'] = item.get('computer__status')\n            item['computer__status'] = _(dict(Computer.STATUS_CHOICES)[item.get('computer__status')])\n\n        return {\n            'total': total,\n            'inner': status,\n            'outer': projects,\n        }\n\n    def checked_ok(self):\n        self.checked = True\n        self.save()\n\n    def save(self, force_insert=False, force_update=False, using=None, update_fields=None):\n        self.description = self.description.replace(\"\\r\\n\", \"\\n\")\n\n        super().save(force_insert, force_update, using, update_fields)\n\n    class Meta:\n        app_label = 'client'\n        verbose_name = _('Error')\n        verbose_name_plural = _('Errors')", "instruction_en": "This code defines a Django model for handling errors in a project. To use this code, create a Django app and add this code to the models.py file. Then, run the migrations to create the necessary database tables. To create a new error, use the ErrorManager.create() method, passing in the computer, project, and description as arguments. To get a queryset of all errors, use the DomainErrorManager.get_queryset() method. To get a queryset of unchecked errors, use the UncheckedManager.get_queryset() method. To get a queryset of errors scoped to a specific user, use the DomainErrorManager.scope() method, passing in the user as an argument. To get the count of unchecked errors, use the Error.unchecked_count() method. To get a dictionary of unchecked errors grouped by project and platform, use the Error.unchecked_by_project() method, passing in the user as an argument. To get a dictionary of errors grouped by project and computer status, use the Error.status_by_project() method, passing in the user as an argument. To mark an error as checked, use the checked_ok() method on an Error instance.", "original_line_number": 25627, "instruction_length": 1259}
{"instruction": "Buat kelas yang dipanggil \"SID\" dengan parameter negeri untuk awalan-SID. Kelas harus menjadi bekas dan harus dihasilkan secara automatik oleh plugin Pythonclass untuk Pyang dari Modul Yang OpenConfig-Network-Instance. Kelas ini harus berdasarkan pada laluan/rangkaian/rangkaian-instance/protokol/protokol/ISIS/level/level/link-state-database/lsp/tlvs/tlv/lanjutan-iPv4-reachability/prefix/prefix/subtlvs/subtlvs/prefix/prefix/prefix-sid/sid. Setiap elemen ahli bekas harus diwakili sebagai pemboleh ubah kelas dengan jenis Yang tertentu. Senarai Segmen-ID awalan hendaklah menjadi segmen IGP-Prefix yang dilampirkan pada awalan IGP. Segmen IGP-Prefix mestilah global (kecuali jika diiklankan secara jelas) dalam domain SR/IGP.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom operator import attrgetter\nfrom pyangbind.lib.yangtypes import RestrictedPrecisionDecimalType\nfrom pyangbind.lib.yangtypes import RestrictedClassType\nfrom pyangbind.lib.yangtypes import TypedListType\nfrom pyangbind.lib.yangtypes import YANGBool\nfrom pyangbind.lib.yangtypes import YANGListType\nfrom pyangbind.lib.yangtypes import YANGDynClass\nfrom pyangbind.lib.yangtypes import ReferenceType\nfrom pyangbind.lib.base import PybindBase\nfrom collections import OrderedDict\nfrom decimal import Decimal\nfrom bitarray import bitarray\nimport six\n\n# PY3 support of some PY2 keywords (needs improved)\nif six.PY3:\n    import builtins as __builtin__\n\n    long = int\nelif six.PY2:\n    import __builtin__\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Prefix Segment-ID list. IGP-Prefix Segment is an IGP segment attached\nto an IGP prefix. An IGP-Prefix Segment is global (unless explicitly\nadvertised otherwise) within the SR/IGP domain.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"extended-ipv4-reachability\",\n                \"prefixes\",\n                \"prefix\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"prefix-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])\n\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance-l2 - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Prefix Segment-ID list. IGP-Prefix Segment is an IGP segment attached\nto an IGP prefix. An IGP-Prefix Segment is global (unless explicitly\nadvertised otherwise) within the SR/IGP domain.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"extended-ipv4-reachability\",\n                \"prefixes\",\n                \"prefix\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"prefix-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])", "instruction_en": "Create a class called \"sid\" with a state parameter for Prefix-SID. The class should be a container and should be auto-generated by the PythonClass plugin for PYANG from YANG module openconfig-network-instance. The class should be based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of the container should be represented as a class variable with a specific YANG type. The Prefix Segment-ID list should be an IGP-Prefix Segment attached to an IGP prefix. An IGP-Prefix Segment should be global (unless explicitly advertised otherwise) within the SR/IGP domain.", "original_line_number": 25634, "instruction_length": 727}
{"instruction": "Kod ini adalah kelas python yang mentakrifkan objek YouPorncom, yang merupakan plugin untuk program yang dipanggil Pyload. Untuk menggunakan plugin ini, buat contoh kelas YouPorncom dan panggil kaedah prosesnya (), lulus dalam objek Pyfile. Kaedah Proses () akan memuat turun fail yang ditentukan oleh objek pyfile dari youporn.com jika ia wujud. Jika fail tidak wujud, kaedah akan memanggil kaedah luar talian (). Kelas YouPorncom juga mempunyai beberapa kaedah penolong, termasuk muat turun_html (), get_file_url (), get_file_name (), dan file_exists ().", "input": "", "output": "# -*- coding: utf-8 -*-\n\nimport re\nfrom module.plugins.Hoster import Hoster\n\n\nclass YoupornCom(Hoster):\n    __name__ = \"YoupornCom\"\n    __type__ = \"hoster\"\n    __pattern__ = r'http://(?:www\\.)?youporn\\.com/watch/.+'\n    __version__ = \"0.2\"\n    __description__ = \"\"\"Youporn.com hoster plugin\"\"\"\n    __author_name__ = \"willnix\"\n    __author_mail__ = \"willnix@pyload.org\"\n\n    def process(self, pyfile):\n        self.pyfile = pyfile\n\n        if not self.file_exists():\n            self.offline()\n\n        pyfile.name = self.get_file_name()\n        self.download(self.get_file_url())\n\n    def download_html(self):\n        url = self.pyfile.url\n        self.html = self.load(url, post={\"user_choice\": \"Enter\"}, cookies=False)\n\n    def get_file_url(self):\n        \"\"\" returns the absolute downloadable filepath\n        \"\"\"\n        if not self.html:\n            self.download_html()\n\n        return re.search(r'(http://download\\.youporn\\.com/download/\\d+\\?save=1)\">', self.html).group(1)\n\n    def get_file_name(self):\n        if not self.html:\n            self.download_html()\n\n        file_name_pattern = r\"<title>(.*) - Free Porn Videos - YouPorn</title>\"\n        return re.search(file_name_pattern, self.html).group(1).replace(\"&amp;\", \"&\").replace(\"/\", \"\") + '.flv'\n\n    def file_exists(self):\n        \"\"\" returns True or False\n        \"\"\"\n        if not self.html:\n            self.download_html()\n        if re.search(r\"(.*invalid video_id.*)\", self.html) is not None:\n            return False\n        else:\n            return True", "instruction_en": "This code is a Python class that defines a YoupornCom object, which is a plugin for a program called pyload. To use this plugin, create an instance of the YoupornCom class and call its process() method, passing in a pyfile object. The process() method will download the file specified by the pyfile object from Youporn.com if it exists. If the file does not exist, the method will call the offline() method. The YoupornCom class also has several helper methods, including download_html(), get_file_url(), get_file_name(), and file_exists().", "original_line_number": 25938, "instruction_length": 556}
{"instruction": "Kod ini adalah skrip python yang memuat data yang diperintahkan dari folder ke pangkalan data PostgreSQL menggunakan PDAL. Untuk menggunakan skrip ini, buat contoh kelas Loaderordered dan panggil kaedah prosesnya (). Kelas loaderorder melanjutkan kelas loader dan mengatasi getFileBlockTable (), loadFromFile (), dan loadFromFileSeQuential () kaedah. Kaedah LoadFromFile () memuatkan fail ke dalam pangkalan data menggunakan PDAL, manakala kaedah LoadFromFileSeQueal () memasukkan blok fail ke dalam jadual Blok Global dalam urutan yang betul. Kaedah Proses () memuatkan fail secara selari menggunakan kaedah LoadFromFile () dan LoadFromFileSeQuential ().", "input": "", "output": "#!/usr/bin/env python\n################################################################################\n#    Created by Oscar Martinez                                                 #\n#    o.rubi@esciencecenter.nl                                                  #\n################################################################################\nimport os, logging\nfrom pointcloud import pdalops, postgresops\nfrom pointcloud.postgres.blocks.Loader import Loader\n\nclass LoaderOrdered(Loader):\n    def getFileBlockTable(self, index):\n        return self.blockTable + '_' + str(index)\n\n    def process(self):\n        logging.info('Starting ordered data loading with PDAL (parallel by python) from ' + self.inputFolder + ' to ' + self.dbName)\n        return self.processMulti(self.inputFiles, self.numProcessesLoad, self.loadFromFile, self.loadFromFileSequential, True)\n\n    def loadFromFile(self, index, fileAbsPath):\n        # Get connection\n        connection = self.getConnection()\n        cursor = connection.cursor()\n        #Create a temporal blocks table for the blocks of the current file\n        fileBlockTable = self.getFileBlockTable(index)\n        self.createBlocksTable(cursor, fileBlockTable, self.indexTableSpace) # We use the index table space for the temporal table\n        \n        # Add point cloud format to poinctcloud_formats table\n        (columns, pcid, compression) = self.addPCFormat(cursor, self.schemaFile, fileAbsPath, self.srid)\n        connection.close()\n\n        pdalCols = []\n        for c in cols:\n            pdalCols.append(self.DM_PDAL[c])\n\n        # Get PDAL config and run PDAL\n        xmlFile = os.path.basename(fileAbsPath) + '.xml'\n        pdalops.PostgreSQLWriter(xmlFile, fileAbsPath, self.getConnectionString(), pcid, pdalCols, fileBlockTable, self.srid, self.blockSize, compression)\n        pdalops.executePDAL(xmlFile)\n        \n    def loadFromFileSequential(self, fileAbsPath, index, numFiles):\n        fileBlockTable = self.getFileBlockTable(index)\n        connection = self.getConnection()\n        cursor = connection.cursor()\n        # Insert the blocks on the global blocks table (with correct order)\n        query = \"INSERT INTO \" + self.blockTable + \" (pa) SELECT pa FROM \" + fileBlockTable + \" ORDER BY id\"\n        postgresops.mogrifyExecute(cursor, query)\n        # Drop the temporal table\n        postgresops.dropTable(cursor, fileBlockTable)\n        connection.close()", "instruction_en": "This code is a Python script that loads ordered data from a folder to a PostgreSQL database using PDAL. To use this script, create an instance of the LoaderOrdered class and call its process() method. The LoaderOrdered class extends the Loader class and overrides its getFileBlockTable(), loadFromFile(), and loadFromFileSequential() methods. The loadFromFile() method loads a file into the database using PDAL, while the loadFromFileSequential() method inserts the blocks of a file into the global blocks table in the correct order. The process() method loads the files in parallel using the loadFromFile() and loadFromFileSequential() methods.", "original_line_number": 25976, "instruction_length": 655}
{"instruction": "Kod ini menyediakan pembalut python di sekitar Tensorflow Ops untuk penyahkodan input LIBSVM kepada tensor. Untuk menggunakannya, hubungi fungsi `decode_libsvm` dengan hujah -hujah berikut:` input` (A `tensor` jenis` string`), `num_features` (` int` yang `> = 1`),` dType` tf.int64`. Fungsi ini mengembalikan tuple objek `tensor` (label, feature_indices, feature_values, feature_shape)`, di mana `label` adalah` tensor` jenis `label_dtype`,` feature_indices` `feature_shape` adalah` tensor` jenis `int64`.", "input": "", "output": "\"\"\"Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\nOriginal C++ source file: libsvm_ops.cc\n\"\"\"\n\nimport collections as _collections\nimport six as _six\n\nfrom tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\nfrom tensorflow.python.eager import context as _context\nfrom tensorflow.python.eager import core as _core\nfrom tensorflow.python.eager import execute as _execute\nfrom tensorflow.python.framework import dtypes as _dtypes\nfrom tensorflow.python.framework import errors as _errors\nfrom tensorflow.python.framework import tensor_shape as _tensor_shape\n\nfrom tensorflow.core.framework import op_def_pb2 as _op_def_pb2\n# Needed to trigger the call to _set_call_cpp_shape_fn.\nfrom tensorflow.python.framework import common_shapes as _common_shapes\nfrom tensorflow.python.framework import op_def_registry as _op_def_registry\nfrom tensorflow.python.framework import ops as _ops\nfrom tensorflow.python.framework import op_def_library as _op_def_library\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n_decode_libsvm_outputs = [\"label\", \"feature_indices\", \"feature_values\",\n                         \"feature_shape\"]\n_DecodeLibsvmOutput = _collections.namedtuple(\n    \"DecodeLibsvm\", _decode_libsvm_outputs)\n\n\n@tf_export('decode_libsvm')\ndef decode_libsvm(input, num_features, dtype=_dtypes.float32, label_dtype=_dtypes.int64, name=None):\n  r\"\"\"Convert LibSVM input to tensors. The output consists of\n\n  a label and a feature tensor. The shape of the label tensor\n  is the same as input and the shape of the feature tensor is\n  `[input_shape, num_features]`.\n\n  Args:\n    input: A `Tensor` of type `string`. Each string is a record in the LibSVM.\n    num_features: An `int` that is `>= 1`. The number of features.\n    dtype: An optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.float32`.\n    label_dtype: An optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (label, feature_indices, feature_values, feature_shape).\n\n    label: A `Tensor` of type `label_dtype`. A tensor of the same shape as input.\n    feature_indices: A `Tensor` of type `int64`. A 2-D int64 tensor of dense_shape [N, ndims].\n    feature_values: A `Tensor` of type `dtype`. A 1-D tensor of any type and dense_shape [N].\n    feature_shape: A `Tensor` of type `int64`. A 1-D int64 tensor of dense_shape [ndims].\n  \"\"\"\n  _ctx = _context._context\n  if _ctx is None or not _ctx._eager_context.is_eager:\n    num_features = _execute.make_int(num_features, \"num_features\")\n    if dtype is None:\n      dtype = _dtypes.float32\n    dtype = _execute.make_type(dtype, \"dtype\")\n    if label_dtype is None:\n      label_dtype = _dtypes.int64\n    label_dtype = _execute.make_type(label_dtype, \"label_dtype\")\n    _, _, _op = _op_def_lib._apply_op_helper(\n        \"DecodeLibsvm\", input=input, num_features=num_features, dtype=dtype,\n        label_dtype=label_dtype, name=name)\n    _result = _op.outputs[:]\n    _inputs_flat = _op.inputs\n    _attrs = (\"dtype\", _op.get_attr(\"dtype\"), \"label_dtype\",\n              _op.get_attr(\"label_dtype\"), \"num_features\",\n              _op.get_attr(\"num_features\"))\n    _execute.record_gradient(\n      \"DecodeLibsvm\", _inputs_flat, _attrs, _result, name)\n    _result = _DecodeLibsvmOutput._make(_result)\n    return _result\n\n  else:\n    try:\n      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n        _ctx._context_handle, _ctx._eager_context.device_name, \"DecodeLibsvm\",\n        name, _ctx._post_execution_callbacks, input, \"dtype\", dtype,\n        \"label_dtype\", label_dtype, \"num_features\", num_features)\n      _result = _DecodeLibsvmOutput._make(_result)\n      return _result\n    except _core._FallbackException:\n      return decode_libsvm_eager_fallback(\n          input, dtype=dtype, label_dtype=label_dtype,\n          num_features=num_features, name=name, ctx=_ctx)\n    except _core._NotOkStatusException as e:\n      if name is not None:\n        message = e.message + \" name: \" + name\n      else:\n        message = e.message\n      _six.raise_from(_core._status_to_exception(e.code, message), None)\n\n\ndef decode_libsvm_eager_fallback(input, num_features, dtype=_dtypes.float32, label_dtype=_dtypes.int64, name=None, ctx=None):\n  r\"\"\"This is the slowpath function for Eager mode.\n  This is for function decode_libsvm\n  \"\"\"\n  _ctx = ctx if ctx else _context.context()\n  num_features = _execute.make_int(num_features, \"num_features\")\n  if dtype is None:\n    dtype = _dtypes.float32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  if label_dtype is None:\n    label_dtype = _dtypes.int64\n  label_dtype = _execute.make_type(label_dtype, \"label_dtype\")\n  input = _ops.convert_to_tensor(input, _dtypes.string)\n  _inputs_flat = [input]\n  _attrs = (\"dtype\", dtype, \"label_dtype\", label_dtype, \"num_features\",\n  num_features)\n  _result = _execute.execute(b\"DecodeLibsvm\", 4, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=_ctx, name=name)\n  _execute.record_gradient(\n      \"DecodeLibsvm\", _inputs_flat, _attrs, _result, name)\n  _result = _DecodeLibsvmOutput._make(_result)\n  return _result\n\n_ops.RegisterShape(\"DecodeLibsvm\")(None)\n\ndef _InitOpDefLibrary(op_list_proto_bytes):\n  op_list = _op_def_pb2.OpList()\n  op_list.ParseFromString(op_list_proto_bytes)\n  _op_def_registry.register_op_list(op_list)\n  op_def_lib = _op_def_library.OpDefLibrary()\n  op_def_lib.add_op_list(op_list)\n  return op_def_lib\n# op {\n#   name: \"DecodeLibsvm\"\n#   input_arg {\n#     name: \"input\"\n#     type: DT_STRING\n#   }\n#   output_arg {\n#     name: \"label\"\n#     type_attr: \"label_dtype\"\n#   }\n#   output_arg {\n#     name: \"feature_indices\"\n#     type: DT_INT64\n#   }\n#   output_arg {\n#     name: \"feature_values\"\n#     type_attr: \"dtype\"\n#   }\n#   output_arg {\n#     name: \"feature_shape\"\n#     type: DT_INT64\n#   }\n#   attr {\n#     name: \"dtype\"\n#     type: \"type\"\n#     default_value {\n#       type: DT_FLOAT\n#     }\n#     allowed_values {\n#       list {\n#         type: DT_FLOAT\n#         type: DT_DOUBLE\n#         type: DT_INT32\n#         type: DT_INT64\n#       }\n#     }\n#   }\n#   attr {\n#     name: \"label_dtype\"\n#     type: \"type\"\n#     default_value {\n#       type: DT_INT64\n#     }\n#     allowed_values {\n#       list {\n#         type: DT_FLOAT\n#         type: DT_DOUBLE\n#         type: DT_INT32\n#         type: DT_INT64\n#       }\n#     }\n#   }\n#   attr {\n#     name: \"num_features\"\n#     type: \"int\"\n#     has_minimum: true\n#     minimum: 1\n#   }\n# }\n_op_def_lib = _InitOpDefLibrary(b\"\\n\\311\\001\\n\\014DecodeLibsvm\\022\\t\\n\\005input\\030\\007\\032\\024\\n\\005label\\\"\\013label_dtype\\032\\023\\n\\017feature_indices\\030\\t\\032\\027\\n\\016feature_values\\\"\\005dtype\\032\\021\\n\\rfeature_shape\\030\\t\\\"\\033\\n\\005dtype\\022\\004type\\032\\0020\\001:\\010\\n\\0062\\004\\001\\002\\003\\t\\\"!\\n\\013label_dtype\\022\\004type\\032\\0020\\t:\\010\\n\\0062\\004\\001\\002\\003\\t\\\"\\027\\n\\014num_features\\022\\003int(\\0010\\001\")", "instruction_en": "The code provides Python wrappers around TensorFlow ops for decoding LibSVM input to tensors. To use it, call the `decode_libsvm` function with the following arguments: `input` (a `Tensor` of type `string`), `num_features` (an `int` that is `>= 1`), `dtype` (an optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.float32`), `label_dtype` (an optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`), and `name` (a name for the operation, optional). The function returns a tuple of `Tensor` objects `(label, feature_indices, feature_values, feature_shape)`, where `label` is a `Tensor` of type `label_dtype`, `feature_indices` is a `Tensor` of type `int64`, `feature_values` is a `Tensor` of type `dtype`, and `feature_shape` is a `Tensor` of type `int64`.", "original_line_number": 26027, "instruction_length": 505}
{"instruction": "Buat kelas MuscletendonComplex yang mengambil parameter otot, keadaan, parameter lampiran, pembetulan mengimbangi, timestep, nama otot, dan sudut sendi. Kelas ini mengandungi fungsi stepupdatestate yang mengemas kini keadaan otot berdasarkan dinamik otot. Pengaktifan otot dikemas kini menggunakan kaedah trapezoid, dan panjang otot dikemas kini menggunakan kaedah yang sama. Kelas ini juga mengandungi fungsi reset_state yang menetapkan semula pembolehubah keadaan otot. Adalah disyorkan untuk menetapkan timestamp lebih rendah daripada 5E-4 untuk mendapatkan hasil yang baik. Model ini dicipta berdasarkan kertas Song dan Geyer 2015.", "input": "", "output": "import numpy as np\n\n\"\"\"\nIt is created for using MTC in Mujoco. The dynamics in this model is not continuous. The integration error will be\naccumulated overtime. And the system might get unstable if the timestep is too large. It is recommended to set the\ntimestamp lower than 5e-4 to get decent results.\n\nThe model is created based on Song's and Geyer's 2015 paper:\nSong, S. and Geyer, H., 2015. A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human\nlocomotion. The Journal of physiology, 593(16), pp.3493-3511.\n\nV0.1\nPassed basic tests. There're slightly difference compared to the simmechanics model.\n\nV0.2\n1. Verified with the simmechanics model. Difference in most of the cases can be ignored.\n2. Changed the integration method from forward Euler to trapezoid.\n3. Muscle force vce etc might vibrate/jitter if in some cases if the timestep is not low enough.\n   Need to improve this in the next version.\n   \n\"\"\"\n\nclass MuscleTendonComplex:\n    def __init__(self, paraMuscle, stateMuscle, paraMusAttach, offsetCorr, timestep, nameMuscle, angJoi):\n        self.frcmax, self.vmax, self.eref, self.lslack, self.lopt, self.tau, self.w, self.c, self.N, self.K = paraMuscle\n        self.stim, self.act, self.lmtc, self.lce, self.vce, self.frcmtc = stateMuscle\n        self.timestep = timestep\n        self.nameMuscle = nameMuscle\n        self.angJoi = angJoi\n        self.offsetCorr = offsetCorr\n        self.r, self.phiref, self.phimaxref, self.rho, self.dirAng, self.phiScale = paraMusAttach\n        self.MR =  0.01\n        self.typeMuscle = self.angJoi.size\n        nJoi = self.typeMuscle\n        self.levelArm = np.zeros(nJoi)\n\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError('Invalid muscle level arm offset correction type. ')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        self.lce = self.lmtc - self.lslack\n        self.lse = self.lmtc - self.lce\n        # unitless parameters\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        # test\n        self.lce_avg = self.lce\n        self.vce_avg = self.vce\n        self.frcmtc_avg = 0\n        self.act_avg = self.act\n        self.frame = 0\n        # self.Fse = 0.0\n        # self.Fbe = 0.0\n        # self.Fpe = 0.0\n        # self.Fce = 0.0\n\n\n    def stepUpdateState(self, angJoi):\n        \"\"\"\n        Muscle Tendon Complex Dynamics\n        update muscle states based on the muscle dynamics\n        Muscle state stim has to be updated outside before this function is called\n        \"\"\"\n        # update lmtc and level arm based on the geometry\n        self.angJoi = angJoi\n        nJoi = self.typeMuscle\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError('Invalid muscle level arm offset correction type. ')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        # update muscle activation\n        # integration, forward-Euler method\n        # self.act = (self.stim - self.act) * self.timestep / self.tau + self.act\n        # integration, trapezoidal method, 2-step\n        self.act = (self.stim - self.actsubstep) * self.timestep / 2.0 / self.tau + self.actsubstep\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n\n        # update lce and lse based on the lmtc\n        # integration, forward-Euler method\n        # self.lce = self.vce * self.timestep + self.lce\n        # integration, trapezoidal method, 2-step\n        self.lce = self.vce * self.timestep / 2.0 + self.lcesubstep\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        self.lse = self.lmtc - self.lce\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        # Serial Elastic element (tendon) force-length relationship\n        if self.Lse > 1.0:\n            Fse = np.power((self.Lse - 1.0) / self.eref, 2)\n        else:\n            Fse = 0.0\n\n        # Parallel Elasticity PE\n        if self.Lce > 1.0:\n            Fpe = np.power((self.Lce - 1.0) / self.w, 2)\n        else:\n            Fpe = 0.0\n\n        # update frcmtc\n        self.frcmtc = Fse * self.frcmax\n        #self.frcmtc =  np.clip(self.frcmtc, 0, self.frcmax)\n\n        # Buffer Elasticity BE\n        if (self.Lce - (1.0 - self.w)) < 0:\n            Fbe = np.power((self.Lce - (1.0 - self.w)) / (self.w / 2), 2)\n        else:\n            Fbe = 0.0\n\n        # Contractile Element force-length relationship\n        tmp = np.power(np.absolute(self.Lce - 1.0) / self.w, 3)\n        Fce = np.exp(tmp * np.log(self.c))\n\n        #Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n        if (Fpe + Fce * self.act) < 1e-10:  # avoid numerical error\n            if (Fse + Fbe) < 1e-10:\n                Fv = 1.0\n            else:\n                Fv = (Fse + Fbe) / 1e-10\n        else:\n            Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n\n        # Contractile Element inverse force-velocity relationship\n        if Fv <= 1.0:\n            # Concentric\n            v = (Fv - 1) / (Fv * self.K + 1.0)\n        elif Fv <= self.N:\n            # excentric\n            tmp = (Fv - self.N) / (self.N - 1.0)\n            v = (tmp + 1.0) / (1.0 - tmp * 7.56 * self.K)\n        else:\n            # excentric overshoot\n            v = ((Fv - self.N) * 0.01 + 1)\n\n        self.vce = v * self.lopt * self.vmax\n        v_frac = self.vce /  self.vmax\n        mr_scale =  self.act * np.absolute(self.frcmax*self.vmax) *self.timestep\n        if self.vce <= 1:\n            self.MR =  0.01 - 0.11*(v_frac) + 0.06*np.exp(-8*v_frac)\n        else:\n            self.MR =  0.23 - 0.16*np.exp(-8*v_frac) \n        self.MR *= mr_scale\n        self.frame += 1\n        self.lce_avg = (self.lce_avg*(self.frame - 1) +  self.lce) / self.frame\n        self.vce_avg = (self.vce_avg*(self.frame - 1) +  self.vce) / self.frame\n        self.frcmtc_avg = (self.frcmtc_avg*(self.frame - 1) +  self.frcmtc) / self.frame\n        self.act_avg = (self.act_avg*(self.frame - 1) +  self.act) / self.frame\n        #self.MR = np.exp(-self.MR)\n        # print(self.MR, np.exp(-self.MR))\n        # self.Fv = Fv\n        # self.Fse = Fse\n        # self.Fbe = Fbe\n        # self.Fpe = Fpe\n        # self.Fce = Fce\n\n    def reset_state(self):\n        self.frame = 0\n        self.lce_avg = 0\n        self.frcmtc_avg = 0\n        self.act_avg = 0\n        self.vce_avg = 0", "instruction_en": "Create a MuscleTendonComplex class that takes in muscle parameters, state, attachment parameters, offset correction, timestep, muscle name, and joint angle. The class contains a stepUpdateState function that updates the muscle states based on the muscle dynamics. The muscle activation is updated using the trapezoidal method, and the muscle length is updated using the same method. The class also contains a reset_state function that resets the muscle state variables. It is recommended to set the timestamp lower than 5e-4 to get decent results. The model is created based on Song's and Geyer's 2015 paper.", "original_line_number": 26207, "instruction_length": 635}
{"instruction": "Untuk memaparkan contoh label teks menggunakan pygame, ikuti langkah -langkah ini:\n\n1. Import modul yang diperlukan: SYS, Pygame, Core, dan Widget.\n2. Tetapkan lebar dan ketinggian tetingkap.\n3. Inisialisasi pygame dan font.\n4. Tetapkan saiz skrin dan jam.\n5. Tetapkan bingkai sesaat dan berjalan berubah menjadi benar.\n6. Buat panel menggunakan kelas widgets.panel dengan saiz grid (3, 10) dan saiz tetingkap.\n7. Tetapkan warna panel ke (155, 155, 155, 255).\n8. Buat label teks menggunakan kelas widgets.TextLabel dengan panel sebagai ibu bapa, kedudukan (1, 2), dan teks yang akan dipaparkan.\n9. Tetapkan warna teks menjadi putih, rentang ke (0, 5), sempadan hitam dengan lebar 8, dan margin hingga 10.\n10. Tentukan fungsi REDRAW yang mengemas kini skrin dan menarik panel dan teks.\n11. Masukkan gelung sementara yang berjalan selagi pemboleh ubah berjalan adalah benar.\n12. Tetapkan jam untuk menandakan bingkai yang ditentukan sesaat.\n13. Panggil fungsi Redraw.\n14. Periksa sebarang peristiwa pygame, seperti berhenti program, dan keluar dari program jika perlu.", "input": "", "output": "import sys\nsys.path.append(\"..\")\nimport pygame\nimport core\nimport widgets\n\n# Text label example.\n\nWINDOW_WIDTH = 1024\nWINDOW_HEIGHT = 728\n\npygame.init()\npygame.font.init\nscreen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\nclock = pygame.time.Clock()\nFPS = 60\nrunning = True\n\nif __name__ == \"__main__\":\t\n    panel = widgets.Panel(core.Grid((3, 10), (WINDOW_WIDTH, WINDOW_HEIGHT)), None, None, (0, 0))\n    panel.set_color((155, 155, 155, 255))\n    text = widgets.TextLabel(panel, (1, 2), core.Text(\n        \"\"\"\n        Lorem ipsum dolor sit amet, \n        consectetur adipiscing elit,\n        sed do eiusmod tempor incididunt\n        ut labore et dolore magna aliqua.\n        Ut enim ad minim veniam, quis\n        nostrud exercitation ullamco laboris\n        nisi ut aliquip ex ea commodo consequat.\n            Duis aute irure dolor in\n            reprehenderit in voluptate velit\n            esse cillum dolore eu fugiat\n            nulla pariatur. Excepteur sint\n            occaecat cupidatat non proident,\n            sunt in culpa qui officia deserunt\n            mollit anim id est laborum.\"\"\", 13, core.BLACK)\n    )\n    text.set_color(core.WHITE) # This is the color of the widget, not to be confused with the color of its text.\n    text.set_span((0, 5))\n    text.set_border(core.BLACK, 8)\n    text.set_margin(10) # Altering the margin because of the border.\n\n    def redraw():\n        pygame.display.flip()\n        screen.fill((0, 0, 0))\n        panel.draw(screen)\n        text.draw(screen)\n\nwhile (running):\n    clock.tick(FPS)\n    redraw()\n    for e in pygame.event.get():\n        if e.type == pygame.QUIT:\n            sys.exit()", "instruction_en": "To display a text label example using Pygame, follow these steps:\n\n1. Import the necessary modules: sys, pygame, core, and widgets.\n2. Set the window width and height.\n3. Initialize Pygame and the font.\n4. Set the screen size and clock.\n5. Set the frames per second and running variable to True.\n6. Create a panel using the widgets.Panel class with a grid size of (3, 10) and the window size.\n7. Set the panel color to (155, 155, 155, 255).\n8. Create a text label using the widgets.TextLabel class with the panel as the parent, position (1, 2), and the text to be displayed.\n9. Set the text color to white, the span to (0, 5), the border to black with a width of 8, and the margin to 10.\n10. Define a redraw function that updates the screen and draws the panel and text.\n11. Enter a while loop that runs as long as the running variable is True.\n12. Set the clock to tick at the specified frames per second.\n13. Call the redraw function.\n14. Check for any Pygame events, such as quitting the program, and exit the program if necessary.", "original_line_number": 26823, "instruction_length": 1066}
{"instruction": "Ujian kod ini dan menunjukkan penggunaan PeritriaxController. Untuk menjalankan simulasi, buat pek sfera dengan jejari dan bilangan sfera yang diberikan, dan masukkan sfera ke simulasi. Kemudian, sediakan enjin untuk simulasi, termasuk forcereSetter, InsertionSortCollider, InteractionLoop, PeritriaxController, dan NewtonIntegrator. Akhirnya, tetapkan langkah masa dan jalankan simulasi. Selepas simulasi selesai, pandangan simulasi boleh dipaparkan. Fungsi TriaxDone digunakan untuk mencetak maklumat mengenai simulasi dan mengubah matlamat PeritriaxController semasa simulasi.", "input": "", "output": "# coding: utf-8\n# 2009  Vclav milauer <eudoxos@arcig.cz>\n\"Test and demonstrate use of PeriTriaxController.\"\nfrom yade import *\nfrom yade import pack,qt\nO.periodic=True\n\nO.cell.hSize=Matrix3(0.1, 0, 0,\n\t\t     0 ,0.1, 0,\n\t\t    0, 0, 0.1)\n\t\t    \nsp=pack.SpherePack()\nradius=5e-3\nnum=sp.makeCloud(Vector3().Zero,O.cell.refSize,radius,.2,500,periodic=True) # min,max,radius,rRelFuzz,spheresInCell,periodic\nO.bodies.append([sphere(s[0],s[1]) for s in sp])\n\n\nO.engines=[\n\tForceResetter(),\n\tInsertionSortCollider([Bo1_Sphere_Aabb()],verletDist=.05*radius),\n\tInteractionLoop(\n\t\t[Ig2_Sphere_Sphere_ScGeom()],\n\t\t[Ip2_FrictMat_FrictMat_FrictPhys()],\n\t\t[Law2_ScGeom_FrictPhys_CundallStrack()]\n\t),\n\t#PeriTriaxController(maxUnbalanced=0.01,relStressTol=0.02,goal=[-1e4,-1e4,0],stressMask=3,globUpdate=5,maxStrainRate=[1.,1.,1.],doneHook='triaxDone()',label='triax'),\n\t#using cell inertia\n\tPeriTriaxController(dynCell=True,mass=0.2,maxUnbalanced=0.01,relStressTol=0.02,goal=(-1e4,-1e4,0),stressMask=3,globUpdate=5,maxStrainRate=(1.,1.,1.),doneHook='triaxDone()',label='triax'),\n\tNewtonIntegrator(damping=.2),\n]\nO.dt=PWaveTimeStep()\nO.run();\nqt.View()\n\nphase=0\ndef triaxDone():\n\tglobal phase\n\tif phase==0:\n\t\tprint 'Here we are: stress',triax.stress,'strain',triax.strain,'stiffness',triax.stiff\n\t\tprint 'Now z will go from 0 to .2 while x and y will be kept the same.'\n\t\ttriax.goal=(-1e4,-1e4,-0.2)\n\t\tphase+=1\n\telif phase==1:\n\t\tprint 'Here we are: stress',triax.stress,'strain',triax.strain,'stiffness',triax.stiff\n\t\tprint 'Done, pausing now.'\n\t\tO.pause()", "instruction_en": "This code tests and demonstrates the use of PeriTriaxController. To run the simulation, create a sphere pack with a given radius and number of spheres, and append the spheres to the simulation. Then, set up the engines for the simulation, including a ForceResetter, InsertionSortCollider, InteractionLoop, PeriTriaxController, and NewtonIntegrator. Finally, set the time step and run the simulation. After the simulation is complete, a view of the simulation can be displayed. The triaxDone function is used to print out information about the simulation and change the goal of the PeriTriaxController during the simulation.", "original_line_number": 27831, "instruction_length": 579}
{"instruction": "Untuk menggerakkan lengan robot dengan kawalan impedans, gunakan fungsi bergerak (k, t) dengan kekakuan yang dikehendaki (k) dan tempoh (t). Untuk menggerakkan pergelangan tangan, gunakan fungsi bergerak (wrist_frame, tool_frame, t, max_wrench) dengan pergelangan tangan dan bingkai alat yang dikehendaki, tempoh (t), dan kekangan sepana maksimum. Untuk memindahkan alat, gunakan fungsi MoveTool (Tool_Frame, T) dengan bingkai dan tempoh alat yang dikehendaki (T). Kod ini menganggap awalan lengan robot tertentu, jadi pastikan untuk menentukan awalan yang betul dalam hujah baris arahan.", "input": "", "output": "#!/usr/bin/env python\n\n# Copyright (c) 2014, Robot Control and Pattern Recognition Group, Warsaw University of Technology\n# All rights reserved.\n# \n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#     * Neither the name of the Warsaw University of Technology nor the\n#       names of its contributors may be used to endorse or promote products\n#       derived from this software without specific prior written permission.\n# \n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL <COPYright HOLDER> BE LIABLE FOR ANY\n# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport roslib\nroslib.load_manifest('velma_controller')\n\nimport rospy\n\nfrom geometry_msgs.msg import *\nfrom cartesian_trajectory_msgs.msg import *\nimport actionlib\nfrom actionlib_msgs.msg import *\nimport tf\nimport tf_conversions.posemath as pm\nimport PyKDL\n\ndef moveImpedance(k, t):\n    global action_impedance_client\n    action_impedance_goal = CartesianImpedanceGoal()\n    action_impedance_goal.trajectory.header.stamp = rospy.Time.now() + rospy.Duration(0.2)\n    action_impedance_goal.trajectory.points.append(CartesianImpedanceTrajectoryPoint(\n    rospy.Duration(t),\n    CartesianImpedance(k,Wrench(Vector3(0.7, 0.7, 0.7),Vector3(0.7, 0.7, 0.7)))))\n    action_impedance_client.send_goal(action_impedance_goal)\n\ndef moveWrist( wrist_frame, tool_frame, t, max_wrench):\n    global action_trajectory_client\n    # we are moving the tool, so: T_B_Wd*T_W_T\n    wrist_pose = pm.toMsg(wrist_frame*tool_frame)\n\n    action_trajectory_goal = CartesianTrajectoryGoal()\n    action_trajectory_goal.trajectory.header.stamp = rospy.Time.now() + rospy.Duration(0.01)\n    action_trajectory_goal.trajectory.points.append(CartesianTrajectoryPoint(\n    rospy.Duration(t),\n    wrist_pose,\n    Twist()))\n    action_trajectory_goal.wrench_constraint = max_wrench\n    action_trajectory_client.send_goal(action_trajectory_goal)\n\ndef moveTool(tool_frame, t):\n    global action_tool_client\n    tool_pose = pm.toMsg(tool_frame)\n    action_tool_goal = CartesianTrajectoryGoal()\n    action_tool_goal.trajectory.header.stamp = rospy.Time.now()\n    action_tool_goal.trajectory.points.append(CartesianTrajectoryPoint(\n    rospy.Duration(t),\n    tool_pose,\n    Twist()))\n    action_tool_client.send_goal(action_tool_goal)\n\n\nif __name__ == '__main__':\n\n    a = []\n    for arg in sys.argv:\n        a.append(arg)\n\n    if (len(a) > 1) and ((a[1]==\"left\") or (\"right\")):\n        prefix = a[1]\n    else:\n        print \"Usage: %s prefix\"%a[0]\n        exit(0)\n\n    rospy.init_node('impedance_riser')\n\n    listener = tf.TransformListener();\n\n    action_impedance_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/cartesian_impedance\", CartesianImpedanceAction)\n    action_impedance_client.wait_for_server()\n\n    action_trajectory_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/cartesian_trajectory\", CartesianTrajectoryAction)\n    action_trajectory_client.wait_for_server()\n\n    action_tool_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/tool_trajectory\", CartesianTrajectoryAction)\n    action_tool_client.wait_for_server()\n\n    rospy.sleep(1.0)\n\n    # save current wrist position\n    time_now = rospy.Time.now() - rospy.Duration(1.0)\n    listener.waitForTransform('torso_base', prefix+'_arm_7_link', time_now, rospy.Duration(4.0))\n    pose = listener.lookupTransform('torso_base', prefix+'_arm_7_link', time_now)\n    T_B_W = pm.fromTf(pose)\n\n    T_W_T = PyKDL.Frame()    # tool transformation\n    print \"setting the tool to %s relative to wrist frame\"%(T_W_T)\n    # move both tool position and wrist position - the gripper holds its position\n    print \"moving wrist\"\n    # we assume that during the initialization there are no contact forces, so we limit the wrench\n    moveWrist( T_B_W, T_W_T, 2.0, Wrench(Vector3(20, 20, 20), Vector3(4, 4, 4)) )\n    print \"moving tool\"\n    moveTool( T_W_T, 2.0 )\n    rospy.sleep(2.0)\n\n    # change the stiffness\n    print \"changing stiffness for door approach\"\n    moveImpedance(Wrench(Vector3(1200.0, 1200.0, 1200.0), Vector3(300.0, 300.0, 300.0)), 4.0)\n    rospy.sleep(4.0)", "instruction_en": "To move the robot arm with impedance control, use the moveImpedance(k, t) function with the desired stiffness (k) and duration (t). To move the wrist, use the moveWrist(wrist_frame, tool_frame, t, max_wrench) function with the desired wrist and tool frame, duration (t), and maximum wrench constraint. To move the tool, use the moveTool(tool_frame, t) function with the desired tool frame and duration (t). The code assumes a specific robot arm prefix, so make sure to specify the correct prefix in the command line argument.", "original_line_number": 27839, "instruction_length": 588}
{"instruction": "Untuk memeriksa sama ada nombor tertentu adalah nombor hodoh, tentukan fungsi isulgynumber (num) yang mengambil num integer positif sebagai input dan pulangan benar jika num adalah nombor hodoh, sebaliknya palsu. Nombor hodoh adalah nombor positif yang faktor utama hanya termasuk 2, 3, 5. Fungsi pemeriksaan rekursif jika NUM dibahagikan dengan 2, 3, atau 5, dan jika ya, membahagikan Num dengan 2, 3, atau 5 dan memanggil dirinya dengan kuota sebagai input. Jika NUM tidak dibahagikan dengan 2, 3, atau 5, fungsi itu kembali palsu. \n\nUntuk mencari nombor N-th hodoh, tentukan fungsi ulgynumber (n) yang mengambil integer positif n sebagai input dan mengembalikan senarai nombor n hodoh pertama. Nombor hodoh adalah nombor positif yang faktor utama hanya termasuk 2, 3, 5. Fungsi ini memulakan senarai panjang n dengan semua elemen yang ditetapkan kepada 1, dan jumlah senarai panjang 3 dengan semua elemen yang ditetapkan ke 0. Perdana dalam prima. Fungsi ini kemudian mengemas kini mengira [j] untuk mengira [j] +1 untuk setiap perdana yang memenuhi nextNum == prime*res [count [j]], dan menetapkan res [i] ke nextnum. Akhirnya, fungsi kembali Res.\n\nUntuk mencari nombor super hodoh nth, tentukan fungsi nthsuperuglynumber (n, primes) yang mengambil integer positif n dan senarai k prima sebagai input dan mengembalikan nombor super hodoh n. Nombor super hodoh adalah nombor positif yang semua faktor utama berada dalam prima senarai utama saiz k. Fungsi ini memulakan senarai res yang panjang n dengan semua elemen yang ditetapkan kepada 1, dan kiraan senarai panjang K dengan semua elemen yang ditetapkan ke 0. Fungsi kemudian berulang dari 1 hingga n-1, mengira nombor super hodoh seterusnya sebagai minimum masa perdana res [kiraan [indeks]] untuk setiap perdana dalam prima, di mana indeks adalah indeks perdana dalam primes. Fungsi ini kemudian mengemas kini kiraan [indeks] untuk mengira [indeks] +1 untuk setiap perdana yang memenuhi nextNum == prime*res [count [index]], dan menetapkan res [__ index] ke NextNum. Akhirnya, fungsi mengembalikan RES [-1].", "input": "", "output": "'''\nCreated on 20151211\nhttps://leetcode.com/problems/ugly-number/\nhttps://leetcode.com/problems/ugly-number-ii/\nhttps://leetcode.com/problems/super-ugly-number/\n@author: Darren\n'''\n'''\nWrite a program to check whether a given number is an ugly number.\n\nUgly numbers are positive numbers whose prime factors only include 2, 3, 5. For example, 6, 8 are ugly while 14 is not ugly since it includes another prime factor 7.\n\nNote that 1 is typically treated as an ugly number.\n'''\ndef isUlgyNumber(num):\n    if not num:\n        return False\n    if num==1:\n        return True\n    if num%2==0:\n        return isUlgyNumber(num//2)\n    if num%3==0:\n        return isUlgyNumber(num//3)\n    if num%5==0:\n        return isUlgyNumber(num//5)\n    return False\n\nprint(isUlgyNumber(14))\n\n'''\nWrite a program to find the n-th ugly number.\n\nUgly numbers are positive numbers whose prime factors only include 2, 3, 5. For example, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 is the sequence of the first 10 ugly numbers.\n\nNote that 1 is typically treated as an ugly number.\n'''\n\ndef ulgyNumber(N):\n    if N<1:\n        raise Exception(\"Invalid Input\")\n    if N==1:\n        return 1\n    res=[1]*N\n    count=[0]*3\n    primes=[2,3,5]\n    for i in range(1,N):\n        nextNum=min([prime*res[count[j]] for j,prime in enumerate(primes)])\n        for j,prime in enumerate(primes):\n            if nextNum==prime*res[count[j]]:\n                count[j]+=1\n        res[i]=nextNum\n    return res\n\nprint(ulgyNumber(10))\n\n'''\nWrite a program to find the nth super ugly number.\n\nSuper ugly numbers are positive numbers whose all prime factors are in the given prime list primes of size k. For example, [1, 2, 4, 7, 8, 13, 14, 16, 19, 26, 28, 32] is the sequence of the first 12 super ugly numbers given primes = [2, 7, 13, 19] of size 4.\n\nNote:\n(1) 1 is a super ugly number for any given primes.\n(2) The given numbers in primes are in ascending order.\n(3) 0 < k  100, 0 < n  106, 0 < primes[i] < 1000.\n'''\ndef nthSuperUglyNumber(n, primes):\n    \"\"\"\n    :type n: int\n    :type primes: List[int]\n    :rtype: int\n    \"\"\"\n    if n==1:\n        return 1\n    res=[1]*n\n    count=[0]*len(primes)\n    for __index in range(1,n):\n        nextNum=min([prime*res[count[index]] for index,prime in enumerate(primes)])\n        for index,prime in enumerate(primes):\n            if nextNum==prime*res[count[index]]:\n                count[index]+=1\n        res[__index]=nextNum\n    return res[-1] \n    \nn=200000\nprimes=[2,3,5,13,19,29,31,41,43,53,59,73,83,89,97,103,107,109,127,137,139,149,163,173,179,193,197,199,211,223,227,229,239,241,251,257,263,269,271,281,317,331,337,347,353,359,367,373,379,389,397,409,419,421,433,449,457,461,463,479,487,509,521,523,541,547,563,569,577,593,599,601,613,619,631,641,659,673,683,701,709,719,733,739,743,757,761,769,773,809,811,829,857,859,881,919,947,953,967,971]\nprint(nthSuperUglyNumber(n, primes))", "instruction_en": "To check whether a given number is an ugly number, define a function isUlgyNumber(num) that takes a positive integer num as input and returns True if num is an ugly number, False otherwise. An ugly number is a positive number whose prime factors only include 2, 3, 5. The function recursively checks if num is divisible by 2, 3, or 5, and if so, divides num by 2, 3, or 5 and calls itself with the quotient as input. If num is not divisible by 2, 3, or 5, the function returns False. \n\nTo find the n-th ugly number, define a function ulgyNumber(N) that takes a positive integer N as input and returns a list of the first N ugly numbers. An ugly number is a positive number whose prime factors only include 2, 3, 5. The function initializes a list res of length N with all elements set to 1, and a list count of length 3 with all elements set to 0. It also initializes a list primes with the values 2, 3, and 5. The function then iterates from 1 to N-1, computing the next ugly number as the minimum of prime times res[count[j]] for each prime in primes, where j is the index of prime in primes. The function then updates count[j] to count[j]+1 for each prime that satisfies nextNum==prime*res[count[j]], and sets res[i] to nextNum. Finally, the function returns res.\n\nTo find the nth super ugly number, define a function nthSuperUglyNumber(n, primes) that takes a positive integer n and a list of k primes as input and returns the nth super ugly number. A super ugly number is a positive number whose all prime factors are in the given prime list primes of size k. The function initializes a list res of length n with all elements set to 1, and a list count of length k with all elements set to 0. The function then iterates from 1 to n-1, computing the next super ugly number as the minimum of prime times res[count[index]] for each prime in primes, where index is the index of prime in primes. The function then updates count[index] to count[index]+1 for each prime that satisfies nextNum==prime*res[count[index]], and sets res[__index] to nextNum. Finally, the function returns res[-1].", "original_line_number": 28329, "instruction_length": 2065}
{"instruction": "Untuk menggunakan kelas VLCClient untuk komunikasi dengan pemain VLC, ikuti langkah -langkah ini:\n\n1. Import modul yang diperlukan: threading, time, urllib, player.client.baseplayer, vlc.meta, vlc.eventtype, giliran, dan util.fileutil.\n\n2. Buat contoh kelas VLCClient.\n\n3. Panggil kaedah start_client () untuk memulakan benang untuk mengendalikan peristiwa VLC.\n\n4. Panggil kaedah set_proxy () untuk membuat pemain VLC baru.\n\n5. Panggil kaedah main () untuk mula bermain trek/stesen yang ditentukan.\n\n6. Panggil kaedah berhenti () untuk menghentikan main balik.\n\n7. Panggil kaedah Seek () untuk melompat ke kedudukan yang ditentukan di trek.\n\n8. Panggil kaedah Play_Pause () untuk bermain/menjeda main balik.\n\n9. Panggil kaedah set_volume () untuk menetapkan tahap kelantangan.\n\n10. Panggil kaedah bisu () untuk meredakan audio.\n\n11. Panggil kaedah semasa () untuk mengembalikan lagu semasa.\n\n12. Panggil kaedah penutupan () untuk menutup pemain.\n\n13. Panggil kaedah get_current_track_time () untuk mengembalikan masa trek semasa.\n\n14. Panggil kaedah get_current_playlist () untuk mengembalikan senarai main semasa.", "input": "", "output": "# Copyright 2016-2021 Peppy Player peppy.player@gmail.com\n# \n# This file is part of Peppy Player.\n# \n# Peppy Player is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# Peppy Player is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with Peppy Player. If not, see <http://www.gnu.org/licenses/>.\n\nimport threading\nimport time\nimport urllib\n\nfrom player.client.baseplayer import BasePlayer\nfrom vlc import Meta\nfrom vlc import EventType\nfrom queue import Queue\nfrom util.fileutil import FILE_PLAYLIST, FILE_AUDIO\n\nclass Vlcclient(BasePlayer):\n    \"\"\" This class extends base player and provides communication with VLC player \n    using Python binding for 'libvlc' library \"\"\"\n        \n    def __init__(self):\n        \"\"\" Initializer. Starts separate threads for handling VLC events \"\"\"\n        \n        self.RADIO_MODE = \"radio\"\n        BasePlayer.__init__(self)\n        self.mode = self.RADIO_MODE\n        self.instance = None\n        self.player = None\n        self.media = None\n        self.current_track = \"\"\n        self.seek_time = \"0\"\n        self.cd_track_id = None\n        self.cd_drive_name = None\n        self.END_REACHED = \"end reached\"\n        self.TRACK_CHANGED = \"track changed\"\n        self.PAUSED = \"paused\"\n        self.player_queue = Queue()\n        self.threads_running = False\n        self.changing_volume = False\n\n    def start_client(self):\n        \"\"\" Start threads. \"\"\"\n        \n        self.threads_running = True\n        thread_1 = threading.Thread(target = self.radio_stream_event_listener)\n        thread_1.start()\n        thread_2 = threading.Thread(target = self.handle_event_queue)\n        thread_2.start()\n\n    def stop_client(self):\n        \"\"\" Stop threads \"\"\"\n\n        with self.lock:\n            self.threads_running = False\n\n    def set_proxy(self, proxy_process, proxy=None):\n        \"\"\" Create new VLC player \"\"\"\n        \n        self.instance = proxy_process\n        self.proxy = proxy\n        self.player = self.instance.media_player_new()\n        player_mgr = self.player.event_manager()\n        player_mgr.event_attach(EventType.MediaPlayerEndReached, self.player_callback, [self.END_REACHED])\n        player_mgr.event_attach(EventType.MediaPlayerPlaying, self.player_callback, [self.TRACK_CHANGED])\n    \n    def player_callback(self, event, data):\n        \"\"\" Player callback method \n        \n        :param event: event to handle\n        :param data: event data\n        \"\"\"\n        if data:\n            self.player_queue.put(data[0])\n    \n    def radio_stream_event_listener(self):\n        \"\"\" Starts the loop for listening VLC events for radio track change \"\"\"\n\n        while self.threads_running:\n            with self.lock:\n                if self.media and self.mode == self.RADIO_MODE:\n                    t = self.media.get_meta(Meta.NowPlaying)\n                    if t and t != self.current_track:\n                        self.current_track = t\n                        if self.enabled:\n                            self.notify_player_listeners({\"current_title\": t})\n            time.sleep(1)\n\n    def handle_event_queue(self):\n        \"\"\" Handling player event queue \"\"\"\n        \n        if not self.enabled:\n            return\n\n        while self.threads_running:\n            d = self.player_queue.get() # blocking line\n            if d  == self.END_REACHED:\n                self.notify_end_of_track_listeners()\n                self.player_queue.task_done()\n            elif d  == self.TRACK_CHANGED:\n                self.track_changed()\n                self.player_queue.task_done()\n                \n    def track_changed(self):\n        \"\"\" Handle track change event \"\"\"\n\n        if not self.enabled:\n            return\n        \n        if self.mode == self.RADIO_MODE: \n            return\n        \n        current = {\"source\": \"player\"}\n        current[\"state\"] = \"playing\"\n        t = self.media.get_meta(Meta.Title)\n        if t == \".\":\n            return\n        if self.cd_track_id and t.startswith(\"cdda:\"):\n            current[\"cd_track_id\"] = self.cd_track_id\n            if self.cd_tracks:\n                t = self.cd_tracks[int(self.cd_track_id) - 1].name\n            else:\n                t = self.cd_drive_name + self.cd_track_title + \" \" + self.cd_track_id\n                \n        m = self.media.get_mrl()\n        m = m[m.rfind(\"/\") + 1:]\n        m = urllib.parse.unquote(m)\n        current[\"file_name\"] = m\n        current[\"current_title\"] = t\n        current[\"Time\"] = str(self.player.get_length()/1000)\n        \n        if not self.seek_time:\n            self.seek_time = \"0\"\n        current[\"seek_time\"] = self.seek_time\n        self.notify_player_listeners(current)\n\n    def set_player_volume_control(self, flag):\n        \"\"\" Player Volume Control type setter\n\n        :param volume: True - player volume cotrol type, False - amixer or hardware volume control type\n        \"\"\"\n        BasePlayer.set_player_volume_control(self, flag)\n        if not self.player_volume_control:\n            self.set_volume(100)\n\n    def play(self, state):\n        \"\"\" Start playing specified track/station. First it cleans the playlist \n        then adds new track/station to the list and then starts playback\n        syntax for CD:\n        self.media = self.instance.media_new(\"cdda:///E:/\", (\":cdda-track=7\"))\n        \n        :param state: button state which contains the track/station info\n        \"\"\"\n        url = None\n        self.enabled = True\n\n        if state == None:\n            if self.state != None:\n                url = getattr(self.state, \"url\", None)\n            else:\n                url = None\n        else:\n            url = getattr(state, \"url\", None)\n            self.state = state    \n\n        if url == None: \n            return      \n\n        url = url.replace(\"\\\\\", \"/\").replace(\"\\\"\", \"\")\n        track_time = getattr(self.state, \"track_time\", None)\n        if track_time == None:\n            track_time = \"0\"\n        else:\n            track_time = str(track_time)\n            if \":\" in track_time:\n                track_time = track_time.replace(\":\", \".\")\n        self.seek_time = track_time\n            \n        s = getattr(self.state, \"playback_mode\", None)\n        \n        if s and s == FILE_PLAYLIST:\n            self.stop()            \n            self.mode = FILE_PLAYLIST\n            self.enabled = True\n        elif s and s == FILE_AUDIO:\n            self.mode = FILE_AUDIO\n        else:\n            self.mode = self.RADIO_MODE\n        \n        if url.startswith(\"http\") and self.mode != self.RADIO_MODE:\n            url = self.encode_url(url)\n        \n        with self.lock:\n            file_name = getattr(self.state, \"file_name\", None)\n            if file_name and file_name.startswith(\"cdda://\"):\n                parts = file_name.split()\n                self.cd_track_id = parts[1].split(\"=\")[1]                \n                self.cd_drive_name = parts[0][len(\"cdda:///\"):]\n                self.media = self.instance.media_new(parts[0], parts[1])\n            else:            \n                self.media = self.instance.media_new(url)\n            self.player.set_media(self.media)            \n            \n            self.player.play()\n            try:\n                self.player.set_time(int(float(self.seek_time)) * 1000)\n            except:\n                pass\n            \n            if self.player_volume_control and getattr(self.state, \"volume\", None) != None:\n                self.set_volume(int(self.state.volume))\n            \n    def stop(self, state=None):\n        \"\"\" Stop playback \"\"\"\n        \n        with self.lock:\n            self.enabled = False\n            self.player.stop()\n    \n    def seek(self, time):\n        \"\"\" Jump to the specified position in the track\n        \n        :param time: time position in track\n        \"\"\"\n        \n        if \":\" in time:\n            self.seek_time = self.get_seconds_from_string(time)\n        else:\n            self.seek_time = time\n        \n        with self.lock:            \n            msec = int(float(self.seek_time) * 1000)\n            t = threading.Thread(target=self.seek_method, args=[msec])\n            t.start()\n\n    def seek_method(self, msec):\n        \"\"\" Seek track thread method\n\n        :param msec: milliseconds for new position\n        \"\"\"\n        self.player.set_time(msec)\n    \n    def play_pause(self, pause_flag=None):\n        \"\"\" Play/Pause playback \n        \n        :param pause_flag: play/pause flag\n        \"\"\" \n        with self.lock:\n            self.seek_time = self.get_current_track_time()\n            self.player.pause()\n    \n    def set_volume(self, level):\n        \"\"\" Set volume.\n        \n        :param level: new volume level\n        \"\"\"\n        self.player.audio_set_volume(int(level))\n        \n        if getattr(self, \"state\", None) != None:\n            if self.state.volume == level:\n                return\n            self.state.volume = level\n\n        v = self.get_volume()\n        if v != int(level): # usually initial volume setting\n            if hasattr(self, \"volume_thread\"):\n                self.volume_thread.join()    \n            self.volume_thread = threading.Thread(target=self.set_volume_level, args=[level])\n            self.volume_thread.start()\n    \n    def set_volume_level(self, level):\n        \"\"\" Set volume level in separate thread\n        \n        :param level: volume level\n        \"\"\"\n        n = 0\n        max_n = 20\n        vol = -2\n        \n        while n < max_n and level != vol:\n            self.player.audio_set_volume(int(level))\n            time.sleep(0.1)\n            vol = self.get_volume()\n            n += 1\n    \n    def get_volume(self):\n        \"\"\"  Return current volume level \n        \n        :return: volume level or -1 if not available\n        \"\"\"\n        with self.lock:\n            return self.player.audio_get_volume()\n    \n    def mute(self):\n        \"\"\" Mute \"\"\"\n        \n        with self.lock:\n            self.player.audio_toggle_mute()\n        \n    def current(self):\n        \"\"\" Return the current song \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\" Shutdown the player \"\"\"\n        with self.lock:\n            self.player.stop()\n        \n    def get_current_track_time(self):\n        \"\"\"  Return current track time\n        \n        :return: current track time\n        \"\"\"\n        t = self.player.get_time()/1000\n        return str(t)\n    \n    def get_current_playlist(self):\n        \"\"\"  Return current playlist\n        \n        :return: current playlist\n        \"\"\"\n        return self.playlist", "instruction_en": "To use the Vlcclient class for communication with VLC player, follow these steps:\n\n1. Import the necessary modules: threading, time, urllib, player.client.baseplayer, vlc.Meta, vlc.EventType, queue, and util.fileutil.\n\n2. Create an instance of the Vlcclient class.\n\n3. Call the start_client() method to start the threads for handling VLC events.\n\n4. Call the set_proxy() method to create a new VLC player.\n\n5. Call the play() method to start playing a specified track/station.\n\n6. Call the stop() method to stop playback.\n\n7. Call the seek() method to jump to a specified position in the track.\n\n8. Call the play_pause() method to play/pause playback.\n\n9. Call the set_volume() method to set the volume level.\n\n10. Call the mute() method to mute the audio.\n\n11. Call the current() method to return the current song.\n\n12. Call the shutdown() method to shutdown the player.\n\n13. Call the get_current_track_time() method to return the current track time.\n\n14. Call the get_current_playlist() method to return the current playlist.", "original_line_number": 28521, "instruction_length": 1115}
{"instruction": "Tukar graf Bel ke objek JSON nod-pautan menggunakan fungsi `to_nodelink (graf)`. Tulis objek JSON yang dihasilkan ke fail menggunakan fungsi `TO_NODELINK_FILE (graf, laluan)`, di mana `graf` adalah graf Bel dan` Path` adalah laluan ke fail. Sebagai alternatif, tulis objek JSON ke fail gzip menggunakan fungsi `to_nodelink_gz (graf, jalan)`. Untuk membaca graf Bel dari objek JSON node-link, gunakan fungsi `dari_nodelink (graph_json_dict)`, di mana `graph_json_dict` adalah objek JSON. Anda juga boleh membaca graf Bel dari fail yang mengandungi nod-pautan JSON menggunakan fungsi `dari_nodelink_file (path)`, di mana `path` adalah laluan ke fail. Untuk membaca graf Bel dari fail gzip yang mengandungi nod-pautan JSON, gunakan fungsi `dari_nodelink_gz (path)`, di mana `jalan` adalah laluan ke fail gzip.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n\"\"\"Conversion functions for BEL graphs with node-link JSON.\"\"\"\n\nimport gzip\nimport json\nfrom io import BytesIO\nfrom itertools import chain, count\nfrom operator import methodcaller\nfrom typing import Any, Mapping, TextIO, Union\n\nfrom networkx.utils import open_file\n\nfrom .utils import ensure_version\nfrom ..constants import (\n    ANNOTATIONS, CITATION, FUSION, GRAPH_ANNOTATION_CURIE, GRAPH_ANNOTATION_LIST, GRAPH_ANNOTATION_MIRIAM, MEMBERS,\n    PARTNER_3P,\n    PARTNER_5P, PRODUCTS, REACTANTS, SOURCE_MODIFIER, TARGET_MODIFIER,\n)\nfrom ..dsl import BaseEntity\nfrom ..language import citation_dict\nfrom ..struct import BELGraph\nfrom ..struct.graph import _handle_modifier\nfrom ..tokens import parse_result_to_dsl\nfrom ..utils import hash_edge, tokenize_version\n\n__all__ = [\n    'to_nodelink',\n    'to_nodelink_file',\n    'to_nodelink_gz',\n    'to_nodelink_jsons',\n    'from_nodelink',\n    'from_nodelink_file',\n    'from_nodelink_gz',\n    'from_nodelink_jsons',\n    'to_nodelink_gz_io',\n    'from_nodelink_gz_io',\n]\n\n\ndef to_nodelink(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert this graph to a node-link JSON object.\n\n    :param graph: BEL Graph\n    \"\"\"\n    graph_json_dict = _to_nodelink_json_helper(graph)\n\n    _prepare_graph_dict(graph_json_dict['graph'])\n\n    return graph_json_dict\n\n\ndef _prepare_graph_dict(g):\n    # Convert annotation list definitions (which are sets) to canonicalized/sorted lists\n    g[GRAPH_ANNOTATION_LIST] = {\n        keyword: list(sorted(values))\n        for keyword, values in g.get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n\n    g[GRAPH_ANNOTATION_CURIE] = list(sorted(g[GRAPH_ANNOTATION_CURIE]))\n    g[GRAPH_ANNOTATION_MIRIAM] = list(sorted(g[GRAPH_ANNOTATION_MIRIAM]))\n\n\n@open_file(1, mode='w')\ndef to_nodelink_file(graph: BELGraph, path: Union[str, TextIO], **kwargs) -> None:\n    \"\"\"Write this graph as node-link JSON to a file.\n\n    :param graph: A BEL graph\n    :param path: A path or file-like\n    \"\"\"\n    graph_json_dict = to_nodelink(graph)\n    json.dump(graph_json_dict, path, ensure_ascii=False, **kwargs)\n\n\ndef to_nodelink_gz(graph, path: str, **kwargs) -> None:\n    \"\"\"Write a graph as node-link JSON to a gzip file.\"\"\"\n    with gzip.open(path, 'wt') as file:\n        json.dump(to_nodelink(graph), file, ensure_ascii=False, **kwargs)\n\n\ndef to_nodelink_jsons(graph: BELGraph, **kwargs) -> str:\n    \"\"\"Dump this graph as a node-link JSON object to a string.\"\"\"\n    return json.dumps(to_nodelink(graph), ensure_ascii=False, **kwargs)\n\n\ndef from_nodelink(graph_json_dict: Mapping[str, Any], check_version: bool = True) -> BELGraph:\n    \"\"\"Build a graph from node-link JSON Object.\"\"\"\n    pybel_version = tokenize_version(graph_json_dict['graph']['pybel_version'])\n    if pybel_version[1] < 14:  # if minor version is less than 14\n        raise ValueError('Invalid NodeLink JSON from old version of PyBEL (v{}.{}.{})'.format(*pybel_version))\n    graph = _from_nodelink_json_helper(graph_json_dict)\n    return ensure_version(graph, check_version=check_version)\n\n\n@open_file(0, mode='r')\ndef from_nodelink_file(path: Union[str, TextIO], check_version: bool = True) -> BELGraph:\n    \"\"\"Build a graph from the node-link JSON contained in the given file.\n\n    :param path: A path or file-like\n    \"\"\"\n    return from_nodelink(json.load(path), check_version=check_version)\n\n\ndef from_nodelink_gz(path: str) -> BELGraph:\n    \"\"\"Read a graph as node-link JSON from a gzip file.\"\"\"\n    with gzip.open(path, 'rt') as file:\n        return from_nodelink(json.load(file))\n\n\ndef from_nodelink_jsons(graph_json_str: str, check_version: bool = True) -> BELGraph:\n    \"\"\"Read a BEL graph from a node-link JSON string.\"\"\"\n    return from_nodelink(json.loads(graph_json_str), check_version=check_version)\n\n\ndef _to_nodelink_json_helper(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert a BEL graph to a node-link format.\n\n    :param graph: BEL Graph\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_data`\n    \"\"\"\n    nodes = sorted(graph, key=methodcaller('as_bel'))\n\n    mapping = dict(zip(nodes, count()))\n\n    return {\n        'directed': True,\n        'multigraph': True,\n        'graph': graph.graph.copy(),\n        'nodes': [\n            _augment_node(node)\n            for node in nodes\n        ],\n        'links': [\n            dict(\n                chain(\n                    data.copy().items(),\n                    [('source', mapping[u]), ('target', mapping[v]), ('key', key)],\n                ),\n            )\n            for u, v, key, data in graph.edges(keys=True, data=True)\n        ],\n    }\n\n\ndef _augment_node(node: BaseEntity) -> BaseEntity:\n    \"\"\"Add the SHA-512 identifier to a node's dictionary.\"\"\"\n    rv = node.copy()\n    rv['id'] = node.md5\n    rv['bel'] = node.as_bel()\n    for m in chain(node.get(MEMBERS, []), node.get(REACTANTS, []), node.get(PRODUCTS, [])):\n        m.update(_augment_node(m))\n    if FUSION in node:\n        node[FUSION][PARTNER_3P].update(_augment_node(node[FUSION][PARTNER_3P]))\n        node[FUSION][PARTNER_5P].update(_augment_node(node[FUSION][PARTNER_5P]))\n    return rv\n\n\ndef _recover_graph_dict(graph: BELGraph):\n    graph.graph[GRAPH_ANNOTATION_LIST] = {\n        keyword: set(values)\n        for keyword, values in graph.graph.get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n    graph.graph[GRAPH_ANNOTATION_CURIE] = set(graph.graph.get(GRAPH_ANNOTATION_CURIE, []))\n    graph.graph[GRAPH_ANNOTATION_MIRIAM] = set(graph.graph.get(GRAPH_ANNOTATION_MIRIAM, []))\n\n\ndef _from_nodelink_json_helper(data: Mapping[str, Any]) -> BELGraph:\n    \"\"\"Return graph from node-link data format.\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_graph`\n    \"\"\"\n    graph = BELGraph()\n    graph.graph = data.get('graph', {})\n    _recover_graph_dict(graph)\n\n    mapping = []\n\n    for node_data in data['nodes']:\n        node = parse_result_to_dsl(node_data)\n        graph.add_node_from_data(node)\n        mapping.append(node)\n\n    for data in data['links']:\n        u = mapping[data['source']]\n        v = mapping[data['target']]\n\n        edge_data = {\n            k: v\n            for k, v in data.items()\n            if k not in {'source', 'target', 'key'}\n        }\n\n        for side in (SOURCE_MODIFIER, TARGET_MODIFIER):\n            side_data = edge_data.get(side)\n            if side_data:\n                _handle_modifier(side_data)\n\n        if CITATION in edge_data:\n            edge_data[CITATION] = citation_dict(**edge_data[CITATION])\n\n        if ANNOTATIONS in edge_data:\n            edge_data[ANNOTATIONS] = graph._clean_annotations(edge_data[ANNOTATIONS])\n\n        graph.add_edge(u, v, key=hash_edge(u, v, edge_data), **edge_data)\n\n    return graph\n\n\ndef to_nodelink_gz_io(graph: BELGraph) -> BytesIO:\n    \"\"\"Get a BEL graph as a compressed BytesIO.\"\"\"\n    bytes_io = BytesIO()\n    with gzip.GzipFile(fileobj=bytes_io, mode='w') as file:\n        s = to_nodelink_jsons(graph)\n        file.write(s.encode('utf-8'))\n    bytes_io.seek(0)\n    return bytes_io\n\n\ndef from_nodelink_gz_io(bytes_io: BytesIO) -> BELGraph:\n    \"\"\"Get BEL from gzipped nodelink JSON.\"\"\"\n    with gzip.GzipFile(fileobj=bytes_io, mode='r') as file:\n        s = file.read()\n    j = s.decode('utf-8')\n    return from_nodelink_jsons(j)", "instruction_en": "Convert a BEL graph to a node-link JSON object using the function `to_nodelink(graph)`. Write the resulting JSON object to a file using the function `to_nodelink_file(graph, path)`, where `graph` is the BEL graph and `path` is the path to the file. Alternatively, write the JSON object to a gzip file using the function `to_nodelink_gz(graph, path)`. To read a BEL graph from a node-link JSON object, use the function `from_nodelink(graph_json_dict)`, where `graph_json_dict` is the JSON object. You can also read a BEL graph from a file containing node-link JSON using the function `from_nodelink_file(path)`, where `path` is the path to the file. To read a BEL graph from a gzip file containing node-link JSON, use the function `from_nodelink_gz(path)`, where `path` is the path to the gzip file.", "original_line_number": 28736, "instruction_length": 806}
{"instruction": "Kod yang diberikan mengandungi beberapa fungsi python yang boleh digunakan untuk memeriksa status robot XARM. Untuk menggunakan fungsi ini, importnya ke dalam skrip Python anda dan gunakannya sebagai penghias untuk fungsi yang ingin anda periksa. Sebagai contoh, untuk memeriksa sama ada robot disambungkan sebelum menjalankan fungsi, gunakan penghias `xarm_is_connected`. Untuk memeriksa sama ada robot siap sebelum menjalankan fungsi, gunakan penghias `xarm_is_ready`. Untuk log panggilan API yang dibuat oleh skrip anda, gunakan penghias `API_LOG`.", "input": "", "output": "# !/usr/bin/env python3\n# Software License Agreement (BSD License)\n#\n# Copyright (c) 2018, UFACTORY, Inc.\n# All rights reserved.\n#\n# Author: Vinman <vinman.wen@ufactory.cc> <vinman.cub@gmail.com>\n\nimport time\nimport functools\nfrom ..core.utils.log import logger\nfrom .code import APIState\n\n\ndef check_modbus_baud(baud=2000000, _type='set', default=None):\n    def _check_modbus_baud(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            code = args[0].checkset_modbus_baud(baud)\n            if code != 0:\n                logger.error('check modbus baud is failed, code={}'.format(code))\n                return code if _type == 'set' else (code, default if default != -99 else [])\n            else:\n                return func(*args, **kwargs)\n        return decorator\n    return _check_modbus_baud\n\n\ndef xarm_is_connected(_type='set'):\n    def _xarm_is_connected(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if args[0].connected:\n                return func(*args, **kwargs)\n            else:\n                logger.error('xArm is not connect')\n                return APIState.NOT_CONNECTED if _type == 'set' else (APIState.NOT_CONNECTED, 'xArm is not connect')\n        return decorator\n    return _xarm_is_connected\n\n\ndef xarm_is_ready(_type='set'):\n    def _xarm_is_ready(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if args[0].connected and kwargs.get('auto_enable', False):\n                if not args[0].ready:\n                    args[0].motion_enable(enable=True)\n                    args[0].set_mode(0)\n                    args[0].set_state(0)\n            if args[0].connected:\n                if args[0].state_is_ready:\n                    return func(*args, **kwargs)\n                else:\n                    logger.error('xArm is not ready')\n                    logger.info('Please check the arm for errors. If so, please clear the error first. '\n                                'Then enable the motor, set the mode and set the state')\n                    return APIState.NOT_READY if _type == 'set' else (APIState.NOT_READY, 'xArm is not ready')\n            else:\n                logger.error('xArm is not connect')\n                return APIState.NOT_CONNECTED if _type == 'set' else (APIState.NOT_CONNECTED, 'xArm is not connect')\n        return decorator\n    return _xarm_is_ready\n\n\ndef xarm_is_pause(_type='set'):\n    def _xarm_is_pause(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            args[0].check_is_pause()\n            return func(*args, **kwargs)\n        return decorator\n    return _xarm_is_pause\n\n\ndef xarm_wait_until_cmdnum_lt_max(only_wait=False):\n    def _xarm_wait_until_cmdnum_lt_max(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            ret = args[0].wait_until_cmdnum_lt_max()\n            if not only_wait and ret is not None:\n                args[0].log_api_info('API -> {} -> code={}'.format(func.__name__, ret), code=ret)\n                return ret\n            return func(*args, **kwargs)\n        return decorator\n    return _xarm_wait_until_cmdnum_lt_max\n\n\ndef xarm_is_not_simulation_mode(ret=0):\n    def _xarm_is_not_simulation_mode(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if not args[0].check_is_simulation_robot():\n                return func(*args, **kwargs)\n            else:\n                return ret\n        return decorator\n    return _xarm_is_not_simulation_mode\n\n\ndef api_log(func):\n    @functools.wraps(func)\n    def decorator(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        logger.info('{}, ret={}, args={}, kwargs={}'.format(func.__name__, ret, args[1:], kwargs))\n        return ret\n    return decorator\n\n\ndef compare_time(time1, time2):\n    try:\n        s_time = time.mktime(time.strptime(time1, '%Y-%m-%d'))\n        e_time = time.mktime(time.strptime(time2, '%Y-%m-%d'))\n        return int(s_time) - int(e_time) > 0\n    except:\n        return False\n\n\ndef compare_version(v1, v2):\n    for i in range(3):\n        if v1[i] > v2[i]:\n            return True\n        elif v1[i] < v2[i]:\n            return False\n    return False", "instruction_en": "The given code contains several Python functions that can be used to check the status of a xArm robot. To use these functions, import them into your Python script and use them as decorators for the functions that you want to check. For example, to check if the robot is connected before running a function, use the `xarm_is_connected` decorator. To check if the robot is ready before running a function, use the `xarm_is_ready` decorator. To log the API calls made by your script, use the `api_log` decorator.", "original_line_number": 28927, "instruction_length": 551}
{"instruction": "Buat fungsi bernama `parse_file` yang mengambil parameter bernama` Datafile`. Fungsi ini harus membaca garis `Datafile` input, dan untuk 10 baris pertama (tidak termasuk header) memecah setiap baris pada\", \"dan kemudian untuk setiap baris, membuat kamus di mana kunci adalah tajuk tajuk medan, dan nilai adalah nilai medan itu dalam baris. Fungsi ini harus mengembalikan senarai kamus, setiap baris data dalam fail menjadi entri senarai tunggal. Nama dan nilai medan tidak boleh mengandungi ruang putih tambahan, seperti ruang atau aksara baru. Anda boleh menggunakan kaedah rentetan python `jalur ()` untuk mengeluarkan ruang putih tambahan. Senarai yang dikembalikan harus mempunyai 10 penyertaan. Akhirnya, tulis fungsi ujian bernama `test` yang menguji pelaksanaan fungsi` parse_file`.", "input": "", "output": "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n# split each line on \",\" and then for each line, create a dictionary\n# where the key is the header title of the field, and the value is the value of that field in the row.\n# The function parse_file should return a list of dictionaries,\n# each data line in the file being a single list entry.\n# Field names and values should not contain extra whitespace, like spaces or newline characters.\n# You can use the Python string method strip() to remove the extra whitespace.\n# You have to parse only the first 10 data lines in this exercise,\n# so the returned list should have 10 entries!\nimport os\n\nDATADIR = \"\"\nDATAFILE = \"beatles-diskography.csv\"\n\n\n# -*- coding: utf-8 -*-\n\ndef parse_file(datafile):\n    data = []\n    with open(datafile, \"r\") as f:\n        cnt = 0\n        key_list = []\n        value_list = []\n        for line in f:\n            #print line\n            if cnt == 0:\n                key_list = line.strip().split(',')\n            else:\n                value_list = line.strip().split(',')\n            if cnt != 0:\n                data_dict = {}\n                for i in xrange(len(key_list)):\n                    data_dict[key_list[i]] = value_list[i]\n                data.append(data_dict)\n            cnt += 1\n            if cnt > 10:\n                break\n    return data\n\n\ndef test():\n    # a simple test of your implemetation\n    datafile = os.path.join(DATADIR, DATAFILE)\n    d = parse_file(datafile)\n    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n\n    assert d[0] == firstline\n    assert d[9] == tenthline\n\n    \ntest()", "instruction_en": "Create a function named `parse_file` that takes a parameter named `datafile`. The function should read the input `datafile` line by line, and for the first 10 lines (not including the header) split each line on \",\" and then for each line, create a dictionary where the key is the header title of the field, and the value is the value of that field in the row. The function should return a list of dictionaries, each data line in the file being a single list entry. Field names and values should not contain extra whitespace, like spaces or newline characters. You can use the Python string method `strip()` to remove the extra whitespace. The returned list should have 10 entries. Finally, write a test function named `test` that tests the implementation of the `parse_file` function.", "original_line_number": 29158, "instruction_length": 789}
{"instruction": "Buat fungsi python bernama \"Transpile\" yang mengambil empat argumen: \"src_dir\", \"dst_dir\", \"model_filename\", dan \"params_filename\". Di dalam fungsi, buat objek cpuplace dan objek pelaksana menggunakan cecair dayung. Kemudian, buat objek skop menggunakan fluid.core.scope (). Muatkan program kesimpulan, nama sasaran suapan, dan ambil sasaran menggunakan fluid.io.load_inference_model () dengan \"src_dir\", \"exe\", \"model_filename\", dan \"params_filename\" argumen. Klon program kesimpulan dan gunakan transpiler kesimpulan menggunakan fluid.inferencetranspiler () dan t.transpile (). Simpan program kesimpulan yang dioptimumkan kepada \"dst_dir\" menggunakan fluid.io.save_persistables () dan \"Inference_Transpiler_Program\" dan \"params_filename\" argumen. Jika \"dst_dir\" tidak wujud, buatnya menggunakan os.makedirs (). Akhirnya, buat fungsi utama yang mengambil argumen menggunakan ArgParse.ArgumentParser () dan memanggil fungsi \"Transpile\" dengan argumen yang dihuraikan.", "input": "", "output": "import os\nimport sys\nimport argparse\nimport paddle.fluid as fluid\n\n\ndef Transpile(src_dir, dst_dir, model_filename, params_filename):\n    place = fluid.CPUPlace()\n    exe = fluid.Executor(place)\n\n    inference_scope = fluid.core.Scope()\n    with fluid.scope_guard(inference_scope):\n        print \"Loading inference_program from \", src_dir\n        [inference_program, feed_target_names,\n         fetch_targets] = fluid.io.load_inference_model(src_dir, exe, model_filename, params_filename)\n\n        inference_transpiler_program = inference_program.clone()\n        # NOTE: Applying the inference transpiler will change the inference_transpiler_program.\n        t = fluid.InferenceTranspiler()\n        # Under the with statement, inference_scope is the global scope.\n        t.transpile(inference_transpiler_program, place)\n\n        #print inference_transpiler_program\n\n        print \"Saving the optimized inference_program to \", dst_dir\n        # There is a bug in fluid.io.save_inference_model, so we can use the following code instead.\n        if not os.path.isdir(dst_dir):\n            os.makedirs(dst_dir)\n        model_path = os.path.join(dst_dir, model_filename)\n        with open(model_path, \"wb\") as f:\n            f.write(inference_transpiler_program.desc.serialize_to_string())\n        fluid.io.save_persistables(exe, dst_dir, inference_transpiler_program, params_filename)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--src_dir', help='Source directory of inference model')\n    parser.add_argument('--dst_dir', help='Dstination directory of inference model')\n    parser.add_argument('--model_filename', default=None, help='The name of model file')\n    parser.add_argument('--params_filename', default=None, help='The name of params file')\n    args = parser.parse_args()\n    Transpile(args.src_dir, args.dst_dir, args.model_filename, args.params_filename)\n\n\nif __name__ == '__main__':\n    main()", "instruction_en": "Create a Python function named \"Transpile\" that takes in four arguments: \"src_dir\", \"dst_dir\", \"model_filename\", and \"params_filename\". Inside the function, create a CPUPlace object and an Executor object using Paddle Fluid. Then, create a Scope object using fluid.core.Scope(). Load the inference program, feed target names, and fetch targets using fluid.io.load_inference_model() with the \"src_dir\", \"exe\", \"model_filename\", and \"params_filename\" arguments. Clone the inference program and apply the inference transpiler using fluid.InferenceTranspiler() and t.transpile(). Save the optimized inference program to \"dst_dir\" using fluid.io.save_persistables() and the \"inference_transpiler_program\" and \"params_filename\" arguments. If \"dst_dir\" does not exist, create it using os.makedirs(). Finally, create a main function that takes in arguments using argparse.ArgumentParser() and calls the \"Transpile\" function with the parsed arguments.", "original_line_number": 29610, "instruction_length": 967}
{"instruction": "Buat kelas bernama Erapi dengan beberapa kaedah untuk memuat, menyimpan, dan mengemas kini data pengguna Erepublik. Kelas harus mempunyai kaedah pembina yang memulakan tetapan konfigurasi yang diperlukan dan objek berguna. Kelas ini juga harus mempunyai kaedah untuk mengira nama pangkat dan kerosakan pengguna berdasarkan mata, kekuatan, kuasa senjata, tahap, dan bonus mereka. Kelas harus dapat memuatkan data dari fail CSV dan mengemas kini data dari pengikis laman web. Kelas ini harus mempunyai kaedah untuk mendapatkan ID, Tahap, Kekuatan, Titik Kedudukan, Kewarganegaraan, dan Nama Pengguna. Akhirnya, kelas harus mempunyai kaedah untuk menulis pengguna baru ke data dengan ID dan nama samaran mereka.", "input": "", "output": "#-*- encoding: utf-8 -*-\n\nimport csv, math, time, re, threading, sys\n\ntry:\n    from urllib.request import urlopen\nexcept ImportError:\n    from urllib import urlopen\n\nclass ErAPI():\n    # Metodo constructor, seteos basicos necesarios de configuracion, instancia objetos utiles\n    def __init__(self):\n        self.data = {}\n        # Data format: {'XXCiro|BNC': {'id': 123456, 'nick': 'XXCiro', 'level': 49, 'strength': 532.5, 'rank_points': 1233354, 'citizenship': 'Argentina'}}\n        \n        # Diccionario de puntos/rango\n        self.rank_required_points = {\n            \"Recruit\": 0,\n            \"Private\": 15,\n            \"Private*\": 45,\n            \"Private**\": 80,\n            \"Private***\": 120,\n            \"Corporal\": 170,\n            \"Corporal*\": 250,\n            \"Corporal**\": 350,\n            \"Corporal***\": 450,\n            \"Sergeant\": 600,\n            \"Sergeant*\": 800,\n            \"Sergeant**\": 1000,\n            \"Sergeant***\": 1400,\n            \"Lieutenant\": 1850,\n            \"Lieutenant*\": 2350,\n            \"Lieutenant**\": 3000,\n            \"Lieutenant***\": 3750,\n            \"Captain\": 5000,\n            \"Captain*\": 6500,\n            \"Captain**\": 9000,\n            \"Captain***\": 12000,\n            \"Major\": 15500,\n            \"Major*\": 20000,\n            \"Major**\": 25000,\n            \"Major***\": 31000,\n            \"Commander\": 40000,\n            \"Commander*\": 52000,\n            \"Commander**\": 67000,\n            \"Commander***\": 85000,\n            \"Lt Colonel\": 110000,\n            \"Lt Colonel*\": 140000,\n            \"Lt Colonel**\": 180000,\n            \"Lt Colonel***\": 225000,\n            \"Colonel\": 285000,\n            \"Colonel*\": 355000,\n            \"Colonel**\": 435000,\n            \"Colonel***\": 540000,\n            \"General\": 660000,\n            \"General*\": 800000,\n            \"General**\": 950000,\n            \"General***\": 1140000,\n            \"Field Marshal\": 1350000,\n            \"Field Marshal*\": 1600000,\n            \"Field Marshal**\": 1875000,\n            \"Field Marshal***\": 2185000,\n            \"Supreme Marshal\": 2550000,\n            \"Supreme Marshal*\": 3000000,\n            \"Supreme Marshal**\": 3500000,\n            \"Supreme Marshal***\": 4150000,\n            \"National Force\": 4900000,\n            \"National Force*\": 5800000,\n            \"National Force**\": 7000000,\n            \"National Force***\": 9000000,\n            \"World Class Force\": 11500000,\n            \"World Class Force*\": 14500000,\n            \"World Class Force**\": 18000000,\n            \"World Class Force***\": 22000000,\n            \"Legendary Force\": 26500000,\n            \"Legendary Force*\": 31500000,\n            \"Legendary Force**\": 37000000,\n            \"Legendary Force***\": 42000000,\n            \"God of War\": 50000000,\n            \"God of War*\": 100000000 ,\n            \"God of War**\": 200000000,\n            \"God of War***\": 500000000,\n            \"Titan\": 1000000000,\n            \"Titan*\": 2000000000,\n            \"Titan**\": 4000000000,\n            \"Titan***\": 10000000000}\n\n        # Lista ordenada de rangos segun importancia\n        self.rank_to_pos = [\n            \"Recruit\",\n            \"Private\",\n            \"Private*\",\n            \"Private**\",\n            \"Private***\",\n            \"Corporal\",\n            \"Corporal*\",\n            \"Corporal**\",\n            \"Corporal***\",\n            \"Sergeant\",\n            \"Sergeant*\",\n            \"Sergeant**\",\n            \"Sergeant***\",\n            \"Lieutenant\",\n            \"Lieutenant*\",\n            \"Lieutenant**\",\n            \"Lieutenant***\",\n            \"Captain\",\n            \"Captain*\",\n            \"Captain**\",\n            \"Captain***\",\n            \"Major\",\n            \"Major*\",\n            \"Major**\",\n            \"Major***\",\n            \"Commander\",\n            \"Commander*\",\n            \"Commander**\",\n            \"Commander***\",\n            \"Lt Colonel\",\n            \"Lt Colonel*\",\n            \"Lt Colonel**\",\n            \"Lt Colonel***\",\n            \"Colonel\",\n            \"Colonel*\",\n            \"Colonel**\",\n            \"Colonel***\",\n            \"General\",\n            \"General*\",\n            \"General**\",\n            \"General***\",\n            \"Field Marshal\",\n            \"Field Marshal*\",\n            \"Field Marshal**\",\n            \"Field Marshal***\",\n            \"Supreme Marshal\",\n            \"Supreme Marshal*\",\n            \"Supreme Marshal**\",\n            \"Supreme Marshal***\",\n            \"National Force\",\n            \"National Force*\",\n            \"National Force**\",\n            \"National Force***\",\n            \"World Class Force\",\n            \"World Class Force*\",\n            \"World Class Force**\",\n            \"World Class Force***\",\n            \"Legendary Force\",\n            \"Legendary Force*\",\n            \"Legendary Force**\",\n            \"Legendary Force***\",\n            \"God of War\",\n            \"God of War*\",\n            \"God of War**\",\n            \"God of War***\",\n            \"Titan\",\n            \"Titan*\",\n            \"Titan**\",\n            \"Titan***\",]\n\n        # Bandera de ejecucion, util en caso de que se decida matar de forma manual los threads para actualizar y guardar los datos\n        self.run = True\n\n        # Se paraleliza la carga de datos en un hilo nuevo, el cual es demonio del invocador en caso de \"muerte prematura\"\n        th = threading.Thread(target=self.data_loader)\n        th.daemon = True\n        th.start()\n\n    # Metodo invocador, carga datos y crea threads para guardar y actualizar informacion, solo llamado desde constructor\n    def data_loader(self):\n        self.load_data()\n\n        self.data_saver_th = threading.Thread(target=self.data_saver)\n        self.data_saver_th.daemon = True\n        self.data_saver_th.start()\n\n        self.data_updater_th = threading.Thread(target=self.data_updater)\n        self.data_updater_th.daemon = True\n        self.data_updater_th.start()\n\n    # Metodo para volcar informacion a archivo fisico, solo llamado de metodo data_loader\n    def data_saver(self):\n        while self.run:\n            self.save_data()\n\n            time.sleep(60)\n\n    # Metodo para actualizar informacion, solo llamado de metodo data_loader\n    def data_updater(self):\n        while self.run:\n            for irc_nick in self.data:\n                self.update_data(irc_nick)\n                time.sleep(30)\n\n            time.sleep(600)\n\n    # ---------------------------------------------------------------------------------- #\n    # @ PUBLIC METHODS                                                                   #\n    # ---------------------------------------------------------------------------------- #\n\n    # Metodo para actualizar informacion local del objeto desde archivo\n    def load_data(self):\n        try:\n            f = open('data/er_nick-data.csv', 'rt')\n            reader = csv.reader(f)\n            for nick_irc,id,nick_er,level,strength,rank_points,citizenship in reader:\n                self.data[nick_irc] = {'id': int(id), 'nick': nick_er, 'level': int(level), 'strength': float(strength), 'rank_points': int(rank_points), 'citizenship': citizenship}\n            f.close()\n        except:\n            pass\n\n    # Metodo para guardar informacion local del objeto en archivo\n    def save_data(self):\n        try:\n            f = open('data/er_nick-data.csv', 'wt')\n            writer = csv.writer(f)\n            for u in self.data:\n                writer.writerow([u, self.data[u]['id'], self.data[u]['nick'], self.data[u]['level'], self.data[u]['strength'], self.data[u]['rank_points'], self.data[u]['citizenship']])\n            f.close()\n        except:\n            pass\n\n    # Metodo scraper para actualizar informacion local del objeto del nick de irc especificado\n    def update_data(self, irc_nick):\n        try:\n            id = self.data[irc_nick]['id']\n\n            c = urlopen('http://www.erepublik.com/es/citizen/profile/%d' % id)\n            page = c.read()\n            c.close()\n\n            self.data[irc_nick]['nick'] = re.search('<meta name=\"title\" content=\"(.+?) - Ciudadano del Nuevo Mundo\" \\/>', page.decode('utf-8')).group(1)\n            self.data[irc_nick]['level'] = int(re.search('<strong class=\"citizen_level\">(.+?)<\\/strong>', page.decode('utf-8'), re.DOTALL).group(1))\n            self.data[irc_nick]['strength'] = float(re.search('<span class=\"military_box_info mb_bottom\">(.+?)</span>', page.decode('utf-8'), re.DOTALL).group(1).strip('\\r\\n\\t ').replace(',',''))\n            self.data[irc_nick]['rank_points'] = int(re.search('<span class=\"rank_numbers\">(.+?) \\/', page.decode('utf-8'), re.DOTALL).group(1).replace(',',''))\n            self.data[irc_nick]['citizenship'] = re.search('<a href=\"http\\:\\/\\/www.erepublik.com\\/es\\/country\\/society\\/([^ \\t\\n\\x0B\\f\\r]+?)\">', page.decode('utf-8')).group(1)\n        except:\n            pass\n\n    # Metodo para actualizar informacion local del objeto con nick de irc e id especificados, fuerza actualizacion del mismo\n    def reg_nick_write(self, nick, id):\n        if(nick.lower() in self.data.keys()):\n            self.data[nick.lower()]['id'] = int(id)\n        else:\n            self.data[nick.lower()] = {'id': int(id), 'nick': nick, 'level': 1, 'strength': 0, 'rank_points': 0, 'citizenship': ''}\n\n        self.update_data(nick.lower())        \n\n    # Metodo para obtener ID del nick de irc especificado\n    def get_id(self, nick):\n        return self.data[nick.lower()]['id']\n\n    # Metodo para obtener LEVEL del nick de irc especificado\n    def get_level(self, nick):\n        return self.data[nick.lower()]['level']\n\n    # Metodo para obtener STRENGTH del nick de irc especificado\n    def get_strength(self, nick):\n        return self.data[nick.lower()]['strength']\n\n    # Metodo para obtener RANK POINTS del nick de irc especificado\n    def get_rank_points(self, nick):\n        return self.data[nick.lower()]['rank_points']\n\n    # Metodo para obtener CITIZENSHIP del nick de irc especificado\n    def get_citizenship(self, nick):\n        return self.data[nick.lower()]['citizenship']\n\n    # Metodo para obtener NICK INGAME del nick de irc especificado\n    def get_nick(self, nick):\n        return self.data[nick.lower()]['nick']\n\n    # Metodo para obtener RANK NAME del nick de irc especificado\n    def calculate_rank_name(self, rank_points):\n        index = 0\n\n        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:\n            if(self.rank_to_pos.index(k) > index):\n                index = self.rank_to_pos.index(k)\n\n        return self.rank_to_pos[index]\n\n    # Metodo para calcular DAO del nick de irc especificado segun datos adicionales\n    def calculate_damage(self, rank_points, strength, weapon_power, level, bonus):\n        index = 0\n\n        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:\n            if(self.rank_to_pos.index(k) > index):\n                index = self.rank_to_pos.index(k)\n\n        return(math.trunc(((index / 20) + 0.3) * ((strength / 10) + 40) * (1 + (weapon_power / 100)) * (1.1 if level > 99 else 1) * bonus))", "instruction_en": "Create a class named ErAPI with several methods to load, save, and update data of eRepublik users. The class should have a constructor method that initializes the necessary configuration settings and useful objects. The class should also have a method to calculate the rank name and damage of a user based on their rank points, strength, weapon power, level, and bonus. The class should be able to load data from a CSV file and update data from a website scraper. The class should have methods to get the ID, level, strength, rank points, citizenship, and nickname of a user. Finally, the class should have a method to write a new user to the data with their ID and nickname.", "original_line_number": 29688, "instruction_length": 708}
{"instruction": "Kod ini adalah plugin pemodelan JuniPerFPC yang mengumpulkan maklumat jadual dari Jadual Kandungan Juniper. Untuk menggunakan program ini, import modul yang diperlukan dan hubungi kelas `Juniperfpcmap`, yang memetakan jadual FPC Juniper ke model. Kaedah `Proses` kelas ini mengumpul maklumat SNMP dari peranti dan mengembalikan objek` RELMAP` yang mengandungi data yang diproses. Pastikan untuk membekalkan data SNMP untuk jadual `jnxcontentstable`,` jnxoperatingTable`, dan `jnxcontainerstable`. Jika tiada data dibekalkan, mesej amaran akan dilog masuk. Objek `RELMAP` mengandungi maklumat mengenai FPC, termasuk keadaan, suhu, penggunaan CPU, uptime, dan penggunaan memori. Nilai `fpcuptime` ditukar dari milisaat hingga jam. Nilai `fpcstate` diubah dari nilai angka ke rentetan status menggunakan kamus` operatingStateLookup`. Atribut `id` dari` ObjectMap` ditetapkan ke deskripsi FPC dan indeks SNMP, dengan ruang digantikan oleh garis bawah.", "input": "", "output": "##########################################################################\n# Author:               Jane Curry,  jane.curry@skills-1st.co.uk\n# Date:                 February 28th, 2011\n# Revised:\t\tExtra debugging added Aug 23, 2011\n#\n# JuniperFPC modeler plugin\n#\n# This program can be used under the GNU General Public License version 2\n# You can find full information here: http://www.zenoss.com/oss\n#\n##########################################################################\n\n__doc__ = \"\"\"JuniperFPCMap\n\nGather table information from Juniper Contents tables\n\"\"\"\n\nimport re\nfrom Products.DataCollector.plugins.CollectorPlugin import SnmpPlugin, GetMap, GetTableMap\n\nclass JuniperFPCMap(SnmpPlugin):\n    \"\"\"Map Juniper FPC table to model.\"\"\"\n    maptype = \"JuniperFPCMap\"\n    modname = \"ZenPacks.ZenSystems.Juniper.JuniperFPC\"\n    relname = \"JuniperFP\"\n    compname = \"\"\n\n    snmpGetTableMaps = (\n        GetTableMap('jnxContentsTable',\n                    '.1.3.6.1.4.1.2636.3.1.8.1',\n                    {\n                        '.1': 'containerIndex',\n                        '.5': 'FPCType',\n                        '.6': 'FPCDescr',\n                        '.7': 'FPCSerialNo',\n                        '.8': 'FPCRevision',\n                        '.10': 'FPCPartNo',\n                        '.11': 'FPCChassisId',\n                        '.12': 'FPCChassisDescr',\n                        '.13': 'FPCChassisCLEI',\n                    }\n        ),\n        GetTableMap('jnxOperatingTable',\n                    '.1.3.6.1.4.1.2636.3.1.13.1',\n                    {\n                        '.6': 'FPCState',\n                        '.7': 'FPCTemp',\n                        '.8': 'FPCCPU',\n                        '.13': 'FPCUpTime',\n                        '.15': 'FPCMemory',\n                    }\n        ),\n        GetTableMap('jnxContainersTable',\n                    '.1.3.6.1.4.1.2636.3.1.6.1',\n                    {\n                        '.1': 'containerIndex',\n                        '.3': 'containerLevel',\n                        '.4': 'containerNextLevel',\n                        '.5': 'containerType',\n                        '.6': 'containerDescr',\n                    }\n        ),\n    )\n\n    def process(self, device, results, log):\n        \"\"\"collect snmp information from this device\"\"\"\n        log.info('processing %s for device %s', self.name(), device.id)\n        getdata, tabledata = results\n        rm = self.relMap()\n        contentsTable = tabledata.get('jnxContentsTable')\n        operatingTable = tabledata.get('jnxOperatingTable')\n        containersTable = tabledata.get('jnxContainersTable')\n\n# If no data supplied then simply return\n        if not contentsTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for contents', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n        if not operatingTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for operating system', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n        if not containersTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for containers', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n \n        for oid, data in contentsTable.items():\n            try:\n                om = self.objectMap(data)\n                FPCDescr = om.FPCDescr\n#                log.info(' FPCDescr is %s ' % (om.FPCDescr))\n                isaFPC = re.match(r'(.*FPC.*)', FPCDescr.upper())\n                if not isaFPC:\n                    continue\n                else:\n                    for oid1, data1 in operatingTable.items():\n                        if oid1 == oid:\n                            om.FPCState = data1['FPCState']\n                            om.FPCTemp = data1['FPCTemp']\n                            om.FPCCPU = data1['FPCCPU']\n                            om.FPCUpTime = data1['FPCUpTime']\n                            om.FPCMemory = data1['FPCMemory']\n                            for oid2, data2 in containersTable.items():\n#                                log.info( ' oid is %s - oid2 is %s - data is %s' % (oid, oid2 , data2))\n                                if oid.startswith(oid2):\n                                    om.containerDescr = data2['containerDescr']\n                                    if data2['containerLevel'] == 1:\n                                        om.containerDescr = '....' + om.containerDescr\n                                    elif data2['containerLevel'] == 2:\n                                        om.containerDescr = '........' + om.containerDescr\n                                    om.containerParentIndex = data2['containerNextLevel']\n                                    if om.containerParentIndex != 0:\n                                        for oid3, data3 in  containersTable.items():\n                                            if oid3.endswith(str(om.containerParentIndex)):\n                                                om.containerParentDescr = data3['containerDescr']\n                            om.snmpindex = oid1.strip('.')\n# Convert FPCUpTime from milliseconds to hours\n                    om.FPCUpTime = om.FPCUpTime / 1000 / 60 / 60 /24\n# Transform numeric FPCState into a status string via operatingStateLookup\n                    if (om.FPCState < 1 or om.FPCState > 7):\n                        om.FPCState = 1\n                    om.FPCState = self.operatingStateLookup[om.FPCState]\n                    om.id = self.prepId( om.FPCDescr.replace(' ','_') + '_' + str( om.snmpindex.replace('.','_') ) )\n            except (KeyError, IndexError, AttributeError, TypeError), errorInfo:\n                log.warn( ' Error in %s modeler plugin %s' % ( self.name(), errorInfo))\n                continue\n            rm.append(om)\n#            log.info('rm %s' % (rm) )\n\n        return rm\n\n    operatingStateLookup = { 1: 'Unknown',\n                             2: 'Running',\n                             3: 'Ready',\n                             4: 'Reset',\n                             5: 'RunningAtFullSpeed (Fan)',\n                             6: 'Down',\n                             7: 'Standby'\n                           }", "instruction_en": "This code is a JuniperFPC modeler plugin that gathers table information from Juniper Contents tables. To use this program, import the necessary modules and call the `JuniperFPCMap` class, which maps the Juniper FPC table to a model. The `process` method of this class collects SNMP information from a device and returns a `relMap` object containing the processed data. Make sure to supply SNMP data for the `jnxContentsTable`, `jnxOperatingTable`, and `jnxContainersTable` tables. If no data is supplied, a warning message will be logged. The `relMap` object contains information about the FPC, including its state, temperature, CPU usage, uptime, and memory usage. The `FPCUpTime` value is converted from milliseconds to hours. The `FPCState` value is transformed from a numeric value to a status string using the `operatingStateLookup` dictionary. The `id` attribute of the `objectMap` is set to the FPC description and SNMP index, with spaces replaced by underscores.", "original_line_number": 29854, "instruction_length": 947}
{"instruction": "Buat skrip Python yang menghubungkan ke pangkalan data SQLite, mencipta tiga jadual (stesen, pemerhatian, dan masa), dan memaparkannya dengan data dari fail CSV dan API JSON. Skrip harus menggunakan modul CSV, JSON, Permintaan, Numpy, dan SQLITE3. Fail CSV harus mengandungi data mengenai stesen cuaca, dan API JSON harus menyediakan data mengenai pemerhatian cuaca. Jadual stesen harus mempunyai lajur untuk ID, pendek, nama, lon, lat, ketinggian, dan json. Jadual pemerhatian harus mempunyai lajur untuk id, local_date_time, local_date_time_full, pecahan_t, delta_t, air_temp, rain_trace, rel_hum, wind_dir, dan wind_spd_kmh. Jadual masa harus mempunyai lajur untuk id dan local_date_time_full. Skrip harus memasukkan data ke dalam jadual menggunakan pertanyaan SQL, dan harus mencetak pertanyaan seperti yang dilaksanakan. Akhirnya, skrip harus mencetak nama -nama jadual dalam pangkalan data dan mengeksport setiap jadual ke fail CSV.", "input": "", "output": "#!/usr/bin/env python\n\nimport csv\nimport json\nimport requests\nimport numpy as np\nimport sqlite3\n\ncsv_filename = \"testdata/vic_usgs/south_west_stations.csv\"\ndb_filename = \"testdata/vic_usgs/south_west_stations.db\"\n\n# open database\nconn = sqlite3.connect(db_filename)\nc = conn.cursor()\n\n# Create stations table\nc.execute('''CREATE TABLE IF NOT EXISTS stations \n             (id text, short text, name text, lon real, lat real, height real, json text, UNIQUE(id))''')\n\n# Create observations table\nc.execute('''CREATE TABLE IF NOT EXISTS observations \n             (id text, local_date_time text, local_date_time_full text,  \n              apparent_t real, delta_t real, air_temp real, rain_trace text, rel_hum real, wind_dir text, wind_spd_kmh real,\n              UNIQUE(id, local_date_time_full))''')\n\n# Create time point table\nc.execute('''CREATE TABLE IF NOT EXISTS time \n             (id text, local_date_time_full text, UNIQUE(local_date_time_full))''')\n\n# stations\nwith open(csv_filename, 'rb') as csvfile:\n    stations = csv.DictReader(csvfile)\n    for station in stations:\n        id = station[\"ID\"]\n        short = station[\"Short\"]\n        name = station[\"Name\"]\n        lon = float(station[\"Lon\"])\n        lat = float(station[\"Lat\"])\n        height = float(station[\"Height\"])\n        json = station[\"Json\"]\n\n        # Insert a row of data into stations\n        query_str = \"INSERT OR IGNORE INTO stations VALUES ('%s', '%s', '%s', %f, %f, %f, '%s')\" % (id, short, name, lon, lat, height, json)\n        print query_str\n        c.execute( query_str )\n\n        # Update observations table\n        r = requests.get(json)\n        stationdata = r.json()\n        data = stationdata[\"observations\"][\"data\"]\n        nrows = len(data)\n        for i in range(nrows):\n            apparent_t = data[i]['apparent_t']\n            if apparent_t is None:\n                apparent_t = 0\n\n            delta_t = data[i]['delta_t']\n            if delta_t is None:\n                delta_t = 0\n\n            air_temp = data[i]['air_temp']\n            if air_temp is None:\n                air_temp = 0\n\n            rain_trace = data[i]['rain_trace']\n            if rain_trace is None:\n                rain_trace = ''\n\n            rel_hum = data[i]['rel_hum']\n            if rel_hum is None:\n                rel_hum = 0\n\n            wind_spd_kmh = data[i]['wind_spd_kmh']\n            if wind_spd_kmh is None:\n                wind_spd_kmh = 0\n\n            query_str = \"INSERT OR IGNORE INTO observations VALUES ('%s', '%s', '%s', %0.2f, %0.2f, %0.2f, '%s', %0.2f, '%s', %0.2f)\" % \\\n                (id, data[i]['local_date_time'], data[i]['local_date_time_full'], apparent_t, delta_t, \\\n                    air_temp, rain_trace, rel_hum, data[i]['wind_dir'], wind_spd_kmh)\n            print query_str\n            c.execute(query_str)\n\n            query_str = \"INSERT OR IGNORE INTO time VALUES (%s, '%s')\" % (id, data[i]['local_date_time_full'])\n            print query_str\n            c.execute(query_str)\n\n# commit the change\nconn.commit()\n# close database\nconn.close()\n\n\n\n\n# TESTING: print out to check\n\"\"\"\nconn = sqlite3.connect(db_filename)\nc = conn.cursor()\nc.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = c.fetchall()\nprint tables\nfor table_name in tables:\n    table_name = table_name[0]\n    table = pd.read_sql_query(\"SELECT * from %s\" % table_name, conn)\n    #print table\n    table.to_csv('testdata/vic_usgs/south_west_stations_' + table_name + '.csv', index_label='index')\n# close database\nconn.close()\n\"\"\"", "instruction_en": "Create a Python script that connects to a SQLite database, creates three tables (stations, observations, and time), and populates them with data from a CSV file and a JSON API. The script should use the csv, json, requests, numpy, and sqlite3 modules. The CSV file should contain data about weather stations, and the JSON API should provide data about weather observations. The stations table should have columns for id, short, name, lon, lat, height, and json. The observations table should have columns for id, local_date_time, local_date_time_full, apparent_t, delta_t, air_temp, rain_trace, rel_hum, wind_dir, and wind_spd_kmh. The time table should have columns for id and local_date_time_full. The script should insert data into the tables using SQL queries, and should print out the queries as they are executed. Finally, the script should print out the names of the tables in the database and export each table to a CSV file.", "original_line_number": 30228, "instruction_length": 938}
{"instruction": "Buat kelas Python yang dipanggil TV4Play yang mewarisi dari Perkhidmatan dan OpenGraphThumbmixin. Kelas harus mempunyai pembina yang mengambil URL sebagai hujah. Kelas harus mempunyai kaedah yang dipanggil GET yang mengambil parameter pilihan. Kaedah ini harus menggunakan URL untuk mendapatkan data, cari ID video, dan muat turun video. Kaedah ini juga harus mengendalikan pengesahan dan geoblocking. Kelas harus mempunyai kaedah yang dipanggil find_all_episodes yang mengambil parameter pilihan. Kaedah ini harus menggunakan URL untuk mendapatkan data, mencari semua episod, dan mengembalikan senarai URL.", "input": "", "output": "# ex:ts=4:sw=4:sts=4:et\n# -*- tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*-\nfrom __future__ import absolute_import\nimport re\nimport os\nimport xml.etree.ElementTree as ET\nimport json\nimport copy\n\nfrom svtplay_dl.utils.urllib import urlparse, parse_qs, quote_plus\nfrom svtplay_dl.service import Service, OpenGraphThumbMixin\nfrom svtplay_dl.utils import is_py2_old, filenamify\nfrom svtplay_dl.log import log\nfrom svtplay_dl.fetcher.hls import hlsparse, HLS\nfrom svtplay_dl.fetcher.rtmp import RTMP\nfrom svtplay_dl.fetcher.hds import hdsparse\nfrom svtplay_dl.subtitle import subtitle\nfrom svtplay_dl.error import ServiceError\n\n\nclass Tv4play(Service, OpenGraphThumbMixin):\n    supported_domains = ['tv4play.se', 'tv4.se']\n\n    def __init__(self, url):\n        Service.__init__(self, url)\n        self.subtitle = None\n        self.cookies = {}\n\n    def get(self, options):\n        data = self.get_urldata()\n\n        vid = findvid(self.url, data)\n        if vid is None:\n            yield ServiceError(\"Can't find video id for %s\" % self.url)\n            return\n\n        if options.username and options.password:\n            data = self.http.request(\"get\", \"https://www.tv4play.se/session/new?https=\")\n            auth_token = re.search('name=\"authenticity_token\" ([a-z]+=\"[^\"]+\" )?value=\"([^\"]+)\"', data.text)\n            if not auth_token:\n                yield ServiceError(\"Can't find authenticity_token needed for user / password\")\n                return\n            url = \"https://www.tv4play.se/session\"\n            postdata = {\"user_name\" : options.username, \"password\": options.password, \"authenticity_token\":auth_token.group(2), \"https\": \"\"}\n            data = self.http.request(\"post\", url, data=postdata, cookies=self.cookies)\n            self.cookies = data.cookies\n            fail = re.search(\"<p class='failed-login'>([^<]+)</p>\", data.text)\n            if fail:\n                yield ServiceError(fail.group(1))\n                return\n        url = \"http://premium.tv4play.se/api/web/asset/%s/play\" % vid\n        data = self.http.request(\"get\", url, cookies=self.cookies)\n        if data.status_code == 401:\n            xml = ET.XML(data.content)\n            code = xml.find(\"code\").text\n            if code == \"SESSION_NOT_AUTHENTICATED\":\n                yield ServiceError(\"Can't access premium content\")\n            elif code == \"ASSET_PLAYBACK_INVALID_GEO_LOCATION\":\n                yield ServiceError(\"Can't downoad this video because of geoblocked.\")\n            else:\n                yield ServiceError(\"Can't find any info for that video\")\n            return\n        if data.status_code == 404:\n            yield ServiceError(\"Can't find the video api\")\n            return\n        xml = ET.XML(data.content)\n        ss = xml.find(\"items\")\n        if is_py2_old:\n            sa = list(ss.getiterator(\"item\"))\n        else:\n            sa = list(ss.iter(\"item\"))\n\n        if xml.find(\"live\").text:\n            if xml.find(\"live\").text != \"false\":\n                options.live = True\n        if xml.find(\"drmProtected\").text == \"true\":\n            yield ServiceError(\"We cant download DRM protected content from this site.\")\n            return\n\n        if options.output_auto:\n            directory = os.path.dirname(options.output)\n            options.service = \"tv4play\"\n            title = \"%s-%s-%s\" % (options.output, vid, options.service)\n            title = filenamify(title)\n            if len(directory):\n                options.output = os.path.join(directory, title)\n            else:\n                options.output = title\n\n        if self.exclude(options):\n            yield ServiceError(\"Excluding video\")\n            return\n\n        for i in sa:\n            if i.find(\"mediaFormat\").text == \"mp4\":\n                base = urlparse(i.find(\"base\").text)\n                parse = urlparse(i.find(\"url\").text)\n                if \"rtmp\" in base.scheme:\n                    swf = \"http://www.tv4play.se/flash/tv4playflashlets.swf\"\n                    options.other = \"-W %s -y %s\" % (swf, i.find(\"url\").text)\n                    yield RTMP(copy.copy(options), i.find(\"base\").text, i.find(\"bitrate\").text)\n                elif parse.path[len(parse.path)-3:len(parse.path)] == \"f4m\":\n                    streams = hdsparse(copy.copy(options), self.http.request(\"get\", i.find(\"url\").text, params={\"hdcore\": \"3.7.0\"}).text, i.find(\"url\").text)\n                    if streams:\n                        for n in list(streams.keys()):\n                            yield streams[n]\n            elif i.find(\"mediaFormat\").text == \"smi\":\n                yield subtitle(copy.copy(options), \"smi\", i.find(\"url\").text)\n\n        url = \"http://premium.tv4play.se/api/web/asset/%s/play?protocol=hls\" % vid\n        data = self.http.request(\"get\", url, cookies=self.cookies).content\n        xml = ET.XML(data)\n        ss = xml.find(\"items\")\n        if is_py2_old:\n            sa = list(ss.getiterator(\"item\"))\n        else:\n            sa = list(ss.iter(\"item\"))\n        for i in sa:\n            if i.find(\"mediaFormat\").text == \"mp4\":\n                parse = urlparse(i.find(\"url\").text)\n                if parse.path.endswith(\"m3u8\"):\n                    streams = hlsparse(i.find(\"url\").text, self.http.request(\"get\", i.find(\"url\").text).text)\n                    for n in list(streams.keys()):\n                        yield HLS(copy.copy(options), streams[n], n)\n\n    def find_all_episodes(self, options):\n        parse = urlparse(self.url)\n        show = parse.path[parse.path.find(\"/\", 1)+1:]\n        if not re.search(\"%\", show):\n            show = quote_plus(show)\n        data = self.http.request(\"get\", \"http://webapi.tv4play.se/play/video_assets?type=episode&is_live=false&platform=web&node_nids=%s&per_page=99999\" % show).text\n        jsondata = json.loads(data)\n        episodes = []\n        n = 1\n        for i in jsondata[\"results\"]:\n            try:\n                days = int(i[\"availability\"][\"availability_group_free\"])\n            except (ValueError, TypeError):\n                days = 999\n            if days > 0:\n                video_id = i[\"id\"]\n                url = \"http://www.tv4play.se/program/%s?video_id=%s\" % (\n                    show, video_id)\n                episodes.append(url)\n                if n == options.all_last:\n                    break\n                n += 1\n\n        return episodes\n\n\ndef findvid(url, data):\n    parse = urlparse(url)\n    if \"tv4play.se\" in url:\n        try:\n            vid = parse_qs(parse.query)[\"video_id\"][0]\n        except KeyError:\n            return None\n    else:\n        match = re.search(r\"\\\"vid\\\":\\\"(\\d+)\\\",\", data)\n        if match:\n            vid = match.group(1)\n        else:\n            match = re.search(r\"-(\\d+)$\", url)\n            if match:\n                vid = match.group(1)\n            else:\n                match = re.search(r\"meta content='([^']+)' property='og:video'\", data)\n                if match:\n                    match = re.search(r\"vid=(\\d+)&\", match.group(1))\n                    if match:\n                        vid = match.group(1)\n                    else:\n                        log.error(\"Can't find video id for %s\", url)\n                        return\n                else:\n                    return None\n    return vid", "instruction_en": "Create a Python class called Tv4play that inherits from Service and OpenGraphThumbMixin. The class should have a constructor that takes a URL as an argument. The class should have a method called get that takes an options parameter. The method should use the URL to retrieve data, find the video ID, and download the video. The method should also handle authentication and geoblocking. The class should have a method called find_all_episodes that takes an options parameter. The method should use the URL to retrieve data, find all episodes, and return a list of URLs.", "original_line_number": 30262, "instruction_length": 607}
{"instruction": "Kod ini membolehkan menjalankan fungsi pada dataset besar dengan satu set parameter. Keputusan diekstrak dan disimpan dengan cara yang mereka dapat dengan mudah dibandingkan dengan set lain. Data harus dianjurkan sebagai yang berikut: (nama imej boleh diubah tetapi mesti diluluskan sebagai parameter ke fungsi ini). Fungsi `test_function` mengambil hujah-hujah berikut: Fungsi untuk menguji (` -f`), direktori dataset (`-d`), argumen untuk lulus ke fungsi yang diuji (` -p`), bilangan cpu yang digunakan untuk ujian (`-cpu-nb`) Fungsi `Generate_data_list` digunakan untuk membina senarai data dari set data. Fungsi `process_results` digunakan untuk memproses hasilnya. Fungsi `function_launcher` digunakan untuk melancarkan fungsi. Fungsi `init_worker` digunakan untuk memulakan pekerja. Fungsi `get_parser` digunakan untuk memulakan parser.", "input": "", "output": "#!/usr/bin/env python\n#########################################################################################\n#\n# This function allows to run a function on a large dataset with a set of parameters.\n# Results are extracted and saved in a way that they can easily be compared with another set.\n#\n# Data should be organized as the following:\n# (names of images can be changed but must be passed as parameters to this function)\n#\n# data/\n# ......subject_name_01/\n# ......subject_name_02/\n# .................t1/\n# .........................subject_02_anything_t1.nii.gz\n# .........................some_landmarks_of_vertebral_levels.nii.gz\n# .........................subject_02_manual_segmentation_t1.nii.gz\n# .................t2/\n# .........................subject_02_anything_t2.nii.gz\n# .........................some_landmarks_of_vertebral_levels.nii.gz\n# .........................subject_02_manual_segmentation_t2.nii.gz\n# .................t2star/\n# .........................subject_02_anything_t2star.nii.gz\n# .........................subject_02_manual_segmentation_t2star.nii.gz\n# ......subject_name_03/\n#          .\n#          .\n#          .\n#\n# ---------------------------------------------------------------------------------------\n# Copyright (c) 2015 Polytechnique Montreal <www.neuro.polymtl.ca>\n# Author: Sara Dupont, Benjamin De Leener\n# Modified: 2015-09-30\n#\n# About the license: see the file LICENSE.TXT\n#########################################################################################\nimport sys\nimport commands\nimport platform\nimport signal\nfrom time import time, strftime\nfrom msct_parser import Parser\nimport sct_utils as sct\nimport os\nimport copy_reg\nimport types\nimport pandas as pd\nimport json\n\n\n# get path of the toolbox\n# TODO: put it back below when working again (julien 2016-04-04)\n# <<<\n# OLD\n# status, path_sct = commands.getstatusoutput('echo $SCT_DIR')\n# NEW\npath_script = os.path.dirname(__file__)\npath_sct = os.path.dirname(path_script)\n# >>>\n# append path that contains scripts, to be able to load modules\nsys.path.append(path_sct + '/scripts')\nsys.path.append(path_sct + '/testing')\n\n\ndef _pickle_method(method):\n    \"\"\"\n    Author: Steven Bethard (author of argparse)\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    func_name = method.im_func.__name__\n    obj = method.im_self\n    cls = method.im_class\n    cls_name = ''\n    if func_name.startswith('__') and not func_name.endswith('__'):\n        cls_name = cls.__name__.lstrip('_')\n    if cls_name:\n        func_name = '_' + cls_name + func_name\n    return _unpickle_method, (func_name, obj, cls)\n\n\ndef _unpickle_method(func_name, obj, cls):\n    \"\"\"\n    Author: Steven Bethard\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    for cls in cls.mro():\n        try:\n            func = cls.__dict__[func_name]\n        except KeyError:\n            pass\n        else:\n            break\n    return func.__get__(obj, cls)\n\ncopy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)\n\n\ndef generate_data_list(folder_dataset, json_requirements=None, verbose=1):\n    \"\"\"\n    Construction of the data list from the data set\n    This function return a list of directory (in folder_dataset) in which the contrast is present.\n    :return data:\n    \"\"\"\n    data_subjects, subjects_dir = [], []\n\n    # each directory in folder_dataset should be a directory of a subject\n    for subject_dir in os.listdir(folder_dataset):\n        if not subject_dir.startswith('.') and os.path.isdir(folder_dataset + subject_dir):\n            if read_json(folder_dataset + subject_dir, json_requirements=json_requirements):\n                data_subjects.append(folder_dataset + subject_dir + '/')\n                subjects_dir.append(subject_dir)\n\n    if not data_subjects:\n        sct.printv('ERROR: No subject data were found in ' + folder_dataset + '. '\n                   'Please organize your data correctly or provide a correct dataset.',\n                   verbose=verbose, type='error')\n\n    return data_subjects, subjects_dir\n\ndef read_json(path_dir, json_requirements=None, fname_json='dataset_description.json'):\n    path_dir = sct.slash_at_the_end(path_dir, slash=1)\n    if fname_json not in os.listdir(path_dir) and json_requirements is not None:\n        accept_subject = False\n    elif json_requirements is None:\n        accept_subject = True\n    else:\n        json_file = open(path_dir+fname_json)\n        dic_info = json.load(json_file)\n        json_file.close()\n        # pass keys and items to lower case\n        dic_info = dict((k.lower(), v.lower()) for k, v in dic_info.iteritems())\n        # if no condition is not verified, accept subject\n        accept_subject = True\n        # read requirements:\n        list_conditions = json_requirements.split(',')\n        for condition in list_conditions:\n            key, val = condition.split('=')\n            key, val = key.lower(), val.lower()\n            # if key do not exist, do not accept subject\n            if key not in dic_info.keys():\n                accept_subject = False\n            # if value for this key is not the one required, do not accept subject\n            elif dic_info[key] != val:\n                accept_subject = False\n\n    return accept_subject\n\n\ndef process_results(results, subjects_name, function, folder_dataset, parameters):\n    try:\n        results_dataframe = pd.concat([result[2] for result in results])\n        results_dataframe.loc[:, 'subject'] = pd.Series(subjects_name, index=results_dataframe.index)\n        results_dataframe.loc[:, 'script'] = pd.Series([function]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'dataset'] = pd.Series([folder_dataset]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'parameters'] = pd.Series([parameters] * len(subjects_name), index=results_dataframe.index)\n        return results_dataframe\n    except KeyboardInterrupt:\n        return 'KeyboardException'\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        sys.exit(2)\n\n\ndef function_launcher(args):\n    import importlib\n    # append local script to PYTHONPATH for import\n    sys.path.append(os.path.abspath(os.curdir))\n    script_to_be_run = importlib.import_module('test_' + args[0])  # import function as a module\n    try:\n        output = script_to_be_run.test(*args[1:])\n    except:\n        import traceback\n        print('%s: %s' % ('test_' + args[0], traceback.format_exc()))\n        # output = (1, 'ERROR: Function crashed', 'No result')\n        from pandas import DataFrame\n        status_script = 1\n        output_script = 'ERROR: Function crashed.'\n        output = (status_script, output_script, DataFrame(data={'status': int(status_script), 'output': output_script}, index=['']))\n    return output\n    # return script_to_be_run.test(*args[1:])\n\n\ndef init_worker():\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\ndef test_function(function, folder_dataset, parameters='', nb_cpu=None, json_requirements=None, verbose=1):\n    \"\"\"\n    Run a test function on the dataset using multiprocessing and save the results\n    :return: results\n    # results are organized as the following: tuple of (status, output, DataFrame with results)\n    \"\"\"\n\n    # generate data list from folder containing\n    data_subjects, subjects_name = generate_data_list(folder_dataset, json_requirements=json_requirements)\n\n    # All scripts that are using multithreading with ITK must not use it when using multiprocessing on several subjects\n    os.environ[\"ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS\"] = \"1\"\n\n    from multiprocessing import Pool\n\n    # create datasets with parameters\n    import itertools\n    data_and_params = itertools.izip(itertools.repeat(function), data_subjects, itertools.repeat(parameters))\n\n    pool = Pool(processes=nb_cpu, initializer=init_worker)\n\n    try:\n        async_results = pool.map_async(function_launcher, data_and_params).get(9999999)\n        # results = process_results(async_results.get(9999999), subjects_name, function, folder_dataset, parameters)  # get the sorted results once all jobs are finished\n        pool.close()\n        pool.join()  # waiting for all the jobs to be done\n        results = process_results(async_results, subjects_name, function, folder_dataset, parameters)  # get the sorted results once all jobs are finished\n    except KeyboardInterrupt:\n        print \"\\nWarning: Caught KeyboardInterrupt, terminating workers\"\n        pool.terminate()\n        pool.join()\n        # return\n        # raise KeyboardInterrupt\n        # sys.exit(2)\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        pool.terminate()\n        pool.join()\n        # raise Exception\n        # sys.exit(2)\n\n    return results\n\n\ndef get_parser():\n    # Initialize parser\n    parser = Parser(__file__)\n\n    # Mandatory arguments\n    parser.usage.set_description(\"\")\n    parser.add_option(name=\"-f\",\n                      type_value=\"str\",\n                      description=\"Function to test.\",\n                      mandatory=True,\n                      example=\"sct_propseg\")\n\n    parser.add_option(name=\"-d\",\n                      type_value=\"folder\",\n                      description=\"Dataset directory.\",\n                      mandatory=True,\n                      example=\"dataset_full/\")\n\n    parser.add_option(name=\"-p\",\n                      type_value=\"str\",\n                      description=\"Arguments to pass to the function that is tested. Please put double-quotes if there are spaces in the list of parameters.\\n\"\n                                  \"Image paths must be contains in the arguments list.\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-json\",\n                      type_value=\"str\",\n                      description=\"Requirements on center, study, ... that must be satisfied by the json file of each tested subjects\\n\"\n                                  \"Syntax:  center=unf,study=errsm,gm_model=0\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-cpu-nb\",\n                      type_value=\"int\",\n                      description=\"Number of CPU used for testing. 0: no multiprocessing. If not provided, \"\n                                  \"it uses all the available cores.\",\n                      mandatory=False,\n                      default_value=0,\n                      example='42')\n\n    parser.add_option(name=\"-log\",\n                      type_value='multiple_choice',\n                      description=\"Redirects Terminal verbose to log file.\",\n                      mandatory=False,\n                      example=['0', '1'],\n                      default_value='1')\n\n    parser.add_option(name=\"-v\",\n                      type_value=\"multiple_choice\",\n                      description=\"Verbose. 0: nothing, 1: basic, 2: extended.\",\n                      mandatory=False,\n                      example=['0', '1', '2'],\n                      default_value='1')\n\n    return parser\n\n\n# ====================================================================================================\n# Start program\n# ====================================================================================================\nif __name__ == \"__main__\":\n\n    # get parameters\n    parser = get_parser()\n    arguments = parser.parse(sys.argv[1:])\n    function_to_test = arguments[\"-f\"]\n    dataset = arguments[\"-d\"]\n    dataset = sct.slash_at_the_end(dataset, slash=1)\n    parameters = ''\n    if \"-p\" in arguments:\n        parameters = arguments[\"-p\"]\n    json_requirements = None\n    if \"-json\" in arguments:\n        json_requirements = arguments[\"-json\"]\n    nb_cpu = None\n    if \"-cpu-nb\" in arguments:\n        nb_cpu = arguments[\"-cpu-nb\"]\n    create_log = int(arguments['-log'])\n    verbose = arguments[\"-v\"]\n\n    # start timer\n    start_time = time()\n    # create single time variable for output names\n    output_time = strftime(\"%y%m%d%H%M%S\")\n    print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # build log file name\n    if create_log:\n        file_log = 'results_test_'+function_to_test+'_'+output_time\n        orig_stdout = sys.stdout\n        fname_log = file_log+'.log'\n        handle_log = file(fname_log, 'w')\n        # redirect to log file\n        sys.stdout = handle_log\n        print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # get path of the toolbox\n    path_script = os.path.dirname(__file__)\n    path_sct = os.path.dirname(path_script)\n\n    # fetch true commit number and branch (do not use commit.txt which is wrong)\n    path_curr = os.path.abspath(os.curdir)\n    os.chdir(path_sct)\n    sct_commit = commands.getoutput('git rev-parse HEAD')\n    if not sct_commit.isalnum():\n        print 'WARNING: Cannot retrieve SCT commit'\n        sct_commit = 'unknown'\n        sct_branch = 'unknown'\n    else:\n        sct_branch = commands.getoutput('git branch --contains '+sct_commit).strip('* ')\n    # with open (path_sct+\"/version.txt\", \"r\") as myfile:\n    #     version_sct = myfile.read().replace('\\n', '')\n    # with open (path_sct+\"/commit.txt\", \"r\") as myfile:\n    #     commit_sct = myfile.read().replace('\\n', '')\n    print 'SCT commit/branch: '+sct_commit+'/'+sct_branch\n    os.chdir(path_curr)\n\n    # check OS\n    platform_running = sys.platform\n    if (platform_running.find('darwin') != -1):\n        os_running = 'osx'\n    elif (platform_running.find('linux') != -1):\n        os_running = 'linux'\n    print 'OS: '+os_running+' ('+platform.platform()+')'\n\n    # check hostname\n    print 'Hostname:', platform.node()\n\n    # Check number of CPU cores\n    from multiprocessing import cpu_count\n    # status, output = sct.run('echo $ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS', 0)\n    print 'CPU cores: ' + str(cpu_count())  # + ', Used by SCT: '+output\n\n    # check RAM\n    sct.checkRAM(os_running, 0)\n\n    # test function\n    try:\n        results = test_function(function_to_test, dataset, parameters, nb_cpu, json_requirements, verbose)\n        pd.set_option('display.max_rows', 500)\n        pd.set_option('display.max_columns', 500)\n        pd.set_option('display.width', 1000)\n        results_subset = results.drop('script', 1).drop('dataset', 1).drop('parameters', 1).drop('output', 1)\n        results_display = results_subset\n\n        # save panda structure\n        if create_log:\n            results_subset.to_pickle(file_log+'.pickle')\n\n        # mean\n        results_mean = results_subset[results_subset.status != 200].mean(numeric_only=True)\n        results_mean['subject'] = 'Mean'\n        results_mean.set_value('status', float('NaN'))  # set status to NaN\n        # results_display = results_display.append(results_mean, ignore_index=True)\n\n        # std\n        results_std = results_subset[results_subset.status != 200].std(numeric_only=True)\n        results_std['subject'] = 'STD'\n        results_std.set_value('status', float('NaN'))  # set status to NaN\n        # results_display = results_display.append(results_std, ignore_index=True)\n\n        # count tests that passed\n        count_passed = results_subset.status[results_subset.status == 0].count()\n        count_crashed = results_subset.status[results_subset.status == 1].count()\n        # count tests that ran\n        count_ran = results_subset.status[results_subset.status != 200].count()\n\n        # results_display = results_display.set_index('subject')\n        # jcohenadad, 2015-10-27: added .reset_index() for better visual clarity\n        results_display = results_display.set_index('subject').reset_index()\n\n        print '\\nCommand: \"' + function_to_test + ' ' + parameters\n        print 'Dataset: ' + dataset\n        # display general results\n        print '\\nGLOBAL RESULTS:'\n        elapsed_time = time() - start_time\n        print 'Duration: ' + str(int(round(elapsed_time)))+'s'\n        # display results\n        print 'Passed: ' + str(count_passed) + '/' + str(count_ran)\n        print 'Crashed: ' + str(count_crashed) + '/' + str(count_ran)\n        # build mean/std entries\n        dict_mean = results_mean.to_dict()\n        dict_mean.pop('status')\n        dict_mean.pop('subject')\n        print 'Mean: ' + str(dict_mean)\n        dict_std = results_std.to_dict()\n        dict_std.pop('status')\n        dict_std.pop('subject')\n        print 'STD: ' + str(dict_std)\n\n        # print detailed results\n        print '\\nDETAILED RESULTS:'\n        print results_display.to_string()\n        print 'Status: 0: Passed | 1: Crashed | 99: Failed | 200: Input file(s) missing | 201: Ground-truth file(s) missing'\n\n    except Exception as err:\n        print err\n\n    # stop file redirection\n    if create_log:\n        sys.stdout.close()\n        sys.stdout = orig_stdout\n        # display log file to Terminal\n        handle_log = file(fname_log, 'r')\n        print handle_log.read()", "instruction_en": "This code allows running a function on a large dataset with a set of parameters. Results are extracted and saved in a way that they can easily be compared with another set. The data should be organized as the following: (names of images can be changed but must be passed as parameters to this function). The function `test_function` takes the following arguments: function to test (`-f`), dataset directory (`-d`), arguments to pass to the function that is tested (`-p`), number of CPU used for testing (`-cpu-nb`), and requirements on center, study, ... that must be satisfied by the json file of each tested subjects (`-json`). The function `generate_data_list` is used to construct the data list from the data set. The function `process_results` is used to process the results. The function `function_launcher` is used to launch the function. The function `init_worker` is used to initialize the worker. The function `get_parser` is used to initialize the parser.", "original_line_number": 30659, "instruction_length": 842}
{"instruction": "Kod yang diberikan mengandungi fungsi untuk mengekstrak ciri -ciri dari imej dan mengesan kereta dalam video menggunakan pengelas Vektor Sokongan Linear (SVM). Untuk menggunakan kod ini, anda perlu menyediakan senarai imej kenderaan dan bukan kenderaan, dan kemudian hubungi fungsi `extract_features` untuk mengekstrak ciri-ciri dari imej-imej ini. Selepas itu, anda boleh melatih pengelas SVM linear menggunakan ciri dan label yang diekstrak. Sebaik sahaja pengelas dilatih, anda boleh menggunakan fungsi `Process_image` untuk mengesan kereta dalam video. Video output akan disimpan di lokasi yang ditentukan.", "input": "", "output": "import matplotlib.image as mpimg\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport cv2\r\nimport glob\r\nfrom skimage.feature import hog\r\nfrom skimage import color, exposure\r\nimport random\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.svm import LinearSVC\r\nimport time\r\nfrom moviepy.editor import VideoFileClip \r\nfrom scipy.ndimage.measurements import label\r\nfrom IPython.display import HTML\r\n\r\ndef load_data(my_list):\r\n    new_list = []\r\n    for image in my_list:\r\n        img = cv2.imread(image)\r\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n        new_list.append(img)\r\n    return new_list\r\n\r\n# Define a function to return HOG features and visualization\r\ndef get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\r\n    if vis == True:\r\n        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\r\n                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \r\n                                  visualise=True, feature_vector=False)\r\n        return features, hog_image\r\n    else:      \r\n        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\r\n                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \r\n                       visualise=False, feature_vector=feature_vec)\r\n        return features\r\n\r\n\r\ndef bin_spatial(img, size=(32, 32)):\r\n    color1 = cv2.resize(img[:,:,0], size).ravel()\r\n    color2 = cv2.resize(img[:,:,1], size).ravel()\r\n    color3 = cv2.resize(img[:,:,2], size).ravel()\r\n    return np.hstack((color1, color2, color3))\r\n\r\ndef color_hist(img, nbins=32):    #bins_range=(0, 256)\r\n    # Compute the histogram of the color channels separately\r\n    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\r\n    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\r\n    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\r\n    # Concatenate the histograms into a single feature vector\r\n    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\r\n    # Return the individual histograms, bin_centers and feature vector\r\n    return hist_features\r\n            \r\n# Define a function to extract features from a list of images\r\n# Have this function call bin_spatial() and color_hist()\r\ndef extract_features(imgs, \r\n                     color_space='RGB', \r\n                     spatial_size=(32, 32),\r\n                     hist_bins=32, orient=9, \r\n                     pix_per_cell=8, \r\n                     cell_per_block=2,\r\n                     hog_channel=0,\r\n                    spatial_feat=True, \r\n                     hist_feat=True, \r\n                     hog_feat=True):\r\n    # Create a list to append feature vectors to\r\n    features = []\r\n    # Iterate through the list of images\r\n    for image in imgs:\r\n        file_features = []\r\n        # Read in each one by one\r\n        #image = mpimg.imread(file)\r\n        # apply color conversion if other than 'RGB'\r\n        if color_space != 'RGB':\r\n            if color_space == 'HSV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\r\n            elif color_space == 'LUV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\r\n            elif color_space == 'HLS':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\r\n            elif color_space == 'YUV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\r\n            elif color_space == 'YCrCb':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\r\n        else: feature_image = np.copy(image)      \r\n\r\n        if spatial_feat == True:\r\n            spatial_features = bin_spatial(feature_image, size=spatial_size)\r\n            file_features.append(spatial_features)\r\n        if hist_feat == True:\r\n            # Apply color_hist()\r\n            hist_features = color_hist(feature_image, nbins=hist_bins)\r\n            file_features.append(hist_features)\r\n        if hog_feat == True:\r\n        # Call get_hog_features() with vis=False, feature_vec=True\r\n            if hog_channel == 'ALL':\r\n                hog_features = []\r\n                for channel in range(feature_image.shape[2]):\r\n                    hog_features.append(get_hog_features(feature_image[:,:,channel], \r\n                                        orient, pix_per_cell, cell_per_block, \r\n                                        vis=False, feature_vec=True))\r\n                hog_features = np.ravel(hog_features)        \r\n            else:\r\n                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \r\n                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\r\n            # Append the new feature vector to the features list\r\n            file_features.append(hog_features)\r\n        features.append(np.concatenate(file_features))\r\n    # Return list of feature vectors\r\n    return features\r\n\r\ndef add_heat(heatmap, bbox_list):\r\n    # Iterate through list of bboxes\r\n    for box in bbox_list:\r\n        # Add += 1 for all pixels inside each bbox\r\n        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\r\n        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\r\n\r\n    # Return updated heatmap\r\n    return heatmap\r\n\r\ndef apply_threshold(heatmap, threshold):\r\n    # Zero out pixels below the threshold\r\n    heatmap[heatmap <= threshold] = 0\r\n    # Return thresholded map\r\n    return heatmap\r\n\r\ndef draw_labeled_bboxes(img, labels):\r\n    # Iterate through all detected cars\r\n    for car_number in range(1, labels[1]+1):\r\n        # Find pixels with each car_number label value\r\n        nonzero = (labels[0] == car_number).nonzero()\r\n        # Identify x and y values of those pixels\r\n        nonzeroy = np.array(nonzero[0])\r\n        nonzerox = np.array(nonzero[1])\r\n        # Define a bounding box based on min/max x and y\r\n        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\r\n        # Draw the box on the image\r\n        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\r\n    # Return the image\r\n    return img\r\n\r\nimg_boxes = []\r\ndef convert_color(img, conv='RGB2YCrCb'):\r\n    if conv == 'RGB2YCrCb':\r\n        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\r\n    if conv == 'BGR2YCrCb':\r\n        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\r\n    if conv == 'RGB2LUV':\r\n        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\r\n\r\n# Define a single function that can extract features using hog sub-sampling and make predictions\r\ndef find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):\r\n    \r\n    draw_img = np.copy(img)\r\n    #img = img.astype(np.float32)/255\r\n    \r\n    heat_map = np.zeros_like(img[:,:,0]).astype(np.float)\r\n    \r\n    \r\n    img_tosearch = img[ystart:ystop,:,:]\r\n    ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\r\n    if scale != 1:\r\n        imshape = ctrans_tosearch.shape\r\n        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\r\n        \r\n    ch1 = ctrans_tosearch[:,:,0]\r\n    ch2 = ctrans_tosearch[:,:,1]\r\n    ch3 = ctrans_tosearch[:,:,2]\r\n\r\n    # Define blocks and steps as above\r\n    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\r\n    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \r\n    nfeat_per_block = orient*cell_per_block**2\r\n    \r\n    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\r\n    window = 64\r\n    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\r\n    cells_per_step = 2  # Instead of overlap, define how many cells to step\r\n    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\r\n    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\r\n    \r\n    # Compute individual channel HOG features for the entire image\r\n    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    \r\n    for xb in range(nxsteps):\r\n        for yb in range(nysteps):\r\n            ypos = yb*cells_per_step\r\n            xpos = xb*cells_per_step\r\n            # Extract HOG for this patch\r\n            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\r\n\r\n            xleft = xpos*pix_per_cell\r\n            ytop = ypos*pix_per_cell\r\n\r\n            # Extract the image patch\r\n            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\r\n          \r\n            # Get color features\r\n            spatial_features = bin_spatial(subimg, size=spatial_size)\r\n            hist_features = color_hist(subimg, nbins=hist_bins)\r\n\r\n            # Scale features and make a prediction\r\n            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \r\n            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \r\n            test_prediction = svc.predict(test_features)\r\n            \r\n            if test_prediction == 1:\r\n                xbox_left = np.int(xleft*scale)\r\n                ytop_draw = np.int(ytop*scale)\r\n                win_draw = np.int(window*scale)\r\n                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6)\r\n                img_boxes.append(((xbox_left, ytop_draw+ystart),(xbox_left+win_draw, ytop_draw+win_draw+ystart)))\r\n                heat_map[ytop_draw+ystart:ytop_draw+win_draw+ystart, xbox_left:xbox_left+win_draw] +=1\r\n                \r\n    return draw_img, heat_map\r\n    \r\ndef process_image(img):\r\n    # Find final boxes from heatmap using label function\r\n    out_img, heatmap = find_cars(img, \r\n                      ystart=YSTART, \r\n                      ystop=YSTOP,\r\n                      scale=SCALE, \r\n                      svc = SVC, \r\n                      X_scaler = X_scaler,\r\n                      orient= ORIENTATION, \r\n                      pix_per_cell = PIX_PER_CELL,\r\n                      cell_per_block= CELL_PER_BLOCK, \r\n                      spatial_size = SPATIAL_SIZE,\r\n                      hist_bins = HIST_BINS)\r\n    labels = label(heatmap)\r\n    draw_img = draw_labeled_bboxes(np.copy(img), labels)\r\n    return draw_img   \r\n\r\nif __name__ == \"__main__\":\r\n\r\n    vehicles_images = glob.glob('../../../vehicles/vehicles/*/*.png')\r\n    non_vehicles_images = glob.glob('../../../non-vehicles/non-vehicles/*/*.png')\r\n    cars = load_data(vehicles_images)\r\n    non_cars = load_data(non_vehicles_images)\r\n\r\n        \"\"\"Parameters\"\"\"\r\n    COLOR_SPACE = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\r\n    ORIENTATION = 9  # HOG orientations\r\n    PIX_PER_CELL = 8 # HOG pixels per cell\r\n    CELL_PER_BLOCK = 2 # HOG cells per block\r\n    HOG_CHANNEL = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\r\n    SPATIAL_SIZE = (16, 16) # Spatial binning dimensions\r\n    HIST_BINS = 16    # Number of histogram bins\r\n    IS_SPATIAL_FEAT = True # Spatial features on or off\r\n    IS_HIST_FEAT = True # Histogram features on or off\r\n    IS_HOG_FEAT = True # HOG features on or off\r\n\r\n    t=time.time()\r\n\r\n    car_features = extract_features(cars, \r\n                                    color_space = COLOR_SPACE, \r\n                                    spatial_size= SPATIAL_SIZE, \r\n                                    hist_bins = HIST_BINS, \r\n                                    orient = ORIENTATION, \r\n                                    pix_per_cell = PIX_PER_CELL, \r\n                                    cell_per_block = CELL_PER_BLOCK, \r\n                                    hog_channel = HOG_CHANNEL,\r\n                                    spatial_feat = IS_SPATIAL_FEAT , \r\n                                    hist_feat = IS_HIST_FEAT,\r\n                                    hog_feat = IS_HOG_FEAT)\r\n\r\n    notcar_features = extract_features(non_cars, \r\n                                    color_space = COLOR_SPACE, \r\n                                    spatial_size= SPATIAL_SIZE, \r\n                                    hist_bins = HIST_BINS, \r\n                                    orient = ORIENTATION, \r\n                                    pix_per_cell = PIX_PER_CELL, \r\n                                    cell_per_block = CELL_PER_BLOCK, \r\n                                    hog_channel = HOG_CHANNEL,\r\n                                    spatial_feat = IS_SPATIAL_FEAT , \r\n                                    hist_feat = IS_HIST_FEAT,\r\n                                    hog_feat = IS_HOG_FEAT)\r\n\r\n    print(time.time()-t, 'Seconds to compute features...')\r\n\r\n\r\n    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \r\n    # Fit a per-column scaler\r\n    X_scaler = StandardScaler().fit(X)\r\n    # Apply the scaler to X\r\n    scaled_X = X_scaler.transform(X)\r\n\r\n    # Define the labels vector\r\n    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\r\n\r\n\r\n    # Split up data into randomized training and test sets\r\n    rand_state = np.random.randint(0, 100)\r\n    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\r\n\r\n    print('Using:',orient,'orientations',pix_per_cell,'pixels per cell and', cell_per_block,'cells per block')\r\n    print('Feature vector length:', len(X_train[0]))\r\n    # Use a linear SVC \r\n    SVC = LinearSVC()\r\n    # Check the training time for the SVC\r\n    SVC.fit(X_train, y_train)\r\n    t2 = time.time()\r\n    print(round(t2-t, 2), 'Seconds to train SVC...')\r\n    # Check the score of the SVC\r\n    print('Test Accuracy of SVC = ', round(SVC.score(X_test, y_test), 4))\r\n\r\n    clip1 = VideoFileClip('../project_video.mp4')\r\n    video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\r\n\r\n    video_output = '../output_videos/project_video.mp4'", "instruction_en": "The given code contains functions to extract features from images and detect cars in a video using a linear support vector machine (SVM) classifier. To use this code, you need to provide a list of vehicle and non-vehicle images, and then call the `extract_features` function to extract features from these images. After that, you can train a linear SVM classifier using the extracted features and labels. Once the classifier is trained, you can use the `process_image` function to detect cars in a video. The output video will be saved in the specified location.", "original_line_number": 30805, "instruction_length": 610}
{"instruction": "Kod yang diberikan mentakrifkan fungsi yang dipanggil \"make_tile\" yang mencipta model 3D jubin dengan dimensi tertentu, jenis bata, saiz, dan tahap terperinci menggunakan API Python Blender. Fungsi ini mengambil beberapa argumen kata kunci termasuk \"dimensi\", \"bata_type\", \"brick_size\", \"circle_verts\", \"type\", \"detail\", dan \"bme\", dan mengembalikan objek bmesh yang mewakili jubin. Untuk menggunakan fungsi ini, panggilnya dengan hujah -hujah yang dikehendaki dan simpan objek BMESH yang dikembalikan.", "input": "", "output": "# Copyright (C) 2020 Christopher Gearhart\n# chris@bblanimation.com\n# http://bblanimation.com/\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\n# System imports\nimport bpy\nimport bmesh\nimport math\nimport numpy as np\n\n# Blender imports\nfrom mathutils import Vector\n\n# Module imports\nfrom .generator_utils import *\n\n\ndef make_tile(dimensions:dict, brick_type:str, brick_size:list, circle_verts:int=None, type:str=None, detail:str=\"LOW\", bme:bmesh=None):\n    \"\"\"\n    create inverted slope brick with bmesh\n\n    Keyword Arguments:\n        dimensions   -- dictionary containing brick dimensions\n        brick_type   -- cm.brick_type\n        brick_size   -- size of brick (e.g. standard 2x4 -> [2, 4, 3])\n        circle_verts -- number of vertices per circle of cylinders\n        type         -- type of tile in ('TILE', 'TILE_GRILL')\n        detail       -- level of brick detail (options: ('FLAT', 'LOW', 'HIGH'))\n        bme          -- bmesh object in which to create verts\n\n    \"\"\"\n    # create new bmesh object\n    bme = bmesh.new() if not bme else bme\n\n    # get halfScale\n    d = Vector((dimensions[\"half_width\"], dimensions[\"half_width\"], dimensions[\"half_height\"]))\n    d.z = d.z * (brick_size[2] if flat_brick_type(brick_type) else 1)\n    # get scalar for d in positive xyz directions\n    scalar = Vector((brick_size[0] * 2 - 1,\n                     brick_size[1] * 2 - 1,\n                     1))\n    d_scaled = vec_mult(d, scalar)\n    # get thickness of brick from inside to outside\n    thick_xy = dimensions[\"thickness\"] - (dimensions[\"tick_depth\"] if \"High\" in detail and min(brick_size) != 1 else 0)\n    thick = Vector((thick_xy, thick_xy, dimensions[\"thickness\"]))\n\n    # create cube\n    if \"GRILL\" in type:\n        coord1 = -d\n        coord1.z += dimensions[\"slit_height\"]\n        coord2 = d_scaled\n        coord2.z = coord1.z\n        v1, v4, v3, v2 = make_rectangle(coord1, coord2, face=False, bme=bme)[1]\n    else:\n        sides = [1, 1 if detail == \"FLAT\" else 0, 1, 1, 1, 1]\n        coord1 = -d\n        coord1.z += dimensions[\"slit_height\"]\n        coord2 = d_scaled\n        v1, v2, v3, v4, v5, v6, v7, v8 = make_cube(coord1, coord2, sides, bme=bme)[1]\n\n    # make verts for slit\n    slit_depth = Vector([dimensions[\"slit_depth\"]]*2)\n    coord1 = -d\n    coord1.xy += slit_depth\n    coord2 = Vector((d_scaled.x, d_scaled.y, -d.z + dimensions[\"slit_height\"]))\n    coord2.xy -= slit_depth\n    v9, v10, v11, v12, v13, v14, v15, v16 = make_cube(coord1, coord2, [0, 1 if detail == \"FLAT\" and \"GRILL\" not in type else 0, 1, 1, 1, 1], bme=bme)[1]\n    # connect slit to outer cube\n    bme.faces.new((v14, v4, v1, v13))\n    bme.faces.new((v15, v3, v4, v14))\n    bme.faces.new((v16, v2, v3, v15))\n    bme.faces.new((v13, v1, v2, v16))\n\n    # add details\n    if \"GRILL\" in type:\n        if brick_size[0] < brick_size[1]:\n            add_grill_details(dimensions, brick_size, thick, scalar, d, v4, v1, v2, v3, v9, v10, v11, v12, bme)\n        else:\n            add_grill_details(dimensions, brick_size, thick, scalar, d, v1, v2, v3, v4, v9, v10, v11, v12, bme)\n\n    elif detail != \"FLAT\":\n        # making verts for hollow portion\n        coord1 = -d + Vector((thick.x, thick.y, 0))\n        coord2 = vec_mult(d, scalar) - thick\n        v17, v18, v19, v20, v21, v22, v23, v24 = make_cube(coord1, coord2, [1, 0, 1, 1, 1, 1], flip_normals=True, bme=bme)[1]\n        # connect hollow portion to verts for slit\n        bme.faces.new((v18, v17, v9, v10))\n        bme.faces.new((v19, v18, v10, v11))\n        bme.faces.new((v20, v19, v11, v12))\n        bme.faces.new((v17, v20, v12, v9))\n\n        # add supports\n        if max(brick_size[:2]) > 2:\n            add_supports(dimensions, dimensions[\"height\"], brick_size, brick_type, circle_verts, type, detail, d, scalar, thick, bme)\n\n    return bme", "instruction_en": "The given code defines a function called \"make_tile\" that creates a 3D model of a tile with specified dimensions, brick type, size, and level of detail using the Blender Python API. The function takes in several keyword arguments including \"dimensions\", \"brick_type\", \"brick_size\", \"circle_verts\", \"type\", \"detail\", and \"bme\", and returns a BMesh object representing the tile. To use this function, call it with the desired arguments and store the returned BMesh object.", "original_line_number": 30900, "instruction_length": 502}
{"instruction": "Untuk menjalankan masalah VRP, lakukan perkara berikut:\n1. Import TsplibParser, Argparser, VRPCenter, TSppainter, dan modul pembalakan.\n2. Membina logger dengan tahap log info dan formatter log.\n3. Tentukan fungsi yang dipanggil \"Run\" yang mengambil objek TSplibParser sebagai hujah.\n4. Di dalam fungsi \"Run\", buat objek VRPCenter dengan objek TSPLIBParser sebagai parameter.\n5. Log koordinat setiap nod dalam objek TSplibParser.\n6. Tetapkan matriks koordinat objek tsppainter ke atribut Cities_coord objek TsplibParser dan lukis peta.\n7. Log lokasi setiap loker dalam objek TsplibParser dan lukis loker pada peta.\n8. Log lokasi dan tuntutan setiap titik penghantaran dalam objek TsplibParser dan hitung jumlah permintaan.\n9. Mulakan objek VRPCenter.\n10. Tentukan fungsi utama yang menghidupkan argumen baris perintah menggunakan modul ArgParser dan membaca fail TSP menggunakan modul TsplibParser.\n11. Log maklumat formulasi masalah.\n12. Panggil fungsi \"lari\" dengan objek TsplibParser sebagai hujah.\n13. Jika berjalan sebagai modul utama, hubungi fungsi utama.", "input": "", "output": "from TsplibParser import parser as tspparser\nfrom ArgParser import parser as argparser\nfrom VRPCenter import VRPCenter\nfrom TspPainter import tspPainter\nimport logging\n\n# construct the logger\nlogger = logging.getLogger(\"logger\")\nlogger.setLevel(logging.INFO)\nlogFormatter = logging.Formatter(\"%(asctime)s [%(threadName)s] [%(levelname)s]  %(message)s\")\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nlogger.addHandler(consoleHandler)\n\ndef run(tspparser):\n    center = VRPCenter(tspparser)\n    logger.info(\"Nodes: \")\n    for i in range(1, len(tspparser.cities_coord)):\n        logger.info(\"Node \" + str(i) + \" coordinate is \" + str(tspparser.cities_coord[i][0]) + \", \" + str(tspparser.cities_coord[i][1]))\n    tspPainter.coord_mat = tspparser.cities_coord\n    tspPainter.drawMap()\n\n    logger.info(\"Lockers: \")\n    for i in range(0, len(tspparser.lockers)):\n        logger.info(tspparser.lockers[i])\n    tspPainter.drawLockers(tspparser.lockers)\n\n    logger.info(\"Delivers: \")\n    for i in range(0, len(tspparser.delivers)):\n        logger.info(tspparser.delivers[i])\n    logger.info(\"Demands: \")\n    demands = 0\n    for i in range(0, len(tspparser.demands)):\n        demands += tspparser.demands[i]\n        logger.info(\"Node {} {}\".format(i, tspparser.demands[i]))\n    logger.info(\"Total demands is: {}\".format(demands))\n\n    center.start()\n\ndef main():\n    args = argparser.parse_args()\n    tspparser.read_file(args.tsp_file[0])\n\n    logger.info(\"-------------------------------------------\")\n    logger.info(\"Problem formulation information\")\n    logger.info(\"-------------------------------------------\")\n    logger.info(\"Name: \" + tspparser.name)\n    logger.info(\"Comment: \" + tspparser.comment)\n    logger.info(\"Type: \" + tspparser.type)\n    # run vrp center\n    run(tspparser)\n\nif __name__ == \"__main__\":\n    main()", "instruction_en": "To run a VRP problem, do the following:\n1. Import TsplibParser, ArgParser, VRPCenter, TspPainter, and logging modules.\n2. Construct a logger with a log level of INFO and a log formatter.\n3. Define a function called \"run\" that takes a TsplibParser object as an argument.\n4. Inside the \"run\" function, create a VRPCenter object with the TsplibParser object as a parameter.\n5. Log the coordinates of each node in the TsplibParser object.\n6. Set the coordinate matrix of the TspPainter object to the cities_coord attribute of the TsplibParser object and draw the map.\n7. Log the locations of each locker in the TsplibParser object and draw the lockers on the map.\n8. Log the locations and demands of each delivery point in the TsplibParser object and calculate the total demand.\n9. Start the VRPCenter object.\n10. Define a main function that parses command line arguments using the ArgParser module and reads a TSP file using the TsplibParser module.\n11. Log the problem formulation information.\n12. Call the \"run\" function with the TsplibParser object as an argument.\n13. If running as the main module, call the main function.", "original_line_number": 31315, "instruction_length": 1062}
{"instruction": "Kod ini mengandungi ujian unit untuk kelas CollectionQuery. Untuk menggunakan kod ini, buat fail python baru dan import kelas Unittest dan CollectionQuery. Kemudian, buat kelas TestCollectionClass baru yang mewarisi dari Unittest.testcase. Di dalam kelas ini, tentukan kaedah ujian yang memanggil kaedah kelas koleksi dan gunakan pernyataan untuk mengesahkan bahawa keputusan yang diharapkan dikembalikan. Gunakan kaedah test_archive_center untuk menguji kaedah archive_center kelas koleksi, kaedah test_keyword untuk menguji kaedah kata kunci, kaedah test_valid_formats untuk menguji kaedah format dengan format yang sah, kaedah test_invalid_format Untuk menguji kaedah Concept_ID dengan ID konsep yang tidak sah.", "input": "", "output": "import unittest\n\nfrom cmr.queries import CollectionQuery\n\nclass TestCollectionClass(unittest.TestCase):\n\n    def test_archive_center(self):\n        query = CollectionQuery()\n        query.archive_center(\"LP DAAC\")\n\n        self.assertIn(\"archive_center\", query.params)\n        self.assertEqual(query.params[\"archive_center\"], \"LP DAAC\")\n    \n    def test_keyword(self):\n        query = CollectionQuery()\n        query.keyword(\"AST_*\")\n\n        self.assertIn(\"keyword\", query.params)\n        self.assertEqual(query.params[\"keyword\"], \"AST_*\")\n    \n    def test_valid_formats(self):\n        query = CollectionQuery()\n        formats = [\n            \"json\", \"xml\", \"echo10\", \"iso\", \"iso19115\",\n            \"csv\", \"atom\", \"kml\", \"native\", \"dif\", \"dif10\",\n            \"opendata\", \"umm_json\", \"umm_json_v1_1\" \"umm_json_v1_9\"]\n\n        for _format in formats:\n            query.format(_format)\n            self.assertEqual(query._format, _format)\n    \n    def test_invalid_format(self):\n        query = CollectionQuery()\n\n        with self.assertRaises(ValueError):\n            query.format(\"invalid\")\n            query.format(\"jsonn\")\n            query.format(\"iso19116\")\n    \n    def test_valid_concept_id(self):\n        query = CollectionQuery()\n\n        query.concept_id(\"C1299783579-LPDAAC_ECS\")\n        self.assertEqual(query.params[\"concept_id\"], [\"C1299783579-LPDAAC_ECS\"])\n        \n        query.concept_id([\"C1299783579-LPDAAC_ECS\", \"C1441380236-PODAAC\"])\n        self.assertEqual(query.params[\"concept_id\"], [\"C1299783579-LPDAAC_ECS\", \"C1441380236-PODAAC\"])\n    \n    def test_invalid_concept_id(self):\n        query = CollectionQuery()\n\n        with self.assertRaises(ValueError):\n            query.concept_id(\"G1327299284-LPDAAC_ECS\")\n        \n        with self.assertRaises(ValueError):\n            query.concept_id([\"C1299783579-LPDAAC_ECS\", \"G1327299284-LPDAAC_ECS\"])", "instruction_en": "This code contains unit tests for the CollectionQuery class. To use this code, create a new Python file and import the unittest and CollectionQuery classes. Then, create a new TestCollectionClass class that inherits from unittest.TestCase. Within this class, define test methods that call methods of the CollectionQuery class and use assertions to verify that the expected results are returned. Use the test_archive_center method to test the archive_center method of the CollectionQuery class, the test_keyword method to test the keyword method, the test_valid_formats method to test the format method with valid formats, the test_invalid_format method to test the format method with invalid formats, the test_valid_concept_id method to test the concept_id method with valid concept IDs, and the test_invalid_concept_id method to test the concept_id method with invalid concept IDs.", "original_line_number": 31585, "instruction_length": 714}
{"instruction": "Buat kelas yang dipanggil BerkaitanManagerListSerializerfield yang mewarisi dari ListSerializerField. Kelas harus mempunyai kaedah __init__ yang mengambil siri, sort_by, dan use_prefetch sebagai argumen. Kelas juga harus mempunyai kaedah pra_value yang mengambil bidang dan tidak termasuk sebagai argumen, dan kaedah set_value yang mengambil nilai sebagai argumen. Kaedah set_value harus memeriksa sama ada nilai itu tidak ada, queryset, atau pengurus, dan kemudian tetapkan item, _native_items, dan atribut _python_items dengan sewajarnya. Akhirnya, kaedah SET_VALUE harus gelung melalui nilai dan menambah setiap item ke kelas menggunakan kaedah add_item.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom collections import Iterable\ntry:\n    from django.db.models.query import QuerySet\n    from django.db.models import Manager\nexcept ImportError:\n    QuerySet = None\n    Manager = None\n\nfrom aserializer.fields import ListSerializerField\nfrom aserializer.django.utils import get_local_fields, get_related_fields\n\n\nclass RelatedManagerListSerializerField(ListSerializerField):\n\n    def __init__(self, serializer, sort_by=None, use_prefetch=False, *args, **kwargs):\n        super(RelatedManagerListSerializerField, self).__init__(serializer=serializer, sort_by=sort_by, *args, **kwargs)\n        self.use_prefetch = use_prefetch\n\n    def pre_value(self, fields=None, exclude=None, **extras):\n        super(RelatedManagerListSerializerField, self).pre_value(fields=fields, exclude=exclude, **extras)\n        self.use_prefetch = extras.get('use_prefetch', self.use_prefetch)\n\n    def set_value(self, value):\n        if value is None:\n            return\n        elif isinstance(value, Iterable):\n            values = value\n        elif isinstance(value, (QuerySet, Manager)):\n            # if using prefetch_related, we can't use only as it will re-fetch the data\n            if not self.use_prefetch and (self.only_fields or self.exclude):\n                local_fields = get_local_fields(value.model)\n                related_fields = get_related_fields(value.model)\n                only_fields = [f.name for f in local_fields]\n                if self.only_fields:\n                    only_fields = [f for f in only_fields if f in self.only_fields]\n                exclude_fields = [f.name for f in local_fields if f.name in self.exclude]\n                if exclude_fields:\n                    only_fields = [f for f in only_fields if f not in exclude_fields]\n                only_fields += [f.name for f in related_fields]\n                # .only() returns a QuerySet of RelatedDjangoModel_Deferred objects?\n                values = value.only(*only_fields)\n            else:\n                values = value.all()\n        else:\n            return\n        self.items[:] = []\n        self._native_items[:] = []\n        self._python_items[:] = []\n        for item in values:\n            self.add_item(source=item)", "instruction_en": "Create a class called RelatedManagerListSerializerField that inherits from ListSerializerField. The class should have an __init__ method that takes in a serializer, sort_by, and use_prefetch as arguments. The class should also have a pre_value method that takes in fields and exclude as arguments, and a set_value method that takes in a value as an argument. The set_value method should check if the value is None, an Iterable, a QuerySet, or a Manager, and then set the items, _native_items, and _python_items attributes accordingly. Finally, the set_value method should loop through the values and add each item to the class using the add_item method.", "original_line_number": 31675, "instruction_length": 657}
{"instruction": "Buat kelas QuerySet yang menambah fungsi tambahan ke senarai asas. Ia harus memegang contoh satu jenis JSSObject dan menggunakan API Senarai Python sambil menambah beberapa kaedah penolong tambahan di atas. Kelas harus mempunyai kaedah untuk disusun mengikut id dan nama, mengambil semua data objek yang terkandung dari JSS, menyimpan semua data objek yang terkandung ke JSS, membatalkan masa cache untuk semua kandungan, dan mengembalikan penjana nama dan ID kandungan. Ia juga harus mempunyai kaedah kelas untuk membina pertanyaan dari respons penyenaraian.", "input": "", "output": "#!/usr/bin/env python\n# Copyright (C) 2014-2017 Shea G Craig\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"queryset.py\n\nClass that adds some extra functionality to a basic list. Used as the\nresult of all queries in python-jss.\n\"\"\"\n\n\nfrom __future__ import absolute_import\nfrom collections import defaultdict\ntry:\n    import cPickle  # Python 2.X\nexcept ImportError:\n    import _pickle as cPickle  # Python 3+\nimport datetime\nimport os\n\nfrom .jssobject import DATE_FMT, Identity\n\n\nSTR_FMT = \"{0:>{1}} | {2:>{3}} | {4:>{5}}\"\n\n\nclass QuerySet(list):\n    \"\"\"A list style collection of JSSObjects.\n\n    Listing operations retrieve minimal or overview information for most\n    object types. For example, we may want to see all the Computers on\n    the JSS but that does not mean we want to do a full object GET for\n    each one.\n\n    QuerySets hold instances of a single type of JSSObject, and use the\n    python list API, while adding some extra helper-methods on top.\n    \"\"\"\n\n    def __init__(self, objects):\n        \"\"\"Construct a list of JSSObjects.\n\n        Args:\n            objects (sequence of JSSObjects):\n                Sequences must be of a single class.\n        \"\"\"\n        if objects and not len({i.__class__ for i in objects}) == 1:\n            raise ValueError\n        super(QuerySet, self).__init__(objects)\n        self.sort()\n        self.contained_class = objects[0].__class__ if objects else None\n\n    def __str__(self):\n        \"\"\"Make data human readable.\"\"\"\n        # Make ID, Name first, no matter what.\n        sort_keys = [\"id\", \"name\"]\n        if self:\n            sort_keys.extend([\n                key for key in self[0]._basic_identity.keys() if\n                key not in sort_keys])\n\n        # Build a dict of max lengths per column for table output.\n        lengths = defaultdict(int)\n        for item in self:\n            for key in sort_keys:\n                val = item._basic_identity[key] or \"\"\n                length = max(len(key), len(val))\n                if length > lengths[key]:\n                    lengths[key] = length\n\n        # Build a format string for row output.\n        format_strings = []\n        for key in sort_keys:\n            length = lengths[key]\n            format_strings.append(\"{{data[{}]:>{}}}\".format(key, length))\n\n        cached = 'cached'\n        cached_format = '| {{cached:>{}}} |'.format(len(cached))\n\n        fmt = \"| \" + \" | \".join(format_strings) + cached_format\n\n        # Begin building output with header lines.\n        # Contained class can be None\n        contained_name = self.contained_class.__name__ if self.contained_class is not None else \"Empty\"\n        results = [\"{} QuerySet\".format(contained_name)]\n        headers = {key: key for key in lengths}\n        header_line = fmt.format(data=headers, cached=\"cached\")\n        bar = len(header_line) * '-'\n        results.extend([bar, header_line, bar])\n\n        str_cached = (\n            lambda i: str(i.cached) if isinstance(i.cached, bool) else 'True')\n\n        table = [\n            fmt.format(data=item._basic_identity, cached=str_cached(item)) for\n            item in self]\n        results.extend(table)\n\n        results.append(bar)\n        return \"\\n\".join(results)\n\n    def __repr__(self):\n        \"\"\"Make data human readable.\"\"\"\n        return \"QuerySet({})\".format(super(QuerySet, self).__repr__())\n\n    def sort(self, **kwargs):\n        \"\"\"Sort list elements by ID.\n        \"\"\"\n        super(QuerySet, self).sort(key=lambda k: int(k.id))\n\n    def sort_by_name(self):\n        \"\"\"Sort list elements by name.\"\"\"\n        super(QuerySet, self).sort(key=lambda k: k.name.upper())\n\n    def retrieve_all(self):\n        \"\"\"Tell each contained object to retrieve its data from the JSS\n\n        This can take a long time given a large number of objects,\n        and depending on the size of each object.\n\n        Returns:\n            self (QuerySet) to allow method chaining.\n        \"\"\"\n        for obj in self:\n            if not obj.cached:\n                obj.retrieve()\n\n        return self\n\n    def save_all(self):\n        \"\"\"Tell each contained object to save its data to the JSS\n\n        This can take a long time given a large number of objects,\n        and depending on the size of each object.\n\n        Returns:\n            self (QuerySet) to allow method chaining.\n        \"\"\"\n        for obj in self:\n            obj.save()\n\n        return self\n\n    def invalidate(self):\n        \"\"\"Clear the cache datetime for all contents.\n\n        This causes objects to retrieve their data again when accessed.\n        \"\"\"\n        for i in self: i.cached = False\n\n    def names(self):\n        \"\"\"Return a generator of contents names\"\"\"\n        return (item.name for item in self)\n\n    def ids(self):\n        \"\"\"Return a generator of contents ids\"\"\"\n        return (item.id for item in self)\n\n    @classmethod\n    def from_response(cls, obj_class, response, jss=None, **kwargs):\n        \"\"\"Build a QuerySet from a listing Response.\"\"\"\n        response_objects = (\n            i for i in response if i is not None and i.tag != \"size\")\n\n        dicts = (\n            {child.tag: child.text for child in item} for item in\n            response_objects)\n        identities = (Identity(d) for d in dicts)\n\n        objects = [obj_class(jss, data=i, **kwargs) for i in identities]\n\n        return cls(objects)", "instruction_en": "Create a QuerySet class that adds extra functionality to a basic list. It should hold instances of a single type of JSSObject and use the python list API while adding some extra helper-methods on top. The class should have methods to sort by ID and name, retrieve all contained objects' data from the JSS, save all contained objects' data to the JSS, invalidate the cache datetime for all contents, and return generators of contents' names and IDs. It should also have a class method to build a QuerySet from a listing Response.", "original_line_number": 31887, "instruction_length": 559}
{"instruction": "Kod ini mewujudkan arkib yang serasi dengan pelaksanaan ESP32-S* ROM DFU. Untuk menggunakannya, jalankan skrip dengan arahan \"tulis\" dan berikan hujah berikut:\n-\"-o/-output-fail\": nama fail untuk menyimpan imej DFU output (diperlukan)\n-\"--pid\": Pengenalpastian produk heksadesimal (diperlukan)\n-\"--JSON\": Fail pilihan untuk memuatkan kamus \"flash_files\" dengan <dorder> <file> item\n-\"-Saiz Bahagian\": Saiz partition untuk fail yang lebih besar (lalai ialah 512kb)\n- \"Fail\": Senarai <dress> <fail> pasangan yang akan ditambah ke imej DFU. Fail-fail tersebut dibahagikan kepada partisi yang lebih kecil daripada saiz \"saiz bahagian\" untuk mengelakkan masa keluar semasa memadamkan kawasan besar.", "input": "", "output": "#!/usr/bin/env python\n#\n# Copyright 2020-2021 Espressif Systems (Shanghai) CO LTD\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This program creates archives compatible with ESP32-S* ROM DFU implementation.\n#\n# The archives are in CPIO format. Each file which needs to be flashed is added to the archive\n# as a separate file. In addition to that, a special index file, 'dfuinfo0.dat', is created.\n# This file must be the first one in the archive. It contains binary structures describing each\n# subsequent file (for example, where the file needs to be flashed/loaded).\n\nfrom __future__ import print_function, unicode_literals\n\nimport argparse\nimport hashlib\nimport json\nimport os\nimport struct\nimport zlib\nfrom collections import namedtuple\nfrom functools import partial\n\nfrom future.utils import iteritems\n\ntry:\n    import typing\nexcept ImportError:\n    # Only used for type annotations\n    pass\n\ntry:\n    from itertools import izip as zip  # type: ignore\nexcept ImportError:\n    # Python 3\n    pass\n\n# CPIO (\"new ASCII\") format related things\nCPIO_MAGIC = b'070701'\nCPIO_STRUCT = b'=6s' + b'8s' * 13\nCPIOHeader = namedtuple(\n    'CPIOHeader',\n    [\n        'magic',\n        'ino',\n        'mode',\n        'uid',\n        'gid',\n        'nlink',\n        'mtime',\n        'filesize',\n        'devmajor',\n        'devminor',\n        'rdevmajor',\n        'rdevminor',\n        'namesize',\n        'check',\n    ],\n)\nCPIO_TRAILER = 'TRAILER!!!'\n\n\ndef make_cpio_header(\n    filename_len, file_len, is_trailer=False\n):  # type: (int, int, bool) -> CPIOHeader\n    \"\"\" Returns CPIOHeader for the given file name and file size \"\"\"\n\n    def as_hex(val):  # type: (int) -> bytes\n        return '{:08x}'.format(val).encode('ascii')\n\n    hex_0 = as_hex(0)\n    mode = hex_0 if is_trailer else as_hex(0o0100644)\n    nlink = as_hex(1) if is_trailer else hex_0\n    return CPIOHeader(\n        magic=CPIO_MAGIC,\n        ino=hex_0,\n        mode=mode,\n        uid=hex_0,\n        gid=hex_0,\n        nlink=nlink,\n        mtime=hex_0,\n        filesize=as_hex(file_len),\n        devmajor=hex_0,\n        devminor=hex_0,\n        rdevmajor=hex_0,\n        rdevminor=hex_0,\n        namesize=as_hex(filename_len),\n        check=hex_0,\n    )\n\n\n# DFU format related things\n# Structure of one entry in dfuinfo0.dat\nDFUINFO_STRUCT = b'<I I 64s 16s'\nDFUInfo = namedtuple('DFUInfo', ['address', 'flags', 'name', 'md5'])\nDFUINFO_FILE = 'dfuinfo0.dat'\n# Structure which gets added at the end of the entire DFU file\nDFUSUFFIX_STRUCT = b'<H H H H 3s B'\nDFUSuffix = namedtuple(\n    'DFUSuffix', ['bcd_device', 'pid', 'vid', 'bcd_dfu', 'sig', 'len']\n)\nESPRESSIF_VID = 12346\n# This CRC32 gets added after DFUSUFFIX_STRUCT\nDFUCRC_STRUCT = b'<I'\n\n\ndef dfu_crc(data, crc=0):  # type: (bytes, int) -> int\n    \"\"\" Calculate CRC32/JAMCRC of data, with an optional initial value \"\"\"\n    uint32_max = 0xFFFFFFFF\n    return uint32_max - (zlib.crc32(data, crc) & uint32_max)\n\n\ndef pad_bytes(b, multiple, padding=b'\\x00'):  # type: (bytes, int, bytes) -> bytes\n    \"\"\" Pad 'b' to a length divisible by 'multiple' \"\"\"\n    padded_len = (len(b) + multiple - 1) // multiple * multiple\n    return b + padding * (padded_len - len(b))\n\n\nclass EspDfuWriter(object):\n    def __init__(self, dest_file, pid, part_size):  # type: (typing.BinaryIO, int, int) -> None\n        self.dest = dest_file\n        self.pid = pid\n        self.part_size = part_size\n        self.entries = []  # type: typing.List[bytes]\n        self.index = []  # type: typing.List[DFUInfo]\n\n    def add_file(self, flash_addr, path):  # type: (int, str) -> None\n        \"\"\"\n        Add file to be written into flash at given address\n\n        Files are split up into chunks in order avoid timing-out during erasing large regions. Instead of adding\n        \"app.bin\" at flash_addr it will add:\n        1. app.bin   at flash_addr  # sizeof(app.bin) == self.part_size\n        2. app.bin.1 at flash_addr + self.part_size\n        3. app.bin.2 at flash_addr + 2 * self.part_size\n        ...\n\n        \"\"\"\n        f_name = os.path.basename(path)\n        with open(path, 'rb') as f:\n            for i, chunk in enumerate(iter(partial(f.read, self.part_size), b'')):\n                n = f_name if i == 0 else '.'.join([f_name, str(i)])\n                self._add_cpio_flash_entry(n, flash_addr, chunk)\n                flash_addr += len(chunk)\n\n    def finish(self):  # type: () -> None\n        \"\"\" Write DFU file \"\"\"\n        # Prepare and add dfuinfo0.dat file\n        dfuinfo = b''.join([struct.pack(DFUINFO_STRUCT, *item) for item in self.index])\n        self._add_cpio_entry(DFUINFO_FILE, dfuinfo, first=True)\n\n        # Add CPIO archive trailer\n        self._add_cpio_entry(CPIO_TRAILER, b'', trailer=True)\n\n        # Combine all the entries and pad the file\n        out_data = b''.join(self.entries)\n        cpio_block_size = 10240\n        out_data = pad_bytes(out_data, cpio_block_size)\n\n        # Add DFU suffix and CRC\n        dfu_suffix = DFUSuffix(0xFFFF, self.pid, ESPRESSIF_VID, 0x0100, b'UFD', 16)\n        out_data += struct.pack(DFUSUFFIX_STRUCT, *dfu_suffix)\n        out_data += struct.pack(DFUCRC_STRUCT, dfu_crc(out_data))\n\n        # Finally write the entire binary\n        self.dest.write(out_data)\n\n    def _add_cpio_flash_entry(\n        self, filename, flash_addr, data\n    ):  # type: (str, int, bytes) -> None\n        md5 = hashlib.md5()\n        md5.update(data)\n        self.index.append(\n            DFUInfo(\n                address=flash_addr,\n                flags=0,\n                name=filename.encode('utf-8'),\n                md5=md5.digest(),\n            )\n        )\n        self._add_cpio_entry(filename, data)\n\n    def _add_cpio_entry(\n        self, filename, data, first=False, trailer=False\n    ):  # type: (str, bytes, bool, bool) -> None\n        filename_b = filename.encode('utf-8') + b'\\x00'\n        cpio_header = make_cpio_header(len(filename_b), len(data), is_trailer=trailer)\n        entry = pad_bytes(\n            struct.pack(CPIO_STRUCT, *cpio_header) + filename_b, 4\n        ) + pad_bytes(data, 4)\n        if not first:\n            self.entries.append(entry)\n        else:\n            self.entries.insert(0, entry)\n\n\ndef action_write(args):  # type: (typing.Mapping[str, typing.Any]) -> None\n    writer = EspDfuWriter(args['output_file'], args['pid'], args['part_size'])\n    for addr, f in args['files']:\n        print('Adding {} at {:#x}'.format(f, addr))\n        writer.add_file(addr, f)\n    writer.finish()\n    print('\"{}\" has been written. You may proceed with DFU flashing.'.format(args['output_file'].name))\n    if args['part_size'] % (4 * 1024) != 0:\n        print('WARNING: Partition size of DFU is not multiple of 4k (4096). You might get unexpected behavior.')\n\n\ndef main():  # type: () -> None\n    parser = argparse.ArgumentParser()\n\n    # Provision to add \"info\" command\n    subparsers = parser.add_subparsers(dest='command')\n    write_parser = subparsers.add_parser('write')\n    write_parser.add_argument('-o', '--output-file',\n                              help='Filename for storing the output DFU image',\n                              required=True,\n                              type=argparse.FileType('wb'))\n    write_parser.add_argument('--pid',\n                              required=True,\n                              type=lambda h: int(h, 16),\n                              help='Hexa-decimal product indentificator')\n    write_parser.add_argument('--json',\n                              help='Optional file for loading \"flash_files\" dictionary with <address> <file> items')\n    write_parser.add_argument('--part-size',\n                              default=os.environ.get('ESP_DFU_PART_SIZE', 512 * 1024),\n                              type=lambda x: int(x, 0),\n                              help='Larger files are split-up into smaller partitions of this size')\n    write_parser.add_argument('files',\n                              metavar='<address> <file>', help='Add <file> at <address>',\n                              nargs='*')\n\n    args = parser.parse_args()\n\n    def check_file(file_name):  # type: (str) -> str\n        if not os.path.isfile(file_name):\n            raise RuntimeError('{} is not a regular file!'.format(file_name))\n        return file_name\n\n    files = []\n    if args.files:\n        files += [(int(addr, 0), check_file(f_name)) for addr, f_name in zip(args.files[::2], args.files[1::2])]\n\n    if args.json:\n        json_dir = os.path.dirname(os.path.abspath(args.json))\n\n        def process_json_file(path):  # type: (str) -> str\n            '''\n            The input path is relative to json_dir. This function makes it relative to the current working\n            directory.\n            '''\n            return check_file(os.path.relpath(os.path.join(json_dir, path), start=os.curdir))\n\n        with open(args.json) as f:\n            files += [(int(addr, 0),\n                       process_json_file(f_name)) for addr, f_name in iteritems(json.load(f)['flash_files'])]\n\n    files = sorted([(addr, f_name.decode('utf-8') if isinstance(f_name, type(b'')) else f_name) for addr, f_name in iteritems(dict(files))],\n                   key=lambda x: x[0])  # remove possible duplicates and sort based on the address\n\n    cmd_args = {'output_file': args.output_file,\n                'files': files,\n                'pid': args.pid,\n                'part_size': args.part_size,\n                }\n\n    {'write': action_write\n     }[args.command](cmd_args)\n\n\nif __name__ == '__main__':\n    main()", "instruction_en": "This code creates archives compatible with ESP32-S* ROM DFU implementation. To use it, run the script with the \"write\" command and provide the following arguments:\n- \"-o/--output-file\": the filename for storing the output DFU image (required)\n- \"--pid\": the hexadecimal product identifier (required)\n- \"--json\": an optional file for loading a \"flash_files\" dictionary with <address> <file> items\n- \"--part-size\": the partition size for larger files (default is 512KB)\n- \"files\": a list of <address> <file> pairs to be added to the DFU image. The files are split up into smaller partitions of size \"part-size\" to avoid timing-out during erasing large regions.", "original_line_number": 32104, "instruction_length": 693}
{"instruction": "Buat model untuk permainan dengan model plot yang mempunyai urlfield, medan jawapan dengan max_digits = 3 dan decimal_places = 2, dan medan tempoh dengan Timefield. Buat model slidervalue dengan kunci asing kepada model pengguna, medan timestamp dengan zon waktu lalai.now, kunci asing kepada model bulat, medan bulat dengan positivesmallintegerfield, dan medan nilai dengan decimal_places = 2, max_digits = 3, dan null = true. Buat model bulat dengan kunci asing kepada model pengguna, kunci asing kepada model plot, medan pusingan dengan positivesmallintegerfield, medan meneka dengan max_digits = 3 dan decimal_places = 2, medan start_time dengan auto_now = benar dan null = benar, medan akhir dengan Perpuluhan (0). Model bulat juga harus mempunyai kaedah round_data yang mengembalikan kamus dengan pelbagai bidang, dan kaedah get_guess yang mengembalikan meneka sebagai terapung atau -1 jika ia adalah batal. Model bulat harus mempunyai kekangan unik_together pada medan pengguna dan round_order. Kod ini harus mengimport perpuluhan, model dari django.db, tetapan dari django.conf, dan zon waktu dari django.utils. Kod ini juga harus mengimport Calculate_score dari game.contrib.calculate dan kawalan dari game.control.models.", "input": "", "output": "from decimal import Decimal\nfrom django.db import models\nfrom django.conf import settings\nfrom django.utils import timezone\n\n\n# Create your models here.\nfrom game.contrib.calculate import calculate_score\nfrom game.control.models import Control\n\n\nclass Plot(models.Model):\n    plot = models.URLField()\n    answer = models.DecimalField(max_digits=3, decimal_places=2)\n    duration = models.TimeField(null=True)\n\n    def __str__(self):\n        return self.plot\n\n\nclass SliderValue(models.Model):\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n    timestamp = models.DateTimeField(default=timezone.now)\n    this_round = models.ForeignKey(\"Round\", null=True)\n    round_order = models.PositiveSmallIntegerField(null=True)\n    value = models.DecimalField(decimal_places=2, max_digits=3, null=True)\n\n    def __str__(self):\n        return \"Slider {}\".format(self.id)\n\nclass Round(models.Model):\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n    plot = models.ForeignKey(Plot, on_delete=models.CASCADE)\n\n    round_order = models.PositiveSmallIntegerField()\n    guess = models.DecimalField(max_digits=3, decimal_places=2, null=True)\n\n    # start time of the round\n    # end time of the round\n    start_time = models.DateTimeField(auto_now=True, null=True)\n    end_time = models.DateTimeField(null=True)\n\n    # todo: change control treatment\n    score = models.DecimalField(max_digits=3, decimal_places=2, default=Decimal(0))\n\n    def __str__(self):\n        return self.user.username\n\n    def round_data(self):\n        played_rounds = self.__class__.objects.filter(user=self.user, round_order__lte=self.round_order,\n                                                      guess__gte=Decimal(0.0))\n        score = calculate_score(played_rounds)\n        this_round = self.__class__.objects.filter(user=self.user, round_order=self.round_order,\n                                                   guess__gte=Decimal(0.0))\n        round_score = calculate_score(this_round)\n\n        data = {'username': self.user.username, 'cumulative_score': score,\n                'avatar': self.user.avatar, 'task_path': self.plot.plot, 'correct_answer': self.plot.answer,\n                'independent_guess': self.guess, 'round_id': self.round_order, 'score': round_score,\n                'game_id': None, 'condition': None, 'following': None, 'revised_guess': None\n                }\n\n        if self.user.game_type == 'c':\n            game = Control.objects.get(user=self.user)\n            data['game_id'] = game.id\n            data['condition'] = 'control'\n\n        return data\n\n    def get_guess(self):\n        return float(self.guess) if self.guess else -1\n\n    class Meta:\n        unique_together = (('user', 'round_order',),)", "instruction_en": "Create models for a game with a Plot model that has a URLField, an answer field with max_digits=3 and decimal_places=2, and a duration field with TimeField. Create a SliderValue model with a foreign key to the user model, a timestamp field with default timezone.now, a foreign key to the Round model, a round_order field with PositiveSmallIntegerField, and a value field with decimal_places=2, max_digits=3, and null=True. Create a Round model with a foreign key to the user model, a foreign key to the Plot model, a round_order field with PositiveSmallIntegerField, a guess field with max_digits=3 and decimal_places=2, a start_time field with auto_now=True and null=True, an end_time field with null=True, and a score field with max_digits=3 and decimal_places=2 with default Decimal(0). The Round model should also have a round_data method that returns a dictionary with various fields, and a get_guess method that returns the guess as a float or -1 if it is null. The Round model should have a unique_together constraint on the user and round_order fields. The code should import Decimal, models from django.db, settings from django.conf, and timezone from django.utils. The code should also import calculate_score from game.contrib.calculate and Control from game.control.models.", "original_line_number": 32255, "instruction_length": 1231}
{"instruction": "Simpan Tahap:\n\n1. Pilih tahap bangunan untuk disimpan.\n2. Memohon penapisan terlebih dahulu untuk melangkau unsur-unsur bukan peringkat.\n3. Jalankan skrip.\n4. Lihat sifat bergantung seperti lokasi titik akhir, kepala peringkat, dan pemimpin akan disimpan untuk digunakan semula.\n5. Unsur bukan peringkat akan dilangkau dengan dialog.\n6. Tahap yang disimpan akan disimpan dalam fail yang dinamakan \"LevelPlacement\" dalam direktori sementara.\n7. Diuji Revit API: 2020.\n8. Skrip ini adalah sebahagian daripada Pyrevitplus: Sambungan untuk Pyrevit.\n9. Untuk maklumat lanjut, lawati github.com/ejs-ejs atau hubungi @ejs-ejs.", "input": "", "output": "\"\"\"\nSave Levels\n\nSave the view dependant properties - \nendpoint locations, level heads and leaders\nof the selected building levels for re-use\n\nNon-level elements will be skipped with dialog,\n so it's advisable to apply filtering beforehead\n\nTESTED REVIT API: 2020\n\n@ejs-ejs\nThis script is part of PyRevitPlus: Extensions for PyRevit\ngithub.com/ejs-ejs | @ejs-ejs\n\n--------------------------------------------------------\nRevitPythonWrapper: revitpythonwrapper.readthedocs.io\npyRevit: github.com/eirannejad/pyRevit\n\n\"\"\"\n\nimport os\nimport pickle\nfrom tempfile import gettempdir\nfrom collections import namedtuple\n\nimport rpw\nfrom rpw import doc, uidoc, DB, UI\n\nPoint = namedtuple('Point', ['X', 'Y','Z'])\n\nAxis = namedtuple('Axis', ['Name', 'Start', 'End','StartBubble', 'EndBubble', 'StartBubbleVisible', 'EndBubbleVisible'])\n\ntempfile = os.path.join(gettempdir(), 'LevelPlacement')\n\ncView = doc.ActiveView\n\n\n\nif not(cView.ViewType == DB.ViewType.Section or cView == DB.ViewType.Elevation):\n    UI.TaskDialog.Show('pyRevitPlus', 'View type \\'{}\\' not supported'.format(cView.ViewType))\n    \nelse:\n    experimental = True\n    UI.TaskDialog.Show('pyRevitPlus', 'Support for \\'{}\\' view type is experimental!'.format(cView.ViewType))\n        \n        \n    selection = rpw.ui.Selection()\n\n#if len(selection) <> 1:\n#            UI.TaskDialog.Show('pyRevitPlus', 'Select a single grid line!')\n#            exit(0);\n\n    n=0\n    LevelLines = dict()\n\n    for cLevel in selection:\n        el = cLevel.unwrap()\n        if isinstance(el, DB.Level):\n            curves=el.GetCurvesInView(DB.DatumExtentType.ViewSpecific, cView)\n            if len(curves) <> 1:\n                UI.TaskDialog.Show('pyRevitPlus', 'The level line is defind by {} curves, unable to proceed', len(curves))\n            else:\n                cLevelLine = {'Name':'', 'Start': Point(0,0,0), 'End': Point(0,0,0), 'StartBubble': False, 'StartBubbleVisible': False, 'EndBubble': False, 'EndBubbleVisible': False}\n        \n                cCurve = curves[0]\n            \n                leader0 = el.GetLeader(DB.DatumEnds.End0, cView)\n                if leader0:\n                    tmp = leader0.Elbow\n                    cLevelLine['Leader0Elbow'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader0.End\n                    cLevelLine['Leader0End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader0.Anchor\n                    cLevelLine['Leader0Anchor'] = Point(tmp.X, tmp.Y,tmp.Z)\n                \n            \n                leader1 = el.GetLeader(DB.DatumEnds.End1, cView)\n                if leader1:\n                    tmp = leader1.Elbow\n                    cLevelLine['Leader1Elbow'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader1.End\n                    cLevelLine['Leader1End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader1.Anchor\n                    cLevelLine['Leader1Anchor'] = Point(tmp.X, tmp.Y,tmp.Z)\n        \n                cLevelLine['Name'] = el.Name\n        \n                tmp = cCurve.GetEndPoint(0)\n                cLevelLine['Start'] = Point(tmp.X, tmp.Y,tmp.Z)\n                tmp = cCurve.GetEndPoint(1)\n                cLevelLine['End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                if el.HasBubbleInView(DB.DatumEnds.End0, cView):\n                    cLevelLine['StartBubble']=True\n                if el.HasBubbleInView(DB.DatumEnds.End1, cView):\n                    cLevelLine['EndBubble']=True\n                if el.IsBubbleVisibleInView(DB.DatumEnds.End0, cView):\n                    cLevelLine['StartBubbleVisible']=True\n                if el.IsBubbleVisibleInView(DB.DatumEnds.End1, cView):\n                    cLevelLine['EndBubbleVisible']=True\n                #if isinstance(cCurve, DB.Arc):\n                #    tmp = cCurve.Center\n                #    cLevelLine['Center'] = Point(tmp.X, tmp.Y,tmp.Z)\n            \n                LevelLines[cLevelLine['Name']] = cLevelLine\n                n += 1\n        else:\n            #if isinstance(el, DB.MultiSegmentGrid):\n            #    UI.TaskDialog.Show('pyRevitPlus', 'Skipping yet unsupported Multi-Segment grid \\'{}\\''.format(el.Name))\n            #else: \n            UI.TaskDialog.Show('pyRevitPlus', 'Skipping non- level element \\'{}\\''.format(el.Name))\n        \n    if n<>1:\n        msg = 'Saved {} level placements to {}'.format(n,tempfile)\n    else:\n        msg = 'Saved level \\'{}\\' placement to {}'.format(cLevelLine['Name'],tempfile)\n \n    if n>0:\n        with open(tempfile, 'wb') as fp:\n            pickle.dump(LevelLines, fp)\n        # close(fp)\n        UI.TaskDialog.Show('pyRevitPlus', msg)\n    else:\n        UI.TaskDialog.Show('pyRevitPlus', 'Nothing to save')", "instruction_en": "Save Levels:\n\n1. Select the building levels to save.\n2. Apply filtering beforehand to skip non-level elements.\n3. Run the script.\n4. View dependant properties such as endpoint locations, level heads, and leaders will be saved for re-use.\n5. Non-level elements will be skipped with dialog.\n6. The saved levels will be stored in a file named \"LevelPlacement\" in the temporary directory.\n7. Tested Revit API: 2020.\n8. This script is part of PyRevitPlus: Extensions for PyRevit.\n9. For more information, visit github.com/ejs-ejs or contact @ejs-ejs.", "original_line_number": 32257, "instruction_length": 619}
{"instruction": "Import modul dan kelas yang diperlukan dari fail yang diperlukan. Tentukan kelas bernama DescriptionParser yang mewarisi dari kelas DescriptionParserbase. Tentukan kaedah get_content yang mengambil URL sebagai input dan mengembalikan kandungan halaman. Tentukan kaedah __init__ yang memulakan kelas dan menetapkan kandungan halaman. Tentukan kaedah get_tag yang mengembalikan tag untuk rentetan yang diberikan. Tentukan kaedah bersih yang menghilangkan aksara yang tidak diingini dari rentetan. Tentukan kaedah get_title yang mengembalikan tajuk rentetan yang diberikan. Tentukan kaedah get_original_title yang mengembalikan tajuk asal rentetan yang diberikan. Tentukan kaedah parse_season_from_title yang mengekstrak nombor musim dari rentetan yang diberikan. Tentukan kaedah get_episodes_num yang mengembalikan bilangan episod dalam rentetan yang diberikan. Tentukan kaedah date_added_duration yang mengembalikan tempoh sejak tarikh ditambah. Tentukan kaedah parse yang mengepam kandungan halaman dan ekstrak maklumat yang diperlukan. Tentukan fungsi write_tvshow_nfo yang menulis fail nfo rancangan TV. Tentukan fungsi write_tvshow yang menulis fail rancangan TV. Tentukan fungsi write_tvshow_item yang menulis item rancangan TV. Tentukan fungsi get_session yang mengembalikan sesi. Tentukan fungsi muat turun_torrent yang memuat turun fail torrent. Tentukan fungsi write_pages yang menulis halaman. Tentukan fungsi penapis yang menapis tajuk. Tentukan fungsi carian_generate yang menghasilkan carian. Tentukan fungsi larian yang menjalankan program.", "input": "", "output": "# coding: utf-8\nimport log\nfrom log import debug\n\n\nfrom settings import Settings\nfrom base import *\nimport feedparser, urllib2, re\nfrom bs4 import BeautifulSoup\nfrom nfowriter import *\nfrom strmwriter import *\nimport requests, filesystem\n\n###################################################################################################\nclass DescriptionParser(DescriptionParserBase):\n\t#==============================================================================================\n\tdef get_content(self, url):\n\t\tpage = urllib2.urlopen(url)\n\t\treturn page\n\t\n\t#==============================================================================================\n\tdef __init__(self, url):\n\t\tInformer.__init__(self)\n\t\t\n\t\tself._dict = dict()\n\t\tself.content = self.get_content(url)\n\t\t#html_doc = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n<html>' + content.encode('utf-8') + '\\n</html>'\n\t\tself.soup = BeautifulSoup(self.content, 'html.parser')\n\t\tself.OK = self.parse()\n\t\t\t\t\n\t#==============================================================================================\n\tdef get_tag(self, x):\n\t\treturn {\n\t\t\tu': ': u'year',\n\t\t\tu': ': u'genre',\n\t\t\tu': ': u'plot',\n\t\t\tu': ': u'director',\n\t\t\tu': ': u'runtime',\n\t\t\tu': ': u'country',\n\t\t}.get(x, u'')\n\t\t\n\t#==============================================================================================\n\tdef clean(self, title):\n\t\ttry:\n\t\t\ttitle = title.split(u' -')[0]\n\t\t\ttitle = title.split(u' TV-')[0]\n\t\t\ttitle = title.split(u' [')[0]\n\t\texcept:\n\t\t\tpass\n\t\treturn title.strip()\n\t\n\t#==============================================================================================\t\n\tdef get_title(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search('^(.+?) /', full_title).group(1)\n\t\t\treturn self.clean(found)\n\t\texcept AttributeError:\n\t\t\treturn full_title\n\t\n\t#==============================================================================================\t\n\tdef get_original_title(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search('^.+? / (.+)', full_title).group(1)\n\t\t\treturn self.clean(found)\n\t\texcept AttributeError:\n\t\t\treturn full_title\n\t\n\t#==============================================================================================\n\tdef parse_season_from_title(self, title):\n\t\ttry:\n\t\t\tfound = re.search(r\"(\\d) \\[\\d+\\D+\\d+\\]\", title)\n\t\t\tif found:\n\t\t\t\ttry:\n\t\t\t\t\tself._dict['season'] = int(found.group(1))\n\t\t\t\t\treturn\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\t\tparts = title.split(u'-')\n\t\t\tif len(parts) == 1:\n\t\t\t\tparts = title.split(u'TV-')\n\t\t\tif len(parts) > 1:\n\t\t\t\tfound = re.search('([0-9]+)', parts[1]).group(1)\n\t\t\t\tself._dict['season'] = int(found)\n\t\texcept:\n\t\t\tpass\n\t\n\t#==============================================================================================\n\tdef get_episodes_num(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search(' \\[([0-9]+) ', full_title).group(1)\n\t\t\treturn int(found)\n\t\texcept AttributeError:\n\t\t\treturn 1\n\n\tdef date_added_duration(self):\n\t\tul = self.soup.find('ul', class_='story_inf')\n\t\tif ul:\n\t\t\tfor li in ul.find_all('li'):\n\t\t\t\ttxt = li.get_text()\n\t\t\t\tparts = txt.split(':')\n\t\t\t\tif len(parts) > 1 and parts[0] == u'':\n\t\t\t\t\tdate, t = parts[1].split(',')\t# \t\td\tu' 30-09-2012'\tunicode\n\n\t\t\t\t\tfrom datetime import datetime, timedelta\n\n\t\t\t\t\tday = timedelta(1)\n\t\t\t\t\tyesterday = datetime.today() - day\n\n\t\t\t\t\t#date = ' 30-09-2012'\n\n\t\t\t\t\tif u'' in date:\n\t\t\t\t\t\td = datetime.today()\n\t\t\t\t\telif u'' in date:\n\t\t\t\t\t\td = yesterday\n\t\t\t\t\telse:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\td = datetime.strptime(date.strip(), '%d-%m-%Y')\n\t\t\t\t\t\texcept TypeError:\n\t\t\t\t\t\t\td = datetime.today()\n\n\t\t\t\t\tdt = datetime.today() - d\n\t\t\t\t\treturn dt\n\t\t\n\t#==============================================================================================\n\tdef parse(self):\n\t\ttag = u''\n\t\tself._dict['gold'] = False\n\t\tself._dict['season'] = 1\n\t\t\n\t\tfor title in self.soup.select('#news-title'):\n\t\t\tfull_title = title.get_text()\n\t\t\tdebug(full_title)\n\t\t\tself._dict['title'] = self.get_title(full_title)\n\t\t\tself._dict['originaltitle'] = self.get_original_title(full_title)\n\t\t\tself.parse_season_from_title(full_title)\n\t\t\tself._dict['episodes'] = self.get_episodes_num(full_title)\n\t\t\n\t\tfor b in self.soup.select('div.xfinfodata b'):\n\t\t\ttry:\n\t\t\t\ttext = b.get_text()\n\t\t\t\ttag = self.get_tag(text)\n\t\t\t\tif tag != '':\n\t\t\t\t\tspan = b.find_next_sibling('span')\n\t\t\t\t\tself._dict[tag] = span.get_text().strip()\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor div in self.soup.select('div.story_c'):\n\t\t\ttry:\n\t\t\t\ttext = div.get_text()\n\t\t\t\ttext = text.split(u':')[1]\n\t\t\t\ttext = text.split(u'')[0]\n\t\t\t\ttext = text.split(u'')[0]\n\t\t\t\ttext = text.strip()\n\t\t\t\tself._dict['plot'] = text\n\t\t\t\t#debug('---')\n\t\t\t\t#debug(text)\n\t\t\t\t#debug('---')\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor b in self.soup.select('div.story_h .rcol sup b'):\n\t\t\ttry:\n\t\t\t\ttext = b.get_text()\n\t\t\t\ttext = text.split(' ')[0]\n\t\t\t\tself._dict['rating'] = float(text) * 2\n\t\t\t\tdebug('rating: ' + str(self._dict['rating']))\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor img in self.soup.select('span.poster img'):\n\t\t\ttry:\n\t\t\t\tself._dict['thumbnail'] = img['src'].strip()\n\t\t\t\tdebug(self._dict['thumbnail'])\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfanart = []\n\t\tfor a in self.soup.select('ul.clr li a'):\n\t\t\ttry:\n\t\t\t\tdebug(a['href'])\n\t\t\t\tfanart.append(a['href'].strip())\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\tif len(fanart) != 0:\n\t\t\tself._dict['fanart'] = fanart\n\t\t# else:\n\t\t# \tdt = self.date_added_duration()\n\t\t# \tif dt and dt.days <= 14:\n\t\t# \t\treturn False\n\t\t\t\n\t\tfor img in self.soup.select('div.video_info a img'):\n\t\t\ttry:\n\t\t\t\tself._dict['studio'] = img['alt'].strip()\n\t\t\t\tdebug(self._dict['studio'])\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\ttags = []\n\t\tfor a in self.soup.select('a[href*=\"https://tr.anidub.com/tags/\"]'):\n\t\t\ttags.append(a.get_text().strip())\n\t\tif len(tags) > 0:\n\t\t\tself._dict['tag'] = tags\n\n\t\t\t\t\n\t\treturn True\n\n###################################################################################################\ndef write_tvshow_nfo(parser, tvshow_api, tvshow_path):\n\ttry:\n\t\tif write_tvshow_nfo.favorites:\n\t\t\tparser.Dict().get('tag', []).append('favorites')\n\texcept:\n\t\tpass\n\n\tNFOWriter(parser, tvshow_api=tvshow_api).write_tvshow_nfo(tvshow_path)\n\treturn\n\n###################################################################################################\ndef write_tvshow(content, path, settings):\n\twith filesystem.save_make_chdir_context(path, 'Anidub.write_tvshow'):\n\t\td = feedparser.parse(content)\n\n\t\tcnt = 0\n\t\tsettings.progress_dialog.update(0, 'anidub', path)\n\n\t\tfor item in d.entries:\n\t\t\twrite_tvshow_item(item, path, settings)\n\n\t\t\tcnt += 1\n\t\t\tsettings.progress_dialog.update(cnt * 100 / len(d.entries), 'anidub', path)\n\n\ndef write_tvshow_item(item, path, settings, path_out=[]):\n\tdebug('-------------------------------------------------------------------------')\n\tdebug(item.link)\n\tparser = DescriptionParser(item.link)\n\tif parser.parsed():\n\t\ttitle = parser.get_value('title')\n\t\tdebug(title)\n\t\toriginaltitle = parser.get_value('originaltitle')\n\t\tdebug(originaltitle)\n\t\tseason = parser.get_value('season')\n\n\t\tfrom downloader import TorrentDownloader\n\t\tTorrentDownloader(item.link, settings.torrents_path(), settings).download()\n\n\t\tdebug('Episodes: ' + str(parser.get_value('episodes')))\n\n\t\ttvshow_path = make_fullpath(title, '')\n\n\t\ttvshow_path = filesystem.join(path, tvshow_path)\n\t\tdebug(tvshow_path)\n\n\t\tpath_out.append(tvshow_path)\n\t\tsettings.update_paths.add(tvshow_path)\n\n\t\twith filesystem.save_make_chdir_context(tvshow_path, 'Anidub.write_tvshow_item'):\n\t\t\ttvshow_api = TVShowAPI.get_by(originaltitle, title)\n\t\t\twrite_tvshow_nfo(parser, tvshow_api, tvshow_path)\n\n\t\tseason_path = filesystem.join(tvshow_path, u'Season ' + unicode(season))\n\t\tdebug(season_path)\n\n\t\twith filesystem.save_make_chdir_context(season_path, 'Anidub.write_tvshow_item_2'):\n\n\t\t\tepisodes = tvshow_api.episodes(season)\n\n\t\t\tif len(episodes) < parser.get_value('episodes'):\n\t\t\t\tfor i in range(len(episodes) + 1, parser.get_value('episodes') + 1):\n\t\t\t\t\tepisodes.append({\n\t\t\t\t\t\t'title': title,\n\t\t\t\t\t\t'showtitle': title,\n\t\t\t\t\t\t'short': 's%02de%02d' % (season, i),\n\t\t\t\t\t\t'episode': i,\n\t\t\t\t\t\t'season': season\n\t\t\t\t\t})\n\n\t\t\tfor episode in episodes:\n\t\t\t\ttitle = episode['title']\n\t\t\t\tshortName = episode['short']\n\t\t\t\tepisodeNumber = episode['episode']\n\n\t\t\t\tif episodeNumber <= parser.get_value('episodes'):\n\t\t\t\t\tfilename = str(episodeNumber) + '. ' + 'episode_' + shortName\n\t\t\t\t\tdebug(filename)\n\n\t\t\t\t\tep = tvshow_api.Episode(season, episodeNumber)\n\t\t\t\t\tif ep:\n\t\t\t\t\t\tepisode = ep\n\n\t\t\t\t\tSTRMWriter(item.link).write(filename, season_path, episodeNumber=episodeNumber, settings=settings)\n\t\t\t\t\tNFOWriter(parser, tvshow_api=tvshow_api).write_episode(episode, filename, season_path)\n\n\telse:\n\t\tskipped(item)\n\tdel parser\n\n\ndef get_session(settings):\n\ts = requests.Session()\n\tdata = {\"login_name\": settings.anidub_login, \"login_password\": settings.anidub_password, \"login\": \"submit\"}\n\theaders = {\n\t\t'Host':\t\t\t\t\t\t\t'tr.anidub.com',\n\t\t'Origin':\t\t\t\t\t\t'https://tr.anidub.com',\n\t\t'Referer':\t\t\t\t\t\t'https://tr.anidub.com/',\n\t\t'Upgrade-Insecure-Requests':\t'1',\n\t\t'User-Agent':\t\t\t\t\t'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132'\n\t}\n\tlogin = s.post(\"https://tr.anidub.com/\", data=data, headers=headers)\n\tdebug('Login status: %d' % login.status_code)\n\tif 'login_name' in login.content:\n\t\tdebug('Login failed')\n\n\treturn s\n\t\ndef download_torrent(url, path, settings):\n\tfrom base import save_hashes\n\tsave_hashes(path)\n\n\turl = urllib2.unquote(url)\n\tdebug('download_torrent:' + url)\n\n\ts = get_session(settings)\n\n\tpage = s.get(url)\n\t#debug(page.text.encode('utf-8'))\n\tsoup = BeautifulSoup(page.text, 'html.parser')\n\n\ttry:\n\t    a = soup.select_one('#tv720 div.torrent_h a')\n\texcept TypeError:\n\t\ta = None\n\n\ttry:\n\t\tif a is None:\n\t\t\ta = soup.select_one('div.torrent_h > a')\n\texcept TypeError:\n\t\ta = None\n\n\tif a is not None:\n\t\thref = 'https://tr.anidub.com' + a['href']\n\t\tdebug(s.headers)\n\t\tr = s.get(href, headers={'Referer': url})\n\t\tdebug(r.headers)\n\t\t\n\t\tif 'Content-Type' in r.headers:\n\t\t\tif not 'torrent' in r.headers['Content-Type']:\n\t\t\t\treturn False\n\t\t\n\t\ttry:\n\t\t\twith filesystem.fopen(path, 'wb') as torr:\n\t\t\t\tfor chunk in r.iter_content(100000):\n\t\t\t\t\ttorr.write(chunk)\n\t\t\tsave_hashes(path)\n\t\t\treturn True\n\t\texcept: \n\t\t\tpass\n\n\treturn False\n\ndef write_pages(url, path, settings, params={}, filter_fn=None, dialog_title = None, path_out=[]):\n\ts = get_session(settings)\n\tif params:\n\t\tpage = s.post(url, data=params)\n\telse:\n\t\tpage = s.get(url)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tpage_no = 1\n\n\tcnt = 0\n\t\n\tclass Item:\n\t\tdef __init__(self, link, title):\n\t\t\tself.link = link\n\t\t\tself.title = title\n\t\n\twith filesystem.save_make_chdir_context(path, 'Anidub.write_pages'):\n\t\twhile True:\n\t\t\tif params:\n\t\t\t\tselector = soup.select('div.search_post > div.text > h2 > a')\n\t\t\telse:\n\t\t\t\tselector = soup.select('article.story > div.story_h > div.lcol > h2 > a')\n\n\t\t\tif not selector:\n\t\t\t\tbreak\n\t\n\t\t\tsettings.progress_dialog.update(0, dialog_title, path)\n\t\n\t\t\tfor a in selector:\n\t\t\t\tlog.debug(a['href'])\n\t\t\t\tlink = a['href']\n\t\t\t\ttitle = a.get_text()\n\t\t\t\tif filter_fn and filter_fn(title):\n\t\t\t\t\tcontinue\n\n\t\t\t\twrite_tvshow_item(Item(link, title), path, settings, path_out)\n\t\n\t\t\t\tcnt += 1\n\t\t\t\tsettings.progress_dialog.update(cnt * 100 / len(selector), dialog_title, path)\n\n\t\t\tif not 'favorites' in url:\n\t\t\t\tbreak\n\t\n\t\t\tpage_no += 1\n\t\t\tpage = s.get(url + 'page/%d/' % page_no)\n\t\n\t\t\tif page.status_code == requests.codes.ok:\n\t\t\t\tsoup = BeautifulSoup(page.text, 'html.parser')\n\t\t\telse:\n\t\t\t\tbreak\n\n\treturn cnt\n\n\ndef write_favorites(path, settings):\n\twrite_pages('https://tr.anidub.com/favorites/', path, settings, dialog_title=u' AniDUB')\n\n\ndef search_generate(what, settings, path_out):\n\tdef filter(title):\n\t\tif what not in title:\n\t\t\treturn True\n\n\t\treturn False\n\n\twrite_tvshow_nfo.favorites = False\n\treturn write_pages('https://tr.anidub.com/index.php?do=search', \n\t\t\t\tsettings.anime_tvshow_path(), settings, \n\t\t\t\t{'do': 'search',\n\t\t\t\t'subaction': 'search',\n\t\t\t\t'story': what.encode('utf-8')}, filter,\n\t\t\t\tdialog_title=u' AniDUB',\n\t\t\t\tpath_out=path_out)\n\t\n\n###################################################################################################\ndef run(settings):\n\tif settings.anime_save:\n\t\tif settings.anidub_rss:\n\t\t\twrite_tvshow_nfo.favorites = False\n\t\t\twrite_tvshow(settings.anidub_url, settings.anime_tvshow_path(), settings)\n\t\tif settings.anidub_favorite:\n\t\t\twrite_tvshow_nfo.favorites = True\n\t\t\twrite_favorites(settings.anime_tvshow_path(), settings)\n\n\nif __name__ == '__main__':\n\tsettings = Settings('../media_library')\n\trun(settings)", "instruction_en": "Import the necessary modules and classes from the required files. Define a class named DescriptionParser that inherits from the DescriptionParserBase class. Define a get_content method that takes a URL as input and returns the page content. Define an __init__ method that initializes the class and sets the content of the page. Define a get_tag method that returns the tag for a given string. Define a clean method that removes unwanted characters from a string. Define a get_title method that returns the title of a given string. Define a get_original_title method that returns the original title of a given string. Define a parse_season_from_title method that extracts the season number from a given string. Define a get_episodes_num method that returns the number of episodes in a given string. Define a date_added_duration method that returns the duration since the date was added. Define a parse method that parses the page content and extracts the required information. Define a write_tvshow_nfo function that writes the TV show NFO file. Define a write_tvshow function that writes the TV show file. Define a write_tvshow_item function that writes the TV show item. Define a get_session function that returns a session. Define a download_torrent function that downloads a torrent file. Define a write_pages function that writes the pages. Define a filter function that filters the titles. Define a search_generate function that generates the search. Define a run function that runs the program.", "original_line_number": 32275, "instruction_length": 1553}
{"instruction": "Kod Python ini adalah program yang menjejaki masa yang dibelanjakan untuk aktiviti menggunakan aplikasi Hamster. Untuk menggunakan program ini, jalankan skrip Python dan aplikasi hamster. Program ini akan membuat item dok yang memaparkan masa yang dibelanjakan untuk aktiviti semasa dan membolehkan anda mengakses gambaran dan keutamaan aplikasi Hamster. Program ini secara automatik akan mengemas kini masa yang dihabiskan setiap minit. Untuk menghentikan program ini, keluar dari aplikasi Hamster dan tutup skrip Python.", "input": "", "output": "#!/usr/bin/env python\n\n#\n#  Copyright (C) 2010 Toms Baugis\n#\n#  Original code from Banshee control,\n#  Copyright (C) 2009-2010 Jason Smith, Rico Tzschichholz\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n\nimport atexit\nimport gobject\nimport sys, os\nfrom subprocess import Popen\n\n\ntry:\n\timport gtk\n\tfrom dockmanager.dockmanager import DockManagerItem, DockManagerSink, DOCKITEM_IFACE\n\tfrom signal import signal, SIGTERM\n\tfrom sys import exit\nexcept ImportError, e:\n\tprint e\n\texit()\n\n\nfrom hamster import client\nfrom hamster.utils import stuff, i18n\ni18n.setup_i18n()\n\n\nclass HamsterItem(DockManagerItem):\n    def __init__(self, sink, path):\n        DockManagerItem.__init__(self, sink, path)\n\n        self.storage = client.Storage()\n        self.storage.connect(\"facts-changed\", lambda storage: self.refresh_hamster())\n        self.storage.connect(\"activities-changed\", lambda storage: self.refresh_hamster())\n\n        self.id_map = {} #menu items\n\n        self.update_text()\n        self.add_actions()\n        gobject.timeout_add_seconds(60, self.refresh_hamster)\n\n\n    def refresh_hamster(self):\n        try:\n            self.update_text()\n        finally:  # we want to go on no matter what, so in case of any error we find out about it sooner\n            return True\n\n\n    def update_text(self):\n        today = self.storage.get_todays_facts()\n\n        if today and today[-1].end_time is None:\n            fact = today[-1]\n\n            self.set_tooltip(\"%s - %s\" % (fact.activity, fact.category))\n            self.set_badge(stuff.format_duration(fact.delta, human=False))\n        else:\n            self.set_tooltip(_(\"No activity\"))\n            self.reset_badge()\n\n    def menu_pressed(self, menu_id):\n        if self.id_map[menu_id] == _(\"Overview\"):\n            Popen([\"hamster-time-tracker\", \"overview\"])\n        elif self.id_map[menu_id] == _(\"Preferences\"):\n            Popen([\"hamster-time-tracker\", \"preferences\"])\n\n        self.add_actions() # TODO - figure out why is it that we have to regen all menu items after each click\n\n\n    def add_actions(self):\n        # first clear the menu\n        for k in self.id_map.keys():\n            self.remove_menu_item(k)\n\n        self.id_map = {}\n        # now add buttons\n        self.add_menu_item(_(\"Overview\"), \"\")\n        self.add_menu_item(_(\"Preferences\"), \"preferences-desktop-personal\")\n\n\nclass HamsterSink(DockManagerSink):\n    def item_path_found(self, pathtoitem, item):\n        if item.Get(DOCKITEM_IFACE, \"DesktopFile\", dbus_interface=\"org.freedesktop.DBus.Properties\").endswith (\"hamster-time-tracker.desktop\"):\n            self.items[pathtoitem] = HamsterItem(self, pathtoitem)\n\nhamstersink = HamsterSink()\n\ndef cleanup():\n\thamstersink.dispose()\n\nif __name__ == \"__main__\":\n\tmainloop = gobject.MainLoop(is_running=True)\n\n\tatexit.register (cleanup)\n\tsignal(SIGTERM, lambda signum, stack_frame: exit(1))\n\n\twhile mainloop.is_running():\n\t\tmainloop.run()", "instruction_en": "This Python code is a program that tracks time spent on activities using the Hamster app. To use this program, run the Python script and the Hamster app. The program will create a dock item that displays the time spent on the current activity and allows you to access the Hamster app's overview and preferences. The program will automatically update the time spent every minute. To stop the program, exit the Hamster app and close the Python script.", "original_line_number": 32629, "instruction_length": 522}
{"instruction": "Kod ini melakukan penilaian letupan. Untuk menggunakannya, ikuti langkah -langkah ini:\n\n1. Import perpustakaan yang diperlukan: glob, sys, csv, dan tabulasi dari bio.blast.applications.\n2. Tentukan fungsi berikut:\n   - CARGA_CSV (file_name): Mewujudkan senarai senarai dengan fail CSV.\n   - Crea_CompParacion (tabla_ref, estructura = 'star', comparacion = 'bi'): Membuat senarai perbandingan (kod) bergantung kepada argumen.\n   - Imprime_CompParacion (Listas): Mencetak perbandingan sebagai format yang boleh dibaca.\n   - Imprime_Referencia (Claves): Mencetak perbandingan sebagai format yang boleh dibaca.\n   - CREA_DICCIONARIO (Tabla): Mencipta Kamus Kod: Organisme.\n3. Dalam badan program, pastikan pengguna masukkan nama fail CSV dan muatkan data menggunakan carga_csv ().\n4. Tunjukkan data CSV menggunakan tabulate ().\n5. Tunjukkan pilihan: parameter letupan, parameter perbandingan, lari.\n6. Jika pengguna memilih pilihan 1, pastikan mereka memasukkan perbandingan menggunakan indeks organisma dan arahan (bidirectional atau unidirectional). Gunakan crea_comparacion () untuk membuat senarai perbandingan.\n7. Jika pengguna memilih pilihan 2, pastikan mereka masukkan nilai E yang dikehendaki untuk larian letupan. Gunakan ncbiblastpcommandline () untuk membuat arahan untuk perbandingan.\n8. Jalankan arahan dan maklumkan kepada pengguna apabila kerja selesai.", "input": "", "output": "#! /usr/bin/env python\n\n\"\"\" DESCRIPTION\n\n\"\"\"\n\nimport glob, sys, csv\nfrom tabulate import tabulate\nfrom Bio.Blast.Applications import NcbiblastpCommandline\n\n\"\"\"--- FUNCTIONS ---\"\"\"\n\ndef carga_csv(file_name):\n    \"\"\" creates a list of lists with a csv file \"\"\"\n    tabla = []    \n    cr = csv.reader(open(file_name,\"rb\"))\n    for row in cr:    \n        tabla.append(row)\n    return tabla      \n\ndef crea_comparacion(tabla_ref, estructura = 'star', comparacion = 'bi'):\n    \"\"\" creates comparisons lists (code) depending on arguments \"\"\"\n    lista = []\n    tabla = list(tabla_ref)\n    if estructura == 'star':\n        nodo = tabla.pop()\n        for organismo in tabla:\n            lista.append([nodo[1],organismo[1]])\n            if comparacion == 'bi':\n                lista.append([organismo[1], nodo[1]])\n    else:\n        comps = estructura.split(',')\n        for comp in comps:\n            pareja = comp.split('-')\n            query = tabla[int(pareja[0])][1]\n            db = tabla[int(pareja[1])][1]\n            lista.append([query, db])\n            if comparacion == 'bi':\n                lista.append([db, query])\n    return lista\n\ndef imprime_comparacion(listas):\n    \"\"\" prints the comparison as a readable format\"\"\"\n    print 'COMPARISONS\\n-----------\\n'\n    for lista in listas:\n        print lista[0] + ' --> ' + lista[1]\n    print '\\n'\n\ndef imprime_referencia(claves):\n    \"\"\" prints the comparison as a readable format\"\"\"\n    print 'REFERENCE\\n---------'\n    n = 0\n    for key, val in claves.items():\n        print n, '. ', key, '\\t', val\n        n=n+1\n    print '\\n'\n\ndef crea_diccionario(tabla):\n    \"\"\" creates a dictionary of code:organism\"\"\"\n    diccionario={}\n    for row in tabla:\n        diccionario[row[1]]=row[0]\n    return diccionario\n    \n\"\"\"--- PROGRAM BODY ---\"\"\"\nprint '----------------\\nBLAST EVALUATION\\n----------------'\nblast_eval = 1e-05\ncomparison_list = []\n# charge csv file\nnombre_csv = raw_input('Please enter the CSV file name: ')\norganismos = carga_csv(nombre_csv)\nreferencia = crea_diccionario(organismos)\ncomparison_list = crea_comparacion(organismos)\n# present csv data\nprint '\\nCSV data\\n--------'\nprint tabulate(organismos, headers=[\"Organism\",\"Code\", \"Genome File\", \"Database folder\"]) + '\\n'\n# present options: blast parameters, comparison parameters, run\nwhile 1:\n    \n    imprime_referencia(referencia)\n    imprime_comparacion(comparison_list)\n    print 'CHOOSE AN OPTION\\n----------------\\n1) Comparisons\\n2) Run\\n3) Quit'\n    user_in = raw_input('Option: ')\n    if user_in == '1':\n        imprime_referencia(referencia)\n        print ('Please enter the comparisons using the organism index.\\n' +\n              'Format: \"-\" between indices; \",\" between comparisons; no spaces.\\n')\n        nueva_comparacion = raw_input('Comparisons: ')\n        print 'Choose \"bi\" for bidirectional or \"uni\" for unidirectional; no quotation marks.'\n        tipo_comparacion = raw_input('Direction: ')\n        comparison_list = crea_comparacion(organismos, nueva_comparacion, tipo_comparacion)\n    elif user_in == '2':\n        blast_eval = raw_input('\\nPlease write the desired E value for BLAST runs; 1e-5 suggested.\\nE_value: ')\n        print '\\nBLAST+ commands to be runned...\\n'\n        break\n    elif user_in == '3': quit()\n    else: print ('Incorrect option, try again.\\n')\n\n# create commands for comparisons\ncomandos = []\nfor pair in comparison_list:\n    nombre = referencia[pair[0]].split()\n    comandos.append([(nombre[0]+'_'+nombre[1]+'.faa'), ('db_'+pair[1]+'/db_'+pair[1]), (pair[0]+'_'+pair[1]+'.xml')])\nprint tabulate(comandos, headers=[\"Genome file\",\"Database\", \"Product file\"]) + '\\n'\nraw_input('Press ENTER to continue')\n# run commands, inform data created\nfor comando in comandos:\n    blastp_cline = NcbiblastpCommandline(query=comando[0], db=comando[1], evalue=blast_eval ,outfmt=5, out=comando[2])\n    print 'File ' + comando[2] + ' is currently in progess...'\n    stdout, stderr = blastp_cline()\nprint 'WORK COMPLETED\\n--------------'", "instruction_en": "This code performs BLAST evaluation. To use it, follow these steps:\n\n1. Import the necessary libraries: glob, sys, csv, and tabulate from Bio.Blast.Applications.\n2. Define the following functions:\n   - carga_csv(file_name): creates a list of lists with a csv file.\n   - crea_comparacion(tabla_ref, estructura = 'star', comparacion = 'bi'): creates comparisons lists (code) depending on arguments.\n   - imprime_comparacion(listas): prints the comparison as a readable format.\n   - imprime_referencia(claves): prints the comparison as a readable format.\n   - crea_diccionario(tabla): creates a dictionary of code:organism.\n3. In the program body, prompt the user to enter the CSV file name and load the data using carga_csv().\n4. Present the CSV data using tabulate().\n5. Present the options: blast parameters, comparison parameters, run.\n6. If the user selects option 1, prompt them to enter the comparisons using the organism index and the direction (bidirectional or unidirectional). Use crea_comparacion() to create the comparison list.\n7. If the user selects option 2, prompt them to enter the desired E value for BLAST runs. Use NcbiblastpCommandline() to create the commands for comparisons.\n8. Run the commands and inform the user when the work is completed.", "original_line_number": 32812, "instruction_length": 1365}
{"instruction": "Buat aplikasi Flask dengan pangkalan data dan fungsi e -mel. Tentukan empat model: waterhistory, pengguna, aplikasi, dan act_code. Model pengguna mempunyai hubungan satu-ke-banyak dengan Waterhistory, Application, dan ACT_CODE. Model pengguna mempunyai atribut untuk nama, e -mel, telefon, is_aktif, peranan, flat_number, kata laluan, e -mel_verified, dan Phone_verified. Model Waterhistory mempunyai atribut untuk ID, hw_counter, cw_counter, user_id, pengguna, dan bulan. Model aplikasi mempunyai atribut untuk id, user_id, pengguna, pautan, dibuat, dan is_aktif. Model act_code mempunyai atribut untuk id, user_id, pengguna, kod, dan dibuat. Model penyedia_oAuth mempunyai atribut untuk id, nama, pengguna_key, consumer_secret, request_token_params, base_url, request_token_url, access_token_method, access_token_url, dan authorize_url.", "input": "", "output": "# -*- coding: utf-8 -*-\r\nfrom app import db, mail\r\nfrom sqlalchemy.orm import backref, relationship\r\nfrom datetime import datetime\r\nfrom sqlalchemy_utils import PasswordType\r\nfrom sqlalchemy.sql.schema import ForeignKey\r\nfrom flask_mail import Message\r\nfrom config import HOSTNAME\r\n\r\nclass Waterhistory(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    hw_counter = db.Column(db.Integer)\r\n    cw_counter = db.Column(db.Integer)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref=backref('history', lazy='dynamic'))\r\n    month = db.Column(db.Integer)\r\n    \r\n    def __init__(self, hw_counter, cw_counter, user):\r\n        self.hw_counter = hw_counter\r\n        self.cw_counter = cw_counter\r\n        self.user = user\r\n        self.month = datetime.utcnow().month\r\n    \r\nclass User(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    name = db.Column(db.String(80))\r\n    email = db.Column(db.String(120), unique=True)\r\n    phone = db.Column(db.String(12), unique=True)\r\n    is_active = db.Column(db.Boolean, default = True)\r\n    role = db.Column(db.String(20), default = 'user')\r\n    flat_number = db.Column(db.Integer)\r\n    password = db.Column(PasswordType(schemes=['pbkdf2_sha512', 'md5_crypt'], deprecated=['md5_crypt']))\r\n    email_verified = db.Column(db.Boolean, default = False)\r\n    phone_verified = db.Column(db.Boolean, default = False)   \r\n    \r\n    def __init__(self, name, email, phone, flat_number, password):\r\n        self.name = name\r\n        self.email = email\r\n        self.phone = phone\r\n        self.flat_number = flat_number\r\n        self.password = password\r\n        \r\n    def is_authenticated(self):\r\n        return True\r\n \r\n    def is_anonymous(self):\r\n        return False\r\n \r\n    def get_id(self):\r\n        return unicode(self.id)\r\n    \r\n    def _gen_act_code(self):\r\n        import binascii, os\r\n        act_code = binascii.hexlify(os.urandom(48))\r\n        raw = Act_code(self, act_code)\r\n        db.session.add(raw)\r\n        db.session.commit()\r\n        return act_code\r\n    \r\n    def verify_email(self, act_code = None):\r\n        if act_code is None:\r\n            act_code = self._gen_act_code()\r\n            link = 'http://' + HOSTNAME + '/activate/' + act_code\r\n            msg = Message('Hello!', recipients=[self.email], sender=('WC Project', 'wc@wc.local'))\r\n            msg.html = '<a href=\"' + link + '\">Link</a>'\r\n            mail.send(msg)\r\n            return True\r\n        \r\n        saved_codes = Act_code.query.filter_by(user = self)\r\n        \r\n        for saved_code in saved_codes:\r\n            if saved_code.code == act_code and (datetime.now() - saved_code.created).seconds <= 43200:\r\n                self.email_verified = True\r\n                db.session.commit()\r\n                return True\r\n        \r\n        def verify_phone(self, act_code):\r\n            pass\r\n        \r\n        return False\r\n        \r\n    def __repr__(self):\r\n        return 'User %r' % self.name\r\n    \r\nclass Application(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref = backref('apps', lazy = 'dynamic'))\r\n    link = db.Column(db.String(100))\r\n    created = db.Column(db.DateTime)\r\n    is_active = db.Column(db.Boolean())\r\n    \r\n    def __init__(self, user, link, created = None, is_active = True):\r\n        self.user = user\r\n        self.link = link\r\n        if created is None:\r\n            created = datetime.utcnow()\r\n        self.created = created\r\n        self.is_active = is_active\r\n        \r\nclass Act_code(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref = backref('codes', lazy = 'dynamic'))\r\n    code = db.Column(db.String(50))\r\n    created = db.Column(db.DateTime)\r\n    \r\n    def __init__(self, user, code):\r\n        self.user = user\r\n        self.code = code\r\n        self.created = datetime.now()\r\n    \r\nclass Providers_oAuth(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    name = db.Column(db.String(20))\r\n    consumer_key = db.Column(db.String(120))\r\n    consumer_secret = db.Column(db.String(120))\r\n    request_token_params = db.Column(db.String(120))\r\n    base_url = db.Column(db.String(120))\r\n    request_token_url = db.Column(db.String(120))\r\n    access_token_method = db.Column(db.String(10))\r\n    access_token_url = db.Column(db.String(120))\r\n    authorize_url = db.Column(db.String(120))", "instruction_en": "Create a Flask application with a database and email functionality. Define four models: Waterhistory, User, Application, and Act_code. The User model has a one-to-many relationship with Waterhistory, Application, and Act_code. The User model has attributes for name, email, phone, is_active, role, flat_number, password, email_verified, and phone_verified. The Waterhistory model has attributes for id, hw_counter, cw_counter, user_id, user, and month. The Application model has attributes for id, user_id, user, link, created, and is_active. The Act_code model has attributes for id, user_id, user, code, and created. The Providers_oAuth model has attributes for id, name, consumer_key, consumer_secret, request_token_params, base_url, request_token_url, access_token_method, access_token_url, and authorize_url.", "original_line_number": 33243, "instruction_length": 838}
{"instruction": "Kod yang diberikan adalah skrip python yang membersihkan imej lama dari pendaftaran kontena Google. Untuk menggunakannya, ikuti langkah -langkah ini:\n\n1. Pasang python dan kebergantungan yang diperlukan (aiogoogle, aiohttp, dan hailtop.aiogoogle).\n2. Simpan kod ke fail bernama \"Cleanup_GCR.PY\".\n3. Buka terminal atau command prompt dan navigasi ke direktori yang mengandungi fail.\n4. Jalankan skrip dengan perintah \"Python Cleanup_GCR.PY [Project]\", di mana [Projek] adalah nama projek Google Cloud anda.\n\nSkrip akan menyambung ke pendaftaran kontena anda, menyenaraikan semua imej, dan memadam sebarang imej yang berusia lebih dari 7 hari atau tidak mempunyai tag yang berkaitan. Ia akan menyimpan 10 imej terbaru untuk setiap repositori. Skrip akan dijalankan secara tidak segerak, menggunakan sehingga 8 benang selari untuk mempercepatkan proses pembersihan. Kemajuan pembersihan akan dilog masuk ke konsol.", "input": "", "output": "import sys\nimport time\nimport logging\nimport asyncio\nimport aiohttp\nimport hailtop.aiogoogle as aiogoogle\n\nlog = logging.getLogger(__name__)\n\n\nclass AsyncIOExecutor:\n    def __init__(self, parallelism):\n        self._semaphore = asyncio.Semaphore(parallelism)\n\n    async def _run(self, fut, aw):\n        async with self._semaphore:\n            try:\n                fut.set_result(await aw)\n            except asyncio.CancelledError:  # pylint: disable=try-except-raise\n                raise\n            except Exception as e:  # pylint: disable=broad-except\n                fut.set_exception(e)\n\n    def submit(self, aw):\n        fut = asyncio.Future()\n        asyncio.ensure_future(self._run(fut, aw))\n        return fut\n\n    async def gather(self, aws):\n        futs = [self.submit(aw) for aw in aws]\n        return [await fut for fut in futs]\n\n\nclass CleanupImages:\n    def __init__(self, client):\n        self._executor = AsyncIOExecutor(8)\n        self._client = client\n\n    async def cleanup_digest(self, image, digest, tags):\n        log.info(f'cleaning up digest {image}@{digest}')\n\n        await self._executor.gather([\n            self._client.delete(f'/{image}/manifests/{tag}')\n            for tag in tags])\n\n        await self._executor.submit(self._client.delete(f'/{image}/manifests/{digest}'))\n\n        log.info(f'cleaned up digest  {image}@{digest}')\n\n    async def cleanup_image(self, image):\n        log.info(f'cleaning up image {image}')\n\n        log.info(f'listing tags for {image}')\n\n        result = await self._executor.submit(self._client.get(f'/{image}/tags/list'))\n        manifests = result['manifest']\n        manifests = [(digest, int(data['timeUploadedMs']) / 1000, data['tag']) for digest, data in manifests.items()]\n\n        log.info(f'got {len(manifests)} manifests for {image}')\n\n        # sort is ascending, oldest first\n        manifests = sorted(manifests, key=lambda x: x[1])\n\n        # keep the most recent 10\n        manifests = manifests[:-10]\n\n        now = time.time()\n        await asyncio.gather(*[\n            self.cleanup_digest(image, digest, tags)\n            for digest, time_uploaded, tags in manifests\n            if (now - time_uploaded) >= (7 * 24 * 60 * 60) or len(tags) == 0])\n\n        log.info(f'cleaned up image  {image}')\n\n    async def run(self):\n        images = await self._executor.submit(self._client.get('/tags/list'))\n        await asyncio.gather(*[\n            self.cleanup_image(image)\n            for image in images['child']\n        ])\n\n\nasync def main():\n    logging.basicConfig(level=logging.INFO)\n\n    if len(sys.argv) != 2:\n        raise ValueError('usage: cleanup_gcr <project>')\n    project = sys.argv[1]\n\n    async with aiogoogle.ContainerClient(\n            project=project,\n            timeout=aiohttp.ClientTimeout(total=60)) as client:\n        cleanup_images = CleanupImages(client)\n        await cleanup_images.run()\n\n\nasyncio.run(main())", "instruction_en": "The given code is a Python script that cleans up old images from a Google Container Registry. To use it, follow these steps:\n\n1. Install Python and the required dependencies (aiogoogle, aiohttp, and hailtop.aiogoogle).\n2. Save the code to a file named \"cleanup_gcr.py\".\n3. Open a terminal or command prompt and navigate to the directory containing the file.\n4. Run the script with the command \"python cleanup_gcr.py [project]\", where [project] is the name of your Google Cloud project.\n\nThe script will connect to your Container Registry, list all images, and delete any images that are more than 7 days old or have no associated tags. It will keep the 10 most recent images for each repository. The script will run asynchronously, using up to 8 parallel threads to speed up the cleanup process. The progress of the cleanup will be logged to the console.", "original_line_number": 33274, "instruction_length": 911}
{"instruction": "Kod yang diberikan adalah perpustakaan shoutbox yang diubahsuai yang membolehkan pengubahsuaian selanjutnya untuk shoutbox. Untuk menghasilkan fail HTML, jalankan kod tanpa sebarang argumen. Untuk memasukkan baris baru dari baris arahan, jalankan \"psogen.py input tanpa nama 'teks'\". Kod ini mengandungi fungsi untuk menghasilkan dan menulis data HTML ke fail HTML statik, membaca data dari fail, simpan kandungan shoutbox baru, dan sediakan garis kandungan. Untuk menguji atau menghasilkan fail HTML statik, jalankan kod dengan pembolehubah \"__Name__\" ditetapkan ke \"__Main__\".", "input": "", "output": "#!/usr/bin/python\n\n# Modificated ShoutBox Library\n#   enables further modifications for the ShoutBox\n#   Run without to generate htmlfile\n#   Run the following to enter a new line from command line\n#     psogen.py input Anonymous default \"Text\"\n\nimport os, re\n\nimport messages, broadcast\n\ndatafilename = os.environ[\"SHOUTBOX_CHATFILE\"]\nhtmlfilename = os.environ[\"SHOUTBOX_GEN_HTMLFILE\"]\n\nstyle=(\"name { font-weight: bold; font-family:Tahoma } \"\n       \"data { font-family: Tahoma } \"\n       \"data.def { color: #000000 } \"\n       \"data.blue { color: #0000FF } \"\n       \"data.green { color: #008000 } \"\n       \"data.orange { color: #FF8040 } \"\n       \"data.red { color: #FF0000 }\")\n\ntry:\n     raw_dest =  os.environ[\"SHOUTBOX_BROADCAST_DESTINATIONS\"]\n     finished_dest = re.sub ( '#' , '\"' , raw_dest )\n     broadcast_destination = eval ( finished_dest )\nexcept KeyError:\n     broadcast_destination = False\n\n\n#--------------\n#  Generates Shoutbox-HTML-Frame  ...\n#           Imports:\n#               content    -   String  containing preformatted data\n#--------------\ndef generate_html(content):\n    htmlstring =   \"<html><head><meta http-equiv='cache-control' content='no-cache'><meta name='GENERATOR' content='PyShoutOut'><title>Shout-Out Data</title>\"\n    htmlstring +=  \"<style type='text/css'>\" + style  + \"</style></head><body>\"\n    htmlstring +=  content\n    htmlstring +=  \"</body></html>\"\n    return htmlstring\n\n#--------------\n#   Generates HTML Data based on given content  and write it to static html file\n#          Imports:\n#               content    -   String  containing preformatted data\n#--------------\ndef generate_html_into_file(content):\n    htmlstring = generate_html ( content )\n\n    htmlfile = open( htmlfilename , 'w' )\n    htmlfile.write( htmlstring )\n    htmlfile.close()\n\n#--------------\n# Generates HTML Data based on datafilename 's content\n#--------------\ndef generate_html_from_file():\n    old =  read_data_file()\n    generate_html_into_file( old   )\n\n#--------------\n# Generates and Displays generated HTML\n#--------------\ndef generate_html_to_display_from_file():\n    old =  read_data_file()\n    htmlstring = generate_html ( old )\n    print htmlstring\n\n#--------------\n#  Reads Data file from datafilename given name\n#--------------\ndef read_data_file():\n    datafile = open(datafilename, 'r')\n    old = datafile.read()\n    datafile.close()\n    return old\n\n#--------------\n# Function for saving new Shoubox-Content & Regenerate static HTML file -- usually called by HTML-Form\n#--------------\ndef process_form( name , indata , color ):\n    content = save_input(  name , indata , color )\n\n    if broadcast_destination == False:\n          generate_html_into_file ( content )\n\n\n#--------------\n# Acutally Saves SB-Content to datafile\n#--------------\ndef save_input( name , indata , color ):\n\n    content = prepare_line ( name, indata, color  )\n\n    if broadcast_destination != False:\n        return writeToNetwork( content , broadcast_destination )\n    else:\n        return writeToDisk ( content )\n\ndef writeToNetwork ( content , broadcast_destination ):\n        message = messages.shoutbox_message()\n\tmessage.set(content)\n        casting = broadcast.broadcast( )\n\tcasting.setDestination(broadcast_destination)\n\tcasting.set( message.get_message() )\n\tcasting.send()\n\treturn None\n\ndef writeToDisk ( content ):\n        old = read_data_file()\n        finalcontent = content  + old\n        datafile = open(datafilename, 'r+')\n        datafile.write(finalcontent)\n        datafile.close()\n\treturn finalcontent\n\n\ndef prepare_line ( name, indata, color  ):\n    datapass = re.sub(\"<\", \"&lt;\", indata)\n    data = re.sub(\">\", \"&gt;\", datapass)\n    content = \"<name>\" + name + \":</name>&nbsp;&nbsp;&nbsp;<data class='\" + color + \"'>\" + data + \"</data><br>\\n\"\n    return content\n\n#--------------\n#  Testing or Generating static HTML File\n#--------------\nif __name__ == \"__main__\":\n  import sys\n  if sys.argv.count(\"input\") >= 1 :\n     save_input(  sys.argv[2] ,  sys.argv[3] ,  sys.argv[4] )\n     generate_html_to_display_from_file()\n     print \"Entered Text.\"\n\n  generate_html_from_file ()\n  print \"Generated HTML-Shoutbox File.\"", "instruction_en": "The given code is a modified ShoutBox library that enables further modifications for the ShoutBox. To generate an HTML file, run the code without any arguments. To enter a new line from the command line, run \"psogen.py input Anonymous default 'Text'\". The code contains functions to generate and write HTML data to a static HTML file, read data from a file, save new ShoutBox content, and prepare a line of content. To test or generate a static HTML file, run the code with the \"__name__\" variable set to \"__main__\".", "original_line_number": 33557, "instruction_length": 578}
{"instruction": "Kod ini menyediakan modul Parser CLI yang bertanggungjawab untuk parsing model arahan yang ditakrifkan dalam clicommands.yaml dan menyediakan fungsi untuk pengesahan input pengguna, menggunakan pemegang pelaksanaan untuk perintah CLI atau ekspansi makro, dan menentukan kemungkinan ARG Match untuk perintah auto-siap berdasarkan konteks. Untuk menggunakan modul, instantiate kelas CLIUTIL dan panggil kaedahnya, seperti get_match () untuk mendapatkan senarai kemungkinan perlawanan untuk konteks arahan yang diberikan, dan validate_command_and_execute () untuk mengesahkan dan melaksanakan konteks arahan penuh. Modul ini juga termasuk kaedah untuk mencetak graf dan hasil arahan.", "input": "", "output": "#------------------------------------------------------------------------------\n#                            cli_parser.py\n#------------------------------------------------------------------------------\n'''\n@author : rgiyer\nDate    : October 20th, 2014\n\nThis module is responsible for parsing command model defined in\ncliCommands.yaml and providing functions for:\n       - Validation of user-input\n       - invoking execution handle for CLI commands or macro expansions\n       - determine possible arg match for command auto-completion based \n         on context\n'''\n\n# Standard Python libraries\nimport os\nimport re\nimport inspect\nimport subprocess\n\n# Packages required for openclos\nimport yaml\n\n# openclos classes\nimport util\n\n# cli related classes\nfrom cli_handle_impl import CLIImplementor\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\nclass CLICommand:\n\n    def __init__ ( self, cmd_access, cmd_handle, cmd_macro, cmd_desc ):\n        self.cmd_access = cmd_access\n        self.cmd_handle = cmd_handle\n        self.cmd_macro  = cmd_macro\n        self.cmd_desc   = cmd_desc\n\n# end class CLICommand\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\nclass CLIUtil:\n\n    def __init__ ( self ):\n        commandConfFile = os.path.join ( util.configLocation, \n                                         'cliCommands.yaml' )\n        self.yaml_file_stream = open ( commandConfFile, 'r' )\n        raw_graph = yaml.load ( self.yaml_file_stream )\n        self.cmd_graph = {}\n        self.indentation = 8\n        self.dump_cmd ( raw_graph )\n        self.yaml_file_stream.close ()\n        \n\n#------------------------------------------------------------------------------\n    def get_implementor_handle ( self, class_instance, handle_name ):\n        handles = inspect.getmembers ( class_instance, \n                                       predicate = inspect.ismethod )\n        for function_tuple in handles:\n            if ( handle_name == function_tuple [ 0 ] ):\n                return function_tuple [ 1 ]\n\n        # no match found\n        return 0\n\n#------------------------------------------------------------------------------\n    # Parse through the dictionary iteratively:\n    def dump_cmd ( self,\n                   cmds,\n                   cmd_root=\"\", \n                   cmd_access=\"READ\",\n                   cmd_handle=\"\",\n                   cmd_macro=\"\",\n                   cmd_desc=\"\" ):\n\n        for cmd in cmds:\n            if ( cmd_root == \"\" ):\n                cmd_compound = cmd\n            else:\n                cmd_compound = cmd_root + \"_\" + cmd\n\n            cmd_data = cmds [ cmd ]\n\n            # Get command access\n            if cmd_data.has_key ( \"Access\" ):\n                cmd_access = cmd_data [ \"Access\" ]\n\n            # Get command handler\n            if cmd_data.has_key ( \"Handle\" ):\n                cmd_handle = cmd_data [ \"Handle\" ]\n            elif ( cmd_handle != \"\" ):\n                cmd_handle = \"\"\n\n            # Get command macro\n            if cmd_data.has_key ( \"Macro\" ):\n                cmd_macro = cmd_data [ \"Macro\" ]\n            elif ( cmd_macro != \"\" ):\n                cmd_macro = \"\"\n                \n\n            # Get command description\n            if cmd_data.has_key ( \"Desc\" ):\n                cmd_desc = cmd_data [ \"Desc\" ]\n            elif ( cmd_desc != \"\" ):\n                cmd_desc = \"\"\n\n            # Parse the arguments\n            if cmd_data.has_key ( \"Args\" ):\n                cmd_args = cmd_data [ \"Args\" ]\n                self.dump_cmd ( cmd_args, \n                                cmd_compound, \n                                cmd_access,\n                                cmd_handle,\n                                cmd_macro,\n                                cmd_desc )\n\n            if cmd_data.has_key ( \"Handle\" ):\n                self.cmd_graph [ cmd_compound ] = CLICommand ( cmd_access, \n                                                               cmd_handle,\n                                                               cmd_macro,\n                                                               cmd_desc )\n                if ( len ( cmd_compound ) > self.indentation ):\n                    self.indentation = len ( cmd_compound )\n\n#------------------------------------------------------------------------------\n    def normalize_command ( self, cmd ):\n        return cmd.replace ( \" \", \"_\" )\n\n#------------------------------------------------------------------------------\n    def get_indentation ( self, cmd ):\n        return ( self.indentation + 8 - len ( cmd ) )\n\n#------------------------------------------------------------------------------\n    def suffix_macro_to_cmd ( self, macro_list, cmd ):\n        ret_cmd = []\n        for macro in macro_list:\n            ret_cmd.append ( self.normalize_command ( cmd + \"_\" + macro ) )\n        return ret_cmd\n\n#------------------------------------------------------------------------------\n    def get_macro_list ( self, class_instance, macro_txt, add_help=None ):\n        fn_macro = self.get_implementor_handle ( class_instance, macro_txt )\n        return fn_macro ( add_help )\n\n#------------------------------------------------------------------------------\n    def include_macro ( self, macro_list, ret_list ):\n        for item in macro_list:\n            ret_list.append ( item )\n\n#------------------------------------------------------------------------------\n    def string_has_enter ( self, string ):\n        if ( re.search ( \"<enter>\", string ) != None ):\n            return 1\n        else:\n            return 0\n\n#------------------------------------------------------------------------------\n    def add_enter_instruction ( self, result_list ):\n        if ( len ( result_list ) ):\n            string = result_list [ 0 ]\n            if ( self.string_has_enter ( string ) == 1 ):\n                return 0\n\n        result_list.insert ( 0, \" <enter>\" + \" \" * self.get_indentation ( \"<enter\" ) + \"Execute the current command\" )\n        \n\n#------------------------------------------------------------------------------\n    def match_macro ( self, macro_list, needle, ret_list ):\n        for haystack in macro_list:\n            if ( len ( needle ) == len ( haystack ) ):\n                if ( re.match ( needle, haystack ) != None ):\n                    self.add_enter_instruction ( ret_list )\n            elif ( len ( needle ) < len ( haystack ) ):\n                if ( re.match ( needle, haystack ) != None ):\n                    ret_list.append ( haystack )\n            else:\n                print \"\"\n\n#------------------------------------------------------------------------------\n    def option_exists ( self, consider_option, ret_list ):\n        for option in ret_list:\n            if ( re.match ( option, consider_option ) != None ):\n                return 1\n        return 0\n\n#------------------------------------------------------------------------------\n    def complete_command ( self,\n                           part_cmd, \n                           full_cmd, \n                           end_index, \n                           cmd_helper, \n                           ret_list ):\n        unmatched_string = full_cmd [ end_index: ]\n\n        # This is an adjustment for \"<space>\" before tab / ? keypress\n        if ( part_cmd [ -1 ] == \"_\" ):\n            part_cmd = part_cmd [ 0:-1 ]\n            unmatched_string = \"_\" + unmatched_string\n    \n        if ( unmatched_string [ 0 ] == \"_\" ):\n            # attach possible matches\n            possible_option = unmatched_string.replace ( \"_\", \" \" ) + ( \" \" * self.get_indentation ( full_cmd ) )\n            possible_option = possible_option + \"<\" + cmd_helper.cmd_desc + \">\"\n            ret_list.append ( possible_option )\n        else:\n            # Get part of the command from part_cmd\n            match_object = re.search ( \"_\", part_cmd )\n            while ( match_object != None ):\n                part_cmd = part_cmd [ match_object.end (): ]\n                match_object = re.search ( \"_\", part_cmd )\n        \n            # Get rest of the command from unmatched_string\n            match_object = re.search ( \"_\", unmatched_string )\n            if ( match_object != None ):\n                unmatched_string = unmatched_string [ :(match_object.end()-1)]\n\n            complete_word = part_cmd + unmatched_string\n            if ( self.option_exists ( complete_word, ret_list ) == 0 ):\n                 ret_list.append ( complete_word )\n\n        return ret_list\n        \n\n#------------------------------------------------------------------------------\n    def get_all_cmds ( self ):\n        ret_list = []\n        for cmd in self.cmd_graph:\n            cmd_str = cmd.replace ( \"_\", \" \" )\n            cmd_str = cmd_str + ( \" \" * self.get_indentation ( cmd ) ) + \"<\" + self.cmd_graph [ cmd ].cmd_desc + \">\"\n            ret_list.append ( cmd_str )\n        return ret_list\n\n#------------------------------------------------------------------------------\n# Lot of reference here to needle and haystack, needle being the current\n# command context of the CLI, and haystack being the command model dict\n# created during CLIUtil instantiation\n#------------------------------------------------------------------------------\n    def get_match ( self, cmd ):\n        if  ( len ( cmd ) == 0 or re.search ( \"[a-z|A-Z|0-9]\", cmd ) == None ):\n            return self.get_all_cmds ()\n\n        # chomp input string\n        if ( cmd [ -1 ] == \" \" ):\n            cmd = cmd [ 0:-1 ]\n\n        needle = self.normalize_command ( cmd )\n        ret_list = []\n\n        for haystack in self.cmd_graph:\n            len_haystack = len ( haystack )\n            len_needle   = len ( needle )\n            cmd_helper = self.cmd_graph [ haystack ]\n\n            # Case 1: Full command is provided, without macro expansion\n            if ( len_needle == len_haystack ):\n                # check if we have a match\n                if ( re.match ( needle, haystack ) != None ):\n                    if ( cmd_helper.cmd_macro != \"\" ):\n                        self.include_macro ( self.get_macro_list ( CLIImplementor (), cmd_helper.cmd_macro, \"add help\" ), ret_list )\n                    else:\n                        self.add_enter_instruction ( ret_list )\n\n            # Case 2: Full command is provided with macro expansion\n            elif ( len_needle > len_haystack ):\n                match_object = re.match ( haystack, needle )\n                if ( match_object != None ):\n                    # Match exists - so get the macro\n                    cmd_macro = needle [ match_object.end (): ]\n                    if ( cmd_macro [ 0 ] == \"_\" and len ( cmd_macro ) > 1 ):\n                        cmd_macro = cmd_macro [ 1: ]\n\n                    if ( cmd_helper.cmd_macro != \"\" ):\n                        cmd_macro_list = self.get_macro_list ( CLIImplementor(),\n                                                          cmd_helper.cmd_macro )\n                        self.match_macro ( cmd_macro_list, cmd_macro, ret_list )\n\n            # Case 3: Part command is provided\n            elif ( len_needle < len_haystack ):\n                match_object = re.match ( needle, haystack )\n                if ( match_object != None ):\n                    # Match exists - get rest of the command\n                    balance_cmd = haystack [ match_object.end (): ]\n                    self.complete_command ( needle, \n                                            haystack, \n                                            match_object.end (), \n                                            self.cmd_graph [ haystack ],\n                                            ret_list )\n                \n\n        return ret_list\n\n#------------------------------------------------------------------------------\n    def chomp ( self, token ):\n        match_object = re.search ( \"[a-z|A-Z|0-9]\", token )\n        if ( match_object != None ):\n            token = token [ ( match_object.end () - 1): ]\n\n        token = token [ ::-1 ]\n        match_object = re.search ( \"[a-z|A-Z|0-9]\", token )\n        if ( match_object != None ):\n            token = token [ ( match_object.end () - 1): ]\n\n        token = token [ ::-1 ]\n\n        return token\n\n#------------------------------------------------------------------------------\n    def validate_command_and_execute ( self, full_cmd_context ):\n        # We will do the validation again in case this function is called\n        # outside the CLI context\n        best_cmd_match = \"\"\n        best_cmd_args  = \"\"\n        best_cmd_handle = None\n\n        for command in self.cmd_graph:\n            match_object = re.match ( command, \n                           self.normalize_command ( full_cmd_context ) )\n            if ( match_object != None ):\n                # Okay - we found a match. Get macros if included\n                command_args = \"\"\n                # TODO - different impl here for multiple args support\n                if ( len ( full_cmd_context ) > len ( command ) ):\n                    command_args = self.chomp ( full_cmd_context [ match_object.end (): ] )\n                if ( len ( best_cmd_match ) < len ( command ) ):\n                    best_cmd_match = command\n                    best_cmd_args  = command_args\n                    best_cmd_handle = self.get_implementor_handle ( CLIImplementor (), self.cmd_graph [ command ].cmd_handle )\n\n        if ( best_cmd_handle != 0 ):\n            return best_cmd_handle ( best_cmd_args )\n        else:\n            print self.cmd_graph [ best_cmd_match ].cmd_handle + \" not implemented\"\n\n#------------------------------------------------------------------------------\n    def print_results ( self, result_list ):\n        for result in result_list:\n            print \"\\t\" + result\n\n#------------------------------------------------------------------------------\n    def print_command_graph ( self, cmd_dict ):\n        for keys in cmd_dict:\n            print keys + \"=>\"\n            cmd = cmd_dict [ keys ]\n            if ( cmd.cmd_desc != \"\" ):\n                print \"    \" + cmd.cmd_desc\n            print \"    \" + cmd.cmd_access\n            if ( cmd.cmd_macro != \"\" ):\n                fn_macro = self.get_implementor_handle ( CLIImplementor (), \n                                                        cmd.cmd_macro )\n                if ( fn_macro != 0 ):\n                    print fn_macro ()\n                else:\n                    print \"    Macro not implemented\"\n            if ( cmd.cmd_handle != \"\" ):\n                fn_handle = self.get_implementor_handle ( CLIImplementor (), \n                                                     cmd.cmd_handle )\n                if ( fn_handle != 0 ):\n                    fn_handle ()\n                else:\n                    print \"    Handler not implemented\"\n\n# end class CLIUtil\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\n#------------------------------------------------------------------------------\n#                              MAIN\n#------------------------------------------------------------------------------\ncli_util = CLIUtil ()\n    \n\nmatch_options = [ \"create cabling\",\n#                  \"create cabling-plan\",\n#                  \"create cabling-\",\n#                  \"create cabling\",\n#                  \"create cabling-plan pod\",\n#                  \"create cabling-plan pod pod_2\",\n#                  \"create\",\n#                  \"create dev\",\n#                  \"create device-config\",\n#                  \"create device-config p\",\n#                  \"create device-config pod\",\n#                  \"create device-config pod pod_1\",\n#                  \"run\",\n#                  \"update password\",\n#                  \"run r\",\n#                  \"run RE\",\n#                  \"create cab\",\n                  \"create pods\",\n                  \"create pods from\",\n                  \"create pods from-file\",\n                  \"\" ]\n\nif __name__ == '__main__':\n    for match in match_options:\n        print \"Matching results for \" + match + \" is:\"\n        cli_util.print_results ( cli_util.get_match ( match ) )\n        print \"------------------------------------------------------\"", "instruction_en": "This code provides a CLI parser module that is responsible for parsing command models defined in cliCommands.yaml and providing functions for validation of user-input, invoking execution handle for CLI commands or macro expansions, and determining possible arg match for command auto-completion based on context. To use the module, instantiate the CLIUtil class and call its methods, such as get_match() to get a list of possible matches for a given command context, and validate_command_and_execute() to validate and execute a full command context. The module also includes methods for printing the command graph and results.", "original_line_number": 34781, "instruction_length": 680}
{"instruction": "Buat projek Django dengan model papan, webhook, dan jambatan. Model papan mempunyai medan untuk nama, webhook_activate, trello_board_id, trello_token, created_at, dan updated_at. Model WebHook mempunyai bidang untuk nama, e -webhook_url, icon_url, nama pengguna, papan, created_at, dan updated_at. Model jambatan mempunyai bidang untuk webhook, papan, acara, created_at, dan dikemas kini_at. Medan peristiwa adalah charfield dengan panjang maksimum 700 dan pilihan dari tuple event_choices. Kaedah events_as_list mengembalikan medan peristiwa sebagai senarai.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom ast import literal_eval\n\nfrom django.db import models\nfrom django.conf import settings\n\n\nclass Board(models.Model):\n\n    name = models.CharField(max_length=100)\n    webhook_activate = models.BooleanField(default=False)\n    trello_board_id = models.CharField(max_length=100)\n    trello_token = models.CharField(max_length=100)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        ordering = [\"name\"]\n        verbose_name_plural = \"boards\"\n\n    def __str__(self):\n        return self.name\n\n    def __unicode__(self):\n        return \"{}\".format(self.name)\n\n\nclass Webhook(models.Model):\n\n    name = models.CharField(max_length=50)\n    incoming_webhook_url = models.CharField(max_length=300, unique=True)\n\n    icon_url = models.CharField(\n        max_length=250,\n        default=\"http://maffrigby.com/wp-content/uploads/2015/05/trello-icon.png\",\n    )\n    username = models.CharField(max_length=30, default=\"Matterllo\")\n\n    board = models.ManyToManyField(Board)\n\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        ordering = [\"name\"]\n        verbose_name_plural = \"webhooks\"\n\n    def __str__(self):\n        return \"{} :: {}\".format(self.name, self.incoming_webhook_url)\n\n    def __unicode__(self):\n        return \"{} :: {}\".format(self.name, self.incoming_webhook_url)\n\n\nclass Bridge(models.Model):\n    EVENT_CHOICES = (\n        # card\n        (\"addAttachmentToCard\", \"addAttachmentToCard\"),\n        (\"addLabelToCard\", \"addLabelToCard\"),\n        (\"addMemberToCard\", \"addMemberToCard\"),\n        (\"commentCard\", \"commentCard\"),\n        (\"copyCard\", \"copyCard\"),\n        (\"createCard\", \"createCard\"),\n        (\"emailCard\", \"emailCard\"),\n        (\"moveCardFromBoard\", \"moveCardFromBoard\"),\n        (\"moveCardToBoard\", \"moveCardToBoard\"),\n        (\"removeLabelFromCard\", \"removeLabelFromCard\"),\n        (\"removeMemberFromCard\", \"removeMemberFromCard\"),\n        (\n            \"updateCard\",\n            \"updateCard (include moveCardToList, renameCard, renameCardDesc, updateCardDueDate, removeCardDueDate, archiveCard, unarchiveCard)\",\n        ),\n        # checklist\n        (\"addChecklistToCard\", \"addChecklistToCard\"),\n        (\"createCheckItem\", \"createCheckItem\"),\n        (\"updateCheckItemStateOnCard\", \"updateCheckItemStateOnCard\"),\n        # list\n        (\"archiveList\", \"archiveList\"),\n        (\"createList\", \"createList\"),\n        (\"moveListFromBoard\", \"moveCardFromBoard\"),\n        (\"moveListToBoard\", \"moveListToBoard\"),\n        (\"renameList\", \"renameList\"),\n        (\"updateList\", \"updateList\"),\n    )\n\n    webhook = models.ForeignKey(Webhook, on_delete=models.CASCADE)\n    board = models.ForeignKey(Board, on_delete=models.CASCADE)\n\n    events = models.CharField(max_length=700)\n\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        verbose_name_plural = \"bridges\"\n\n    def __str__(self):\n        return \"{}::{}\".format(self.board, self.webhook)\n\n    def __unicode__(self):\n        return \"{}::{}\".format(self.board, self.webhook)\n\n    def events_as_list(self):\n        return literal_eval(self.events)", "instruction_en": "Create a Django project with a Board, Webhook, and Bridge model. The Board model has fields for name, webhook_activate, trello_board_id, trello_token, created_at, and updated_at. The Webhook model has fields for name, incoming_webhook_url, icon_url, username, board, created_at, and updated_at. The Bridge model has fields for webhook, board, events, created_at, and updated_at. The events field is a CharField with a max length of 700 and choices from the EVENT_CHOICES tuple. The events_as_list method returns the events field as a list.", "original_line_number": 35479, "instruction_length": 559}
{"instruction": "Untuk menjalankan contoh pekerja Weimar atau memulakan pelayan graf Weimar, gunakan modul ArgParse untuk menghuraikan argumen baris arahan. Jika memulakan contoh pekerja, gunakan fungsi start_worker () dari modul remote.worker dan tentukan bilangan benang yang berjalan di dalam pekerja. Jika memulakan pelayan, tentukan alamat IP penyelaras HyperDex dan nombor port menggunakan argumen hyperdex_ip dan hyperdex_port, dan gunakan fungsi start_server () dari modul jauh. Jika hujah hyperdex_ip dan hyperdex_port tidak ditentukan semasa memulakan pelayan, cetak mesej ralat dan keluar. Jika hujah benang ditentukan semasa memulakan pelayan, isi -imetinya kerana ia hanya merujuk kepada proses pekerja.", "input": "", "output": "'''\nCreated on Mar 17, 2014\n\n@author: mschilonka\n'''\nimport argparse, sys\nfrom remote import server as Server\nfrom remote import worker as Worker\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-w\", \"--worker\", help=\"Starts a weimar worker instance.\", action=\"store_true\")\n    parser.add_argument(\"-t\", \"--threads\",type=int, dest='threads', help=\"The number of threads running in one a worker (Default=3).\")\n    parser.add_argument(\"-s\", \"--server\", help=\"Starts a weimar graph server.\", action=\"store_true\")\n    parser.add_argument(\"-i\", \"--hyperdex-ip\",type=str ,dest='hyperdex_ip',  help='The HyperDex coordinator IP address. Must be specified if a server is started.')\n    parser.add_argument(\"-p\", \"--hyperdex-port\",type=int ,dest='hyperdex_port', help=\"The HyperDex coordinator port number. Must be specified if a server is started.\")\n    args = parser.parse_args()\n\n    if args.worker:\n        if(args.threads is None):\n            args.threads = 3\n        Worker.start_worker(args.threads)\n    elif args.server:\n        if(args.hyperdex_ip is None or args.hyperdex_port is None):\n            print('When starting a Weimar server, please specify the HyperDex\\'s coordinators ip and port.')\n            parser.print_help()\n            sys.exit(1)\n        if(args.threads is not None):\n            print('--threads only refers to a worker process and will be omitted.')\n        Server.start_server(args.hyperdex_ip, args.hyperdex_port)", "instruction_en": "To run a Weimar worker instance or start a Weimar graph server, use the argparse module to parse command line arguments. If starting a worker instance, use the start_worker() function from the remote.worker module and specify the number of threads running in the worker. If starting a server, specify the HyperDex coordinator IP address and port number using the hyperdex_ip and hyperdex_port arguments, respectively, and use the start_server() function from the remote.server module. If the hyperdex_ip and hyperdex_port arguments are not specified when starting a server, print an error message and exit. If the threads argument is specified when starting a server, omit it as it only refers to a worker process.", "original_line_number": 35666, "instruction_length": 699}
{"instruction": "Buat objek zonemixing baru dengan sifat berikut: \n- Nama: rentetan\n- Nama Zon: senarai objek\n- Nama Jadual: senarai objek\n- Kaedah pengiraan kadar aliran reka bentuk: \"aliran/zon\"\n- Kadar aliran reka bentuk: nombor sebenar\n- Kadar aliran setiap kawasan lantai zon: nombor sebenar\n- Kadar aliran setiap orang: nombor sebenar\n- perubahan udara sejam: nombor sebenar\n- Sumber Zon Nama: senarai objek\n- Suhu Delta: 10.1\n- Nama Jadual Suhu Delta: senarai objek\n- Nama Jadual Suhu Zon Minimum: Senarai Objek\n- Jadual Suhu Zon Maksimum Nama: Senarai Objek\n- Jadual Suhu Zon Sumber Minimum Nama: Senarai Objek\n- Jadual suhu zon sumber maksimum Nama: senarai objek\n- Jadual Suhu Luar Minimum Nama: Senarai Objek\n- Jadual Suhu Luar Maksimum Nama: Senarai Objek", "input": "", "output": "import os\nimport tempfile\nimport unittest\nimport logging\nfrom pyidf import ValidationLevel\nimport pyidf\nfrom pyidf.idf import IDF\nfrom pyidf.zone_airflow import ZoneMixing\n\nlog = logging.getLogger(__name__)\n\nclass TestZoneMixing(unittest.TestCase):\n\n    def setUp(self):\n        self.fd, self.path = tempfile.mkstemp()\n\n    def tearDown(self):\n        os.remove(self.path)\n\n    def test_create_zonemixing(self):\n\n        pyidf.validation_level = ValidationLevel.error\n\n        obj = ZoneMixing()\n        # alpha\n        var_name = \"Name\"\n        obj.name = var_name\n        # object-list\n        var_zone_name = \"object-list|Zone Name\"\n        obj.zone_name = var_zone_name\n        # object-list\n        var_schedule_name = \"object-list|Schedule Name\"\n        obj.schedule_name = var_schedule_name\n        # alpha\n        var_design_flow_rate_calculation_method = \"Flow/Zone\"\n        obj.design_flow_rate_calculation_method = var_design_flow_rate_calculation_method\n        # real\n        var_design_flow_rate = 0.0\n        obj.design_flow_rate = var_design_flow_rate\n        # real\n        var_flow_rate_per_zone_floor_area = 0.0\n        obj.flow_rate_per_zone_floor_area = var_flow_rate_per_zone_floor_area\n        # real\n        var_flow_rate_per_person = 0.0\n        obj.flow_rate_per_person = var_flow_rate_per_person\n        # real\n        var_air_changes_per_hour = 0.0\n        obj.air_changes_per_hour = var_air_changes_per_hour\n        # object-list\n        var_source_zone_name = \"object-list|Source Zone Name\"\n        obj.source_zone_name = var_source_zone_name\n        # real\n        var_delta_temperature = 10.1\n        obj.delta_temperature = var_delta_temperature\n        # object-list\n        var_delta_temperature_schedule_name = \"object-list|Delta Temperature Schedule Name\"\n        obj.delta_temperature_schedule_name = var_delta_temperature_schedule_name\n        # object-list\n        var_minimum_zone_temperature_schedule_name = \"object-list|Minimum Zone Temperature Schedule Name\"\n        obj.minimum_zone_temperature_schedule_name = var_minimum_zone_temperature_schedule_name\n        # object-list\n        var_maximum_zone_temperature_schedule_name = \"object-list|Maximum Zone Temperature Schedule Name\"\n        obj.maximum_zone_temperature_schedule_name = var_maximum_zone_temperature_schedule_name\n        # object-list\n        var_minimum_source_zone_temperature_schedule_name = \"object-list|Minimum Source Zone Temperature Schedule Name\"\n        obj.minimum_source_zone_temperature_schedule_name = var_minimum_source_zone_temperature_schedule_name\n        # object-list\n        var_maximum_source_zone_temperature_schedule_name = \"object-list|Maximum Source Zone Temperature Schedule Name\"\n        obj.maximum_source_zone_temperature_schedule_name = var_maximum_source_zone_temperature_schedule_name\n        # object-list\n        var_minimum_outdoor_temperature_schedule_name = \"object-list|Minimum Outdoor Temperature Schedule Name\"\n        obj.minimum_outdoor_temperature_schedule_name = var_minimum_outdoor_temperature_schedule_name\n        # object-list\n        var_maximum_outdoor_temperature_schedule_name = \"object-list|Maximum Outdoor Temperature Schedule Name\"\n        obj.maximum_outdoor_temperature_schedule_name = var_maximum_outdoor_temperature_schedule_name\n\n        idf = IDF()\n        idf.add(obj)\n        idf.save(self.path, check=False)\n\n        with open(self.path, mode='r') as f:\n            for line in f:\n                log.debug(line.strip())\n\n        idf2 = IDF(self.path)\n        self.assertEqual(idf2.zonemixings[0].name, var_name)\n        self.assertEqual(idf2.zonemixings[0].zone_name, var_zone_name)\n        self.assertEqual(idf2.zonemixings[0].schedule_name, var_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].design_flow_rate_calculation_method, var_design_flow_rate_calculation_method)\n        self.assertAlmostEqual(idf2.zonemixings[0].design_flow_rate, var_design_flow_rate)\n        self.assertAlmostEqual(idf2.zonemixings[0].flow_rate_per_zone_floor_area, var_flow_rate_per_zone_floor_area)\n        self.assertAlmostEqual(idf2.zonemixings[0].flow_rate_per_person, var_flow_rate_per_person)\n        self.assertAlmostEqual(idf2.zonemixings[0].air_changes_per_hour, var_air_changes_per_hour)\n        self.assertEqual(idf2.zonemixings[0].source_zone_name, var_source_zone_name)\n        self.assertAlmostEqual(idf2.zonemixings[0].delta_temperature, var_delta_temperature)\n        self.assertEqual(idf2.zonemixings[0].delta_temperature_schedule_name, var_delta_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_zone_temperature_schedule_name, var_minimum_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_zone_temperature_schedule_name, var_maximum_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_source_zone_temperature_schedule_name, var_minimum_source_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_source_zone_temperature_schedule_name, var_maximum_source_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_outdoor_temperature_schedule_name, var_minimum_outdoor_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_outdoor_temperature_schedule_name, var_maximum_outdoor_temperature_schedule_name)", "instruction_en": "Create a new ZoneMixing object with the following properties: \n- Name: a string\n- Zone Name: an object-list\n- Schedule Name: an object-list\n- Design Flow Rate Calculation Method: \"Flow/Zone\"\n- Design Flow Rate: a real number\n- Flow Rate per Zone Floor Area: a real number\n- Flow Rate per Person: a real number\n- Air Changes per Hour: a real number\n- Source Zone Name: an object-list\n- Delta Temperature: 10.1\n- Delta Temperature Schedule Name: an object-list\n- Minimum Zone Temperature Schedule Name: an object-list\n- Maximum Zone Temperature Schedule Name: an object-list\n- Minimum Source Zone Temperature Schedule Name: an object-list\n- Maximum Source Zone Temperature Schedule Name: an object-list\n- Minimum Outdoor Temperature Schedule Name: an object-list\n- Maximum Outdoor Temperature Schedule Name: an object-list", "original_line_number": 35825, "instruction_length": 750}
{"instruction": "Buat titik akhir API yang menyenaraikan semua pemain 1v1 atau pemain global. Titik akhir harus menerima parameter rentetan 'Leaderboard_Type' yang boleh sama ada '1v1' atau 'global'. Titik akhir juga harus menerima dua halaman Parameter Parameter Pilihan [Nombor] 'dan' Size] '. Nilai lalai untuk 'halaman [nombor]' hendaklah 1 dan nilai lalai untuk 'saiz [saiz]' hendaklah 5000. Sambutan juga harus mengandungi medan 'jenis' yang boleh sama ada 'ranked1v1' atau 'global'. Pemain harus disusun dengan penarafan mereka dalam urutan menurun. Sekiranya terdapat ralat, titik akhir harus mengembalikan respons JSON dengan medan 'kesilapan' yang mengandungi pelbagai objek ralat dengan medan 'tajuk' dan 'status' mereka.", "input": "", "output": "from faf.api import LeaderboardSchema\nfrom faf.api import LeaderboardStatsSchema\nfrom flask import request\nfrom pymysql.cursors import DictCursor\n\nfrom api import app\nfrom api.error import ApiException, ErrorCode\nfrom api.error import Error\nfrom api.query_commons import fetch_data\nfrom faf import db\n\nMAX_PAGE_SIZE = 5000\n\nSELECT_EXPRESSIONS = {\n    'id': 'r.id',\n    'login': 'l.login',\n    'mean': 'r.mean',\n    'deviation': 'r.deviation',\n    'num_games': 'r.numGames',\n    'is_active': 'r.is_active',\n    'rating': 'ROUND(r.mean - 3 * r.deviation)',\n    'ranking': '@rownum:=@rownum+1'\n}\n\nTABLE1V1 = 'ladder1v1_rating r JOIN login l on r.id = l.id, (SELECT @rownum:=%(row_num)s) n'\nTABLEGLOBAL = 'global_rating r JOIN login l on r.id = l.id, (SELECT @rownum:=%(row_num)s) n'\n\n\n@app.route('/leaderboards/<string:leaderboard_type>')\ndef leaderboards_type(leaderboard_type):\n    \"\"\"\n        Lists all ranked 1v1 or global players.\n\n        **Example Request**:\n\n        **Default Values**:\n            page[number]=1\n\n            page[size]=5000\n\n        .. sourcecode:: http\n\n           GET /leaderboards/1v1 /leaderboards/global\n           Accept: application/vnd.api+json\n\n        **Example Response**:\n\n        .. sourcecode:: http\n\n            HTTP/1.1 200 OK\n            Vary: Accept\n            Content-Type: text/javascript\n\n            {\n              \"data\": [\n                {\n                  \"attributes\": {\n                    \"deviation\": 48.4808,\n                    \"id\": \"781\",\n                    \"login\": \"Zock\",\n                    \"mean\": 2475.69,\n                    \"num_games\": 1285,\n                    \"ranking\": 1,\n                    \"rating\": 2330,\n                    \"won_games\": 946\n                  },\n                  \"id\": \"781\",\n                  \"type\": \"ranked1v1\"\n                },\n                ...\n              ]\n            }\n\n        :param page[number]: The page number being requested (EX.: /leaderboards/1v1?page[number]=2)\n        :type page[number]: int\n        :param page[size]: The total amount of players to grab by default (EX.: /leaderboards/1v1?page[size]=10)\n        :type page[size]: int\n        :param leaderboard_type: Finds players in the 1v1 or global rating\n        :type leaderboard_type: 1v1 OR global\n        :status 200: No error\n\n        \"\"\"\n    sort_field = request.values.get('sort')\n    if sort_field:\n        raise ApiException([Error(ErrorCode.QUERY_INVALID_SORT_FIELD, sort_field)])\n\n    page = int(request.values.get('page[number]', 1))\n    page_size = int(request.values.get('page[size]', MAX_PAGE_SIZE))\n    row_num = (page - 1) * page_size\n    select = SELECT_EXPRESSIONS\n\n    args = {'row_num': row_num}\n\n    rating = find_leaderboard_type(leaderboard_type, select)\n\n    return fetch_data(LeaderboardSchema(), rating['table'], rating['select'], MAX_PAGE_SIZE, request, sort='-rating',\n                      args=args, where='is_active = 1 AND r.numGames > 0')\n\n\n@app.route('/leaderboards/<string:leaderboard_type>/<int:player_id>')\ndef leaderboards_type_get_player(leaderboard_type, player_id):\n    \"\"\"\n        Gets a global or 1v1 player. Player must be active, played more than one ranked game, and must have statistics associated\n        with he/she.\n\n        **Example Request**:\n\n        .. sourcecode:: http\n\n           GET /leaderboards/1v1/781 /leaderboards/global/781\n\n        **Example Response**:\n\n        .. sourcecode:: http\n\n            HTTP/1.1 200 OK\n            Vary: Accept\n            Content-Type: text/javascript\n\n            {\n              \"data\": {\n                \"attributes\": {\n                  \"deviation\": 48.4808,\n                  \"id\": \"781\",\n                  \"login\": \"Zock\",\n                  \"mean\": 2475.69,\n                  \"num_games\": 1285,\n                  \"ranking\": 1,\n                  \"rating\": 2330,\n                  \"won_games\": 946\n                },\n                \"id\": \"781\",\n                \"type\": \"leaderboard\"\n              }\n            }\n\n        :param leaderboard_type: Finds players in the 1v1 or global rating\n        :type leaderboard_type: 1v1 OR global\n        :param player_id: Player ID\n        :type player_id: int\n\n        :status 200: No error\n        :status 404: No entry with this id was found\n\n        \"\"\"\n    select_expressions = SELECT_EXPRESSIONS.copy()\n\n    select = select_expressions\n\n    rating = find_leaderboard_type(leaderboard_type, select)\n\n    select_expressions['ranking'] = \"\"\"(SELECT count(*) FROM \"\"\" + rating['tableName'] + \"\"\"\n                                        WHERE ROUND(mean - 3 * deviation) >= ROUND(r.mean - 3 * r.deviation)\n                                        AND is_active = 1\n                                        AND numGames > 0)\n                                        \"\"\"\n\n    result = fetch_data(LeaderboardSchema(), rating['table'], rating['select'], MAX_PAGE_SIZE, request,\n                        many=False, where='r.id=%(id)s', args=dict(id=player_id, row_num=0))\n\n    if 'id' not in result['data']:\n        return {'errors': [{'title': 'No entry with this id was found'}]}, 404\n\n    return result\n\n\n@app.route(\"/leaderboards/<string:rating_type>/stats\")\ndef rating_stats(rating_type):\n    \"\"\"\n    Gets all player stats sorted by rankings.\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n       GET /leaderboards/1v1/stats /leaderboards/global/stats\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Vary: Accept\n        Content-Type: text/javascript\n\n        {\n          \"data\": {\n            \"attributes\": {\n              \"rating_distribution\": {\n                \"-100\": 206,\n                \"-200\": 192,\n                \"-300\": 137,\n                \"-400\": 77,\n                \"-500\": 15,\n                \"-600\": 10,\n                \"-700\": 1,\n                \"0\": 268,\n                \"100\": 282,\n                \"1000\": 122,\n                \"1100\": 86,\n                \"1200\": 72,\n                \"1300\": 55,\n                \"1400\": 42,\n                \"1500\": 35,\n                \"1600\": 25,\n                \"1700\": 15,\n                \"1800\": 14,\n                \"1900\": 7,\n                \"200\": 284,\n                \"2000\": 5,\n                \"2100\": 2,\n                \"2200\": 1,\n                \"2300\": 2,\n                \"300\": 316,\n                \"400\": 296,\n                \"500\": 239,\n                \"600\": 238,\n                \"700\": 208,\n                \"800\": 177,\n                \"900\": 140\n              }\n            },\n            \"id\": \"/leaderboards/1v1/stats\",\n            \"type\": \"leaderboard_stats\"\n          }\n        }\n\n    :status 200: No error\n\n    \"\"\"\n\n    rating = find_leaderboard_type(rating_type)\n\n    with db.connection:\n        cursor = db.connection.cursor(DictCursor)\n        cursor.execute(\"\"\"\n        SELECT\n            FLOOR((mean - 3 * deviation)/100) * 100 AS `rating`,\n            count(*) as count\n        FROM \"\"\" + rating['tableName'] + \"\"\"\n        WHERE `is_active` = 1\n            AND mean BETWEEN 0 AND 3000\n            AND deviation <= 240\n            AND numGames > 0\n        GROUP BY `rating`\n        ORDER BY CAST(`rating` as SIGNED) ASC;\n        \"\"\")\n\n        result = cursor.fetchall()\n\n    data = dict(id='/leaderboards/' + rating_type + '/stats', rating_distribution={})\n\n    for item in result:\n        data['rating_distribution'][str(int(item['rating']))] = item['count']\n\n    return LeaderboardStatsSchema().dump(data, many=False).data\n\n\ndef find_leaderboard_type(rating_type, select=None):\n    rating = {}\n\n    if rating_type == '1v1':\n        rating['table'] = TABLE1V1\n        rating['select'] = append_select_expression()\n        rating['tableName'] = 'ladder1v1_rating'\n    elif rating_type == 'global':\n        rating['table'] = TABLEGLOBAL\n        rating['select'] = select\n        rating['tableName'] = 'global_rating'\n    else:\n        raise ApiException([Error(ErrorCode.QUERY_INVALID_RATING_TYPE, rating_type)])\n\n    return rating\n\n\ndef append_select_expression():\n    select = SELECT_EXPRESSIONS.copy()\n    select['won_games'] = 'r.winGames'\n    select['lost_games'] = 'r.numGames - r.winGames'\n    select['winning_percentage'] = 'ROUND((r.winGames/r.numGames) * 100)'\n    return select", "instruction_en": "Create an API endpoint that lists all ranked 1v1 or global players. The endpoint should accept a string parameter 'leaderboard_type' which can be either '1v1' or 'global'. The endpoint should also accept two optional query parameters 'page[number]' and 'page[size]'. The default value for 'page[number]' should be 1 and the default value for 'page[size]' should be 5000. The endpoint should return a JSON response containing an array of player objects with their attributes such as 'id', 'login', 'mean', 'deviation', 'num_games', 'ranking', 'rating', and 'is_active'. The response should also contain a 'type' field which can be either 'ranked1v1' or 'global'. The players should be sorted by their rating in descending order. If there is an error, the endpoint should return a JSON response with an 'errors' field containing an array of error objects with their 'title' and 'status' fields.", "original_line_number": 36019, "instruction_length": 715}
