{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade unbabel-comet\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Verify that GPU is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "def evaluate_translations(source_file, target_file, sample_size=None):\n",
    "    \"\"\"\n",
    "    Evaluate translation quality using COMET model in QE mode\n",
    "    \"\"\"\n",
    "    # Import COMET\n",
    "    from comet import download_model, load_from_checkpoint\n",
    "    \n",
    "    # Download the QE model that doesn't require references\n",
    "    print(\"Downloading COMET QE model (this may take a minute)...\")\n",
    "    model_path = download_model(\"Unbabel/wmt20-comet-qe-da\")  # Using QE model instead\n",
    "    model = load_from_checkpoint(model_path)\n",
    "    print(\"Model loaded successfully\")\n",
    "    \n",
    "    # Load source and target data\n",
    "    print(f\"Loading source file: {source_file}\")\n",
    "    with open(source_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Loading target file: {target_file}\")\n",
    "    with open(target_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        target_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    # Prepare data for COMET - QE mode only needs src and mt\n",
    "    samples = []\n",
    "    \n",
    "    # Determine number of samples to evaluate - now using full dataset by default\n",
    "    if sample_size:\n",
    "        eval_size = min(sample_size, min(len(source_data), len(target_data)))\n",
    "    else:\n",
    "        eval_size = min(len(source_data), len(target_data))\n",
    "    \n",
    "    print(f\"Preparing {eval_size} samples for evaluation...\")\n",
    "    \n",
    "    for i in range(eval_size):\n",
    "        source_text = source_data[i].get(\"instruction\", \"\")\n",
    "        target_text = target_data[i].get(\"instruction\", \"\")\n",
    "        \n",
    "        if source_text and target_text:\n",
    "            samples.append({\n",
    "                \"src\": source_text,\n",
    "                \"mt\": target_text,\n",
    "                \"index\": i\n",
    "            })\n",
    "    \n",
    "    # Run the model in batches to prevent memory issues\n",
    "    all_scores = []\n",
    "    batch_size = 8  # Small batch size to prevent CUDA OOM errors\n",
    "    \n",
    "    print(\"Running COMET evaluation...\")\n",
    "    for i in tqdm(range(0, len(samples), batch_size)):\n",
    "        batch = samples[i:i+batch_size]\n",
    "        # Use GPU (gpus=1) since we're in Colab GPU environment\n",
    "        model_output = model.predict(batch, batch_size=batch_size, gpus=1)\n",
    "        all_scores.extend(model_output.scores)\n",
    "    \n",
    "    # Collect scores\n",
    "    scores = []\n",
    "    for i, score in enumerate(all_scores):\n",
    "        item_index = samples[i][\"index\"]\n",
    "        scores.append({\n",
    "            \"index\": item_index,\n",
    "            \"source\": samples[i][\"src\"][:100] + \"...\" if len(samples[i][\"src\"]) > 100 else samples[i][\"src\"],\n",
    "            \"translation\": samples[i][\"mt\"][:100] + \"...\" if len(samples[i][\"mt\"]) > 100 else samples[i][\"mt\"],\n",
    "            \"score\": score\n",
    "        })\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_score = sum(all_scores) / len(all_scores) if all_scores else 0\n",
    "    \n",
    "    # Export to CSV for further analysis\n",
    "    df = pd.DataFrame(scores)\n",
    "    df.to_csv(\"comet_full_evaluation_results.csv\", index=False)\n",
    "    print(f\"Results saved to comet_full_evaluation_results.csv\")\n",
    "    \n",
    "    return {\n",
    "        \"samples\": scores,\n",
    "        \"average_score\": avg_score,\n",
    "        \"num_evaluated\": len(samples)\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Check if files exist\n",
    "    source_file = \"cleaned_train1.jsonl\"\n",
    "    target_file = \"final_train1.jsonl\"\n",
    "    \n",
    "    if not os.path.exists(source_file):\n",
    "        print(f\"Error: File {source_file} not found\")\n",
    "    elif not os.path.exists(target_file):\n",
    "        print(f\"Error: File {target_file} not found\")\n",
    "    else:\n",
    "        # Evaluate the entire dataset\n",
    "        print(\"Evaluating entire dataset - this may take a while...\")\n",
    "        results = evaluate_translations(\n",
    "            source_file=source_file, \n",
    "            target_file=target_file,\n",
    "            sample_size=None  # Set to None to evaluate all samples\n",
    "        )\n",
    "\n",
    "        # Display results\n",
    "        print(f\"\\n=== EVALUATION SUMMARY ===\")\n",
    "        print(f\"Average COMET score: {results['average_score']:.4f}\")\n",
    "        print(f\"Number of samples evaluated: {results['num_evaluated']}\")\n",
    "\n",
    "        # Display top examples\n",
    "        print(\"\\n=== Top 5 Translations ===\")\n",
    "        for i, sample in enumerate(sorted(results['samples'], key=lambda x: x['score'], reverse=True)[:5]):\n",
    "            print(f\"\\nSample {i+1} (Score: {sample['score']:.4f}):\")\n",
    "            print(f\"Source: {sample['source']}\")\n",
    "            print(f\"Translation: {sample['translation']}\")\n",
    "        \n",
    "        # Display worst translations\n",
    "        print(\"\\n=== Worst 5 Translations ===\")\n",
    "        for i, sample in enumerate(sorted(results['samples'], key=lambda x: x['score'])[:5]):\n",
    "            print(f\"\\nSample {i+1} (Score: {sample['score']:.4f}):\")\n",
    "            print(f\"Source: {sample['source']}\")\n",
    "            print(f\"Translation: {sample['translation']}\")\n",
    "            \n",
    "        # Generate quality distribution statistics\n",
    "        scores = [s['score'] for s in results['samples']]\n",
    "        score_ranges = {\n",
    "            \"Excellent (0.8-1.0)\": len([s for s in scores if s >= 0.8]),\n",
    "            \"Good (0.6-0.8)\": len([s for s in scores if 0.6 <= s < 0.8]),\n",
    "            \"Average (0.4-0.6)\": len([s for s in scores if 0.4 <= s < 0.6]),\n",
    "            \"Poor (0.2-0.4)\": len([s for s in scores if 0.2 <= s < 0.4]),\n",
    "            \"Very Poor (0-0.2)\": len([s for s in scores if s < 0.2])\n",
    "        }\n",
    "        \n",
    "        print(\"\\n=== Quality Distribution ===\")\n",
    "        for range_name, count in score_ranges.items():\n",
    "            percentage = (count / len(scores)) * 100\n",
    "            print(f\"{range_name}: {count} samples ({percentage:.2f}%)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    \n",
    "    # Fallback to LaBSE evaluation if COMET fails\n",
    "    print(\"\\nFalling back to LaBSE similarity evaluation for the entire dataset...\")\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer, util\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading LaBSE model...\")\n",
    "    # Configure to use GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SentenceTransformer(\"sentence-transformers/LaBSE\", device=device)\n",
    "    \n",
    "    def get_semantic_similarity(text_en, text_ms):\n",
    "        embeddings_en = model.encode(text_en, convert_to_tensor=True)\n",
    "        embeddings_ms = model.encode(text_ms, convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(embeddings_en, embeddings_ms).item()\n",
    "        return cosine_score\n",
    "    \n",
    "    # Evaluate with LaBSE\n",
    "    source_file = \"cleaned_train3.jsonl\"\n",
    "    target_file = \"final_train3.jsonl\"\n",
    "    \n",
    "    with open(source_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    with open(target_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        target_data = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    # Evaluate all samples\n",
    "    eval_size = min(len(source_data), len(target_data))\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Evaluating {eval_size} samples with LaBSE...\")\n",
    "    \n",
    "    # Process in batches to show progress\n",
    "    batch_size = 50\n",
    "    for i in tqdm(range(0, eval_size, batch_size), desc=\"Evaluating batches\"):\n",
    "        batch_end = min(i + batch_size, eval_size)\n",
    "        for j in range(i, batch_end):\n",
    "            source_text = source_data[j].get(\"instruction\", \"\")\n",
    "            target_text = target_data[j].get(\"instruction\", \"\")\n",
    "            \n",
    "            if source_text and target_text:\n",
    "                score = get_semantic_similarity(source_text, target_text)\n",
    "                results.append({\n",
    "                    \"index\": j,\n",
    "                    \"source\": source_text[:100] + \"...\" if len(source_text) > 100 else source_text,\n",
    "                    \"translation\": target_text[:100] + \"...\" if len(target_text) > 100 else target_text,\n",
    "                    \"score\": score\n",
    "                })\n",
    "    \n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"labse_full_evaluation_results.csv\", index=False)\n",
    "    print(\"Results saved to labse_full_evaluation_results.csv\")\n",
    "    \n",
    "    # Calculate average score\n",
    "    avg_score = sum(item[\"score\"] for item in results) / len(results) if results else 0\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n=== LaBSE EVALUATION SUMMARY ===\")\n",
    "    print(f\"Average similarity score: {avg_score:.4f}\")\n",
    "    print(f\"Number of samples evaluated: {len(results)}\")\n",
    "    \n",
    "    # Generate quality distribution statistics\n",
    "    scores = [s['score'] for s in results]\n",
    "    score_ranges = {\n",
    "        \"Excellent (0.8-1.0)\": len([s for s in scores if s >= 0.8]),\n",
    "        \"Good (0.6-0.8)\": len([s for s in scores if 0.6 <= s < 0.8]),\n",
    "        \"Average (0.4-0.6)\": len([s for s in scores if 0.4 <= s < 0.6]),\n",
    "        \"Poor (0.2-0.4)\": len([s for s in scores if 0.2 <= s < 0.4]),\n",
    "        \"Very Poor (0-0.2)\": len([s for s in scores if s < 0.2])\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Quality Distribution ===\")\n",
    "    for range_name, count in score_ranges.items():\n",
    "        percentage = (count / len(scores)) * 100\n",
    "        print(f\"{range_name}: {count} samples ({percentage:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
