{"instruction": "To analyze velocity data and find extrema and saddle points, follow these steps:\n\n1. Import necessary libraries: yt, matplotlib.pyplot, numpy, and yt.analysis_modules.halo_finding.api.\n2. Define four functions: derivx, derivy, deriv2x, and deriv2y to calculate first and second derivatives of velocity with respect to x and y.\n3. Define a function mixed_deriv to calculate the second mixed derivative of velocity.\n4. Define a function hess to create a Hessian matrix for each point.\n5. Define a function determinant to find the determinant of each Hessian matrix.\n6. Define a function magnitude to find the magnitude of velocity at each point.\n7. Define a function extrema to find extrema and saddle points using the determinant and magnitude.\n8. Define a function jacobian to create a Jacobian matrix for each point.\n9. Define a function evals to obtain eigenvalues for all points' Jacobian matrices and then check the extrema.\n10. Define a function main to load files and call the hess function.\n11. Call the main function to analyze the velocity data and find extrema and saddle points.", "input": "", "output": "import yt\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import pylab\nfrom yt.analysis_modules.halo_finding.api import HaloFinder\nfrom pylab import*\nfrom numpy import ma\nfrom numpy import linalg as LA\n\n\n#derives vel with respect to x\ndef derivx(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelxdx = np.zeros((320,320))\n\t\n\tfor i in range(len(vel)):\n\t\t\n\t\tfor x in range(len(vel)):\n\t\t\t\n\t\t\tif 0 < i < len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-1/2) * vel[i-1][x]) + ((1/2) * vel[i+1][x])\n\t\t\t\n\t\t\telif i == 0:\n\t\t\t\tvelxdx[i,x] = (((-3/2) * vel[i][x]) + (2 * vel[i+1][x]) + ((-1/2) * vel[i+2][x]))\n\t\t\t\n\t\t\telif i == len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-3/2) * vel[i][x]) + (2 * vel[i-1][x]) + ((-1/2) * vel[i-2][x])\n\n\treturn velxdx/distance\n\n\n#derives vel with respect to y\ndef derivy(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelydy = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < x < len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((-1/2) * vel[i][x-1]) + ((1/2) * vel[i][x+1]))\n\t\t\t\n\t\t\telif x == 0:\n\t\t\t\tvelydy[i,x] = (((-3/2)*vel[i][x]) + (2*vel[i][x+1]) + ((-1/2) * vel[i][x + 2]))\n\t\t\t\n\t\t\telif x == len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((-3/2)*vel[i][x]) + (2*vel[i][x-1]) + ((-1/2) * vel[i][x-2]))\n\n\treturn velydy/distance\n\n\n#second derivative of vel with respect to x\ndef deriv2x(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelxdx = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < i < len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = (vel[i-1][x]) + (-2 * vel[i][x]) + (vel[i+1][x])\n\n\t\t\telif i == 0:\n\t\t\t\tvelxdx[i,x] = ((2 * vel[i][x]) + (-5 * vel[i+1][x]) + (4* vel[i+2][x]) + (-1 * vel[i+3][x]))\n\n\t\t\telif i == len(vel) - 1:\n\t\t\t\tvelxdx[i,x] = ((-3/2) * vel[i][x]) + (2 * vel[i-1][x]) + ((-1/2) * vel[i-2][x])\n\n\treturn velxdx/distance\n\n\n#second derivative of vel with respect to y\ndef deriv2y(vel,xcoords):\n\tdistance = xcoords[1][0] - xcoords[0][0]\n\tvelydy = np.zeros((320,320))\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < x < len(vel) - 1:\n\t\t\t\tvelydy[i,x] = ((vel[i][x-1]) + (-2 * vel[i][x]) + (vel[i][x+1]))\n\n\t\t\telif x == 0:\n\t\t\t\tvelydy[i,x] = (((2)*vel[i][x]) + (-5 * vel[i][x+1]) + ((4) * vel[i][x+2]) + (-1 * vel[i][x+3]))\n\n\t\t\telif x == len(vel) - 1:\n\t\t\t\tvelydy[i,x] = (((2) * vel[i][x]) + (-5 * vel[i][x - 1]) + ((4) * vel[i][x-2]) + (-1 * vel[i][x-3]))\n\n\treturn velydy/distance\n\n\n#second derivative of a mixed derivative\ndef mixed_deriv(xcoords, ycoords, vel):\n\tdistx = xcoords[1][0] - xcoords[0][0]\n\tdisty = ycoords[0][1] - ycoords[0][0]\n\tmixed = np.zeros((320,320))\n\tveldx = derivx(vel, xcoords)\n\tveldy = derivy(veldx, xcoords)  #takes deriv of vel with respect to x and derives that in the y direction\n\n\tfor i in range(len(vel)):\n\n\t\tfor x in range(len(vel)):\n\n\t\t\tif 0 < i < len(vel) - 1 and 0 < x < len(vel) - 1:\n\t\t\t\tmixed[i][x] = ((vel[i+1][x+1]) - (vel[i+1][x-1]) - (vel[i-1][x+1]) + (vel[i-1][x-1]))/(4*distx*disty)\n\n\t\t\t#if on edges derives with respect to x first\n\t\t\telif i == 0 or i == len(vel) - 1 or x == 0 or x == len(vel) - 1:\n\t\t\t\tmixed[i][x]=veldy[i][x]\n\n\treturn mixed\n\n#create hessian matrix for each point\ndef hess(xcoords, ycoords, vel):\n\tveldx = deriv2x(vel, xcoords)  #retrieves the second derivatives of the velocity in the x direction\n\tveldy = deriv2y(vel, xcoords)  #retrieves the second derivatives of the velocity in the y direction\n\tmixed = mixed_deriv(xcoords, ycoords, vel) #retrieves the second mixed derivatives of the velocity \n\thessian = np.zeros((2,2))\n\tallhessian = [[[] for j in range(320)] for i in range(320)]\n\tfor j in range(len(veldx)):\n\n\t\tfor k in range(len(veldx)):\n\n\t\t\tfor i in range(len(hessian)):\n\n\t\t\t\tfor x in range(len(hessian)):\n\n\t\t\t\t\tif i == 0 and x == 1:\n\t\t\t\t\t\thessian[i,x] = mixed[j,k]\n\t\t\t\t\t\thessian[i+1][x-1] = mixed[j,k]\n\n\t\t\t\t\telif x == 0 and i == 0:\n\t\t\t\t\t\thessian[i,x] = veldx[j,k]\n\n\t\t\t\t\telif x == 1 and i == 1:\n\t\t\t\t\t\thessian[i,x] = veldy[j,k] \n\n\t\t\t\tallhessian[j][k] = hessian\n\tallhessian = np.array(allhessian)\n\treturn allhessian\n\n\n#find determinant\ndef determinant(allhessian):\n\tdeters = np.zeros((320,320))\n\n\tfor j in range(len(allhessian)):\n\n\t\tfor k in range(len(allhessian)):\n\n\t\t\tx = allhessian[j,k]\n\n\t\t\tdeters[j,k] = (x[0,0]*x[1,1]) - (x[1,0]*x[0,1])\n\n\treturn deters\n\n\n#find magnitude\ndef magnitude(velx,vely, xcoords):\n\tmag = np.zeros((320,320))\n\tyderiv = derivy(vely, xcoords)\n\txderiv = derivx(velx, xcoords)\n\n\tfor i in range(len(xderiv)):\n\n\t\tfor x in range(len(xderiv)):\n\t\t\tmag[i][x] = (((yderiv[i,x]**2) + (xderiv[i,x]**2))**.5)\n\n\treturn mag\n\n\n#finds extrema and saddlepoints\ndef extrema(allhessian, velx, vely, xcoords):\n\tdeters = determinant(allhessian)\n\textrem = np.zeros((320,320))\n\tmag = magnitude(velx, vely, xcoords)\n\n\tfor j in range(len(extrem)):\n\n\t\tfor k in range(len(extrem)):\n\n\t\t\tif mag[j][k] == 0:\n\t\t\t\tif deters[j,k] < 0:\n\t\t\t\t\textrem[j, k] = -1\n\n\t\t\t\telif deters[j,k] == 0:\n\t\t\t\t\textrem[j,k] = 0\n\n\t\t\t\telse:\n\t\t\t\t\tx = allhessian[j,k]\n\t\t\t\t\tif deter[j,k] > 0 and x[0,0] > 0:\n\t\t\t\t\t\textem[j, k] = -2\n\t\t\t\t\telif deter[j,k] > 0 and x[0,0] < 0:\n\t\t\t\t\t\textrem[j, k] = 2\n\treturn extrem\t\t\t\t\n\n\n#creates jacobia matrix for each point\t\t\t\t\t\t\ndef jacobian(xcoords,velx, vely):\n\txx = derivx(velx, xcoords)\n\txy = derivy(velx, xcoords)\n\tyx = derivx(vely, xcoords)\n\tyy = derivy(vely, xcoords)\n\tjacob = np.zeros ((2,2))\n\talljacob = [[[] for j in range(320)] for i in range(320)]\n\n\tfor j in range(len(xx)):\n\n\t\tfor k in range(len(xx)):\n\n\t\t\tfor i in range(len(jacob)):\n\n\t\t\t\tfor c in range(len(jacob)):\n\n\t\t\t\t\tif c == 0 and i == 0:\n\t\t\t\t\t\tjacob [i][c] = xx[j][k] \n\n\t\t\t\t\telif c == 1 and i == 0:\n\t\t\t\t\t\tjacob[i][c] = xy[j][k]\n\n\t\t\t\t\telif c ==1 and i == 1:\n\t\t\t\t\t\tjacob[i][c] = yy[j][k]\n\n\t\t\t\talljacob[j][k] = jacob\n\talljacob = np.array(alljacob)\t\t\t\n\treturn alljacob\n\n\n#obtains eigenvalues for all points' jacobian matrices and then checks the extrema\ndef evals(alljacob):\n\teigen = [[[] for j in range(320)] for i in range(320)]\n\textrema = np.zeros((320,320))\n\n\tfor j in range(len(alljacob)):\n\n\t\tfor k in range(len(alljacob)):\n\n\t\t\tx = alljacob[j,k]\n\t\t\teigen[j][k] = LA.eigvalsh(x)\n\t\t\ty = eigen [j][k]\n\n\t\t\tif y[0]>0 and y[1] > 0:\n\t\t\t\textrema[j,k] = 2\n\n\t\t\telif y[0]<0 and y[1] <0:\n\t\t\t\textrema[j,k] = -2\n\n\t\t\telif y[0]*y[1] < 0:\n\t\t\t\textrema[j,k] = 3\n\n\treturn extrema\n\n#loads files and calls hess function\ndef main():\n\tds = yt.load(\"kh_mhd_Ma=0.803333333333At=0.0hdf5_chk_0000\")\n\tad = ds.covering_grid(level=0, left_edge=ds.index.grids[0].LeftEdge, \n\t\t\t\t\t\t\t\t\t\tdims=ds.domain_dimensions)\n\n\txcoords = np.array(ad[\"x\"])\n\tycoords = np.array(ad[\"y\"])\n\tvelx = np.array(ad[\"velx\"])\n\tvely = np.array(ad[\"vely\"])\n\n\tds1 = yt.load(\"kh_mhd_Ma=0.803333333333At=0.0hdf5_chk_0001\")\n\tdd = ds1.covering_grid(level=0, left_edge=ds1.index.grids[0].LeftEdge, dims=ds1.domain_dimensions)\n\n\txcoords1 = np.array(dd[\"x\"])\n\tycoords1 = np.array(dd[\"y\"])\n\tvelx1 = np.array(dd[\"velx\"])\n\tvely1 = np.array(dd[\"vely\"])\n\n\t#creates Hessian matrix for x velocity for file 1\n\textrema(hess(xcoords, ycoords, velx), velx, vely, xcoords)\n\n\t#creates Hessian marix for y velocity for file 1\n\t(extrema(hess(xcoords, ycoords, vely), velx, vely, xcoords))\n\n\t#prints extrema for file1\n\tprint(evals(jacobian(xcoords, velx, vely)))\n\t'''plt.figure()\n\tplt.scatter(xcoords, ycoords,c=evals(jacobian(xcoords, velx, vely)), marker= 'o',edgecolor='none')\n\tcb = plt.colorbar()\n\tcb.set_label('Extrema')\n\tplt.show()'''\n\n\nmain()", "original_line_number": 243, "instruction_length": 1089}
{"instruction": "The given code is a Python module that provides functions to interact with video4linux2 (v4l2) devices. To use this module, import it into your Python script and call the functions as needed. The functions include:\n\n- `list_devices()`: lists all v4l2 devices currently connected to the system.\n- `list_resolutions(device)`: lists all supported resolutions for a given v4l2 device.\n- `get_brightness(device)`: gets the current brightness value for a given v4l2 device.\n- `set_brightness(device, value)`: sets the brightness value for a given v4l2 device.\n- `get_contrast(device)`: gets the current contrast value for a given v4l2 device.\n- `set_contrast(device, value)`: sets the contrast value for a given v4l2 device.\n- `get_saturation(device)`: gets the current saturation value for a given v4l2 device.\n- `set_saturation(device, value)`: sets the saturation value for a given v4l2 device.\n- `get_hue(device)`: gets the current hue value for a given v4l2 device.\n- `set_hue(device, value)`: sets the hue value for a given v4l2 device.", "input": "", "output": "# Copyright (c) 2013 Calin Crisan\n# This file is part of motionEye.\n#\n# motionEye is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>. \n\nimport fcntl\nimport logging\nimport os.path\nimport re\nimport stat\nimport subprocess\nimport time\nimport utils\n\n\n_resolutions_cache = {}\n_ctrls_cache = {}\n_ctrl_values_cache = {}\n\n_DEV_V4L_BY_ID = '/dev/v4l/by-id/'\n\n\ndef find_v4l2_ctl():\n    try:\n        return subprocess.check_output('which v4l2-ctl', shell=True).strip()\n    \n    except subprocess.CalledProcessError: # not found\n        return None\n\n\ndef list_devices():\n    global _resolutions_cache, _ctrls_cache, _ctrl_values_cache\n    \n    logging.debug('listing v4l2 devices...')\n    \n    try:\n        output = ''\n        started = time.time()\n        p = subprocess.Popen('v4l2-ctl --list-devices 2>/dev/null', shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n        fd = p.stdout.fileno()\n        fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n        fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n        while True:\n            try:\n                data = p.stdout.read(1024)\n                if not data:\n                    break\n            \n            except IOError:\n                data = ''\n                time.sleep(0.01)\n\n            output += data\n\n            if len(output) > 10240:\n                logging.warn('v4l2-ctl command returned more than 10k of output')\n                break\n            \n            if time.time() - started > 3:\n                logging.warn('v4l2-ctl command ran for more than 3 seconds')\n                break\n\n    except subprocess.CalledProcessError:\n        logging.debug('failed to list devices (probably no devices installed)')\n        return []\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n    name = None\n    devices = []\n    for line in output.split('\\n'):\n        if line.startswith('\\t'):\n            device = line.strip()\n            persistent_device = find_persistent_device(device)\n            devices.append((device, persistent_device, name))\n        \n            logging.debug('found device %(name)s: %(device)s, %(persistent_device)s' % {\n                    'name': name, 'device': device, 'persistent_device': persistent_device})\n\n        else:\n            name = line.split('(')[0].strip()\n    \n    # clear the cache\n    _resolutions_cache = {}\n    _ctrls_cache = {}\n    _ctrl_values_cache = {}\n\n    return devices\n\n\ndef list_resolutions(device):\n    global _resolutions_cache\n    \n    device = utils.make_str(device)\n    \n    if device in _resolutions_cache:\n        return _resolutions_cache[device]\n    \n    logging.debug('listing resolutions of device %(device)s...' % {'device': device})\n    \n    resolutions = set()\n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --list-formats-ext | grep -vi stepwise | grep -oE \"[0-9]+x[0-9]+\" || true' % {\n            'device': device}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n\n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n        \n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n    \n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n    \n    except:\n        pass # nevermind\n\n    for pair in output.split('\\n'):\n        pair = pair.strip()\n        if not pair:\n            continue\n        \n        width, height = pair.split('x')\n        width = int(width)\n        height = int(height)\n\n        if (width, height) in resolutions:\n            continue # duplicate resolution\n\n        if width < 96 or height < 96: # some reasonable minimal values\n            continue\n        \n        if width % 16 or height % 16: # ignore non-modulo 16 resolutions\n            continue\n\n        resolutions.add((width, height))\n        \n        logging.debug('found resolution %(width)sx%(height)s for device %(device)s' % {\n                'device': device, 'width': width, 'height': height})\n    \n    if not resolutions:\n        logging.debug('no resolutions found for device %(device)s, using common values' % {'device': device})\n\n        # no resolution returned by v4l2-ctl call, add common default resolutions\n        resolutions = utils.COMMON_RESOLUTIONS\n\n    resolutions = list(sorted(resolutions, key=lambda r: (r[0], r[1])))\n    _resolutions_cache[device] = resolutions\n    \n    return resolutions\n\n\ndef device_present(device):\n    device = utils.make_str(device)\n    \n    try:\n        st = os.stat(device)\n        return stat.S_ISCHR(st.st_mode)\n\n    except:\n        return False\n    \n\ndef find_persistent_device(device):\n    device = utils.make_str(device)\n    \n    try:\n        devs_by_id = os.listdir(_DEV_V4L_BY_ID)\n\n    except OSError:\n        return device\n    \n    for p in devs_by_id:\n        p = os.path.join(_DEV_V4L_BY_ID, p)\n        if os.path.realpath(p) == device:\n            return p\n    \n    return device\n\n\ndef get_brightness(device):\n    return _get_ctrl(device, 'brightness')\n\n\ndef set_brightness(device, value):\n    _set_ctrl(device, 'brightness', value)\n\n\ndef get_contrast(device):\n    return _get_ctrl(device, 'contrast')\n\n\ndef set_contrast(device, value):\n    _set_ctrl(device, 'contrast', value)\n\n\ndef get_saturation(device):\n    return _get_ctrl(device, 'saturation')\n\n\ndef set_saturation(device, value):\n    _set_ctrl(device, 'saturation', value)\n\n\ndef get_hue(device):\n    return _get_ctrl(device, 'hue')\n\n\ndef set_hue(device, value):\n    _set_ctrl(device, 'hue', value)\n\n\ndef _get_ctrl(device, control):\n    global _ctrl_values_cache\n    \n    device = utils.make_str(device)\n    \n    if not device_present(device):\n        return None\n    \n    if device in _ctrl_values_cache and control in _ctrl_values_cache[device]:\n        return _ctrl_values_cache[device][control]\n    \n    controls = _list_ctrls(device)\n    properties = controls.get(control)\n    if properties is None:\n        logging.warn('control %(control)s not found for device %(device)s' % {\n                'control': control, 'device': device})\n        \n        return None\n    \n    value = int(properties['value'])\n    \n    # adjust the value range\n    if 'min' in properties and 'max' in properties:\n        min_value = int(properties['min'])\n        max_value = int(properties['max'])\n        \n        value = int(round((value - min_value) * 100.0 / (max_value - min_value)))\n    \n    else:\n        logging.warn('min and max values not found for control %(control)s of device %(device)s' % {\n                'control': control, 'device': device})\n    \n    logging.debug('control %(control)s of device %(device)s is %(value)s%%' % {\n            'control': control, 'device': device, 'value': value})\n    \n    return value\n\n\ndef _set_ctrl(device, control, value):\n    global _ctrl_values_cache\n    \n    device = utils.make_str(device)\n    \n    if not device_present(device):\n        return\n\n    controls = _list_ctrls(device)\n    properties = controls.get(control)\n    if properties is None:\n        logging.warn('control %(control)s not found for device %(device)s' % {\n                'control': control, 'device': device})\n        \n        return\n    \n    _ctrl_values_cache.setdefault(device, {})[control] = value\n\n    # adjust the value range\n    if 'min' in properties and 'max' in properties:\n        min_value = int(properties['min'])\n        max_value = int(properties['max'])\n        \n        value = int(round(min_value + value * (max_value - min_value) / 100.0))\n    \n    else:\n        logging.warn('min and max values not found for control %(control)s of device %(device)s' % {\n                'control': control, 'device': device})\n    \n    logging.debug('setting control %(control)s of device %(device)s to %(value)s' % {\n            'control': control, 'device': device, 'value': value})\n\n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --set-ctrl %(control)s=%(value)s' % {\n            'device': device, 'control': control, 'value': value}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n        \n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n\n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n\ndef _list_ctrls(device):\n    global _ctrls_cache\n    \n    device = utils.make_str(device)\n\n    if device in _ctrls_cache:\n        return _ctrls_cache[device]\n    \n    output = ''\n    started = time.time()\n    p = subprocess.Popen('v4l2-ctl -d \"%(device)s\" --list-ctrls' % {\n            'device': device}, shell=True, stdout=subprocess.PIPE, bufsize=1)\n\n    fd = p.stdout.fileno()\n    fl = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)\n\n    while True:\n        try:\n            data = p.stdout.read(1024)\n            if not data:\n                break\n        \n        except IOError:\n            data = ''\n            time.sleep(0.01)\n\n        output += data\n\n        if len(output) > 10240:\n            logging.warn('v4l2-ctl command returned more than 10k of output')\n            break\n\n        if time.time() - started > 3:\n            logging.warn('v4l2-ctl command ran for more than 3 seconds')\n            break\n\n    try:\n        # try to kill the v4l2-ctl subprocess\n        p.kill()\n\n    except:\n        pass # nevermind\n\n    controls = {}\n    for line in output.split('\\n'):\n        if not line:\n            continue\n        \n        match = re.match('^\\s*(\\w+)\\s+\\(\\w+\\)\\s+\\:\\s*(.+)', line)\n        if not match:\n            continue\n        \n        (control, properties) = match.groups()\n        properties = dict([v.split('=', 1) for v in properties.split(' ') if v.count('=')])\n        controls[control] = properties\n    \n    _ctrls_cache[device] = controls\n    \n    return controls", "original_line_number": 480, "instruction_length": 1036}
{"instruction": "The given code contains several classes that perform different tasks. To use these classes, you need to create an instance of the desired class and call its main method. For example, to add JavaScript to a PDF document, create an instance of the AddJavascript class, passing the path to the input PDF file as a parameter. Then call the main method of the class. The output PDF file with the added JavaScript will be saved in the same directory as the input file. Similarly, you can use other classes to perform different tasks such as adding a table of contents, optimizing a PDF file, removing metadata, setting PDF document information, etc.", "input": "", "output": "__author__ = 'fahadadeel'\r\nimport jpype\r\nimport re\r\nimport datetime\r\n\r\nclass AddJavascript:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n        self.JavascriptAction=jpype.JClass(\"com.aspose.pdf.JavascriptAction\")\r\n\r\n    def main(self):\r\n\r\n        # Open a pdf document.\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'Template.pdf'\r\n\r\n\r\n        # Adding JavaScript at Document Level\r\n        # Instantiate JavascriptAction with desried JavaScript statement\r\n        javaScript = self.JavascriptAction(\"this.print({bUI:true,bSilent:false,bShrinkToFit:true});\");\r\n\r\n        # Assign JavascriptAction object to desired action of Document\r\n        doc.setOpenAction(javaScript)\r\n        js=self.JavascriptAction(\"app.alert('page 2 is opened')\")\r\n\r\n        # Adding JavaScript at Page Level\r\n        doc.getPages.get_Item(2)\r\n        doc.getActions().setOnOpen(js())\r\n        doc.getPages().get_Item(2).getActions().setOnClose(self.JavascriptAction(\"app.alert('page 2 is closed')\"))\r\n\r\n        # Save PDF Document\r\n        doc.save(self.dataDir + \"JavaScript-Added.pdf\")\r\n\r\n        print \"Added JavaScript Successfully, please check the output file.\"\r\n\r\nclass AddToc:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n        self.TocInfo=jpype.JClass(\"com.aspose.pdf.TocInfo\")\r\n        self.TextFragment=jpype.JClass(\"com.aspose.pdf.TextFragment\")\r\n        self.TextSegment=jpype.JClass(\"com.aspose.pdf.TextSegment\")\r\n        self.Heading=jpype.JClass(\"com.aspose.pdf.Heading\")\r\n\r\n    def main(self):\r\n        # Open a pdf document.\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get access to first page of PDF file\r\n        toc_page = doc.getPages().insert(1)\r\n\r\n        # Create object to represent TOC information\r\n        toc_info = self.TocInfo()\r\n        title = self.TextFragment(\"Table Of Contents\")\r\n        title.getTextState().setFontSize(20)\r\n\r\n        # Set the title for TOC\r\n        toc_info.setTitle(title)\r\n        toc_page.setTocInfo(toc_info)\r\n\r\n        # Create string objects which will be used as TOC elements\r\n        titles = [\"First page\", \"Second page\"]\r\n\r\n        i = 0;\r\n        while (i < 2):\r\n            # Create Heading object\r\n            heading2 = self.Heading(1);\r\n\r\n            segment2 = self.TextSegment\r\n            heading2.setTocPage(toc_page)\r\n            heading2.getSegments().add(segment2)\r\n\r\n            # Specify the destination page for heading object\r\n            heading2.setDestinationPage(doc.getPages().get_Item(i + 2))\r\n\r\n            # Destination page\r\n            heading2.setTop(doc.getPages().get_Item(i + 2).getRect().getHeight())\r\n\r\n            # Destination coordinate\r\n            segment2.setText(titles[i])\r\n\r\n            # Add heading to page containing TOC\r\n            toc_page.getParagraphs().add(heading2)\r\n\r\n            i +=1;\r\n\r\n\r\n        # Save PDF Document\r\n        doc.save(self.dataDir + \"TOC.pdf\")\r\n\r\n        print \"Added TOC Successfully, please check the output file.\"\r\n\r\nclass GetDocumentWindow:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get different document properties\r\n        # Position of document's window - Default: false\r\n        print \"CenterWindow :- \" + str(doc.getCenterWindow())\r\n\r\n        # Predominant reading order; determine the position of page\r\n        # when displayed side by side - Default: L2R\r\n        print \"Direction :- \" + str(doc.getDirection())\r\n\r\n        # Whether window's title bar should display document title.\r\n        # If false, title bar displays PDF file name - Default: false\r\n        print \"DisplayDocTitle :- \" + str(doc.getDisplayDocTitle())\r\n\r\n        #Whether to resize the document's window to fit the size of\r\n        #first displayed page - Default: false\r\n        print \"FitWindow :- \" + str(doc.getFitWindow())\r\n\r\n        # Whether to hide menu bar of the viewer application - Default: false\r\n        print \"HideMenuBar :-\" + str(doc.getHideMenubar())\r\n\r\n        # Whether to hide tool bar of the viewer application - Default: false\r\n        print \"HideToolBar :-\" + str(doc.getHideToolBar())\r\n\r\n        # Whether to hide UI elements like scroll bars\r\n        # and leaving only the page contents displayed - Default: false\r\n        print \"HideWindowUI :-\" + str(doc.getHideWindowUI())\r\n\r\n        # The document's page mode. How to display document on exiting full-screen mode.\r\n        print \"NonFullScreenPageMode :-\" + str(doc.getNonFullScreenPageMode())\r\n\r\n        # The page layout i.e. single page, one column\r\n        print \"PageLayout :-\" + str(doc.getPageLayout())\r\n\r\n        #How the document should display when opened.\r\n        print \"pageMode :-\" + str(doc.getPageMode())\r\n\r\nclass GetPdfFileInfo:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get document information\r\n        doc_info = doc.getInfo();\r\n\r\n        # Show document information\r\n        print \"Author:-\" + str(doc_info.getAuthor())\r\n        print \"Creation Date:-\" + str(doc_info.getCreationDate())\r\n        print \"Keywords:-\" + str(doc_info.getKeywords())\r\n        print \"Modify Date:-\" + str(doc_info.getModDate())\r\n        print \"Subject:-\" + str(doc_info.getSubject())\r\n        print \"Title:-\" + str(doc_info.getTitle())\r\n\r\nclass GetXMPMetadata:\r\n    def __init__(self, dataDir):\r\n        self.dataDir = dataDir\r\n        self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n    def main(self):\r\n\r\n        doc= self.Document()\r\n        pdf = self.Document()\r\n        pdf=self.dataDir + 'input1.pdf'\r\n\r\n        # Get properties\r\n        print \"xmp:CreateDate: \" + str(doc.getMetadata().get_Item(\"xmp:CreateDate\"))\r\n        print \"xmp:Nickname: \" + str(doc.getMetadata().get_Item(\"xmp:Nickname\"))\r\n        print \"xmp:CustomProperty: \" + str(doc.getMetadata().get_Item(\"xmp:CustomProperty\"))\r\n\r\n\r\n\r\nclass Optimize:\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n#            self.OptimizationOptions=jpype.JClass(\"com.aspose.pdf.Document.OptimizationOptions\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            # Optimize for web\r\n            doc.optimize();\r\n\r\n            #Save output document\r\n            doc.save(self.dataDir + \"Optimized_Web.pdf\")\r\n\r\n            print \"Optimized PDF for the Web, please check output file.\"\r\n\r\nclass RemoveMetadata:\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            if (re.findall('/pdfaid:part/',doc.getMetadata())):\r\n                doc.getMetadata().removeItem(\"pdfaid:part\")\r\n\r\n\r\n            if (re.findall('/dc:format/',doc.getMetadata())):\r\n                doc.getMetadata().removeItem(\"dc:format\")\r\n\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"Remove_Metadata.pdf\")\r\n\r\n            print \"Removed metadata successfully, please check output file.\"\r\n\r\nclass SetExpiration:\r\n\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n            self.JavascriptAction=jpype.JClass(\"com.aspose.pdf.JavascriptAction\")\r\n\r\n        def main(self):\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            javascript = self.JavascriptAction(\r\n                \"var year=2014; var month=4;today = new Date();today = new Date(today.getFullYear(), today.getMonth());expiry = new Date(year, month);if (today.getTime() > expiry.getTime())app.alert('The file is expired. You need a new one.');\");\r\n\r\n            doc.setOpenAction(javascript);\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"set_expiration.pdf\");\r\n\r\n            print \"Update document information, please check output file.\"\r\n\r\n\r\nclass SetPdfFileInfo:\r\n\r\n        def __init__(self, dataDir):\r\n            self.dataDir = dataDir\r\n            self.Document = jpype.JClass(\"com.aspose.pdf.Document\")\r\n\r\n        def main(self):\r\n\r\n            doc= self.Document()\r\n            pdf = self.Document()\r\n            pdf=self.dataDir + 'input1.pdf'\r\n\r\n            # Get document information\r\n            doc_info = doc.getInfo();\r\n\r\n            doc_info.setAuthor(\"Aspose.Pdf for java\");\r\n            doc_info.setCreationDate(datetime.today.strftime(\"%m/%d/%Y\"));\r\n            doc_info.setKeywords(\"Aspose.Pdf, DOM, API\");\r\n            doc_info.setModDate(datetime.today.strftime(\"%m/%d/%Y\"));\r\n            doc_info.setSubject(\"PDF Information\");\r\n            doc_info.setTitle(\"Setting PDF Document Information\");\r\n\r\n            # save update document with new information\r\n            doc.save(self.dataDir + \"Updated_Information.pdf\")\r\n\r\n            print \"Update document information, please check output file.\"", "original_line_number": 873, "instruction_length": 643}
{"instruction": "The given code is a Python script that performs various natural language processing tasks on Twitter data. To use this script, you need to import the required libraries such as json, sklearn, nltk, wordcloud, matplotlib, and db. Then, you can define the DATASET_PATH variable to point to the location of the Twitter data file. The calc_frequencies function calculates the frequency of words in a list of words. The get_word_clouds function generates a word cloud for each user in a list of tweets. The save_wordcloud_image function saves a word cloud image to a file. The load_example_data function loads example Twitter data. The get_corpus_of_most_active_users function returns a corpus of tweets from the most active users. Finally, the main function calls the get_word_clouds function and saves the resulting word clouds to a database.", "input": "", "output": "import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport db\nimport os\n\nDATASET_PATH = os.environ['HOME'] + '/nltk_data/corpora/twitter_samples/tweets.20150430-223406.json'\ndef calc_frequencies(words, words_n=50, lang='german'):\n    words = [word for word in words if len(word) > 1]\n    words = [word for word in words if not word.isnumeric()]\n    words = [word.lower() for word in words]\n    # words = [word for word in words if word not in all_stopwords]\n    # Stemming words seems to make matters worse, disabled\n    # stemmer = nltk.stem.snowball.SnowballStemmer(lang)\n    # words = [stemmer.stem(word) for word in words]\n\n    fdist = nltk.FreqDist(words)\n    return fdist.most_common(words_n)\n\ndef get_word_clouds(tweets, users, words_n=50, lang='english'):\n    default_stopwords = set(nltk.corpus.stopwords.words(lang))\n    stopwords_file = '../data/stopwords.txt'\n    custom_stopwords = set(open(stopwords_file, 'r').read().splitlines())\n    all_stopwords = default_stopwords | custom_stopwords\n\n    vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=list(all_stopwords))\n    X = vectorizer.fit_transform(tweets)\n    terms = vectorizer.get_feature_names()\n\n    word_cloud_per_person = {}\n    for doc in range(len(tweets)):\n        feature_index = X[doc, :].nonzero()[1]\n        tfidf_scores = zip(feature_index, [X[doc, x] for x in feature_index])\n        doc_terms = []\n        for word, score in [(terms[i], score) for (i, score) in tfidf_scores]:\n            doc_terms.append((word, score))\n        important_terms = [(word, score) for word, score in sorted(doc_terms, key=lambda x: x[1], reverse=True)][:words_n]\n        word_cloud_per_person[users[doc]] = important_terms\n    return word_cloud_per_person\n\ndef save_wordcloud_image(frequencies, filename):\n    wordcloud = WordCloud(width=1024, height=786, min_font_size=1).fit_words(frequencies)\n    fig = plt.figure()\n    fig.set_figwidth(12)\n    fig.set_figheight(16)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.savefig(filename, facecolor='k', bbox_inches='tight')\n    print('imaged created')\n\ndef load_example_data():\n    tweets = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['text'])\n    return tweets\n\ndef get_corpus_of_most_active_users(n_users=5):\n    tweets = []\n    texts = []\n    with open(DATASET_PATH) as f:\n        for line in f:\n            tweets.append(json.loads(line)['user']['screen_name'])\n            texts.append((json.loads(line)['user']['screen_name'], json.loads(line)['text']))\n\n    users = nltk.FreqDist(tweets).most_common(n_users)\n\n    dict = {}\n    for user, tweet in texts:\n        if user in dict:\n            dict[user] = \" \".join([dict[user],tweet])\n        else:\n            dict[user] = tweet\n\n    corpus = [dict[name] for name, _ in users]\n    user_names = [name for name, _ in users]\n    return  corpus, user_names\n\nif __name__ == \"__main__\":\n    corpus, users = get_corpus_of_most_active_users()\n    word_cloud_per_person = get_word_clouds(corpus, users, words_n=100, lang='english')\n    for user in users:\n        topic_frequencies = word_cloud_per_person[user]\n        print user\n        print topic_frequencies\n        db.save_word_frequencies('test_user_seb', dict(topic_frequencies))\n        exit()\n        # save_wordcloud_image(dict(topic_frequencies), 'plots/word_clouds/' + user + '.png')\n\n\n    # This is an example how to save a word_cloud in the database\n    # user_in_db = 'malechanissen'\n    # db.save_word_frequencies(user_in_db, {'w3':10, 'w4':20})\n    # db.save_word_frequencies(user_in_db, dict(topic_frequencies))\n\n    # db.save_word_frequencies('test_user_seb', {'w3':10, 'w4':20})", "original_line_number": 1275, "instruction_length": 839}
{"instruction": "This code defines several classes for creating HTML modules in an edX course. The classes include HtmlFields, HtmlModule, HtmlDescriptor, AboutFields, AboutModule, AboutDescriptor, StaticTabFields, StaticTabModule, StaticTabDescriptor, CourseInfoFields, CourseInfoModule, and CourseInfoDescriptor. The classes define various fields and methods for creating and editing HTML content in a course, including loading and exporting HTML content, defining display names and help text, and specifying JavaScript and CSS resources.", "input": "", "output": "import copy\nfrom fs.errors import ResourceNotFoundError\nimport logging\nimport os\nimport sys\nfrom lxml import etree\nfrom path import path\n\nfrom pkg_resources import resource_string\nfrom xblock.fields import Scope, String, Boolean, List\nfrom xmodule.editing_module import EditingDescriptor\nfrom xmodule.html_checker import check_html\nfrom xmodule.stringify import stringify_children\nfrom xmodule.x_module import XModule\nfrom xmodule.xml_module import XmlDescriptor, name_to_pathname\nimport textwrap\nfrom xmodule.contentstore.content import StaticContent\nfrom xblock.core import XBlock\n\nlog = logging.getLogger(\"edx.courseware\")\n\n\nclass HtmlFields(object):\n    display_name = String(\n        display_name=\"Display Name\",\n        help=\"This name appears in the horizontal navigation at the top of the page.\",\n        scope=Scope.settings,\n        # it'd be nice to have a useful default but it screws up other things; so,\n        # use display_name_with_default for those\n        default=\"Text\"\n    )\n    data = String(help=\"Html contents to display for this module\", default=u\"\", scope=Scope.content)\n    source_code = String(help=\"Source code for LaTeX documents. This feature is not well-supported.\", scope=Scope.settings)\n    use_latex_compiler = Boolean(\n        help=\"Enable LaTeX templates?\",\n        default=False,\n        scope=Scope.settings\n    )\n\n\nclass HtmlModule(HtmlFields, XModule):\n    js = {\n        'coffee': [\n            resource_string(__name__, 'js/src/javascript_loader.coffee'),\n            resource_string(__name__, 'js/src/collapsible.coffee'),\n            resource_string(__name__, 'js/src/html/display.coffee')\n        ],\n        'js': [\n            resource_string(__name__, 'js/src/html/imageModal.js'),\n            resource_string(__name__, 'js/common_static/js/vendor/draggabilly.pkgd.js')\n        ]\n    }\n    js_module_name = \"HTMLModule\"\n    css = {'scss': [resource_string(__name__, 'css/html/display.scss')]}\n\n    def get_html(self):\n        if self.system.anonymous_student_id:\n            return self.data.replace(\"%%USER_ID%%\", self.system.anonymous_student_id)\n        return self.data\n\n\nclass HtmlDescriptor(HtmlFields, XmlDescriptor, EditingDescriptor):\n    \"\"\"\n    Module for putting raw html in a course\n    \"\"\"\n    mako_template = \"widgets/html-edit.html\"\n    module_class = HtmlModule\n    filename_extension = \"xml\"\n    template_dir_name = \"html\"\n\n    js = {'coffee': [resource_string(__name__, 'js/src/html/edit.coffee')]}\n    js_module_name = \"HTMLEditingDescriptor\"\n    css = {'scss': [resource_string(__name__, 'css/editor/edit.scss'), resource_string(__name__, 'css/html/edit.scss')]}\n\n    # VS[compat] TODO (cpennington): Delete this method once all fall 2012 course\n    # are being edited in the cms\n    @classmethod\n    def backcompat_paths(cls, path):\n        if path.endswith('.html.xml'):\n            path = path[:-9] + '.html'  # backcompat--look for html instead of xml\n        if path.endswith('.html.html'):\n            path = path[:-5]  # some people like to include .html in filenames..\n        candidates = []\n        while os.sep in path:\n            candidates.append(path)\n            _, _, path = path.partition(os.sep)\n\n        # also look for .html versions instead of .xml\n        nc = []\n        for candidate in candidates:\n            if candidate.endswith('.xml'):\n                nc.append(candidate[:-4] + '.html')\n        return candidates + nc\n\n    @classmethod\n    def filter_templates(cls, template, course):\n        \"\"\"\n        Filter template that contains 'latex' from templates.\n\n        Show them only if use_latex_compiler is set to True in\n        course settings.\n        \"\"\"\n        return (not 'latex' in template['template_id'] or course.use_latex_compiler)\n\n    def get_context(self):\n        \"\"\"\n        an override to add in specific rendering context, in this case we need to\n        add in a base path to our c4x content addressing scheme\n        \"\"\"\n        _context = EditingDescriptor.get_context(self)\n        # Add some specific HTML rendering context when editing HTML modules where we pass\n        # the root /c4x/ url for assets. This allows client-side substitutions to occur.\n        _context.update({\n            'base_asset_url': StaticContent.get_base_url_path_for_course_assets(self.location) + '/',\n            'enable_latex_compiler': self.use_latex_compiler,\n        })\n        return _context\n\n    # NOTE: html descriptors are special.  We do not want to parse and\n    # export them ourselves, because that can break things (e.g. lxml\n    # adds body tags when it exports, but they should just be html\n    # snippets that will be included in the middle of pages.\n\n    @classmethod\n    def load_definition(cls, xml_object, system, location):\n        '''Load a descriptor from the specified xml_object:\n\n        If there is a filename attribute, load it as a string, and\n        log a warning if it is not parseable by etree.HTMLParser.\n\n        If there is not a filename attribute, the definition is the body\n        of the xml_object, without the root tag (do not want <html> in the\n        middle of a page)\n        '''\n        filename = xml_object.get('filename')\n        if filename is None:\n            definition_xml = copy.deepcopy(xml_object)\n            cls.clean_metadata_from_xml(definition_xml)\n            return {'data': stringify_children(definition_xml)}, []\n        else:\n            # html is special.  cls.filename_extension is 'xml', but\n            # if 'filename' is in the definition, that means to load\n            # from .html\n            # 'filename' in html pointers is a relative path\n            # (not same as 'html/blah.html' when the pointer is in a directory itself)\n            pointer_path = \"{category}/{url_path}\".format(\n                category='html',\n                url_path=name_to_pathname(location.name)\n            )\n            base = path(pointer_path).dirname()\n            # log.debug(\"base = {0}, base.dirname={1}, filename={2}\".format(base, base.dirname(), filename))\n            filepath = \"{base}/{name}.html\".format(base=base, name=filename)\n            # log.debug(\"looking for html file for {0} at {1}\".format(location, filepath))\n\n            # VS[compat]\n            # TODO (cpennington): If the file doesn't exist at the right path,\n            # give the class a chance to fix it up. The file will be written out\n            # again in the correct format.  This should go away once the CMS is\n            # online and has imported all current (fall 2012) courses from xml\n            if not system.resources_fs.exists(filepath):\n                candidates = cls.backcompat_paths(filepath)\n                # log.debug(\"candidates = {0}\".format(candidates))\n                for candidate in candidates:\n                    if system.resources_fs.exists(candidate):\n                        filepath = candidate\n                        break\n\n            try:\n                with system.resources_fs.open(filepath) as file:\n                    html = file.read().decode('utf-8')\n                    # Log a warning if we can't parse the file, but don't error\n                    if not check_html(html) and len(html) > 0:\n                        msg = \"Couldn't parse html in {0}, content = {1}\".format(filepath, html)\n                        log.warning(msg)\n                        system.error_tracker(\"Warning: \" + msg)\n\n                    definition = {'data': html}\n\n                    # TODO (ichuang): remove this after migration\n                    # for Fall 2012 LMS migration: keep filename (and unmangled filename)\n                    definition['filename'] = [filepath, filename]\n\n                    return definition, []\n\n            except (ResourceNotFoundError) as err:\n                msg = 'Unable to load file contents at path {0}: {1} '.format(\n                    filepath, err)\n                # add more info and re-raise\n                raise Exception(msg), None, sys.exc_info()[2]\n\n    # TODO (vshnayder): make export put things in the right places.\n\n    def definition_to_xml(self, resource_fs):\n        ''' Write <html filename=\"\" [meta-attrs=\"...\"]> to filename.xml, and the html\n        string to filename.html.\n        '''\n\n        # Write html to file, return an empty tag\n        pathname = name_to_pathname(self.url_name)\n        filepath = u'{category}/{pathname}.html'.format(\n            category=self.category,\n            pathname=pathname\n        )\n\n        resource_fs.makedir(os.path.dirname(filepath), recursive=True, allow_recreate=True)\n        with resource_fs.open(filepath, 'w') as filestream:\n            html_data = self.data.encode('utf-8')\n            filestream.write(html_data)\n\n        # write out the relative name\n        relname = path(pathname).basename()\n\n        elt = etree.Element('html')\n        elt.set(\"filename\", relname)\n        return elt\n\n    @property\n    def non_editable_metadata_fields(self):\n        non_editable_fields = super(HtmlDescriptor, self).non_editable_metadata_fields\n        non_editable_fields.append(HtmlDescriptor.use_latex_compiler)\n        return non_editable_fields\n\n\nclass AboutFields(object):\n    display_name = String(\n        help=\"Display name for this module\",\n        scope=Scope.settings,\n        default=\"overview\",\n    )\n    data = String(\n        help=\"Html contents to display for this module\",\n        default=\"\",\n        scope=Scope.content\n    )\n\n\n@XBlock.tag(\"detached\")\nclass AboutModule(AboutFields, HtmlModule):\n    \"\"\"\n    Overriding defaults but otherwise treated as HtmlModule.\n    \"\"\"\n    pass\n\n\n@XBlock.tag(\"detached\")\nclass AboutDescriptor(AboutFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = \"about\"\n    module_class = AboutModule\n\n\nclass StaticTabFields(object):\n    \"\"\"\n    The overrides for Static Tabs\n    \"\"\"\n    display_name = String(\n        display_name=\"Display Name\",\n        help=\"This name appears in the horizontal navigation at the top of the page.\",\n        scope=Scope.settings,\n        default=\"Empty\",\n    )\n    data = String(\n        default=textwrap.dedent(\"\"\"\\\n            <p>This is where you can add additional pages to your courseware. Click the 'edit' button to begin editing.</p>\n        \"\"\"),\n        scope=Scope.content,\n        help=\"HTML for the additional pages\"\n    )\n\n\n@XBlock.tag(\"detached\")\nclass StaticTabModule(StaticTabFields, HtmlModule):\n    \"\"\"\n    Supports the field overrides\n    \"\"\"\n    pass\n\n\n@XBlock.tag(\"detached\")\nclass StaticTabDescriptor(StaticTabFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = None\n    module_class = StaticTabModule\n\n\nclass CourseInfoFields(object):\n    \"\"\"\n    Field overrides\n    \"\"\"\n    items = List(\n        help=\"List of course update items\",\n        default=[],\n        scope=Scope.content\n    )\n    data = String(\n        help=\"Html contents to display for this module\",\n        default=\"<ol></ol>\",\n        scope=Scope.content\n    )\n\n\n@XBlock.tag(\"detached\")\nclass CourseInfoModule(CourseInfoFields, HtmlModule):\n    \"\"\"\n    Just to support xblock field overrides\n    \"\"\"\n    # statuses\n    STATUS_VISIBLE = 'visible'\n    STATUS_DELETED = 'deleted'\n\n\n@XBlock.tag(\"detached\")\nclass CourseInfoDescriptor(CourseInfoFields, HtmlDescriptor):\n    \"\"\"\n    These pieces of course content are treated as HtmlModules but we need to overload where the templates are located\n    in order to be able to create new ones\n    \"\"\"\n    template_dir_name = None\n    module_class = CourseInfoModule", "original_line_number": 1571, "instruction_length": 523}
{"instruction": "Perform an update by calling the function 'perform_update' with the following arguments: 'endpoint' (the URL of the update endpoint), 'current_version' (the current version of the application), 'app_name' (optional, the name of the application), 'password' (optional, the password to decrypt the update archive), 'update_available_callback' (optional, a callback function to be called when an update is available), 'progress_callback' (optional, a callback function to be called to report the progress of the update), and 'update_complete_callback' (optional, a callback function to be called when the update is complete).", "input": "", "output": "from logging import getLogger\nlogger = getLogger('update')\n\nimport contextlib\nimport io\nimport os\nimport platform\nimport requests\nimport tempfile\nfrom wxUI import commonMessageDialogs\nimport widgetUtils\nimport webbrowser\ntry:\n import czipfile as zipfile\nexcept ImportError:\n import zipfile\n\nfrom platform_utils import paths\n\ndef perform_update(endpoint, current_version, app_name='', password=None, update_available_callback=None, progress_callback=None, update_complete_callback=None):\n requests_session = create_requests_session(app_name=app_name, version=current_version)\n available_update = find_update(endpoint, requests_session=requests_session)\n if not available_update:\n  logger.debug(\"No update available\")\n  return False\n available_version = float(available_update['current_version'])\n if not float(available_version) > float(current_version) or platform.system()+platform.architecture()[0][:2] not in available_update['downloads']:\n  logger.debug(\"No update for this architecture\")\n  return False\n available_description = available_update.get('description', None)\n update_url = available_update ['downloads'][platform.system()+platform.architecture()[0][:2]]\n logger.info(\"A new update is available. Version %s\" % available_version)\n donation()\n if callable(update_available_callback) and not update_available_callback(version=available_version, description=available_description): #update_available_callback should return a falsy value to stop the process\n  logger.info(\"User canceled update.\")\n  return\n base_path = tempfile.mkdtemp()\n download_path = os.path.join(base_path, 'update.zip')\n update_path = os.path.join(base_path, 'update')\n downloaded = download_update(update_url, download_path, requests_session=requests_session, progress_callback=progress_callback)\n extracted = extract_update(downloaded, update_path, password=password)\n bootstrap_path = move_bootstrap(extracted)\n execute_bootstrap(bootstrap_path, extracted)\n logger.info(\"Update prepared for installation.\")\n if callable(update_complete_callback):\n  update_complete_callback()\n\ndef create_requests_session(app_name=None, version=None):\n user_agent = ''\n session = requests.session()\n if app_name:\n  user_agent = ' %s/%r' % (app_name, version)\n session.headers['User-Agent'] = session.headers['User-Agent'] + user_agent\n return session\n\ndef find_update(endpoint, requests_session):\n response = requests_session.get(endpoint)\n response.raise_for_status()\n content = response.json()\n return content\n\ndef download_update(update_url, update_destination, requests_session, progress_callback=None, chunk_size=io.DEFAULT_BUFFER_SIZE):\n total_downloaded = total_size = 0\n with io.open(update_destination, 'w+b') as outfile:\n  download = requests_session.get(update_url, stream=True)\n  total_size = int(download.headers.get('content-length', 0))\n  logger.debug(\"Total update size: %d\" % total_size)\n  download.raise_for_status()\n  for chunk in download.iter_content(chunk_size):\n   outfile.write(chunk)\n   total_downloaded += len(chunk)\n   if callable(progress_callback):\n    call_callback(progress_callback, total_downloaded, total_size)\n logger.debug(\"Update downloaded\")\n return update_destination\n\ndef extract_update(update_archive, destination, password=None):\n \"\"\"Given an update archive, extracts it. Returns the directory to which it has been extracted\"\"\"\n with contextlib.closing(zipfile.ZipFile(update_archive)) as archive:\n  if password:\n   archive.setpassword(password)\n  archive.extractall(path=destination)\n logger.debug(\"Update extracted\")\n return destination\n\ndef move_bootstrap(extracted_path):\n working_path = os.path.abspath(os.path.join(extracted_path, '..'))\n if platform.system() == 'Darwin':\n  extracted_path = os.path.join(extracted_path, 'Contents', 'Resources')\n downloaded_bootstrap = os.path.join(extracted_path, bootstrap_name())\n new_bootstrap_path = os.path.join(working_path, bootstrap_name())\n os.rename(downloaded_bootstrap, new_bootstrap_path)\n return new_bootstrap_path\n\ndef execute_bootstrap(bootstrap_path, source_path):\n arguments = r'\"%s\" \"%s\" \"%s\" \"%s\"' % (os.getpid(), source_path, paths.app_path(), paths.get_executable())\n if platform.system() == 'Windows':\n  import win32api\n  win32api.ShellExecute(0, 'open', bootstrap_path, arguments, '', 5)\n else:  \n  import subprocess\n  make_executable(bootstrap_path)\n  subprocess.Popen(['%s %s' % (bootstrap_path, arguments)], shell=True)\n logger.info(\"Bootstrap executed\")\n\ndef bootstrap_name():\n if platform.system() == 'Windows': return 'bootstrap.exe'\n if platform.system() == 'Darwin': return 'bootstrap-mac.sh'\n return 'bootstrap-lin.sh'\n\ndef make_executable(path):\n import stat\n st = os.stat(path)\n os.chmod(path, st.st_mode | stat.S_IEXEC)\n\ndef call_callback(callback, *args, **kwargs):\n# try:\n callback(*args, **kwargs)\n# except:\n#  logger.exception(\"Failed calling callback %r with args %r and kwargs %r\" % (callback, args, kwargs))\n\ndef donation():\n dlg = commonMessageDialogs.donation()\n if dlg == widgetUtils.YES:\n  webbrowser.open_new_tab(\"http://twblue.es/?q=donate\")", "original_line_number": 1878, "instruction_length": 622}
{"instruction": "To use the Watson Language Translation service, import the necessary modules and define the following functions: \"getTranslationService()\" to get the translation service, \"identifyLanguage()\" to identify the language of the input text, \"checkForTranslation()\" to check if translation is supported between the source and target languages, and \"performTranslation()\" to perform the actual translation. Make sure to replace the username and password keys with your own keys for the Watson Language Translation service.", "input": "", "output": "# -*- coding: utf-8 -*-\n# Copyright 2016 IBM Corp. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport json\n\nfrom watson_developer_cloud import LanguageTranslationV2 as LanguageTranslationService\n\n\ndef getTranslationService():\n  return LanguageTranslationService(username='<your username key for the Watson language translation service>',\n                                    password='<your password key for the service>')\n\n\ndef identifyLanguage(app, data):\n  txt = data.encode(\"utf-8\", \"replace\")\n  language_translation = getTranslationService()\n  langsdetected = language_translation.identify(txt)\n\n  app.logger.info(json.dumps(langsdetected, indent=2))\n  primarylang = langsdetected[\"languages\"][0]\n  retData = {key: primarylang[key] for key in ('language', 'confidence')}\n  app.logger.info(json.dumps(retData, indent=2))\n  return retData\n\n\ndef checkForTranslation(app, fromlang, tolang):\n  supportedModels = []\n  lt = getTranslationService()\n  models = lt.list_models()\n  modelList = models.get(\"models\")\n  supportedModels = [model['model_id'] for model in modelList\n                     if fromlang == model['source']\n                     and tolang == model['target']]\n  return supportedModels\n\n\ndef performTranslation(app, txt, primarylang, targetlang):\n  lt = getTranslationService()\n  translation = lt.translate(txt, source=primarylang, target=targetlang)\n  theTranslation = None\n  if translation and (\"translations\" in translation):\n    theTranslation = translation['translations'][0]['translation']\n  return theTranslation", "original_line_number": 1909, "instruction_length": 515}
{"instruction": "This code creates sample-specific co-expression networks for one fold and one repeat of a cross-validation for which fold indices have already been computed. To use it, run the script `setUpSubTypeStratifiedCV_computeNetworks.py` with the following arguments: `aces_dir`, `data_dir`, `fold`, and `repeat`. The data will be stored under `<data_dir>/repeat<repeat idx>` with the specified structure. The script reads expression data and sample labels from a file, and creates networks using the `CoExpressionNetwork` module.", "input": "", "output": "# @Author \n# Chloe-Agathe Azencott\n# chloe-agathe.azencott@mines-paristech.fr\n# April 2016\n\nimport argparse\nimport h5py\nimport numpy as np\nimport os\nimport sys\n\nimport CoExpressionNetwork\n\ndef main():\n    \"\"\" Create sample-specific co-expression networks for one fold and one repeat\n    of a cross-validation for which fold indices have already been computed.\n\n    The data will be stored under\n        <data_dir>/repeat<repeat idx>\n    with the following structure:\n        edges.gz: \n            Gzipped file containing the list of edges of the co-expression networks.\n            Each line is an undirected edge, formatted as:\n                <index of gene 1> <index of gene 2>\n            By convention, the index of gene 1 is smaller than that of gene 2.\n        For k=0..(numFolds-1):\n            <k>/lioness/edge_weights.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the LIONESS co-expression networks\n                for the training samples.\n            <k>/lioness/edge_weights_te.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the LIONESS co-expression networks\n                for the test samples.\n            <k>/regline/edge_weights.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the Regline co-expression networks\n                for the training samples.\n            <k>/regline/edge_weights_te.gz:\n                gzipped file containing the (self.numSamples, numEdges) array\n                describing the edge weights of the Regline co-expression networks\n                for the test samples.\n\n    Parameters\n    ----------\n    aces_dir: path\n        Path to the ACES folder.\n    data_dir: path\n        Path to the folder containing fold indices (under <data_dir>/repeat<repeat_idx>/fold<fold_idx>).\n    fold: int\n        Fold index.\n    repeat: int\n        Repeat index.\n\n    Example\n    -------\n        $ python setUpSubTypeStratifiedCV_computeNetworks.py ACES outputs/U133A_combat_RFS/subtype_stratified 0 0\n    \n    Reference\n    ---------\n    Allahyar, A., and Ridder, J. de (2015).\n    FERAL: network-based classifier with application to breast cancer outcome prediction.\n    Bioinformatics 31, i311--i319.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Build sample-specific co-expression networks\" + \\\n                                     \"for a 10-fold sub-type stratified CV on the RFS data\",\n                                     add_help=True)\n    parser.add_argument(\"aces_dir\", help=\"Path to ACES data\")\n    parser.add_argument(\"data_dir\", help=\"Path to the fold indices\")\n    parser.add_argument(\"fold\", help=\"Index of the fold\", type=int)\n    parser.add_argument(\"repeat\", help=\"Index of the repeat\", type=int)\n    args = parser.parse_args()\n\n    outDir = '%s/repeat%d' % (args.data_dir, args.repeat)\n\n    # Get expression data, sample labels.\n    # Do not normalize the data while loading it (so as not to use test data for normalization).\n    f = h5py.File(\"%s/experiments/data/U133A_combat.h5\" % args.aces_dir)\n    expressionData = np.array(f['U133A_combat_RFS']['ExpressionData'])\n    sampleLabels = np.array(f['U133A_combat_RFS']['PatientClassLabels'])\n    f.close()\n    \n    foldNr = args.fold\n    # Output directory\n    foldDir = \"%s/fold%d\" % (outDir, foldNr)\n\n    # Read train indices from file\n    trIndicesF = '%s/train.indices' % foldDir\n    trIndices = np.loadtxt(trIndicesF, dtype=int)\n    sys.stdout.write(\"Read training indices for fold %d from %s\\n\" % (foldNr, trIndicesF))\n\n    # Read test indices from file\n    teIndicesF = '%s/test.indices' % foldDir\n    teIndices = np.loadtxt(teIndicesF, dtype=int)\n    sys.stdout.write(\"Read training indices for fold %d from %s\\n\" % (foldNr, teIndicesF))\n    print teIndices\n    print teIndices.shape\n\n    # Create networks\n    CoExpressionNetwork.run_whole_data(expressionData, sampleLabels, foldDir,\n                                       trIndices=trIndices, teIndices=teIndices)\n\n\n\nif __name__ == \"__main__\":\n    main()", "original_line_number": 2094, "instruction_length": 522}
{"instruction": "This code is a controller for a differential drive. To use it, you need to create an instance of the DiffController class, passing in the device and name as arguments. Then, call the startup() method to initialize the controller. Finally, call the update() method in a loop to update the controller and handle movement requests. The cmdVelCb() method can be used to handle movement requests. The shutdown() method can be called to stop the controller. The getDiagnostics() method can be used to get a diagnostics status.", "input": "", "output": "#!/usr/bin/env python\n\n\"\"\"\n  diff_controller.py - controller for a differential drive\n  Copyright (c) 2010-2011 Vanadium Labs LLC.  All right reserved.\n\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n      * Redistributions of source code must retain the above copyright\n        notice, this list of conditions and the following disclaimer.\n      * Redistributions in binary form must reproduce the above copyright\n        notice, this list of conditions and the following disclaimer in the\n        documentation and/or other materials provided with the distribution.\n      * Neither the name of Vanadium Labs LLC nor the names of its \n        contributors may be used to endorse or promote products derived \n        from this software without specific prior written permission.\n  \n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n  ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL VANADIUM LABS BE LIABLE FOR ANY DIRECT, INDIRECT,\n  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,\n  OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n  OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\n  ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport rospy\n\nfrom math import sin,cos,pi\n\nfrom geometry_msgs.msg import Quaternion\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg import Odometry\nfrom diagnostic_msgs.msg import *\nfrom tf.broadcaster import TransformBroadcaster\n\nfrom ax12 import *\nfrom controllers import *\nfrom struct import unpack\n\nclass DiffController(Controller):\n    \"\"\" Controller to handle movement & odometry feedback for a differential \n            drive mobile base. \"\"\"\n    def __init__(self, device, name):\n        Controller.__init__(self, device, name)\n        self.pause = True\n        self.last_cmd = rospy.Time.now()\n\n        # parameters: rates and geometry\n        self.rate = rospy.get_param('~controllers/'+name+'/rate',10.0)\n        self.timeout = rospy.get_param('~controllers/'+name+'/timeout',1.0)\n        self.t_delta = rospy.Duration(1.0/self.rate)\n        self.t_next = rospy.Time.now() + self.t_delta\n        self.ticks_meter = float(rospy.get_param('~controllers/'+name+'/ticks_meter'))\n        self.base_width = float(rospy.get_param('~controllers/'+name+'/base_width'))\n\n        self.base_frame_id = rospy.get_param('~controllers/'+name+'/base_frame_id', 'base_link')\n        self.odom_frame_id = rospy.get_param('~controllers/'+name+'/odom_frame_id', 'odom')\n\n        # parameters: PID\n        self.Kp = rospy.get_param('~controllers/'+name+'/Kp', 5)\n        self.Kd = rospy.get_param('~controllers/'+name+'/Kd', 1)\n        self.Ki = rospy.get_param('~controllers/'+name+'/Ki', 0)\n        self.Ko = rospy.get_param('~controllers/'+name+'/Ko', 50)\n\n        # parameters: acceleration\n        self.accel_limit = rospy.get_param('~controllers/'+name+'/accel_limit', 0.1)\n        self.max_accel = int(self.accel_limit*self.ticks_meter/self.rate)\n\n        # output for joint states publisher\n        self.joint_names = [\"base_l_wheel_joint\",\"base_r_wheel_joint\"]\n        self.joint_positions = [0,0]\n        self.joint_velocities = [0,0]\n\n        # internal data            \n        self.v_left = 0                 # current setpoint velocity\n        self.v_right = 0\n        self.v_des_left = 0             # cmd_vel setpoint\n        self.v_des_right = 0\n        self.enc_left = None            # encoder readings\n        self.enc_right = None\n        self.x = 0                      # position in xy plane\n        self.y = 0\n        self.th = 0\n        self.dx = 0                     # speeds in x/rotation\n        self.dr = 0\n        self.then = rospy.Time.now()    # time for determining dx/dy\n\n        # subscriptions\n        rospy.Subscriber(\"cmd_vel\", Twist, self.cmdVelCb)\n        self.odomPub = rospy.Publisher(\"odom\", Odometry, queue_size=5)\n        self.odomBroadcaster = TransformBroadcaster()\n\t\t\n        rospy.loginfo(\"Started DiffController (\"+name+\"). Geometry: \" + str(self.base_width) + \"m wide, \" + str(self.ticks_meter) + \" ticks/m.\")\n\n    def startup(self):\n        if not self.fake:\n            self.setup(self.Kp,self.Kd,self.Ki,self.Ko) \n    \n    def update(self):\n        now = rospy.Time.now()\n        if now > self.t_next:\n            elapsed = now - self.then\n            self.then = now\n            elapsed = elapsed.to_sec()\n\n            if self.fake:\n                x = cos(self.th)*self.dx*elapsed\n                y = -sin(self.th)*self.dx*elapsed\n                self.x += cos(self.th)*self.dx*elapsed\n                self.y += sin(self.th)*self.dx*elapsed\n                self.th += self.dr*elapsed\n            else:\n                # read encoders\n                try:\n                    left, right = self.status()\n                except Exception as e:\n                    rospy.logerr(\"Could not update encoders: \" + str(e))\n                    return\n                rospy.logdebug(\"Encoders: \" + str(left) +\",\"+ str(right))\n\n                # calculate odometry\n                if self.enc_left == None:\n                    d_left = 0\n                    d_right = 0\n                else:\n                    d_left = (left - self.enc_left)/self.ticks_meter\n                    d_right = (right - self.enc_right)/self.ticks_meter\n                self.enc_left = left\n                self.enc_right = right\n\n                d = (d_left+d_right)/2\n                th = (d_right-d_left)/self.base_width\n                self.dx = d / elapsed\n                self.dr = th / elapsed\n\n                if (d != 0):\n                    x = cos(th)*d\n                    y = -sin(th)*d\n                    self.x = self.x + (cos(self.th)*x - sin(self.th)*y)\n                    self.y = self.y + (sin(self.th)*x + cos(self.th)*y)\n                if (th != 0):\n                    self.th = self.th + th\n\n            # publish or perish\n            quaternion = Quaternion()\n            quaternion.x = 0.0 \n            quaternion.y = 0.0\n            quaternion.z = sin(self.th/2)\n            quaternion.w = cos(self.th/2)\n            self.odomBroadcaster.sendTransform(\n                (self.x, self.y, 0), \n                (quaternion.x, quaternion.y, quaternion.z, quaternion.w),\n                rospy.Time.now(),\n                self.base_frame_id,\n                self.odom_frame_id\n                )\n\n            odom = Odometry()\n            odom.header.stamp = now\n            odom.header.frame_id = self.odom_frame_id\n            odom.pose.pose.position.x = self.x\n            odom.pose.pose.position.y = self.y\n            odom.pose.pose.position.z = 0\n            odom.pose.pose.orientation = quaternion\n            odom.child_frame_id = self.base_frame_id\n            odom.twist.twist.linear.x = self.dx\n            odom.twist.twist.linear.y = 0\n            odom.twist.twist.angular.z = self.dr\n            self.odomPub.publish(odom)\n\n            if now > (self.last_cmd + rospy.Duration(self.timeout)):\n                self.v_des_left = 0\n                self.v_des_right = 0\n\n            # update motors\n            if not self.fake:\n                if self.v_left < self.v_des_left:\n                    self.v_left += self.max_accel\n                    if self.v_left > self.v_des_left:\n                        self.v_left = self.v_des_left\n                else:\n                    self.v_left -= self.max_accel\n                    if self.v_left < self.v_des_left:\n                        self.v_left = self.v_des_left\n                \n                if self.v_right < self.v_des_right:\n                    self.v_right += self.max_accel\n                    if self.v_right > self.v_des_right:\n                        self.v_right = self.v_des_right\n                else:\n                    self.v_right -= self.max_accel\n                    if self.v_right < self.v_des_right:\n                        self.v_right = self.v_des_right\n                self.write(self.v_left, self.v_right)\n\n            self.t_next = now + self.t_delta\n \n    def shutdown(self):\n        if not self.fake:\n            self.write(0,0)\n\n    def cmdVelCb(self,req):\n        \"\"\" Handle movement requests. \"\"\"\n        self.last_cmd = rospy.Time.now()\n        if self.fake:\n            self.dx = req.linear.x        # m/s\n            self.dr = req.angular.z       # rad/s\n        else:\n            # set motor speeds in ticks per 1/30s\n            self.v_des_left = int( ((req.linear.x - (req.angular.z * self.base_width/2.0)) * self.ticks_meter) / 30.0)\n            self.v_des_right = int( ((req.linear.x + (req.angular.z * self.base_width/2.0)) * self.ticks_meter) / 30.0)\n\n    def getDiagnostics(self):\n        \"\"\" Get a diagnostics status. \"\"\"\n        msg = DiagnosticStatus()\n        msg.name = self.name\n        msg.level = DiagnosticStatus.OK\n        msg.message = \"OK\"\n        if not self.fake:\n            msg.values.append(KeyValue(\"Left\", str(self.enc_left)))\n            msg.values.append(KeyValue(\"Right\", str(self.enc_right)))\n        msg.values.append(KeyValue(\"dX\", str(self.dx)))\n        msg.values.append(KeyValue(\"dR\", str(self.dr)))\n        return msg\n\n    ###\n    ### Controller Specification: \n    ###\n    ###  setup: Kp, Kd, Ki, Ko (all unsigned char)\n    ###\n    ###  write: left_speed, right_speed (2-byte signed, ticks per frame)\n    ###\n    ###  status: left_enc, right_enc (4-byte signed)\n    ### \n    \n    def setup(self, kp, kd, ki, ko):\n        success = self.device.execute(253, AX_CONTROL_SETUP, [10, kp, kd, ki, ko])\n\n    def write(self, left, right):\n        \"\"\" Send a closed-loop speed. Base PID loop runs at 30Hz, these values\n                are therefore in ticks per 1/30 second. \"\"\"\n        left = left&0xffff\n        right = right&0xffff\n        success = self.device.execute(253, AX_CONTROL_WRITE, [10, left%256, left>>8, right%256, right>>8])\n\n    def status(self):\n        \"\"\" read 32-bit (signed) encoder values. \"\"\"\n        values = self.device.execute(253, AX_CONTROL_STAT, [10])\n        left_values = \"\".join([chr(k) for k in values[0:4] ])        \n        right_values = \"\".join([chr(k) for k in values[4:] ])\n        try:\n            left = unpack('=l',left_values)[0]\n            right = unpack('=l',right_values)[0]\n            return [left, right]\n        except:\n            return None", "original_line_number": 3199, "instruction_length": 520}
{"instruction": "The given code is a Python script that defines a SupportTicket class and several functions. To use this code, import the necessary modules and create an instance of the SupportTicket class. The class has several methods that can be called to get the sender, subject, and content of a support ticket, as well as to update the status and send an email notification. There are also several helper functions for setting the status of a ticket, assigning tickets to users, and re-enabling user accounts. To use these functions, call them with the appropriate arguments.", "input": "", "output": "# Copyright (c) 2013, Web Notes Technologies Pvt. Ltd. and Contributors\n# License: GNU General Public License v3. See license.txt\n\nfrom __future__ import unicode_literals\nimport frappe\n\nfrom erpnext.utilities.transaction_base import TransactionBase\nfrom frappe.utils import now, extract_email_id\nimport json\nimport requests\n\n\nSTANDARD_USERS = (\"Guest\", \"Administrator\")\n\nclass SupportTicket(TransactionBase):\n\n\tdef get_sender(self, comm):\n\t\treturn frappe.db.get_value('Support Email Settings',None,'support_email')\n\n\tdef get_subject(self, comm):\n\t\treturn '[' + self.name + '] ' + (comm.subject or 'No Subject Specified')\n\n\tdef get_content(self, comm):\n\t\tsignature = frappe.db.get_value('Support Email Settings',None,'support_signature')\n\t\tcontent = comm.content\n\t\tif signature:\n\t\t\tcontent += '<p>' + signature + '</p>'\n\t\treturn content\n\n\tdef get_portal_page(self):\n\t\treturn \"ticket\"\n\n\tdef on_update1(self):\n\t\tfrom frappe.utils import get_url, cstr\n\t\tfrappe.errprint(get_url())\n\t\tif get_url()=='http://tailorpad.com':\n\t\t\tpass\n\t\telse:\n\t\t\tpr2 = frappe.db.sql(\"\"\"select name from `tabSupport Ticket`\"\"\")\n\t\t\tfrappe.errprint(pr2)\n\t\t\tfrappe.errprint(\"is feed back saved\")\n\t\t\tif pr2:\n\t\t\t\t# self.login()\n\t\t\t\tfrappe.errprint(\"in if for creation support ticket\")\n\t\t\t\ttest = {}\n\t\t\t\tsupport_ticket = self.get_ticket_details()\n\t\t\t\tself.call_del_keys(support_ticket)\n\t\t\t\t#test['communications'] = []\n\t\t\t\t#self.call_del_keys(support_ticket.get('communications'), test)\n\t\t\t\tself.login()\n\t\t\t\tfrappe.errprint(\"support_ticket\")\n\t\t\t\tfrappe.errprint(support_ticket)\n\t\t\t\tself.tenent_based_ticket_creation(support_ticket)\n\n\t# def on_update(self):\n\t\t# self.send_email()\t\n\n\tdef send_email(self):\n\t\tfrappe.errprint(\"in the sendmail\")\n\t\tfrom frappe.utils.user import get_user_fullname\n\t\tfrom frappe.utils import get_url\n\t\tif self.get(\"__islocal\") and get_url()=='http://tailorpad.com':\n\t\t\t\n\t\t\t# mail_titles = frappe.get_hooks().get(\"login_mail_title\", [])\n\t\t\t# title = frappe.db.get_default('company') or (mail_titles and mail_titles[0]) or \"\"\n\n\t\t\tfull_name = get_user_fullname(frappe.session['user'])\n\t\t\tif full_name == \"Guest\":\n\t\t\t\tfull_name = \"Administrator\"\n\n\t\t\tfirst_name = frappe.db.sql_list(\"\"\"select first_name from `tabUser` where name='%s'\"\"\"%(self.raised_by))\n\t\t\tfrappe.errprint(first_name[0])\n\n\t\t\tmsg=\"Dear  \"+first_name[0]+\"!<br><br>Support Ticket is created successfully for '\"+self.subject+\"'<br><br>Your Support Ticket Number is '\"+self.name+\"' <br><br>Please note for further information. <br><br>Regards, <br>Team TailorPad.\"\n\t\t\tsender = frappe.session.user not in STANDARD_USERS and frappe.session.user or None\n\t\t\tfrappe.sendmail(recipients=self.raised_by, sender=sender, subject=self.subject,\n\t\t\t\tmessage=msg)\n\n\n\tdef login(self):\n\t\tlogin_details = {'usr': 'Administrator', 'pwd': 'admin'}\n\t\turl = 'http://tailorpad.com/api/method/login'\n\t\theaders = {'content-type': 'application/x-www-form-urlencoded'}\n\t\tfrappe.errprint([url, 'data='+json.dumps(login_details)])\n\t\tresponse = requests.post(url, data='data='+json.dumps(login_details), headers=headers)\n\n\tdef get_ticket_details(self):\n\t\t# return frappe.get_doc('Support Ticket', self.name)\n\t\tresponse = requests.get(\"\"\"%(url)s/api/resource/Support Ticket/SUP-00001\"\"\"%{'url':get_url()})\n\t\t\n\t\t# frappe.errprint([\"\"\"%(url)s/api/resource/Support Ticket/%(name)s\"\"\"%{'url':get_url(), 'name':self.name}])\n\t\tfrappe.errprint(response.text)\n\t\treturn eval(response.text).get('data')\n\n\tdef call_del_keys(self, support_ticket):\n\t\tif support_ticket:\n\t\t\tif isinstance(support_ticket, dict):\n\t\t\t\tself.del_keys(support_ticket)\n\n\t\t\tif isinstance(support_ticket, list):\n\t\t\t\tfor comm in support_ticket:\n\t\t\t\t\tself.del_keys(comm)\n\n\tdef del_keys(self, support_ticket):\n\t\tfrappe.errprint(type(support_ticket))\n\t\tdel support_ticket['name']\n\t\tdel support_ticket['creation']\n\t\tdel support_ticket['modified']\n\t\tdel support_ticket['company']\n\n\tdef tenent_based_ticket_creation(self, support_ticket):\n\t\tfrappe.errprint(support_ticket)\n\t\turl = 'http://tailorpad.com/api/resource/Support Ticket'\n\t\t#url = 'http://192.168.5.12:7676/api/method/login'\n\t\theaders = {'content-type': 'application/x-www-form-urlencoded'}\n\t\tfrappe.errprint('data='+json.dumps(support_ticket))\n\t\tresponse = requests.post(url, data='data='+json.dumps(support_ticket), headers=headers)\n\n\t\tfrappe.errprint(response)\n\t\tfrappe.errprint(response.text)\n\n\tdef validate(self):\n\t\t\n\t\tself.update_status()\n\t\tself.set_lead_contact(self.raised_by)\n\n\t\tif self.status == \"Closed\":\n\t\t\tfrom frappe.widgets.form.assign_to import clear\n\t\t\tclear(self.doctype, self.name)\n\t\t#self.on_update1()\n\t\tself.send_email()\n\n\tdef set_lead_contact(self, email_id):\n\t\timport email.utils\n\t\temail_id = email.utils.parseaddr(email_id)\n\t\tif email_id:\n\t\t\tif not self.lead:\n\t\t\t\tself.lead = frappe.db.get_value(\"Lead\", {\"email_id\": email_id})\n\t\t\tif not self.contact:\n\t\t\t\tself.contact = frappe.db.get_value(\"Contact\", {\"email_id\": email_id})\n\n\t\t\tif not self.company:\n\t\t\t\tself.company = frappe.db.get_value(\"Lead\", self.lead, \"company\") or \\\n\t\t\t\t\tfrappe.db.get_default(\"company\")\n\n\tdef update_status(self):\n\t\tstatus = frappe.db.get_value(\"Support Ticket\", self.name, \"status\")\n\t\tif self.status!=\"Open\" and status ==\"Open\" and not self.first_responded_on:\n\t\t\tself.first_responded_on = now()\n\t\tif self.status==\"Closed\" and status !=\"Closed\":\n\t\t\tself.resolution_date = now()\n\t\tif self.status==\"Open\" and status !=\"Open\":\n\t\t\t# if no date, it should be set as None and not a blank string \"\", as per mysql strict config\n\t\t\tself.resolution_date = None\n\n@frappe.whitelist()\ndef set_status(name, status):\n\tst = frappe.get_doc(\"Support Ticket\", name)\n\tst.status = status\n\tst.save()\n\n@frappe.whitelist()\ndef get_admin(name):\n\tadmin = frappe.db.sql(\"select email_id_admin from tabUser  where name='administrator'\")\n\tfrappe.errprint(admin)\n\tfrappe.errprint(frappe.session.get('user'))\n\tif admin:\n\t\tfrappe.errprint(\"if\")\n\t\treturn admin[0][0]\n\telse:\n\t\tfrappe.errprint(\"else\")\n\t  \treturn frappe.session.get('user')\n\n\n@frappe.whitelist()\ndef assing_future(name, assign_in_future,raised_by,assign_to):\n  \tfrappe.errprint(\"in assign future\")\n  \tfrom frappe.utils import get_url, cstr\n\tif get_url()=='http://tailorpad.com':\n\t\tcheck_entry = frappe.db.sql(\"\"\"select assign_to from `tabAssing Master` where name = %s \"\"\", raised_by)\n\t\tfrappe.errprint(\"in assign\")\n\t\tif check_entry :\n\t\t\tfrappe.errprint(\"chk\")\n\t\t\tif assign_in_future=='No':\n\t\t\t\tfrappe.errprint(\"no\")\n\t\t\t\tfrappe.db.sql(\"\"\"delete from `tabAssing Master` where name = %s \"\"\", raised_by)\t\n\t\t\telse :\n\t\t\t\tfrappe.errprint(\"Yes\")\n\t\t\t\tfrappe.db.sql(\"\"\"update `tabAssing Master` set assign_to=%s where name = %s \"\"\",(assign_to,raised_by))\n\t\telse :\n\t\t\tfrappe.errprint(\"not chk\")\n\t\t\tif assign_in_future=='Yes':\n\t\t\t\tfrappe.errprint(\"Yes\")\n\t\t\t\tam = frappe.new_doc(\"Assing Master\")\n\t\t\t\tam.update({\n\t\t\t\t\"name\": raised_by,\n\t\t\t\t\"assign_to\": assign_to,\n\t\t\t\t\"raised_by\":raised_by\n\t\t\t})\n\t\t\tam.insert()\n\ndef auto_close_tickets():\n\tfrappe.db.sql(\"\"\"update `tabSupport Ticket` set status = 'Closed'\n\t\twhere status = 'Replied'\n\t\tand date_sub(curdate(),interval 15 Day) > modified\"\"\")\n\n\n@frappe.whitelist()\ndef reenable(name):\n\tfrappe.errprint(\"calling superadmin\")\n        from frappe.utils import get_url, cstr\n\tfrappe.errprint(get_url())\n\tif get_url()!='http://tailorpad.com':\n\t  frappe.errprint(\"in reenable\")\n  \t  from frappe.utils import get_url, cstr,add_months\n  \t  from frappe import msgprint, throw, _\n  \t  res = frappe.db.sql(\"select validity from `tabUser` where name='Administrator' and no_of_users >0\")\n\t  if  res:\n\t\tres1 = frappe.db.sql(\"select validity_end_date from `tabUser` where '\"+cstr(name)+\"' and validity_end_date <CURDATE()\")\n\t\tif res1:\n\t\t\tbc=\"update `tabUser` set validity_end_date=DATE_ADD((nowdate(), INTERVAL \"+cstr(res[0][0])+\" MONTH) where name = '\"+cstr(name)+\"'\"\n\t\t\tfrappe.db.sql(bc)\n\t  \t\tfrappe.db.sql(\"update `tabUser`set no_of_users=no_of_users-1  where name='Administrator'\")\n\t\telse:\n\t\t\tab=\"update `tabUser` set validity_end_date=DATE_ADD(validity_end_date,INTERVAL \"+cstr(res[0][0])+\" MONTH) where name = '\"+cstr(name)+\"' \"\n\t\t\tfrappe.errprint(ab)\n\t\t\tfrappe.db.sql(ab)\n\t  \t\tfrappe.db.sql(\"update `tabUser`set no_of_users=no_of_users-1  where name='Administrator'\")\n\t  else:\n\t\t    frappe.throw(_(\"Your subscription plan expired .Please purchase an subscription plan and enable user.\"))", "original_line_number": 3244, "instruction_length": 564}
{"instruction": "This code includes several classes and functions for implementing an Encoder-Decoder model with attention mechanism. It also includes a function for calculating masked cross-entropy loss and a class for implementing Focal Loss. To use this code, import the necessary libraries and classes, and then define the input and output sizes, hidden size, number of layers, and other hyperparameters for the Encoder and Decoder. Then, instantiate the Encoder and Decoder classes and train the model using the provided loss functions. Finally, use the trained model to generate predictions on new data.", "input": "", "output": "import numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n\nUSE_CUDA = False\n\nKERN_SIZE = 3\n\n\ndef set_cuda(val):\n    global USE_CUDA\n    USE_CUDA = val\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    seq_range_expand = Variable(seq_range_expand)\n    # if sequence_length.is_cuda:\n    if USE_CUDA:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (sequence_length.unsqueeze(1)\n                         .expand_as(seq_range_expand))\n    return seq_range_expand < seq_length_expand\n\n\ndef masked_cross_entropy(logits, target, length, gamma=0, eps=1e-7):\n    length = Variable(torch.LongTensor(length))\n    if USE_CUDA:\n        length = length.cuda()\n\n    \"\"\"\n    Args:\n        logits: A Variable containing a FloatTensor of size\n            (batch, max_len, num_classes) which contains the\n            unnormalized probability for each class.\n        target: A Variable containing a LongTensor of size\n            (batch, max_len) which contains the index of the true\n            class for each corresponding step.\n        length: A Variable containing a LongTensor of size (batch,)\n            which contains the length of each data in a batch.\n\n    Returns:\n        loss: An average loss value masked by the length.\n    \"\"\"\n\n    # logits_flat: (batch * max_len, num_classes)\n    logits_flat = logits.view(-1, logits.size(-1))\n    # target_flat: (batch * max_len, 1)\n    target_flat = target.view(-1, 1)\n    # probs_flat: (batch * max_len, 1)\n    probs_flat = torch.gather(F.softmax(logits_flat), dim=1, index=target_flat)\n    probs_flat = probs_flat.clamp(eps, 1. - eps)   # prob: [0, 1] -> [eps, 1 - eps]\n    # losses_flat: (batch * max_len, 1)\n    losses_flat = -torch.log(probs_flat) * (1 - probs_flat) ** gamma  # focal loss\n    # losses: (batch, max_len)\n    losses = losses_flat.view(*target.size())\n    # mask: (batch, max_len)\n    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n    losses = losses * mask.float()\n    loss = losses.sum() / length.float().sum()\n    return loss\n\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n        super(EncoderRNN, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n\n    def forward(self, input_seqs, input_lengths, hidden=None):\n        # Note: we run this all at once (over multiple batches of multiple sequences)\n        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lengths)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n        return outputs, hidden\n\n\nclass CNN(nn.Module):\n    def __init__(self, input_size):\n        super(CNN, self).__init__()\n\n        inp_size = input_size\n\n        self.features = nn.Sequential(\n\n            nn.Conv2d(1, 10, kernel_size=(KERN_SIZE, inp_size)),\n            nn.BatchNorm2d(10),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(10, 10, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(10),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n\n            nn.Conv2d(10, 20, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(20),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(20, 20, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(20),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n\n            nn.Conv2d(20, 40, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(40),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(40, 40, kernel_size=(KERN_SIZE, 1)),\n            nn.BatchNorm2d(40),\n            nn.ReLU(inplace=True),\n\n            nn.MaxPool2d((2, 1)),\n        )\n\n        self.n_pool_layers = 3\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(2), out.size(0), out.size(1) * out.size(3))\n        return out\n\n\nclass EncoderCNN(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n        super(EncoderCNN, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.cnn = CNN(input_size)\n        self.gru = nn.GRU(input_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n\n    def forward(self, input_seqs, input_lengths, hidden=None):\n        input_features = self.cnn(input_seqs.transpose(0, 1).unsqueeze(1))\n        features_length = [(l//(2 ** self.cnn.n_pool_layers)) for l in input_lengths]\n        # features_length = input_lengths\n        # print(input_features.size())\n        # print(features_length)\n        # Note: we run this all at once (over multiple batches of multiple sequences)\n        packed = torch.nn.utils.rnn.pack_padded_sequence(input_features, features_length)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs)  # unpack (back to padded)\n        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]  # Sum bidirectional outputs\n        return outputs, hidden\n\n\nclass Attn(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Attn, self).__init__()\n\n        self.method = method\n        self.hidden_size = hidden_size\n\n        if self.method == 'general':\n            self.attn = nn.Linear(self.hidden_size, hidden_size)\n\n        elif self.method == 'concat':\n            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n\n    def forward(self, hidden, encoder_outputs):\n        max_len = encoder_outputs.size(0)\n        this_batch_size = encoder_outputs.size(1)\n\n        # Create variable to store attention energies\n        attn_energies = Variable(torch.zeros(this_batch_size, max_len))  # B x S\n\n        if USE_CUDA:\n            attn_energies = attn_energies.cuda()\n\n        # For each batch of encoder outputs\n        for b in range(this_batch_size):\n            # Calculate energy for each encoder output\n            for i in range(max_len):\n                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n\n        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n        return F.softmax(attn_energies).unsqueeze(1)\n\n    def score(self, hidden, encoder_output):\n\n        if self.method == 'dot':\n            energy = torch.dot(hidden.view(-1), encoder_output.view(-1))\n            return energy\n\n        elif self.method == 'general':\n            energy = self.attn(encoder_output)\n            energy = torch.dot(hidden.view(-1), energy.view(-1))\n            return energy\n\n        elif self.method == 'concat':\n            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n            energy = self.v.dot(energy)\n            return energy\n\n\nclass LuongAttnDecoderRNN(nn.Module):\n\n    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, prior_size=0, dropout=0.1, embedding=None):\n        super(LuongAttnDecoderRNN, self).__init__()\n\n        # Keep for reference\n        self.attn_model = attn_model\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.prior_size = prior_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n\n        # Define layers\n        if np.any(embedding):\n            self.embedding_size = embedding_size = embedding.shape[1]\n            self.embedding = nn.Embedding(output_size, embedding_size)\n            self.embedding.weight = nn.Parameter(torch.from_numpy(embedding).float())\n            self.embedding.requires_grad = True\n        else:\n            self.embedding_size = embedding_size = hidden_size\n            self.embedding = nn.Embedding(output_size, embedding_size)\n\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(embedding_size, hidden_size, n_layers, dropout=dropout)\n        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n        self.out = nn.Linear(hidden_size + prior_size, output_size)\n\n        # Choose attention model\n        if attn_model != 'none':\n            self.attn = Attn(attn_model, hidden_size)\n\n    def forward(self, input_seq, last_hidden, encoder_outputs, prior):\n        # Note: we run this one step at a time\n\n        # Get the embedding of the current input word (last output word)\n        batch_size = input_seq.size(0)\n        embedded = self.embedding(input_seq)\n        embedded = self.embedding_dropout(embedded)\n        embedded = embedded.view(1, batch_size, -1)  # S=1 x B x N\n\n        # Get current hidden state from input word and last hidden state\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n\n        # Calculate attention from current RNN state and all encoder outputs;\n        # apply to encoder outputs to get weighted average\n        attn_weights = self.attn(rnn_output, encoder_outputs)\n        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # B x S=1 x N\n\n        # Attentional vector using the RNN hidden state and context vector\n        # concatenated together (Luong eq. 5)\n        rnn_output = rnn_output.squeeze(0)  # S=1 x B x N -> B x N\n        context = context.squeeze(1)  # B x S=1 x N -> B x N\n        concat_input = torch.cat((rnn_output, context), 1)\n        concat_output = F.tanh(self.concat(concat_input))\n\n        # Finally predict next token (Luong eq. 6, without softmax)\n        if prior is None:\n            output = self.out(concat_output)\n        else:\n            output = self.out(torch.cat((concat_output, prior), 1))\n\n        # Return final output, hidden state, and attention weights (for visualization)\n        return output, hidden, attn_weights\n\n\n# https://github.com/DingKe/pytorch_workplace/blob/master/focalloss/loss.py\ndef one_hot(index, classes):\n    size = index.size() + (classes,)\n    view = index.size() + (1,)\n\n    mask = torch.Tensor(*size).fill_(0)\n    index = index.view(*view)\n    ones = 1.\n\n    if isinstance(index, Variable):\n        ones = Variable(torch.Tensor(index.size()).fill_(1))\n        mask = Variable(mask, volatile=index.volatile)\n\n    return mask.scatter_(1, index, ones)\n\n\nclass FocalLoss(nn.Module):\n\n    def __init__(self, gamma=0, eps=1e-7):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, input, target):\n        y = one_hot(target, input.size(-1))\n        logit = F.softmax(input)\n        logit = logit.clamp(self.eps, 1. - self.eps)\n\n        loss = -1 * y * torch.log(logit) # cross entropy\n        loss = loss * (1 - logit) ** self.gamma # focal loss\n\n        return loss.sum()", "original_line_number": 3600, "instruction_length": 592}
{"instruction": "This code performs library screening result analysis using the pycas package. To use it, run the script with the required arguments: input data file path, control experiment labels, treatment experiment labels, method to be used in the analysis, whether to consider barcodes, and the result output directory. The input data file should have columns named \"gene\", \"guide\", and \"barcode\". The control and treatment experiment labels should be separated by space and have the same length. The method should be either \"sunbird\" or \"mw\". The result will be saved in a csv file named \"pycas_analysis_sunbird.csv\" in the specified output directory or the current work directory if not specified.", "input": "", "output": "#! /bin/env python3\n__all__ = ['pycaspattern']\n\n# ------------------\n# Libraries\n# ------------------\nimport argparse\nimport os\nimport sys\nsys.path.append('/gpfs/user/liuzh/Code/crispr')\nimport pandas as pd\nfrom pycas.analysis import Screening\nfrom pycas.utils.decorator import helpstring\nfrom pycas.utils.decorator import AppendHelp\n\n\n# ------------------\n# Functions\n# ------------------\n\ndef pycasanalysis(filename,\n                  ctrl_label,\n                  exp_label,\n                  method,\n                  hasbarcode,\n                  out_dir):\n    file_type = 'csv'\n    if filename.split('.')[-1] == 'txt':\n        sep = '\\t'\n        file_type = 'txt'\n    elif filename.split('.')[-1] == 'csv':\n        sep = ','\n        file_type = 'csv'\n    else:\n        raise ValueError('Input data file should be txt or csv')\n    if file_type == 'csv':\n        data = pd.read_csv(filename, header=0)\n    else:\n        data = pd.read_table(filename, header=0, sep='\\t')\n    for x in ['gene', 'guide', 'barcode']:\n        if x not in data.columns:\n            raise ValueError('Input data file should contain column named as: ' + x)\n    if len(ctrl_label) != len(exp_label):\n        raise ValueError('Input control labels and treatment labels should be of the same length.')\n    if out_dir != '' and not os.path.exists(out_dir):\n        os.mkdir(out_dir)\n    if method not in ['sunbird', 'mw']:\n        raise ValueError('The test method should be in: sunbird mw.')\n    analysis = Screening(data, ctrl_label, exp_label, hasbarcode=hasbarcode)\n    if method == 'sunbird':\n        analysis.sunbird(10)\n        analysis.test['sunbird'].to_csv(\n            os.path.join(out_dir, 'pycas_analysis_sunbird.csv')\n        )\n    else:\n        pass\n\n\n\n# ------------------\n# Main\n# ------------------\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        prog='pycasanalysis',\n        description='Analysis the library screening result.'\n    )\n    parser.add_argument(\n        '-i', '--input', required=True,\n        help='Input data file path, with columns: gene, guide <, barcode>, [labels]. (column names should be in the csv file).'\n    )\n    parser.add_argument(\n        '-c', '--control-label', nargs='+',\n        help='Control experiment labels, separeted by space.'\n    )\n    parser.add_argument(\n        '-t', '--treat-label', nargs='+',\n        help='Treatment experiment labels, separeted by space.'\n    )\n    parser.add_argument(\n        '-m', '--method', default='sunbird',\n        help='Method to be used in the analysis: sunbird, mw.'\n    )\n    parser.add_argument(\n        '--has-barcode', action='store_true',\n        help='Input data should be tested consider barcodes.'\n    )\n    parser.add_argument(\n        '--out-dir', default='',\n        help='Result output directory，default is current work directory.'\n    )\n    args = parser.parse_args()\n    def analysis(**args):\n        pycasanalysis(\n            filename=args['input'],\n            ctrl_label=args['control_label'],\n            exp_label=args['treat_label'],\n            method=args['method'],\n            hasbarcode=args['has_barcode'],\n            out_dir=args['out_dir']\n        )\n    analysis(**vars(args))\n\n\n# ------------------\n# EOF\n# ------------------", "original_line_number": 3602, "instruction_length": 688}
{"instruction": "Connect to a database using pymssql by creating a Dao class with methods to connect, create, insert, select, and close the database connection. The connect method should take in the host, user, password, database, and charset as parameters and establish a connection to the database. The create method should execute a given SQL statement to create a table in the database. The insert method should execute a given SQL statement to insert data into the table. The select method should execute a given SQL statement to retrieve data from the table and return the results. Finally, the close method should close the database connection.", "input": "", "output": "# -*- coding: utf-8 -*-\n\nimport pymssql\n\n\nclass Dao:\n    def __init__(self):\n        self.conn = None\n        self.cur = None\n\n    def connect(self):\n        # 数据库连接信息\n        self.conn = pymssql.connect(host=\"localhost:59318\", user=\"eachen\", password=\"123456\", database=\"mydata\",\n                                    charset=\"utf8\")\n        # host = \"localhost:59318\", user = \"eachen\", pwd = \"123456\", db = \"mydata\"\n        self.cur = self.conn.cursor()\n        if not self.cur:\n            raise (NameError, \"数据库连接失败\")\n        else:\n            print(\"数据库连接成功\")\n\n    def create(self, sql):\n        # print(sql)\n        try:\n            self.cur.execute(sql)\n            self.conn.commit()\n        except:\n            print('create failed')\n        else:\n            print('create succeed')\n\n    def insert(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        self.conn.commit()\n\n    def select(self, sql):\n        # print(sql)\n        self.cur.execute(sql)\n        # fetchall()是接收全部的返回结果行\n        return self.cur.fetchall()\n\n    def close(self):\n        self.conn.close()", "original_line_number": 3655, "instruction_length": 634}
{"instruction": "Download tweets from a list of users and save them to a CSV file. \n\n1. Install tweepy and unicodecsv libraries in Python.\n2. Create a file named \"users.txt\" and add the list of users whose tweets you want to download.\n3. Run the Python script with the following command: python script.py [maxTweets] [fName] [sinceId] [last_id]\n4. Replace [maxTweets] with the maximum number of tweets you want to download.\n5. Replace [fName] with the name of the CSV file where you want to save the tweets.\n6. Replace [sinceId] and [last_id] with the tweet IDs if you want to download tweets between a specific range. Otherwise, use \"-1\" for both.\n7. Wait for the script to finish downloading the tweets and saving them to the CSV file.", "input": "", "output": "#!/usr/bin/python\nimport tweepy\nimport sys\nimport os\nimport codecs\nimport unicodecsv as csv\n\n# API and ACCESS KEYS\nAPI_KEY = 'jz3feMK2gN0kaN377FsTXY7uY'\nAPI_SECRET = 'sGfCEayfwORloC9SvHy6BmDjifUsUEIF0EF51SgiYUgs054n7H'\n\n# Don't buffer stdout, so we can tail the log output redirected to a file\nsys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n\n# Max Tweets\nmaxTweets = int(sys.argv[1])\n\n# Filename\nfName = sys.argv[2]\n\ntweetsPerQry = 200\n\n# List of users read from users.txt\nusers = []\n\n#open users.txt file and gets the list of users\nwith open('users.txt', 'r') as f:\n  for line in f:\n    users.append(line.strip())\n\nsinceId = None\n\nif(len(sys.argv) > 3):\n  if(sys.argv[3] != '-1'):\n    sinceId = sys.argv[3]\n\nlast_id = -1L\n\nif(len(sys.argv) > 4):\n  last_id = long(sys.argv[4])\n\ndef getHashtags(hashes):\n  hashStr = ''\n  for i, h in enumerate(hashes):\n    if i == len(hashes)-1:\n      hashStr = hashStr + h['text']\n    else:\n      hashStr = hashStr + h['text'] + ','\n  return hashStr\n\ndef getMentions(mentions):\n  mentionStr = ''\n  for i, m in enumerate(mentions):\n    if i == len(mentions)-1:\n      mentionStr = mentionStr + m['screen_name']\n    else:\n      mentionStr = mentionStr + m['screen_name'] + ','\n  return mentionStr\n\nauth = tweepy.AppAuthHandler(API_KEY, API_SECRET)\n\napi = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)\n\nif (not api):\n  print (\"Can't Authenticate Bye!\")\n  sys.exit(-1)\n\ntweetCount = 0\nprint(\"Downloading max {0} tweets\".format(maxTweets))\n\nwith open(fName, 'ab') as f:\n  writer = csv.writer(f, encoding='utf-8')\n  writer.writerow(['Username','Created_at','ID','Tweet','RetweetCount','Name','Location','URL','Description','TweetCount','FollowersCount','FriendsCount','hashtags','mentions'])\n  for user in users:\n    tweetCount = 0\n    last_id = 0\n    while tweetCount < maxTweets:\n      print 'User is ' + user + ' Tweet count ' + str(tweetCount) + ' max Tweets ' + str(maxTweets) + ' SinceId ' + str(sinceId) + ' last_id ' + str(last_id)\n      try:\n        if (last_id <= 0):\n          if (not sinceId):\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry)\n          else:\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, since_id=sinceId)\n\n        else:\n          if (not sinceId):\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, max_id=str(last_id - 1))\n          else:\n            new_tweets = api.user_timeline(screen_name=user, count=tweetsPerQry, max_id=str(last_id - 1), since_id=sinceId)\n\n        if not new_tweets:\n          print(\"No more tweets found\")\n          break\n        \n        for tweet in new_tweets:\n          try: \n            hashTags = getHashtags(tweet.entities.get('hashtags'))\n            mentions = getMentions(tweet.entities.get('user_mentions'))\n\n            writer.writerow([tweet.user.screen_name,tweet.created_at,tweet.id_str,tweet.text,str(tweet.retweet_count),tweet.user.name, tweet.user.location, str(tweet.user.url),tweet.user.description,str(tweet.user.statuses_count),str(tweet.user.followers_count),str(tweet.user.friends_count),hashTags,mentions])\n          except tweepy.TweepError as e:\n            print(\"some error : \" + str(e) + \" for user: \" + user)\n            break\n\n        tweetCount += len(new_tweets)\n        print(\"Downloaded {0} tweets\".format(tweetCount))\n        last_id = new_tweets[-1].id\n      except tweepy.TweepError as e:\n        # Just exit if any error\n        print(\"some error : \" + str(e))\n        break\n\nprint (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))", "original_line_number": 3770, "instruction_length": 720}
{"instruction": "Create a custom script to detect and install all MetaToolKit dependencies on Ubuntu. Use the given code as a reference. The script should check for Python essentials and required packages such as numpy, sympy, xlrd, xlwt, matplotlib, PyQt4, Bio, nose, docx, libsbml, cbmpy, java, perl, and blastall. It should also check for optional/recommended packages such as git, cython, libxml2-dev, libxslt-dev, ipython, ipython-notebook, python-suds, and python-flask. The script should generate a bash script that can be run to install the dependencies. The bash script should prompt the user for confirmation before installing the dependencies.", "input": "", "output": "# Detect all MetaToolKit depencies on Ubuntu and create a custom script to install them.\n# Tested on Ubuntu 14.04, 16.04\n# Author Brett G. Olivier (bgoli@users.sourceforge.net)\n# (C) All rights reserved, Brett G. Olivier, Amsterdam 2016.\n\nimport os, subprocess, itertools, stat\n\nUBUNTU = CONDA = False\ntry:\n    print(os.sys.argv)\n    arg = os.sys.argv[1]\nexcept:\n    arg = 'UBUNTU'\nif arg == 'UBUNTU':\n    UBUNTU = True\nelif arg == 'CONDA':\n    CONDA = True\nelse:\n    print('\\nPlease call script with CONDA as argument for Anaconda install script, defaulting to UBUNTU')\n    UBUNTU = True\n\nres = {'Required' : {},\\\n       'Optional' : {}\n       }\n\n# First lets check for some Python essentials\nreqcnt = itertools.count(1,1)\noptcnt = itertools.count(1,1)\n# this should just be there for any sane python build environment\nif UBUNTU:\n    res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install build-essential g++ gfortran python python-dev'\nif CONDA:\n    pass\n    #res['Required'][reqcnt.next()] = 'conda update -y conda # if this is the only required package ignore it'\ntry:\n    import pip\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-pip'\ntry:\n    import numpy\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-numpy'\ntry:\n    import sympy\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-sympy'\ntry:\n    import xlrd\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-xlrd'\ntry:\n    import xlwt\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-xlwt'\ntry:\n    import matplotlib\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-matplotlib'\ntry:\n    import PyQt4\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-qt4'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -y pyqt=4.11.4'\ntry:\n    import Bio\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-biopython'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -y biopython'\ntry:\n    import nose\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install python-nose'\ntry:\n    import docx\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install docx'    \n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'pip install docx'    \ntry:\n    import libsbml\n    if libsbml.LIBSBML_VERSION < 51201:\n        print('\\nPlease update to the latest version of libSBML.\\n')\n        raise ImportError\nexcept ImportError:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install libxml2 libxml2-dev libxslt-dev zlib1g zlib1g-dev bzip2 libbz2-dev'\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install --upgrade python-libsbml'\n    elif CONDA:\n        res['Required'][reqcnt.next()] = 'conda install -c SBMLTeam -y python-libsbml'\ntry:\n    import cbmpy\nexcept:\n    if UBUNTU:\n        res['Required'][reqcnt.next()] = 'sudo -EH pip install --upgrade cbmpy'\n        res['Required'][reqcnt.next()] = 'sudo python -c \"import cbmpy\"'\n    if CONDA:\n        res['Required'][reqcnt.next()] = 'pip install cbmpy'\n        res['Required'][reqcnt.next()] = 'python -c \"import cbmpy\"'\ntry:\n    out = subprocess.call(['java', '-version'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install default-jre'\n\ntry:\n    out = subprocess.call(['perl', '-v'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install perl'\np_script = \"\"\"\\\nmy $rc = 0;\n$rc = eval\n{\n  require XML::Parser;\n  XML::Parser->import();\n  1;\n};\nif ($rc){\n    exit 0\n    } else {\n    exit 1\n    }\n\"\"\"\ntry:\n    PF = file('_test.pl', 'w')\n    PF.write(p_script)\n    PF.close()\n    out = int(subprocess.call(['perl', '_test.pl']))\n    if out:\n        raise OSError\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install libxml-parser-perl'\ntry:\n    out = subprocess.call(['blastall'])\nexcept (OSError):\n    if UBUNTU or CONDA:\n        res['Required'][reqcnt.next()] = 'sudo -E apt-get -y install blast2'\n\n# Optional/recommended\n# https://github.com/bgoli/cbmpy-glpk\nif UBUNTU:\n    res['Optional'][optcnt.next()] = 'sudo apt-get -y install git cython libxml2-dev libxslt-dev'\ntry:\n    import IPython\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install ipython ipython-notebook'\ntry:\n    import suds\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install python-suds'\n    elif CONDA:\n        res['Optional'][optcnt.next()] = 'pip install suds'\ntry:\n    import flask\nexcept ImportError:\n    if UBUNTU:\n        res['Optional'][optcnt.next()] = 'sudo -E apt-get -y install python-flask'\n\nif UBUNTU or CONDA:\n    bash_script=\"\"\"\\\n    while true; do\n        read -p \"Do you wish to install *{}* MetaToolkit dependencies? [y/n]: \" yn\n        case $yn in\n            [Yy]* ) echo \"Installing ...\"; break;;\n            [Nn]* ) exit;;\n            * ) echo \"Please enter y/n.\";;\n        esac\n    done\n    \n    \"\"\"\n    \n    bash_script=\"\"\"\\\n    \n    # {}\n    \n    \"\"\"\n    \n    output = '#!/bin/sh\\n\\n'\n    output += '#MetaToolkit: Ubuntu system requirements check\\n'\n    output += '#=============================================\\n\\n'\n    \n    REQUIRE_USER_INPUT = False\n    \n    for r in res:\n        if len(res[r]) > 0:\n            if REQUIRE_USER_INPUT:\n                output += bash_script.format(r)\n            output += '#{}\\n#{}\\n\\n'.format(r, '-'*len(r))\n            resk = list(res[r])\n            resk.sort()\n            for k in resk:\n                if k != None:\n                    output += '{}\\n'.format(res[r][k])\n            output += '\\n'\n    output += 'exit\\n\\n'\n    fname = 'metatoolkit_install_dependencies.sh'\n    F = file(fname, 'w')\n    F.write(output)\n    F.close()\n    os.chmod(fname, stat.S_IRWXU)\n    \n    print('')\n    print(output)\n    print('\\n\\nInstall script (shown above) saved as file: {}\\nplease examine it carefully and run. Alternatively install individual dependencies manually').format(fname)", "original_line_number": 3955, "instruction_length": 637}
{"instruction": "To run the server part of the package dashboard, follow these steps:\n\n1. Install the required libraries:\n   - google.appengine.api.mail\n   - google.appengine.api.memcache\n   - google.appengine.api.taskqueue\n   - google.appengine.api.urlfetch\n   - google.appengine.api.users\n   - google.appengine.ext.db\n   - google.appengine.ext.webapp\n   - google.appengine.ext.webapp.template\n   - google.appengine.ext.webapp.util\n   - datetime\n   - logging\n   - os\n   - re\n   - sets\n   - urllib2\n\n2. Import the auth module and the const module.\n\n3. Define the Package class, which is the storage model for package info recorded on the server.\n\n4. Define the Project class, which is the storage model for project info recorded on the server.\n\n5. Define the vc_to_web function, which converts a version control system path to a web URL and a check URL.\n\n6. Define the web_to_vc function, which converts a web URL to a version control system path.\n\n7. Define the PackagePage class, which handles requests related to packages.\n\n8. Define the ProjectPage class, which handles requests related to projects.\n\n9. Define the main function, which creates a WSGI application and runs it.\n\n10. Run the main function to start the server.", "input": "", "output": "# Copyright 2010 The Go Authors. All rights reserved.\n# Use of this source code is governed by a BSD-style\n# license that can be found in the LICENSE file.\n\n# This is the server part of the package dashboard.\n# It must be run by App Engine.\n\nfrom google.appengine.api import mail\nfrom google.appengine.api import memcache\nfrom google.appengine.api import taskqueue\nfrom google.appengine.api import urlfetch\nfrom google.appengine.api import users\nfrom google.appengine.ext import db\nfrom google.appengine.ext import webapp\nfrom google.appengine.ext.webapp import template\nfrom google.appengine.ext.webapp.util import run_wsgi_app\nimport datetime\nimport logging\nimport os\nimport re\nimport sets\nimport urllib2\n\n# local imports\nfrom auth import auth\nimport toutf8\nimport const\n\ntemplate.register_template_library('toutf8')\n\n# Storage model for package info recorded on server.\nclass Package(db.Model):\n    path = db.StringProperty()\n    web_url = db.StringProperty()           # derived from path\n    count = db.IntegerProperty()            # grand total\n    week_count = db.IntegerProperty()       # rolling weekly count\n    day_count = db.TextProperty(default='') # daily count\n    last_install = db.DateTimeProperty()\n\n    # data contributed by gobuilder\n    info = db.StringProperty()  \n    ok = db.BooleanProperty()\n    last_ok = db.DateTimeProperty()\n\n    def get_day_count(self):\n        counts = {}\n        if not self.day_count:\n            return counts\n        for d in str(self.day_count).split('\\n'):\n            date, count = d.split(' ')\n            counts[date] = int(count)\n        return counts\n\n    def set_day_count(self, count):\n        days = []\n        for day, count in count.items():\n            days.append('%s %d' % (day, count))\n        days.sort(reverse=True)\n        days = days[:28]\n        self.day_count = '\\n'.join(days)\n\n    def inc(self):\n        count = self.get_day_count()\n        today = str(datetime.date.today())\n        count[today] = count.get(today, 0) + 1\n        self.set_day_count(count)\n        self.update_week_count(count)\n        self.count += 1\n\n    def update_week_count(self, count=None):\n        if count is None:\n            count = self.get_day_count()\n        total = 0\n        today = datetime.date.today()\n        for i in range(7):\n            day = str(today - datetime.timedelta(days=i))\n            if day in count:\n                total += count[day]\n        self.week_count = total\n\n\n# PackageDaily kicks off the daily package maintenance cron job\n# and serves the associated task queue.\nclass PackageDaily(webapp.RequestHandler):\n\n    def get(self):\n        # queue a task to update each package with a week_count > 0\n        keys = Package.all(keys_only=True).filter('week_count >', 0)\n        for key in keys:\n            taskqueue.add(url='/package/daily', params={'key': key.name()})\n\n    def post(self):\n        # update a single package (in a task queue)\n        def update(key):\n            p = Package.get_by_key_name(key)\n            if not p:\n                return\n            p.update_week_count()\n            p.put()\n        key = self.request.get('key')\n        if not key:\n            return\n        db.run_in_transaction(update, key)\n \n\nclass Project(db.Model):\n    name = db.StringProperty(indexed=True)\n    descr = db.StringProperty()\n    web_url = db.StringProperty()\n    package = db.ReferenceProperty(Package)\n    category = db.StringProperty(indexed=True)\n    tags = db.ListProperty(str)\n    approved = db.BooleanProperty(indexed=True)\n\n\nre_bitbucket = re.compile(r'^(bitbucket\\.org/[a-z0-9A-Z_.\\-]+/[a-zA-Z0-9_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?$')\nre_googlecode = re.compile(r'^[a-z0-9\\-]+\\.googlecode\\.com/(svn|hg|git)(/[a-z0-9A-Z_.\\-/]+)?$')\nre_github = re.compile(r'^github\\.com/[a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)+$')\nre_launchpad = re.compile(r'^launchpad\\.net/([a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)?|~[a-z0-9A-Z_.\\-]+/(\\+junk|[a-z0-9A-Z_.\\-]+)/[a-z0-9A-Z_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?$')\n\ndef vc_to_web(path):\n    if re_bitbucket.match(path):\n        m = re_bitbucket.match(path)\n        check_url = 'http://' + m.group(1) + '/?cmd=heads'\n        web = 'http://' + m.group(1) + '/'\n    elif re_github.match(path):\n        m = re_github_web.match(path)\n        check_url = 'https://raw.github.com/' + m.group(1) + '/' + m.group(2) + '/master/'\n        web = 'http://github.com/' + m.group(1) + '/' + m.group(2) + '/'\n    elif re_googlecode.match(path):\n        m = re_googlecode.match(path)\n        check_url = 'http://'+path\n        if not m.group(2):  # append / after bare '/hg' or '/git'\n            check_url += '/'\n        web = 'http://code.google.com/p/' + path[:path.index('.')]\n    elif re_launchpad.match(path):\n        check_url = web = 'https://'+path\n    else:\n        return False, False\n    return web, check_url\n\nre_bitbucket_web = re.compile(r'bitbucket\\.org/([a-z0-9A-Z_.\\-]+)/([a-z0-9A-Z_.\\-]+)')\nre_googlecode_web = re.compile(r'code.google.com/p/([a-z0-9\\-]+)')\nre_github_web = re.compile(r'github\\.com/([a-z0-9A-Z_.\\-]+)/([a-z0-9A-Z_.\\-]+)')\nre_launchpad_web = re.compile(r'launchpad\\.net/([a-z0-9A-Z_.\\-]+(/[a-z0-9A-Z_.\\-]+)?|~[a-z0-9A-Z_.\\-]+/(\\+junk|[a-z0-9A-Z_.\\-]+)/[a-z0-9A-Z_.\\-]+)(/[a-z0-9A-Z_.\\-/]+)?')\nre_striphttp = re.compile(r'https?://(www\\.)?')\n\ndef find_googlecode_vcs(path):\n    # Perform http request to path/hg or path/git to check if they're\n    # using mercurial or git.  Otherwise, assume svn.\n    for vcs in ['git', 'hg']:\n        try:\n            response = urlfetch.fetch('http://'+path+vcs, deadline=1)\n            if response.status_code == 200:\n                return vcs\n        except: pass\n    return 'svn'\n\ndef web_to_vc(url):\n    url = re_striphttp.sub('', url)\n    m = re_bitbucket_web.match(url)\n    if m:\n        return 'bitbucket.org/'+m.group(1)+'/'+m.group(2)\n    m = re_github_web.match(url)\n    if m:\n        return 'github.com/'+m.group(1)+'/'+m.group(2)\n    m = re_googlecode_web.match(url)\n    if m:\n        path = m.group(1)+'.googlecode.com/'\n        vcs = find_googlecode_vcs(path)\n        return path + vcs\n    m = re_launchpad_web.match(url)\n    if m:\n        return m.group(0)\n    return False\n\nMaxPathLength = 100\nCacheTimeout = 3600\n\nclass PackagePage(webapp.RequestHandler):\n    def get(self):\n        if self.request.get('fmt') == 'json':\n            return self.json()\n\n        html = memcache.get('view-package')\n        if not html:\n            tdata = {}\n\n            q = Package.all().filter('week_count >', 0)\n            q.order('-week_count')\n            tdata['by_week_count'] = q.fetch(50)\n\n            q = Package.all()\n            q.order('-last_install')\n            tdata['by_time'] = q.fetch(20)\n\n            q = Package.all()\n            q.order('-count')\n            tdata['by_count'] = q.fetch(100)\n\n            path = os.path.join(os.path.dirname(__file__), 'package.html')\n            html = template.render(path, tdata)\n            memcache.set('view-package', html, time=CacheTimeout)\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        self.response.out.write(html)\n\n    def json(self):\n        json = memcache.get('view-package-json')\n        if not json:\n            q = Package.all()\n            s = '{\"packages\": ['\n            sep = ''\n            for r in q.fetch(1000):\n                s += '%s\\n\\t{\"path\": \"%s\", \"last_install\": \"%s\", \"count\": \"%s\"}' % (sep, r.path, r.last_install, r.count)\n                sep = ','\n            s += '\\n]}\\n'\n            json = s\n            memcache.set('view-package-json', json, time=CacheTimeout)\n        self.response.set_status(200)\n        self.response.headers['Content-Type'] = 'text/plain; charset=utf-8'\n        self.response.out.write(json)\n\n    def can_get_url(self, url):\n        try:\n            urllib2.urlopen(urllib2.Request(url))\n            return True\n        except:\n            return False\n\n    def is_valid_package_path(self, path):\n        return (re_bitbucket.match(path) or\n            re_googlecode.match(path) or\n            re_github.match(path) or\n            re_launchpad.match(path))\n\n    def record_pkg(self, path):\n        # sanity check string\n        if not path or len(path) > MaxPathLength or not self.is_valid_package_path(path):\n            return False\n\n        # look in datastore\n        key = 'pkg-' + path\n        p = Package.get_by_key_name(key)\n        if p is None:\n            # not in datastore - verify URL before creating\n            web, check_url = vc_to_web(path)\n            if not web:\n                logging.error('unrecognized path: %s', path)\n                return False\n            if not self.can_get_url(check_url):\n                logging.error('cannot get %s', check_url)\n                return False\n            p = Package(key_name = key, path = path, count = 0, web_url = web)\n\n        if auth(self.request):\n            # builder updating package metadata\n            p.info = self.request.get('info')\n            p.ok = self.request.get('ok') == \"true\"\n            if p.ok:\n                p.last_ok = datetime.datetime.utcnow()\n        else:\n            # goinstall reporting an install\n            p.inc()\n            p.last_install = datetime.datetime.utcnow()\n\n        # update package object\n        p.put()\n        return True\n\n    def post(self):\n        path = self.request.get('path')\n        ok = db.run_in_transaction(self.record_pkg, path)\n        if ok:\n            self.response.set_status(200)\n            self.response.out.write('ok')\n        else:\n            logging.error('invalid path in post: %s', path)\n            self.response.set_status(500)\n            self.response.out.write('not ok')\n\nclass ProjectPage(webapp.RequestHandler):\n\n    def get(self):\n        admin = users.is_current_user_admin()\n        if self.request.path == \"/project/login\":\n            self.redirect(users.create_login_url(\"/project\"))\n        elif self.request.path == \"/project/logout\":\n            self.redirect(users.create_logout_url(\"/project\"))\n        elif self.request.path == \"/project/edit\" and admin:\n            self.edit()\n        elif self.request.path == \"/project/assoc\" and admin:\n            self.assoc()\n        else:\n            self.list()\n\n    def assoc(self):\n        projects = Project.all()\n        for p in projects:\n            if p.package:\n                continue\n            path = web_to_vc(p.web_url)\n            if not path:\n                continue\n            pkg = Package.get_by_key_name(\"pkg-\"+path)\n            if not pkg:\n                self.response.out.write('no: %s %s<br>' % (p.web_url, path))\n                continue\n            p.package = pkg\n            p.put()\n            self.response.out.write('yes: %s %s<br>' % (p.web_url, path))\n\n    def post(self):\n        if self.request.path == \"/project/edit\":\n            self.edit(True)\n        else:\n            data = dict(map(lambda x: (x, self.request.get(x)), [\"name\",\"descr\",\"web_url\"]))\n            if reduce(lambda x, y: x or not y, data.values(), False):\n                data[\"submitMsg\"] = \"You must complete all the fields.\"\n                self.list(data)\n                return\n            p = Project.get_by_key_name(\"proj-\"+data[\"name\"])\n            if p is not None:\n                data[\"submitMsg\"] = \"A project by this name already exists.\"\n                self.list(data)\n                return\n            p = Project(key_name=\"proj-\"+data[\"name\"], **data)\n            p.put()\n\t\t\n            path = os.path.join(os.path.dirname(__file__), 'project-notify.txt')\n            mail.send_mail(\n                sender=const.mail_from,\n                to=const.mail_submit_to,\n                subject=const.mail_submit_subject,\n                body=template.render(path, {'project': p}))\n\n            self.list({\"submitMsg\": \"Your project has been submitted.\"})\n\n    def list(self, additional_data={}):\n        cache_key = 'view-project-data'\n        tag = self.request.get('tag', None)\n        if tag:\n            cache_key += '-'+tag\n        data = memcache.get(cache_key)\n        admin = users.is_current_user_admin()\n        if admin or not data:\n            projects = Project.all().order('category').order('name')\n            if not admin:\n                projects = projects.filter('approved =', True)\n            projects = list(projects)\n\n            tags = sets.Set()\n            for p in projects:\n                for t in p.tags:\n                    tags.add(t)\n\n            if tag:\n                projects = filter(lambda x: tag in x.tags, projects)\n\n            data = {}\n            data['tag'] = tag\n            data['tags'] = tags\n            data['projects'] = projects \n            data['admin']= admin\n            if not admin:\n                memcache.set(cache_key, data, time=CacheTimeout)\n\n        for k, v in additional_data.items():\n            data[k] = v\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        path = os.path.join(os.path.dirname(__file__), 'project.html')\n        self.response.out.write(template.render(path, data))\n\n    def edit(self, save=False):\n        if save:\n            name = self.request.get(\"orig_name\")\n        else:\n            name = self.request.get(\"name\")\n\n        p = Project.get_by_key_name(\"proj-\"+name)\n        if not p:\n            self.response.out.write(\"Couldn't find that Project.\")\n            return\n\n        if save:\n            if self.request.get(\"do\") == \"Delete\":\n                p.delete()\n            else:\n                pkg_name = self.request.get(\"package\", None)\n                if pkg_name:\n                    pkg = Package.get_by_key_name(\"pkg-\"+pkg_name)\n                    if pkg:\n                        p.package = pkg.key()\n                for f in ['name', 'descr', 'web_url', 'category']:\n                    setattr(p, f, self.request.get(f, None))\n                p.approved = self.request.get(\"approved\") == \"1\"\n                p.tags = filter(lambda x: x, self.request.get(\"tags\", \"\").split(\",\"))\n                p.put()\n            memcache.delete('view-project-data')\n            self.redirect('/project')\n            return\n\n        # get all project categories and tags\n        cats, tags = sets.Set(), sets.Set()\n        for r in Project.all():\n            cats.add(r.category)\n            for t in r.tags:\n                tags.add(t)\n\n        self.response.headers['Content-Type'] = 'text/html; charset=utf-8'\n        path = os.path.join(os.path.dirname(__file__), 'project-edit.html')\n        self.response.out.write(template.render(path, { \n            \"taglist\": tags, \"catlist\": cats, \"p\": p, \"tags\": \",\".join(p.tags) }))\n\n    def redirect(self, url):\n        self.response.set_status(302)\n        self.response.headers.add_header(\"Location\", url)\n\ndef main():\n    app = webapp.WSGIApplication([\n        ('/package', PackagePage),\n        ('/package/daily', PackageDaily),\n        ('/project.*', ProjectPage),\n        ], debug=True)\n    run_wsgi_app(app)\n\nif __name__ == '__main__':\n    main()", "original_line_number": 4362, "instruction_length": 1210}
{"instruction": "This code provides a RollbackDebianGitRepository class that can perform rollbacks. To use it, create an instance of RollbackDebianGitRepository and call its methods to perform Git operations. To remember a ref for rollback, call the rrr() method with the refname, action, and reftype as arguments. To perform a complete rollback, call the rollback() method. Some Git operations are wrapped to automatically remember the ref for rollback, such as create_tag(), commit_dir(), create_branch(), and merge().", "input": "", "output": "# vim: set fileencoding=utf-8 :\n#\n# (C) 2018 Guido Günther <agx@sigxcpu.org>\n\"\"\"A git repository for Debian packages that can roll back operations\"\"\"\n\n\nfrom .. import log\nfrom .. git import GitRepositoryError\nfrom . git import DebianGitRepository\n\n\nclass RollbackError(GitRepositoryError):\n    \"\"\"\n    Error raised if the rollback failed\n    \"\"\"\n    def __init__(self, errors):\n        self.msg = \"Automatic rollback failed\"\n        super(RollbackError, self).__init__(self.msg)\n        self.errors = errors\n\n    def __str__(self):\n        return \"%s %s\" % (self.msg, self.errors)\n\n\nclass RollbackDebianGitRepository(DebianGitRepository):\n    \"\"\"\n    Like a DebianGitRepository but can also perform rollbacks and knows\n    about some of the inner workings upstream vcs_tag, …\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        self.rollbacks = []\n        self.rollback_errors = []\n        DebianGitRepository.__init__(self, *args, **kwargs)\n\n    def has_rollbacks(self):\n        return len(self.rollbacks) > 0\n\n    def rrr(self, refname, action, reftype):\n        \"\"\"\n        Remember ref for rollback\n\n        @param refname: ref to roll back\n        @param action: the rollback action (delete, reset, ...)\n        @param reftype: the reference type (tag, branch, ...)\n        \"\"\"\n        sha = None\n\n        if action == 'reset':\n            try:\n                sha = self.rev_parse(refname)\n            except GitRepositoryError as err:\n                log.warn(\"Failed to rev-parse '%s': %s\" % (refname, err))\n        elif action == 'delete':\n            pass\n        elif action == 'abortmerge':\n            pass\n        else:\n            raise GitRepositoryError(\"Unknown action '%s' for %s '%s'\" % (action, reftype, refname))\n        self.rollbacks.append((refname, reftype, action, sha))\n\n    def rrr_branch(self, branchname, action='reset-or-delete'):\n        if action == 'reset-or-delete':\n            if self.has_branch(branchname):\n                return self.rrr(branchname, 'reset', 'branch')\n            else:\n                return self.rrr(branchname, 'delete', 'branch')\n        else:\n            return self.rrr(branchname, action, 'branch')\n\n    def rrr_tag(self, tagname, action='delete'):\n        return self.rrr(tagname, action, 'tag')\n\n    def rrr_merge(self, commit, action='abortmerge'):\n        return self.rrr(commit, action, 'commit')\n\n    def rollback(self):\n        \"\"\"\n        Perform a complete rollback\n\n        Try to roll back as much as possible and remember what failed.\n        \"\"\"\n        for (name, reftype, action, sha) in self.rollbacks:\n            try:\n                if action == 'delete':\n                    log.info(\"Rolling back %s '%s' by deleting it\" % (reftype, name))\n                    if reftype == 'tag':\n                        self.delete_tag(name)\n                    elif reftype == 'branch':\n                        self.delete_branch(name)\n                    else:\n                        raise GitRepositoryError(\"Don't know how to delete %s '%s'\" % (reftype, name))\n                elif action == 'reset' and reftype == 'branch':\n                    log.info('Rolling back branch %s by resetting it to %s' % (name, sha))\n                    self.update_ref(\"refs/heads/%s\" % name, sha, msg=\"gbp import-orig: failure rollback of %s\" % name)\n                elif action == 'abortmerge':\n                    if self.is_in_merge():\n                        log.info('Rolling back failed merge of %s' % name)\n                        self.abort_merge()\n                    else:\n                        log.info(\"Nothing to rollback for merge of '%s'\" % name)\n                else:\n                    raise GitRepositoryError(\"Don't know how to %s %s '%s'\" % (action, reftype, name))\n            except GitRepositoryError as e:\n                self.rollback_errors.append((name, reftype, action, sha, e))\n        if self.rollback_errors:\n            raise RollbackError(self.rollback_errors)\n\n    # Wrapped methods for rollbacks\n    def create_tag(self, *args, **kwargs):\n        name = kwargs['name']\n        ret = super(RollbackDebianGitRepository, self).create_tag(*args, **kwargs)\n        self.rrr_tag(name)\n        return ret\n\n    def commit_dir(self, *args, **kwargs):\n        import_branch = kwargs['branch']\n        self.rrr_branch(import_branch)\n        return super(RollbackDebianGitRepository, self).commit_dir(*args, **kwargs)\n\n    def create_branch(self, *args, **kwargs):\n        branch = kwargs['branch']\n        ret = super(RollbackDebianGitRepository, self).create_branch(*args, **kwargs)\n        self.rrr_branch(branch, 'delete')\n        return ret\n\n    def merge(self, *args, **kwargs):\n        commit = args[0] if args else kwargs['commit']\n        try:\n            return super(RollbackDebianGitRepository, self).merge(*args, **kwargs)\n        except GitRepositoryError:\n            # Only cleanup in the error case to undo working copy\n            # changes. Resetting the refs handles the other cases.\n            self.rrr_merge(commit)\n            raise", "original_line_number": 4389, "instruction_length": 503}
{"instruction": "To use the code, call it with the following arguments: <filename> <pragma_xml_file> <executable_name> <profiling_iterations> True/False (for output). The code will run the executable for the specified number of iterations and aggregate the information in executable_profile.xml. It will then create nested and flow graphs for each function, as well as a complete call-tree graph. The code will then search for the optimal schedule using a parallel or sequential algorithm with a timing constraint. If a feasible schedule is found, it will be saved in a schedule file.", "input": "", "output": "import sys\nimport pargraph as par\nimport copy\nimport schedule as sched\nimport profiler as pro\nimport time\nimport multiprocessing\nimport itertools\nimport random\nimport threading\n\n\n\n\n\"\"\" Usage: call with <filename> <pragma_xml_file> <executable_name> <profiling_interations> True/False (for output) \"\"\"\n\nif __name__ == \"__main__\":\n\t\n\tpragma_xml = sys.argv[1]\n\texecutable = sys.argv[2]\n\tcount = int(sys.argv[3])\n\toutput = sys.argv[4]\n\texecution_time = float(sys.argv[5])\n\tdeadline = float(sys.argv[6])\n\tmulti = sys.argv[7]\n\n\t#runs count time the executable and aggregates the informations in executable_profile.xml. The single profile outputs are saved as profile+iter.xml\n\tprofile_xml = pro.profileCreator(count, executable)\n\n\t#return the nested dot graphs in code style (one for each function)\n\tvisual_nested_graphs = par.getNesGraph(pragma_xml, profile_xml)\n\n\t#returns the graphs to be visualized and the object graphs in flow style (one for each function)\n\t(visual_flow_graphs, flow_graphs) = par.getParalGraph(pragma_xml, profile_xml) \n\n\ti = 0\n\n\tfor g in visual_nested_graphs:\n\t\tg.write_pdf('graphs/%s_code.pdf'%flow_graphs[i].type)\n\t\tg.write_dot('graphs/%s_code.dot'%flow_graphs[i].type)\n\t\ti += 1\n\n\ti = 0\n\tfor g in visual_flow_graphs:\n\t\tg.write_pdf('graphs/%s_flow.pdf'%flow_graphs[i].type)\n\t\tg.write_dot('graphs/%s_flow.dot'%flow_graphs[i].type)\n\t\ti += 1\n\n\t#creates the flow type graph --> flow.xml\n\tpar.dump_graphs(flow_graphs)\n\t#adding to the original xml the profiling informations --> code.xml\n\tpro.add_profile_xml(profile_xml, pragma_xml)\n\t#creating the total graph with the call-tree\n\tfunc_graph = par.create_complete_graph(visual_flow_graphs, profile_xml)\n\t#creating the graphs with the function calls\n\tfunc_graph.write_pdf('graphs/function_graphs.pdf')\n\tfunc_graph.write_dot('graphs/function_graphs.dot')\n\n\t#creating the expanded graph where the functions are inserted in the flow graph\n\texp_flows = copy.deepcopy(flow_graphs)\n\tpar.explode_graph(exp_flows)\n\tmain_flow = sched.get_main(exp_flows)\n\t\n\t#creating a generator for the expanded graph\n\tgen = sched.generate_task(main_flow)\n\t\n\t#creating a new generator for the expanded graph\n\tsched.make_white(main_flow)\n\n\t#getting the number of physical cores of the machine profiled\n\tmax_flows = sched.get_core_num(profile_xml)\n\tmax_flows = 4\n\t#getting cores of the actual machine, but the problem is multithreading\n\tcores = multiprocessing.cpu_count()\n\tif cores == 1:\n\t\tcores = 2\n\t\n\t#initializing all the lists for the parallel scheduling algorithm\n\ttasks_list = []\n\ttask_list = []\n\tflows_list = []\n\toptimal_flow_list = []\n\tp_list = []\n\tqueue_list = []\n\tresults = []\n\tnum_tasks = 0\n\n\t#getting the number of tasks in the expanded graph and creating a list of task\n\tfor task in gen:\n\t\ttask_list.append(task)\n\t\tnum_tasks += 1\n\n\tif output == 'True':\n\t\tsched.make_white(main_flow)\n\t\tpar.scanGraph(main_flow)\n\t\n\t#starting the parallel or sequential search of the best solution with a timing constrain\n\tif multi == 'parallel':\n\t\tfor core in range(cores):\n\t\t\ttmp = []\n\t\t\toptimal_flow_list.append(tmp)\n\t\t\ttmp_2 = []\n\t\t\tflows_list.append(tmp_2)\n\t\t\trandom.shuffle(task_list)\n\t\t\ttasks_list.append(copy.deepcopy(task_list))\n\t\t\tq = sched.Queue()\n\t\t\tqueue_list.append(q)\n\t\t\tp_list.append(multiprocessing.Process(target = sched.get_optimal_flow, args = (flows_list[core], tasks_list[core], 0, optimal_flow_list[core], num_tasks, max_flows, execution_time, queue_list[core],  )))\n\t\t\tprint \"starting core: \",core\n\t\t\tp_list[core].start()\n\t\t#getting the results from the processes\n\t\tfor queue in queue_list:\n\t\t\tt = queue.q.get()\n\t\t\tresults.append(t)\n\t\t#joining all the processes\n\t\ti = 0\n\t\tfor p in p_list:\n\t\t\tp.join()\n\t\t\tprint \"core \", i, \" joined\"\n\t\t\ti += 1\n\t\t#getting the best result\n\t\toptimal_flow = results[0]\n\t\tbest = 0\n\t\tfor i in range(len(results)):\n\t\t\tprint \"result:\"\n\t\t\tfor flow in results[i]:\n\t\t\t\tflow.dump()\n\t\t\t\tif sched.get_cost(results[i]) < sched.get_cost(optimal_flow):\n\t\t\t\t\tbest = i\n\t\toptimal_flow = results[best]\n\telse:\n\t\t\toptimal_flow = []\n\t\t\tflow_list = []\n\t\t\texecution_time += time.clock()\n\t\t\tprint \"searching best schedule\"\n\t\t\tsched.get_optimal_flow_single(flow_list, task_list, 0, optimal_flow, num_tasks, max_flows, execution_time )\n\n\t\n\n\t#printing the best result\t\n\tprint \"solution:\"\n\tfor flow in optimal_flow:\n\t\tflow.dump(\"\\t\")\n\t\tprint \"\\ttime:\",flow.time\n\n\n\n\t#substitutes \"for tasks\" with splitted versions if present in the optimal flows\n\tpar.add_new_tasks(optimal_flow, main_flow)\n\tsched.make_white(main_flow)\n\tgen_ = sched.generate_task(main_flow)\n\n\n\tt_list = []\n\tfor t in gen_:\n\t\tt_list.append(t)\n\t\t\"\"\"\n\t\tprint t.type,\" @ \", t.start_line, \" has parents:\"\n\t\tfor p in t.parent:\n\t\t\tprint \"\\t \",p.type,\" @ \", p.start_line\n\t\tprint \"and children:\"\n\t\tfor c in t.children:\n\t\t\tprint \"\\t \",c.type,\" @ \", c.start_line\n\t\tprint\n\t\t\"\"\"\n\t\n\t#adds id's to all the tasks to retrive the flow to which they belong\n\tpar.add_flow_id(optimal_flow, t_list)\n\n\t#sets arrival times and deadlines using a modified version of the chetto algorithm\n\tsched.chetto(main_flow, deadline, optimal_flow)\n\t\n\t#checks if the schedule is feasible and in case creates the schedule file\n\tif sched.check_schedule(main_flow):\n\t\tsched.create_schedule(main_flow, len(optimal_flow))\n\t\tsched.make_white(main_flow)\n\t\t#sched.print_schedule(main_flow)\n\telse:\n\t\tprint \"tasks not schedulable, try with more search time\"\n\n\t#prints extended info of the entire pragma graph", "original_line_number": 4401, "instruction_length": 567}
{"instruction": "Create a Packet class that generates a binary packet from provided init_packet_fields. The class should include a method called generate_packet that returns a string representing the init_packet (in binary). The class should also include a dictionary called init_packet_fields that contains the packet fields. The class should use little endian format with standard sizes for python. The class should include an enum called PacketField that contains the packet field values. The class should include constants for unsigned short, unsigned int, unsigned char, and char array. The class should include a private method called __generate_struct_format_string that generates a struct format string based on keys that are present in init_packet_fields. The class should handle fields that require length and fields that require a specific format.", "input": "", "output": "# Copyright (c) 2015, Nordic Semiconductor\r\n# All rights reserved.\r\n#\r\n# Redistribution and use in source and binary forms, with or without\r\n# modification, are permitted provided that the following conditions are met:\r\n#\r\n# * Redistributions of source code must retain the above copyright notice, this\r\n#   list of conditions and the following disclaimer.\r\n#\r\n# * Redistributions in binary form must reproduce the above copyright notice,\r\n#   this list of conditions and the following disclaimer in the documentation\r\n#   and/or other materials provided with the distribution.\r\n#\r\n# * Neither the name of Nordic Semiconductor ASA nor the names of its\r\n#   contributors may be used to endorse or promote products derived from\r\n#   this software without specific prior written permission.\r\n#\r\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\r\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\r\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\r\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\r\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\r\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\r\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\r\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\r\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\r\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\nfrom enum import Enum\r\nimport struct\r\n\r\n\r\nINIT_PACKET_USES_CRC16 = 0\r\nINIT_PACKET_USES_HASH = 1\r\nINIT_PACKET_EXT_USES_ECDS = 2 \r\n\r\n\r\nclass PacketField(Enum):\r\n    PACKET_VERSION = 1\r\n    COMPRESSION_TYPE = 2\r\n    DEVICE_TYPE = 3\r\n    DEVICE_REVISION = 4\r\n    APP_VERSION = 5\r\n    REQUIRED_SOFTDEVICES_ARRAY = 6\r\n    OPT_DATA = 7\r\n    NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID = 8\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH = 9\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH = 10\r\n    NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16 = 11\r\n    NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS = 12\r\n\r\n\r\nclass Packet(object):\r\n    \"\"\"\r\n    Class that implements the INIT packet format.\r\n    http://developer.nordicsemi.com/nRF51_SDK/doc/7.1.0/s110/html/a00065.html\r\n    \"\"\"\r\n\r\n    UNSIGNED_SHORT = \"H\"\r\n    UNSIGNED_INT = \"I\"\r\n    UNSIGNED_CHAR = \"B\"\r\n    CHAR_ARRAY = \"s\"\r\n\r\n    def __init__(self, init_packet_fields):\r\n        \"\"\"\r\n\r\n            :param init_packet_fields: Dictionary with packet fields\r\n        \"\"\"\r\n        self.init_packet_fields = init_packet_fields\r\n\r\n    def generate_packet(self):\r\n        \"\"\"\r\n        Generates a binary packet from provided init_packet_fields provided in constructor.\r\n        This version includes the extended data\r\n\r\n        :return str: Returns a string representing the init_packet (in binary)\r\n\r\n        \"\"\"\r\n        # Create struct format string based on keys that are\r\n        # present in self.init_packet_fields\r\n        format_string = self.__generate_struct_format_string()\r\n        args = []\r\n\r\n        for key in sorted(self.init_packet_fields.keys(), key=lambda x: x.value):\r\n            # Add length to fields that required that\r\n            if key in [PacketField.REQUIRED_SOFTDEVICES_ARRAY,\r\n                       PacketField.OPT_DATA]:\r\n                args.append(len(self.init_packet_fields[key]))\r\n                args.extend(self.init_packet_fields[key])\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID]:\r\n                args.append(self.init_packet_fields[key]) # Extended packet id format\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH]:\r\n                args.append(self.init_packet_fields[key]) # Length of firmware image\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16]:\r\n                args.append(self.init_packet_fields[key])  # CRC-16\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH]:\r\n                args.append(self.init_packet_fields[key])  # SHA-256 hash of firmware image\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS]:\r\n                args.append(self.init_packet_fields[key])  # ECDS of base init packet using Curve P-256 amd SHA-256\r\n            else:\r\n                args.append(self.init_packet_fields[key])\r\n\r\n        return struct.pack(format_string, *args)\r\n\r\n    def __generate_struct_format_string(self):\r\n        format_string = \"<\"  # Use little endian format with standard sizes for python,\r\n        # see https://docs.python.org/2/library/struct.html\r\n\r\n        for key in sorted(self.init_packet_fields.keys(), key=lambda x: x.value):\r\n            if key in [PacketField.PACKET_VERSION,\r\n                       PacketField.COMPRESSION_TYPE,\r\n                       PacketField.DEVICE_TYPE,\r\n                       PacketField.DEVICE_REVISION,\r\n                       ]:\r\n                format_string += Packet.UNSIGNED_SHORT\r\n\r\n            elif key in [PacketField.APP_VERSION]:\r\n                format_string += Packet.UNSIGNED_INT\r\n            elif key in [PacketField.REQUIRED_SOFTDEVICES_ARRAY]:\r\n                array_elements = self.init_packet_fields[key]\r\n                format_string += Packet.UNSIGNED_SHORT  # Add length field to format packet\r\n\r\n                for _ in range(len(array_elements)):\r\n                    format_string += Packet.UNSIGNED_SHORT\r\n            elif key in [PacketField.OPT_DATA]:\r\n                format_string += Packet.UNSIGNED_SHORT  # Add length field to optional data\r\n                format_string += \"{0}{1}\".format(len(self.init_packet_fields[key]), Packet.CHAR_ARRAY)\r\n            elif key in [PacketField.NORDIC_PROPRIETARY_OPT_DATA_EXT_PACKET_ID]:\r\n                format_string += Packet.UNSIGNED_INT # Add the extended packet id field\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_LENGTH:\r\n                format_string += Packet.UNSIGNED_INT # Add the firmware length field\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_HASH:\r\n                format_string += \"32{0}\".format(Packet.CHAR_ARRAY)  # SHA-256 requires 32 bytes\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_FIRMWARE_CRC16:\r\n                format_string += Packet.UNSIGNED_SHORT\r\n            elif key == PacketField.NORDIC_PROPRIETARY_OPT_DATA_INIT_PACKET_ECDS:\r\n                format_string += \"64{0}\".format(Packet.CHAR_ARRAY)  # ECDS based on P-256 using SHA-256 requires 64 bytes\r\n\r\n        return format_string", "original_line_number": 4867, "instruction_length": 841}
{"instruction": "Create an animation of the motion of rigid bodies in a fluid. The animation should display the position and orientation of the rigid bodies over time, as well as their energy distribution. The animation should be generated using the given code, which includes functions for calculating the rotation transform and animating the motion of the rigid bodies. The code also includes a class for defining the properties and behavior of the rigid bodies, as well as parameters for the fluid and beam properties. The animation should be displayed using matplotlib.", "input": "", "output": "import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation\nfrom scipy import constants\n\ndef rotation_transform(axis, angle, ax = None):\n    if ax is None: ax = plt.gca()\n    t_scale = ax.transData\n    t_rotate = mpl.transforms.Affine2D().rotate_deg_around(axis[0], axis[1], angle*180/np.pi)\n    return t_rotate + t_scale\n\ndef animate(spheres, radius, T):\n    r = radius*1e9\n    for sphere in spheres:\n        sphere.x *= 1e9\n        sphere.y *= 1e9\n\n    xmin = np.inf; xmax = -np.inf\n    ymin = np.inf; ymax = -np.inf\n\n    fig,axes = plt.subplots(1,2)\n    plt.subplots_adjust(hspace=0.3)\n    plt.subplot(axes[0])\n    circles = []\n    lines = []\n    for i,sphere in enumerate(spheres):\n        xmin = min(np.min(sphere.x), xmin)\n        xmax = max(np.max(sphere.x), xmax)\n        ymin = min(np.min(sphere.y), ymin)\n        ymax = max(np.max(sphere.y), ymax)\n\n        \n        circles.append(plt.Circle([sphere.x[0], sphere.y[0]], r, animated=True, edgecolor='C{}'.format(i), fc='white', lw=2))\n        lines.append(plt.Line2D([sphere.x[0]-r, sphere.x[0]+r], [sphere.y[0], sphere.y[0]], lw=1.5, color='black', animated=True))\n        plt.gca().add_artist(circles[-1])\n        plt.gca().add_line(lines[-1])\n\n    plt.xlim([xmin-r, xmax+r])\n    plt.ylim([ymin-r, ymax+r])\n    plt.xlabel(\"x (nm)\")\n    plt.ylabel(\"y (nm)\")\n    plt.gca().set_aspect('equal')\n\n\n    ax = plt.gca()\n    title = ax.text(.97,0.03, r\"{0:.2f} $\\mu$s\".format(0.0), transform=ax.transAxes, horizontalalignment='right', fontsize=13, animated=True)\n    def update(t):\n        for i,sphere in enumerate(spheres):\n            circles[i].center = (sphere.x[t], sphere.y[t])\n            lines[i].set_data([sphere.x[t]-r, sphere.x[t]+r], [sphere.y[t], sphere.y[t]])\n            lines[i].set_transform(rotation_transform([sphere.x[t], sphere.y[t]], sphere.phi[t], ax=ax))\n            title.set_text(r\"{0:.2f} $\\mu$s\".format(dt*t*1.e6))\n\n        return circles + lines + [title]\n\n\n    anim = animation.FuncAnimation(plt.gcf(), update, frames=np.arange(0,len(spheres[0].x),1), interval=6, blit=True, repeat=True)\n\n    # plt.figure()\n    # plt.plot(time, phi)\n    # Writer = animation.writers['ffmpeg']\n    # writer = Writer(fps=30, bitrate=7800)\n    # anim.save('test.mp4', writer=writer, dpi=200)\n    # plt.plot(x, y)\n\n    # plt.figure()\n    # rad_data = np.array([(sphere.x**2 + sphere.y**2)**0.5 for sphere in spheres])\n    # plt.hist(rad_data.flatten())\n    # plt.figure()\n    # phi_data = np.array([sphere.phi[4000:] % 2*np.pi for sphere in spheres])\n    # plt.hist(phi_data.flatten())\n\n    plt.subplot(axes[1])\n    plt.plot(spheres[0].x, spheres[0].y)\n    plt.plot(spheres[1].x, spheres[1].y)\n    plt.xlabel(\"x (nm)\")\n    plt.ylabel(\"y (nm)\")\n    plt.xlim([xmin-r, xmax+r])\n    plt.ylim([ymin-r, ymax+r])\n    plt.gca().set_aspect('equal')\n    plt.suptitle(r\"time step: {0:.1f} ns, T = {1} K, $\\mu$ = {2:.2f} mPa$\\cdot$s\".format(dt*1e9, T, mu*1e3))\n\n    plt.figure()\n    kT = constants.k*T\n    KE_x = 0.5*spheres[0].mass*spheres[0].vx**2/kT\n    KE_y = 0.5*spheres[0].mass*spheres[0].vy**2/kT\n    KE_r = 0.5*spheres[0].Iz*spheres[0].omega**2/kT\n    plt.hist(KE_r[np.isfinite(KE_r)], color = 'C2', bins=np.linspace(0,2.5,80), alpha=0.5)\n    plt.hist(KE_x[np.isfinite(KE_x)], color = 'C0', bins=np.linspace(0,2.5,80), alpha=0.5)\n    plt.hist(KE_y[np.isfinite(KE_y)], color = 'C1', bins=np.linspace(0,2.5,80), alpha=0.5)\n    plt.axvline(x = 0.5*constants.k*T/kT, color='black')\n\n    plt.figtext(.85,.8, r\"$\\frac{{1}}{{2}}kT$\", horizontalalignment='right',  verticalalignment='top', fontsize=14)\n    plt.figtext(.85,.70, r\"$\\left< \\frac{{1}}{{2}}mv_x^2 \\right>$: {0:.3f} $kT$\".format(np.average(KE_x)) , horizontalalignment='right', verticalalignment='top', fontsize=14, color='C0')\n    plt.figtext(.85,.6, r\"$\\left< \\frac{{1}}{{2}}mv_y^2 \\right>$: {0:.3f} $kT$\".format(np.average(KE_y)) , horizontalalignment='right',  verticalalignment='top', fontsize=14, color='C1')\n    plt.figtext(.85,.5, r\"$\\left< \\frac{{1}}{{2}}I_z \\omega_z^2 \\right>$: {0:.3f} $kT$\".format(np.average(KE_r)) , horizontalalignment='right',  verticalalignment='top', fontsize=14, color='C2')\n    plt.xlim([0,3*0.5])\n\n    plt.xlabel(\"energy (kT)\")\n    plt.ylabel(\"count\")\n    plt.suptitle(r\"time step: {0:.1f} ns, T = {1} K, $\\mu$ = {2:.2f} mPa$\\cdot$s\".format(dt*1e9, T, mu*1e3))\n\n    plt.figure()\n    plt.plot(time*1e3, spheres[0].phi*180/np.pi)\n    plt.plot(time*1e3, spheres[1].phi*180/np.pi)\n    plt.xlabel(\"time (ms)\")\n    plt.ylabel(\"angle (deg.)\")\n\n    plt.show()\n\nclass rigid_body:\n    def __init__(self, pos, mass, Iz):\n        self.pos = pos\n        self.angle = 0\n        self.mass = mass\n        self.Iz = Iz\n\n        self.velocity = np.zeros(2)\n        self.angular_velocity = 0\n\n        self.F = np.zeros(2)\n        self.prevF = np.zeros(2)\n        self.T = 0\n        self.prevT = 0\n\n        self.predicted_velocity = np.zeros(2)\n        self.predicted_angular_velocity = 0\n\n        self.x = []\n        self.y = []\n        self.vx = []\n        self.vy = []\n        self.omega = []\n        self.phi = []\n\n    def predict(self, dt):\n        self.velocity += (self.prevF/self.mass)*dt/2\n        self.angular_velocity += (self.prevT/self.Iz)*dt/2\n\n        self.pos += self.velocity*dt\n        self.angle += self.angular_velocity*dt\n\n    def correct(self, F, T, dt):\n        self.velocity += (F/self.mass)*dt/2\n        self.angular_velocity += (T/self.Iz)*dt/2\n\n        self.prevF = F\n        self.prevT = T\n\n    def push(self, F, dt):\n        self.velocity += (F/self.mass)*dt\n        self.pos += self.velocity*dt\n\n    def twist(self, T, dt):\n        self.angular_velocity += (T/self.Iz)*dt\n        self.angle += self.angular_velocity*dt\n\n# final time and time step\ntf = 300e-6\ndt = 172e-9\ntf = 100000*dt\ntime = np.arange(0,tf,dt)\n\n# sphere properties\nradius = 150e-9\ndensity = 10490\n\nmass = 4/3*np.pi*radius**3*density\nIz = 2/5*mass*radius**2\n\n# initial conditions\nspheres = [ rigid_body(np.array([-200e-9,0.0]), mass, Iz),\n            rigid_body(np.array([200e-9,0.0]), mass, Iz) ]\n# spheres = [rigid_body(100e-9*np.array([x,0.0]), mass, Iz) for x in np.arange(-8,8,4)]\n# spheres = [rigid_body(100e-9*np.array([x,4.0]), mass, Iz) for x in np.arange(-8,8,4)]\n# spheres.extend([rigid_body(100e-9*np.array([x,-4.0]), mass, Iz) for x in np.arange(-8,8,4)])\n# spheres.extend([rigid_body(100e-9*np.array([x,-8.0]), mass, Iz) for x in np.arange(-8,8,4)])\n\n# fluid properties\nmu = 0.6e-3           # liquid viscosity\ntemp = 320          # temperature\n\nalpha_T = 6*np.pi*mu*radius\nalpha_R = 8*np.pi*mu*radius**3\nbeta_T = (2*alpha_T*constants.k*temp/dt)**0.5\nbeta_R = (2*alpha_R*constants.k*temp/dt)**0.5\n\n# Electrostatic repulsion\nestatic = 0\n\n# beam properties\nr0 = 400e-9       # radius\nI = 0e-12          # intensity\nl = 0e-12         # angular drive\ncenter = np.array([0,-200e-9])   # beam center\n\nfor t in time:\n    # Fadd = np.zeros((len(spheres),2))\n    # for i in range(0,len(spheres)):\n        # for j in range(i+1,len(spheres)):\n            # d = spheres[i].pos - spheres[j].pos\n            # r = np.linalg.norm(d)\n            # Fadd[i] += estatic*d/r**3\n            # Fadd[j] += -estatic*d/r**3\n            # if r < 2*radius:\n                # dv = spheres[i].velocity - spheres[j].velocity\n                # spheres[i].velocity -= np.dot(dv, d)/r**2 * d\n                # spheres[j].velocity += np.dot(dv, d)/r**2 * d\n\n\n\n    for i,sphere in enumerate(spheres):\n        sphere.predict(dt)\n\n        F = -alpha_T*sphere.velocity + beta_T*np.random.normal(size=2)\n        n = sphere.pos - center \n        dist = np.linalg.norm(n)\n        n /= dist \n        that = np.array([n[1], -n[0]])\n\n        # F += I*1*dist*np.exp(dist**2/r0**2)*(1 - dist**2/r0**2)*n\n        # F += l*that# *np.sin(t/59)\n        # F += Fadd[i]\n        T = -alpha_R*sphere.angular_velocity + beta_R*np.random.normal(size=1)[0]\n\n        sphere.correct(F,T,dt)\n\n        sphere.x.append(sphere.pos[0])\n        sphere.y.append(sphere.pos[1])\n        sphere.vx.append(sphere.velocity[0])\n        sphere.vy.append(sphere.velocity[1])\n        sphere.omega.append(sphere.angular_velocity)\n\n        sphere.phi.append(sphere.angle)\n\nfor sphere in spheres:\n    sphere.x = np.asarray(sphere.x)\n    sphere.y = np.asarray(sphere.y)\n    sphere.vx = np.asarray(sphere.vx)\n    sphere.vy = np.asarray(sphere.vy)\n    sphere.phi = np.asarray(sphere.phi)\n    sphere.omega = np.asarray(sphere.omega)\n\nanimate(spheres, radius=radius, T=temp)\n# plt.plot(1e9*spheres[0].x, 1e9*spheres[0].y, '.-')\n# plt.plot(1e9*spheres[1].x, 1e9*spheres[1].y, '.-')\n\nplt.show()", "original_line_number": 5065, "instruction_length": 556}
{"instruction": "The given code provides a function to estimate pairwise similarity from a given model and split processing across multiple processors. To use this function, one needs to provide the BayesDB database object file, table name containing the raw data, model name to estimate from, and the name of the table to insert similarity results into. The function also has optional parameters such as the number of processors to use, the number of rows for which to estimate pairwise similarities, and whether to overwrite the sim_table if it already exists. The function splits the processing evenly among the specified number of processors and calculates the size and offset calculation for each worker query. It then creates the similarity table and inserts the results of ESTIMATEs into the table.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n#   Copyright (c) 2010-2016, MIT Probabilistic Computing Project\n#\n#   Licensed under the Apache License, Version 2.0 (the \"License\");\n#   you may not use this file except in compliance with the License.\n#   You may obtain a copy of the License at\n#\n#       http://www.apache.org/licenses/LICENSE-2.0\n#\n#   Unless required by applicable law or agreed to in writing, software\n#   distributed under the License is distributed on an \"AS IS\" BASIS,\n#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#   See the License for the specific language governing permissions and\n#   limitations under the License.\n\n\"\"\"Speed up BDB queries by parallelizing them.\n\nIntuition\n---------\n\n``ESTIMATE SIMILARITY FROM PAIRWISE t`` will run in ``O(n^2 m v)`` time, where\n``n`` is the number of rows, ``m`` is the number of models, and ``v`` is the\naverage number of views per model. While this query is reasonable for small\ndatasets, on medium-large datasets the query slows down significantly and can\nbecome intractable.  Splitting the processing up among multiple cores can\ngreatly reduce computation time; this module provides functionality to assist\nthis multiprocessing.\n\nCurrently, a multiprocessing equivalent is provided only for\n``ESTIMATE PAIRWISE SIMILARITY``. In fact, this is a query that is most likely\nto require multiprocessing, as datasets frequently have many more rows than\ncolumns.\n\nExample\n-------\n\nFollowing are (very) informal timing statistics with a 200 rows by 4 column\n.cvs file, run on a late 2012 MacBook Pro with a 2.5 GHz 2-core Intel Core i5::\n\n    id,one,two,three,four\n    0,2,3,4,two\n    1,1,5,4,three\n    2,5,1,5,one\n    ...\n    197,0,5,0,five\n    198,5,3,0,three\n    199,4,5,2,three\n\nAfter inserting this .csv data into a table ``t`` and analyzing it quickly::\n\n    bdb.execute('''\n        CREATE GENERATOR t_cc FOR t USING crosscat (\n            GUESS(*),\n            id IGNORE\n        )\n    ''')\n    bdb.execute('INITIALIZE 3 MODELS FOR t_cc')\n    bdb.execute('ANALYZE t_cc MODELS 0-2 FOR 10 ITERATIONS WAIT')\n\nThe corresponding similarity table thus has 200^2 = 40000 rows::\n\n    In [72]: %timeit -n 10 cursor_to_df(bdb.execute('ESTIMATE SIMILARITY FROM PAIRWISE t_cc'))\n    10 loops, best of 3: 9.56 s per loop\n\n    In [73]: %timeit -n 10 parallel.estimate_pairwise_similarity(bdb_file.name, 't', 't_cc', overwrite=True)\n    10 loops, best of 3: 5.16 s per loop  # And values are located in the t_similarity table.\n\nThe approximate 2x speed up is what would be expected from dividing the work\namong two cores. Further speed increases are likely with more powerful\nmachines.\n\n----\n\"\"\"\n\nfrom bayeslite.exception import BayesLiteException as BLE\nfrom bdbcontrib.bql_utils import cursor_to_df\nimport multiprocessing as mp\nfrom bayeslite import bayesdb_open, bql_quote_name\nfrom bayeslite.util import cursor_value\n\n\ndef _query_into_queue(query_string, params, queue, bdb_file):\n    \"\"\"\n    Estimate pairwise similarity of a certain subset of the bdb according to\n    query_string; place it in the multiprocessing Manager.Queue().\n\n    For two technical reasons, this function is defined as a toplevel class and\n    independently creates a bdb handle:\n\n    1) Multiprocessing workers must be pickleable, and thus must be\n       declared as toplevel functions;\n    2) Multiple threads cannot access the same bdb handle, lest concurrency\n       issues arise with corrupt data.\n\n    Parameters\n    ----------\n    query_string : str\n        Name of the query to execute, determined by estimate_similarity_mp.\n    queue : multiprocessing.Manager.Queue\n        Queue to place results into\n    bdb_file : str\n        File location of the BayesDB database. This function will\n        independently open a new BayesDB handler.\n    \"\"\"\n    bdb = bayesdb_open(pathname=bdb_file)\n    res = bdb.execute(query_string, params)\n    queue.put(cursor_to_df(res))\n\n\ndef _chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in xrange(0, len(l), n):\n        yield l[i:i+n]\n\n\ndef estimate_pairwise_similarity(bdb_file, table, model, sim_table=None,\n                                 cores=None, N=None, overwrite=False):\n    \"\"\"\n    Estimate pairwise similarity from the given model, splitting processing\n    across multiple processors, and save results into sim_table.\n\n    Because called methods in this function must also open up separate BayesDB\n    instances, this function accepts a BayesDB filename, rather than an actual\n    bayeslite.BayesDB object.\n\n    Parameters\n    ----------\n    bdb_file : str\n        File location of the BayesDB database object. This function will\n        handle opening the file with bayeslite.bayesdb_open.\n    table : str\n        Name of the table containing the raw data.\n    model : str\n        Name of the metamodel to estimate from.\n    sim_table : str\n        Name of the table to insert similarity results into. Defaults to\n        table name + '_similarity'.\n    cores : int\n        Number of processors to use. Defaults to the number of cores as\n        identified by multiprocessing.num_cores.\n    N : int\n        Number of rows for which to estimate pairwise similarities (so\n        N^2 calculations are done). Should be used just to test small\n        batches; currently, there is no control over what specific pairwise\n        similarities are estimated with this parameter.\n    overwrite : bool\n        Whether to overwrite the sim_table if it already exists. If\n        overwrite=False and the table exists, function will raise\n        sqlite3.OperationalError. Default True.\n    \"\"\"\n    bdb = bayesdb_open(pathname=bdb_file)\n\n    if cores is None:\n        cores = mp.cpu_count()\n\n    if cores < 1:\n        raise BLE(ValueError(\n            \"Invalid number of cores {}\".format(cores)))\n\n    if sim_table is None:\n        sim_table = table + '_similarity'\n\n    # Get number of occurrences in the database\n    count_cursor = bdb.execute(\n        'SELECT COUNT(*) FROM {}'.format(bql_quote_name(table))\n    )\n    table_count = cursor_value(count_cursor)\n    if N is None:\n        N = table_count\n    elif N > table_count:\n        raise BLE(ValueError(\n            \"Asked for N={} rows but {} rows in table\".format(N, table_count)))\n\n    # Calculate the size (# of similarities to compute) and\n    # offset (where to start) calculation for each worker query.\n\n    # Divide sizes evenly, and make the last job finish the remainder\n    sizes = [(N * N) / cores for i in range(cores)]\n    sizes[-1] += (N * N) % cores\n\n    total = 0\n    offsets = [total]\n    for size in sizes[:-1]:\n        total += size\n        offsets.append(total)\n\n    # Create the similarity table. Assumes original table has rowid column.\n    # XXX: tables don't necessarily have an autoincrementing primary key\n    # other than rowid, which is implicit and can't be set as a foreign key.\n    # We ought to ask for an optional user-specified foreign key, but\n    # ESTIMATE SIMILARITY returns numerical values rather than row names, so\n    # changing numerical rownames into that foreign key would be finicky. For\n    # now, we eliminate REFERENCE {table}(foreign_key) from the rowid0 and\n    # rowid1 specs.\n    sim_table_q = bql_quote_name(sim_table)\n    if overwrite:\n        bdb.sql_execute('DROP TABLE IF EXISTS {}'.format(sim_table_q))\n\n    bdb.sql_execute('''\n        CREATE TABLE {} (\n            rowid0 INTEGER NOT NULL,\n            rowid1 INTEGER NOT NULL,\n            value DOUBLE NOT NULL\n        )\n    '''.format(sim_table_q))\n\n    # Define the helper which inserts data into table in batches\n    def insert_into_sim(df):\n        \"\"\"\n        Use the main thread bdb handle to successively insert results of\n        ESTIMATEs into the table.\n        \"\"\"\n        rows = map(list, df.values)\n        insert_sql = '''\n            INSERT INTO {} (rowid0, rowid1, value) VALUES (?, ?, ?)\n        '''.format(sim_table_q)\n        # Avoid sqlite3 500-insert limit by grouping insert statements\n        # into one transaction.\n        with bdb.transaction():\n            for row in rows:\n                bdb.sql_execute(insert_sql, row)\n\n    pool = mp.Pool(processes=cores)\n\n    manager = mp.Manager()\n    queue = manager.Queue()\n\n    # Construct the estimate query template.\n    q_template = '''\n        ESTIMATE SIMILARITY FROM PAIRWISE {} LIMIT ? OFFSET ?\n    ''' .format(bql_quote_name(model))\n\n    for so in zip(sizes, offsets):\n        pool.apply_async(\n            _query_into_queue, args=(q_template, so, queue, bdb_file)\n        )\n\n    # Close pool and wait for processes to finish\n    # FIXME: This waits for all processes to finish before inserting\n    # into the table, which means that memory usage is potentially very\n    # high!\n    pool.close()\n    pool.join()\n\n    # Process returned results\n    while not queue.empty():\n        df = queue.get()\n        insert_into_sim(df)", "original_line_number": 5252, "instruction_length": 788}
{"instruction": "The given code is a Python module for implementing Google reCAPTCHA. To use it, import the necessary modules and functions, and then call the functions as needed. To display the reCAPTCHA HTML, call the \"displayhtml\" function and pass in the public key and any optional parameters. To submit the reCAPTCHA response for verification, call the \"submit\" function and pass in the necessary parameters. To check the reCAPTCHA response in a Django view, call the \"check_captcha\" function and pass in the request object. To generate new reCAPTCHA HTML based on the result of the verification, call the \"new_captcha_html\" function and pass in the result object.", "input": "", "output": "import urllib2, urllib\nfrom   proj.settings import *\n\n\nAPI_SSL_SERVER=\"https://www.google.com/recaptcha/api\"\nAPI_SERVER=\"http://www.google.com/recaptcha/api\"\nVERIFY_SERVER=\"www.google.com\"\n\nclass RecaptchaResponse(object):\n    def __init__(self, is_valid, error_code=None):\n        self.is_valid = is_valid\n        self.error_code = error_code\n\ndef displayhtml (public_key,\n                 use_ssl = False,\n                 error = None):\n    \"\"\"Gets the HTML to display for reCAPTCHA\n\n    public_key -- The public api key\n    use_ssl -- Should the request be sent over ssl?\n    error -- An error message to display (from RecaptchaResponse.error_code)\"\"\"\n\n    error_param = ''\n    if error:\n        error_param = '&error=%s' % error\n\n    if use_ssl:\n        server = API_SSL_SERVER\n    else:\n        server = API_SERVER\n\n    return \"\"\"<script type=\"text/javascript\" src=\"%(ApiServer)s/challenge?k=%(PublicKey)s%(ErrorParam)s\"></script>\n\n<noscript>\n  <iframe src=\"%(ApiServer)s/noscript?k=%(PublicKey)s%(ErrorParam)s\" height=\"300\" width=\"500\" frameborder=\"0\"></iframe><br />\n  <textarea name=\"recaptcha_challenge_field\" rows=\"3\" cols=\"40\"></textarea>\n  <input type='hidden' name='recaptcha_response_field' value='manual_challenge' />\n</noscript>\n\"\"\" % {\n        'ApiServer' : server,\n        'PublicKey' : public_key,\n        'ErrorParam' : error_param,\n        }\n\n\ndef submit (recaptcha_challenge_field,\n            recaptcha_response_field,\n            private_key,\n            remoteip):\n    \"\"\"\n    Submits a reCAPTCHA request for verification. Returns RecaptchaResponse\n    for the request\n\n    recaptcha_challenge_field -- The value of recaptcha_challenge_field from the form\n    recaptcha_response_field -- The value of recaptcha_response_field from the form\n    private_key -- your reCAPTCHA private key\n    remoteip -- the user's ip address\n    \"\"\"\n\n    if not (recaptcha_response_field and recaptcha_challenge_field and\n            len (recaptcha_response_field) and len (recaptcha_challenge_field)):\n        return RecaptchaResponse (is_valid = False, error_code = 'incorrect-captcha-sol')\n    \n\n    def encode_if_necessary(s):\n        if isinstance(s, unicode):\n            return s.encode('utf-8')\n        return s\n\n    params = urllib.urlencode ({\n            'privatekey': encode_if_necessary(private_key),\n            'remoteip' :  encode_if_necessary(remoteip),\n            'challenge':  encode_if_necessary(recaptcha_challenge_field),\n            'response' :  encode_if_necessary(recaptcha_response_field),\n            })\n\n    request = urllib2.Request (\n        url = \"http://%s/recaptcha/api/verify\" % VERIFY_SERVER,\n        data = params,\n        headers = {\n            \"Content-type\": \"application/x-www-form-urlencoded\",\n            \"User-agent\": \"reCAPTCHA Python\"\n            }\n        )\n    \n    httpresp = urllib2.urlopen (request)\n\n    return_values = httpresp.read ().splitlines ();\n    httpresp.close();\n\n    return_code = return_values [0]\n\n    if (return_code == \"true\"):\n        return RecaptchaResponse (is_valid=True)\n    else:\n        return RecaptchaResponse (is_valid=False, error_code = return_values [1])\n\ndef check_captcha(request):\n    \n    captcha_challenge = request.POST.get('recaptcha_challenge_field')\n    captcha_response = request.POST.get('recaptcha_response_field')\n    captcha_result = None\n    ip = None\n    if 'HTTP_X_FORWARDED_FOR' in request.META:\n        ip = request.META['HTTP_X_FORWARDED_FOR']\n    elif 'REMOTE_ADDR' in request.META:\n        ip = request.META['REMOTE_ADDR']\n    if captcha_response is not None and captcha_challenge is not None:\n        captcha_result = submit(captcha_challenge,\n                                captcha_response,\n                                recaptcha_private_key,\n                                ip)\n    return captcha_result\n\ndef new_captcha_html(captcha_result):\n    if captcha_result is None:\n        captcha_html = displayhtml(recaptcha_public_key, use_ssl=True)\n    else:\n        captcha_html = displayhtml(recaptcha_public_key, use_ssl=True, error = captcha_result.error_code)\n    return captcha_html", "original_line_number": 5575, "instruction_length": 653}
{"instruction": "Use the Scratch Project Comments Parser v1.0.0 to parse comments from a Scratch project. Instantiate the CommentsParser class and call the parse method with the project ID and maximum number of comments to parse as arguments. The output will be a tuple of dictionaries, where each dictionary represents a comment and has keys \"user\" and \"msg\" representing the username and message of the comment, respectively. The parser is licensed under GNU General Public License v3 and was created by Scratch user Gaza101.", "input": "", "output": "'''\n    Scratch Project Comments Parser v1.0.0\n    Created for use with SCV Server v2.0.0\n\n    Created by Scratch user, Gaza101.\n    Licensed under GNU General Public License v3.\n    www.garethpw.net\n'''\n\nfrom HTMLParser import HTMLParser\nfrom htmlentitydefs import name2codepoint\nfrom urllib2 import urlopen\n\nclass CommentsParser(HTMLParser):\n    def __init__(self,emap={}):\n        self.emap = emap\n        self.out = []\n        self.nest = []\n        self.comments = str()\n    def aDict(self,a): #Converts attrs into dict format for easier parsing.\n        d = {} #        e.g.    [('class', 'example'),('height', '50px')]\n        for i in a: #   becomes  {'class':('example',),'height':('50px',)}\n            if i[0] in d:\n                d[i[0]] += (i[1],)\n            else:\n                d[i[0]] = (i[1],)\n        return d\n    def isLegal(self,n,r): #Checks the nest based on a set of rules provided.\n        try: # Rule format: [(#tuple of tag nest - can be any length - starts from root tag),(#level of nest, #attr, #value)]\n            if (     tuple([i[0] for i in n][:len(r[0])]) == r[0]\n                 and not (False in [(True in [sr[2] in i for i in n[sr[0]][1][sr[1]]]) for sr in r[1:]]) ):\n                return True\n        except KeyError:\n            pass\n        return False\n    def isCName(self,n): #Checks if the current nest is valid to be the comment username.\n        return self.isLegal(n,[ (\"li\",\"div\",\"div\",\"div\",'a'),\n                                (0,\"class\",\"top-level-reply\"),\n                                (1,\"class\",\"comment\"),\n                                (2,\"class\",\"info\"),\n                                (3,\"class\",\"name\")             ])\n    def isCBody(self,n): #Checks if the current nest is valid to be the comment body.\n        return self.isLegal(n,[ (\"li\",\"div\",\"div\",\"div\"),\n                                (0,\"class\",\"top-level-reply\"),\n                                (1,\"class\",\"comment\"),\n                                (2,\"class\",\"info\"),\n                                (3,\"class\",\"content\")          ])\n    def handle_starttag(self, tag, attrs):\n        il = (self.isCName(self.nest),self.isCBody(self.nest))\n        self.nest.append((tag,self.aDict(attrs)))\n        if il != (self.isCName(self.nest),self.isCBody(self.nest)): #Check if a new comment username or body has begun.\n            self.out.append([]) #If so, append new list to output array.\n        if tag == \"img\": #If the tag is valid to be an emoticon,\n            if (     self.isCBody(self.nest)\n                 and self.isLegal(self.nest,[ tuple(),\n                                              (-1,\"class\",\"easter-egg\") ]) ):\n                try:\n                    self.out[-1].append(self.emap[self.nest[-1][1]['src'][0]]) #Attempt to match with its alias in the emoticon map.\n                except KeyError:\n                    self.out[-1].append(\"_undefined_\") #If alias not found, refer to it as \"_undefined_\"\n            self.nest.pop() #Remove image from nest array since it's most likely without an end tag.\n    def handle_endtag(self,tag):\n        if tag != \"img\": #Ignore img end tags since they will have already been dealt with.\n            self.nest.pop()\n    def handle_data(self,data):\n        if self.isCName(self.nest) or self.isCBody(self.nest): #If we're in valid comment text,\n            self.out[-1].append(data) #Append text to output.\n    def handle_entityref(self,name):\n        if self.isCName(self.nest) or self.isCBody(self.nest): #If we're in valid comment text,\n            self.out[-1].append(unichr(name2codepoint[name])) #Append text to output.\n    def handle_charref(self,name):\n        if self.isCName(self.nest) or self.isCBody(self.nest): #If we're in valid comment text,\n            self.out[-1].append(unichr(int(name[1:],16) if name[0] == 'x' else int(name))) #Append text to output.\n    def parse(self,project_id,max_comments=30): #Parses any data given. Data must be complete.\n        comments = urlopen(\"https://scratch.mit.edu/site-api/comments/project/\"+str(project_id)+'/').read()\n        if self.comments != comments: #If we haven't already parsed this,\n            self.comments = comments\n            self.out = [] #Reinitialise the instance.\n            self.nest = []\n            self.reset() #Reset the parser.\n            self.feed(self.comments) #Feed the parser the data from the comments of the project specified.\n            self.out =  tuple( [{\"user\": u''.join([u''.join([unichr(ord(c)) for c in m]) for m in self.out[i]]), #Convert parsed data into a more usable format. e.g. {'user','Gaza101','msg':'_meow_'}\n                                  \"msg\": u''.join([u''.join([unichr(ord(c)) for c in m]) for m in self.out[i+1]])[23:-12]} for i in range(0,min(len(self.out),max_comments),2)] )\n        return self.out #Output parsed data.", "original_line_number": 5688, "instruction_length": 510}
{"instruction": "This code contains the logic for `aq add network_device`. To add a network device, you need to provide the following required parameters: \"network_device\", \"model\", \"type\", \"ip\", \"interface\", and \"iftype\". You can also provide optional parameters such as \"label\", \"mac\", \"vendor\", \"serial\", \"comments\", \"exporter\", \"chassis\", \"slot\", \"archetype\", \"domain\", \"sandbox\", \"user\", \"justification\", and \"reason\". The code will create a new NetworkDevice object with the provided information and add it to the database. It will also create a new Host object and update the DSDB with the new network device information. Finally, it will generate plenary templates for the network device and host objects.", "input": "", "output": "# -*- cpy-indent-level: 4; indent-tabs-mode: nil -*-\n# ex: set expandtab softtabstop=4 shiftwidth=4:\n#\n# Copyright (C) 2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018  Contributor\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Contains the logic for `aq add network_device`.\"\"\"\n\nfrom sqlalchemy.orm import subqueryload\n\nfrom aquilon.exceptions_ import ArgumentError\nfrom aquilon.aqdb.model import NetworkDevice, Model, Archetype, Chassis, NetworkDeviceChassisSlot\nfrom aquilon.aqdb.model.network import get_net_id_from_ip\nfrom aquilon.worker.broker import BrokerCommand\nfrom aquilon.worker.dbwrappers.dns import grab_address\nfrom aquilon.worker.dbwrappers.location import get_location\nfrom aquilon.worker.dbwrappers.interface import (get_or_create_interface,\n                                                 assign_address,\n                                                 check_netdev_iftype)\nfrom aquilon.worker.dbwrappers.host import create_host\nfrom aquilon.worker.processes import DSDBRunner\nfrom aquilon.worker.templates.switchdata import PlenarySwitchData\nfrom aquilon.worker.dbwrappers.change_management import ChangeManagement\n\n\nclass CommandAddNetworkDevice(BrokerCommand):\n    requires_plenaries = True\n\n    required_parameters = [\"network_device\", \"model\", \"type\",\n                           \"ip\", \"interface\", \"iftype\"]\n\n    def render(self, session, logger, plenaries, network_device, label, model, type, ip,\n               interface, iftype, mac, vendor, serial, comments, exporter, chassis, slot,\n               archetype, domain, sandbox, user, justification, reason, **arguments):\n        dbmodel = Model.get_unique(session, name=model, vendor=vendor,\n                                   compel=True)\n\n        if not dbmodel.model_type.isNetworkDeviceType():\n            raise ArgumentError(\"This command can only be used to \"\n                                \"add network devices.\")\n\n        dblocation = get_location(session, query_options=[subqueryload('parents')], **arguments)\n\n        if chassis:\n            dbchassis = Chassis.get_unique(session, chassis, compel=True)\n            if slot is None:\n                raise ArgumentError(\"The --chassis option requires a --slot.\")\n            if dblocation and dblocation != dbchassis.location:\n                raise ArgumentError(\"{0} conflicts with chassis location \"\n                                    \"{1}.\".format(dblocation, dbchassis.location))\n            dblocation = dbchassis.location\n        elif slot is not None:\n            raise ArgumentError(\"The --slot option requires a --chassis.\")\n\n        dbdns_rec, _ = grab_address(session, network_device, ip,\n                                    allow_restricted_domain=True,\n                                    allow_reserved=True, preclude=True,\n                                    exporter=exporter, require_grn=False)\n        if not label:\n            label = dbdns_rec.fqdn.name\n            try:\n                NetworkDevice.check_label(label)\n            except ArgumentError:\n                raise ArgumentError(\"Could not deduce a valid hardware label \"\n                                    \"from the network device name.  Please specify \"\n                                    \"--label.\")\n\n        # FIXME: What do the error messages for an invalid enum (switch_type)\n        # look like?\n        dbnetdev = NetworkDevice(label=label, switch_type=type,\n                                 location=dblocation, model=dbmodel,\n                                 serial_no=serial, comments=comments)\n        session.add(dbnetdev)\n\n        if chassis:\n            dbslot = session.query(NetworkDeviceChassisSlot).filter_by(chassis=dbchassis,\n                                                                       slot_number=slot).first()\n            if dbslot and dbslot.network_device:\n                raise ArgumentError(\"{0} slot {1} already has network device \"\n                                    \"{2}.\".format(dbchassis, slot,\n                                                  dbslot.network_device.label))\n            if not dbslot:\n                dbslot = NetworkDeviceChassisSlot(chassis=dbchassis, slot_number=slot)\n            dbslot.network_device = dbnetdev\n            session.add(dbslot)\n\n        dbnetdev.primary_name = dbdns_rec\n\n        check_netdev_iftype(iftype)\n\n        dbinterface = get_or_create_interface(session, dbnetdev,\n                                              name=interface, mac=mac,\n                                              interface_type=iftype)\n        dbnetwork = get_net_id_from_ip(session, ip)\n        # TODO: should we call check_ip_restrictions() here?\n        assign_address(dbinterface, ip, dbnetwork, logger=logger)\n\n        if not archetype:\n            hw_section = 'hardware_network_device'\n            if not self.config.has_option(hw_section, 'default_archetype'):\n                raise ArgumentError(\"Cannot determine the archetype for \"\n                                    \"network devices.  Please specify \"\n                                    \"--archetype.\")\n            archetype = self.config.get(hw_section, 'default_archetype')\n\n        dbarchetype = Archetype.get_unique(session, archetype, compel=True)\n\n        dbhost = create_host(session, logger, self.config, dbnetdev,\n                             dbarchetype, domain=domain, sandbox=sandbox,\n                             **arguments)\n\n        session.flush()\n\n        # Validate ChangeManagement\n        cm = ChangeManagement(session, user, justification, reason, logger, self.command, **arguments)\n        cm.consider(dbhost)\n        cm.validate()\n\n        # Add the legacy template separately\n        plenaries.add(dbnetdev, cls=PlenarySwitchData)\n        plenaries.add(dbnetdev)\n        plenaries.add(dbhost)\n\n        with plenaries.transaction():\n            dsdb_runner = DSDBRunner(logger=logger)\n            dsdb_runner.update_host(dbnetdev, None)\n            dsdb_runner.commit_or_rollback(\"Could not add network device to DSDB\")\n\n        return", "original_line_number": 6383, "instruction_length": 696}
{"instruction": "Create a Django web application with views for displaying a 404 page, home page, topics, resources, blog post list, blog post view, leaderboard, user registration, and two API endpoints for checking Github handles and emails. The application should use models and forms from the specified files, and import necessary modules such as requests, http, and JsonResponse. The leaderboard view should retrieve data from the Github API and update the user model accordingly. The registration view should use the RegisterUserForm and redirect to the home page upon successful submission. The API endpoints should check for duplicate Github handles and emails in the Users model and return appropriate responses.", "input": "", "output": "from django import views\nfrom django.shortcuts import render, get_object_or_404\nfrom django.views.generic import TemplateView\nfrom django.views.generic.edit import CreateView\nfrom .models import *\nfrom .forms import *\nimport requests\nimport http\nfrom django.urls import reverse_lazy\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.http import JsonResponse\n\nclass Template404(TemplateView):\n    template_name = \"404.html\"\n\nclass Home(TemplateView):\n    template_name = 'home/home.html'\n\nclass Topics(views.View):\n    def get(self, request, *args, **kwargs):\n        return render(request, \"home/resources/topics.html\", {'topics': Topic.objects.all()})\n\nclass Resources(views.View):\n    def get(self, request, pk, *args, **kwargs):\n        topic = get_object_or_404(Topic, pk=pk)\n        return render(request, \"home/resources/resources.html\", {'resources': topic.resource_set.all(), 'topic' : topic})\n\nclass BlogPostList(views.View):\n    def get(self, request, *args, **kwargs):\n        posts = BlogPost.objects.all()\n        return render(request, \"home/blog/index.html\", {'posts': posts})\n\nclass BlogPostView(views.View):\n    def get(self, request, pk, *args, **kwargs):\n        post = get_object_or_404(BlogPost, pk=pk)\n        return render(request, \"home/blog/blog_post.html\", {'post': post})\n\nclass Leaderboard(views.View):\n    def get(self, request, *args, **kwargs):\n        users = Users.objects.all()\n        for user in users:\n            connected = False\n            while not connected:\n                try:\n                    user_name = user.github_handle\n                    response = requests.get('https://api.github.com/search/issues?sort=created&q=author:{}&type:pr&per_page=100'.format(user_name), verify = False).json()\n                    pr_count = 0\n                    print(response)\n                    for obj in response['items']:\n                        if('pull_request' in obj):\n                            if('2018-09-30T00:00:00Z'<obj['created_at']<'2018-10-31T23:59:59Z'):\n                                pr_count += 1\n                    user.pr_count = pr_count\n                    user.save()\n                    connected = True\n                except:\n                    pass\n        return render(request, 'home/leaderboard.html', {'users': users})\n\nclass RegisterUser(CreateView):\n    form_class = RegisterUserForm\n    template_name = \"home/registeruser.html\"\n    success_url = reverse_lazy('home:home')\n\n@csrf_exempt\ndef GithubEmailCheck(request):\n    github_handle = request.POST.get('github_handle')\n    email = request.POST.get('email')\n    print(\"Received \", github_handle)\n    users = Users.objects.all()\n    for user in users:\n        if user.github_handle == github_handle:\n            return JsonResponse({'message' : 'Duplicate Github Handle'})\n        if user.email == email:\n            return JsonResponse({'message' : 'Duplicate Email'})\n    return JsonResponse({'message' : 'New'})\n\n@csrf_exempt\ndef GithubCheck(request):\n    github_handle = request.POST.get('github_handle')\n    response = requests.get(\"https://api.github.com/users/{}\".format(github_handle), verify = False).json()\n    print(\"https://api.github.com/users/{}\".format(github_handle))\n    if ('login' in response):\n        print(\"Found\")\n        return JsonResponse({'message' : 'Found'})\n    else:\n        return JsonResponse({'message' : 'Not Found'})", "original_line_number": 6521, "instruction_length": 703}
{"instruction": "This code builds VoltDB kits for Linux and Mac OSX. To build the kits, run the script with the following arguments: the commit, tag, or branch for the VoltDB repository, the commit, tag, or branch for the Pro repository, and the commit, tag, or branch for the RabbitMQ repository. Optionally, use the `--nomac` flag to skip building the Mac OSX kit and the `--nocommunity` flag to skip building the community kit. The script will create a release directory and copy the necessary files to it, including the trial license, SHA256 checksum, and Maven jars. Finally, the script will create a candidate symlink and backup the release directory.", "input": "", "output": "#!/usr/bin/env python\n\nimport argparse, datetime, getpass, os, sys, shutil, traceback\nfrom fabric.api import run, cd, local, get, settings, lcd, put\nfrom fabric_ssh_config import getSSHInfoForHost\nfrom fabric.context_managers import shell_env\nfrom fabric.utils import abort\n\n#Login as user test, but build in a directory by real username\nusername = 'test'\nbuilddir = \"/tmp/\" + getpass.getuser() + \"Kits/buildtemp\"\nversion = \"UNKNOWN\"\nnativelibdir = \"/nativelibs/obj\"  #  ~test/libs/... usually\ndefaultlicensedays = 70 #default trial license length\n\n\n################################################\n# CHECKOUT CODE INTO A TEMP DIR\n################################################\n\ndef checkoutCode(voltdbGit, proGit, rbmqExportGit, gitloc):\n    global buildir\n    # clean out the existing dir\n    run(\"rm -rf \" + builddir)\n    # make the build dir again\n    run(\"mkdir -p \" + builddir)\n    # change to it\n    with cd(builddir):\n        # do the checkouts, collect checkout errors on both community &\n        # pro repos so user gets status on both checkouts\n        message = \"\"\n        run(\"git clone -q %s/voltdb.git\" % gitloc)\n        result = run(\"cd voltdb; git checkout %s\" % voltdbGit, warn_only=True)\n        if result.failed:\n            message = \"VoltDB checkout failed. Missing branch %s.\" % rbmqExportGit\n\n        run(\"git clone -q %s/pro.git\" % gitloc)\n        result = run(\"cd pro; git checkout %s\" % proGit, warn_only=True)\n        if result.failed:\n            message += \"\\nPro checkout failed. Missing branch %s.\" % rbmqExportGit\n\n        #rabbitmq isn't mirrored internally, so don't use gitloc\n        run(\"git clone -q git@github.com:VoltDB/export-rabbitmq.git\")\n        result = run(\"cd export-rabbitmq; git checkout %s\" % rbmqExportGit, warn_only=True)\n        # Probably ok to use master for export-rabbitmq.\n        if result.failed:\n            print \"\\nExport-rabbitmg branch %s checkout failed. Defaulting to master.\" % rbmqExportGit\n\n        if len(message) > 0:\n            abort(message)\n\n        return run(\"cat voltdb/version.txt\").strip()\n\n################################################\n# MAKE A RELEASE DIR\n################################################\n\ndef makeReleaseDir(releaseDir):\n    # handle the case where a release dir exists for this version\n    if os.path.exists(releaseDir):\n        shutil.rmtree(releaseDir)\n    # create a release dir\n    os.makedirs(releaseDir)\n    print \"Created dir: \" + releaseDir\n\n\n################################################\n# BUILD THE COMMUNITY VERSION\n################################################\n\ndef buildCommunity():\n    if build_mac:\n        packageMacLib=\"true\"\n    else:\n        packageMacLib=\"false\"\n    with cd(builddir + \"/voltdb\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\")\n        run(\"ant -Djmemcheck=NO_MEMCHECK -Dkitbuild=%s %s clean default dist\" % (packageMacLib,  build_args))\n\n################################################\n# BUILD THE ENTERPRISE VERSION\n################################################\n\ndef buildEnterprise():\n    if build_mac:\n        packageMacLib=\"true\"\n    else:\n        packageMacLib=\"false\"\n    with cd(builddir + \"/pro\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\")\n        run(\"VOLTCORE=../voltdb ant -f mmt.xml -Djmemcheck=NO_MEMCHECK -Dallowreplication=true -DallowDrActiveActive=true -Dlicensedays=%d -Dkitbuild=%s %s clean dist.pro\" % (defaultlicensedays, packageMacLib, build_args))\n\n################################################\n# BUILD THE PRO VERSION\n################################################\n\n#\ndef packagePro(version):\n    print \"Making license\"\n    makeTrialLicense(days=defaultlicensedays, dr_and_xdcr=False, nodes=3)\n    print \"Repacking pro kit\"\n    with cd(builddir + \"/pro/obj/pro\"):\n        run(\"mkdir pro_kit_staging\")\n    with cd(builddir + \"/pro/obj/pro/pro_kit_staging\"):\n        run(\"tar xf ../voltdb-ent-%s.tar.gz\" % version)\n        run(\"mv voltdb-ent-%s voltdb-pro-%s\" % (version, version))\n        run(\"cp %s/pro/trial_*.xml voltdb-pro-%s/voltdb/license.xml\" % (builddir, version))\n        run(\"tar cvfz ../voltdb-pro-%s.tar.gz voltdb-pro-%s\" % (version, version))\n\n################################################\n# BUILD THE RABBITMQ EXPORT CONNECTOR\n################################################\n#Build rabbit MQ Exporter\ndef buildRabbitMQExport(version, dist_type):\n    # Paths to the final kit for unpacking/repacking with rmq export\n    paths = {\n        'community': builddir + \"/voltdb/obj/release\",\n        'ent' : builddir + \"/pro/obj/pro/\"\n        }\n    # Untar\n    with cd(paths[dist_type]):\n        run (\"pwd\")\n        run (\"mkdir -p restage\")\n        run (\"tar xf voltdb-%s-%s.tar.gz -C restage\" % (dist_type, version))\n        run (\"rm -f voltdb-%s-%s.tar.gz\" % (dist_type, version))\n\n    # Build RabbitMQ export jar and put it into the untarred kit\n    with cd(builddir + \"/export-rabbitmq\"):\n        run(\"pwd\")\n        run(\"git status\")\n        run(\"git describe --dirty\", warn_only=True)\n        run(\"VOLTDIST=%s/restage/voltdb-%s-%s ant\" % (paths[dist_type], dist_type, version))\n\n    # Retar\n    with cd(paths[dist_type]):\n        run(\"pwd\")\n        run(\"tar -C restage -czf voltdb-%s-%s.tar.gz voltdb-%s-%s\" % (dist_type, version, dist_type, version))\n        run (\"rm -Rf restage\")\n\n################################################\n# MAKE AN ENTERPRISE TRIAL LICENSE\n################################################\n\n# Must be called after buildEnterprise has been done\ndef makeTrialLicense(days=30, dr_and_xdcr=\"true\", nodes=12):\n    with cd(builddir + \"/pro/tools\"):\n        run(\"./make_trial_licenses.pl -t %d -H %d -W %s\" % (days, nodes, dr_and_xdcr ))\n\n################################################\n# MAKE A SHA256 checksum\n################################################\n\ndef makeSHA256SUM(version, type):\n    with cd(builddir + \"/pro/obj/pro\"):\n        kitname=\"voltdb-\" +  type + \"-\" + version\n        run(\"sha256sum -b %s.tar.gz > %s.SHA256SUM\" % (kitname, kitname))\n\n################################################\n# MAKE AN JAR FILES NEEDED TO PUSH TO MAVEN\n################################################\n\ndef makeMavenJars():\n    with cd(builddir + \"/voltdb\"):\n        run(\"VOLTCORE=../voltdb ant -f build-client.xml maven-jars\")\n\n################################################\n# COPY FILES\n################################################\n\ndef copyFilesToReleaseDir(releaseDir, version, type=None):\n    print \"Copying files to releaseDir\"\n    if type:\n        typeString=\"-\" + type\n    else:\n        typeString=\"\"\n    get(\"%s/pro/obj/pro/voltdb%s-%s.tar.gz\" % (builddir, typeString, version),\n        \"%s/voltdb%s-%s.tar.gz\" % (releaseDir, typeString, version))\n    get(\"%s/pro/obj/pro/voltdb%s-%s.SHA256SUM\" % (builddir, typeString, version),\n        \"%s/voltdb%s-%s.SHA256SUM\" % (releaseDir, typeString, version))\n\ndef copyCommunityFilesToReleaseDir(releaseDir, version, operatingsys):\n    get(\"%s/voltdb/obj/release/voltdb-community-%s.tar.gz\" % (builddir, version),\n        \"%s/voltdb-community-%s.tar.gz\" % (releaseDir, version))\n\n    # add stripped symbols\n    if operatingsys == \"LINUX\":\n        os.makedirs(releaseDir + \"/other\")\n        get(\"%s/voltdb/obj/release/voltdb-%s.sym\" % (builddir, version),\n            \"%s/other/%s-voltdb-voltkv-%s.sym\" % (releaseDir, operatingsys, version))\n\ndef copyTrialLicenseToReleaseDir(releaseDir):\n    get(\"%s/pro/trial_*.xml\" % (builddir),\n        \"%s/license.xml\" % (releaseDir))\n\ndef copyMavenJarsToReleaseDir(releaseDir, version):\n    #The .jars and upload file must be in a directory called voltdb - it is the projectname\n    mavenProjectDir = releaseDir + \"/mavenjars/voltdb\"\n    if not os.path.exists(mavenProjectDir):\n        os.makedirs(mavenProjectDir)\n\n    #Get the voltdbclient-n.n.jar from the recently built community build\n    get(\"%s/voltdb/obj/release/dist-client-java/voltdb/voltdbclient-%s.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s.jar\" % (mavenProjectDir, version))\n    #Get the upload.gradle file\n    get(\"%s/voltdb/tools/kit_tools/upload.gradle\" % (builddir),\n        \"%s/upload.gradle\" % (mavenProjectDir))\n    #Get the src and javadoc .jar files\n    get(\"%s/voltdb/obj/release/voltdbclient-%s-javadoc.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s-javadoc.jar\" % (mavenProjectDir, version))\n    get(\"%s/voltdb/obj/release/voltdbclient-%s-sources.jar\" % (builddir, version),\n        \"%s/voltdbclient-%s-sources.jar\" % (mavenProjectDir, version))\n\n################################################\n# CREATE CANDIDATE SYMLINKS\n################################################\n\ndef createCandidateSysmlink(releaseDir):\n    candidateDir =  os.getenv('HOME') + \"/releases/candidate\";\n    local(\"rm -rf \" + candidateDir)\n    local(\"ln -s %s %s\" % (releaseDir, candidateDir))\n\n################################################\n# BACKUP RELEASE DIR\n################################################\n\ndef backupReleaseDir(releaseDir,archiveDir,version):\n    if not os.path.exists(archiveDir):\n        os.makedirs(archiveDir)\n    # make a backup with the timstamp of the  build\n    timestamp = datetime.datetime.now().strftime(\"%y%m%d-%H%M%S\")\n    local(\"tar -czf %s/%s-%s.tgz %s\" \\\n          % (archiveDir, version, timestamp, releaseDir))\n\n################################################\n# REMOVE NATIVE LIBS FROM SHARED DIRECTORY\n################################################\n\ndef rmNativeLibs():\n    # local(\"ls -l ~\" + username + nativelibdir)\n    local(\"rm -rf ~\" + username + nativelibdir)\n\n\n\n################################################\n# GET THE GIT TAGS OR SHAS TO BUILD FROM\n################################################\n\nparser = argparse.ArgumentParser(description = \"Create a full kit. With no args, will do build of master\")\nparser.add_argument('voltdb_sha', nargs=\"?\", default=\"master\", help=\"voltdb repository commit, tag or branch\" )\nparser.add_argument('pro_sha', nargs=\"?\", default=\"master\", help=\"pro repository commit, tag or branch\" )\nparser.add_argument('rabbitmq_sha', nargs=\"?\", default=\"master\", help=\"rabbitmq repository commit, tag or branch\" )\nparser.add_argument('-g','--gitloc', default=\"git@github.com:VoltDB\", help=\"Repository location. For example: /home/github-mirror\")\nparser.add_argument('--nomac', action='store_true', help=\"Don't build Mac OSX\")\nparser.add_argument('--nocommunity', action='store_true', help=\"Don't build community\")\nargs = parser.parse_args()\n\nproTreeish = args.pro_sha\nvoltdbTreeish = args.voltdb_sha\nrbmqExportTreeish = args.rabbitmq_sha\n\nprint args\n\nbuild_community = not args.nocommunity\nbuild_mac = not args.nomac\n\n#If anything is missing we're going to dump this in oneoffs dir.\nbuild_all = build_community and build_mac\nif voltdbTreeish != proTreeish or not build_all:\n    oneOff = True\nelse:\n    oneOff  = False\n\nrmNativeLibs()\n\ntry:\n    build_args = os.environ['VOLTDB_BUILD_ARGS']\nexcept:\n    build_args=\"\"\n\nprint \"Building with pro: %s and voltdb: %s\" % (proTreeish, voltdbTreeish)\n\nbuild_errors=False\n\nversionCentos = \"unknown\"\nversionMac = \"unknown\"\nreleaseDir = \"unknown\"\n\n# get ssh config [key_filename, hostname]\nCentosSSHInfo = getSSHInfoForHost(\"volt15a\")\nMacSSHInfo = getSSHInfoForHost(\"voltmini\")\nUbuntuSSHInfo = getSSHInfoForHost(\"volt12d\")\n\n# build community kit on the mini so that .so can be picked up for unified kit\nif build_mac or build_community:\n    try:\n        with settings(user=username,host_string=MacSSHInfo[1],disable_known_hosts=True,key_filename=MacSSHInfo[0]):\n            versionMac = checkoutCode(voltdbTreeish, proTreeish, rbmqExportTreeish, args.gitloc)\n            buildCommunity()\n    except Exception as e:\n        print traceback.format_exc()\n        print \"Could not build MAC kit. Exception: \" + str(e) + \", Type: \" + str(type(e))\n        build_errors=True\n\n# build kits on 15f\ntry:\n    with settings(user=username,host_string=CentosSSHInfo[1],disable_known_hosts=True,key_filename=CentosSSHInfo[0]):\n        versionCentos = checkoutCode(voltdbTreeish, proTreeish, rbmqExportTreeish, args.gitloc)\n        if build_mac:\n            assert versionCentos == versionMac\n\n        if oneOff:\n            releaseDir = \"%s/releases/one-offs/%s-%s-%s\" % \\\n                (os.getenv('HOME'), versionCentos, voltdbTreeish, proTreeish)\n        else:\n            releaseDir = os.getenv('HOME') + \"/releases/\" + voltdbTreeish\n        makeReleaseDir(releaseDir)\n        print \"VERSION: \" + versionCentos\n        if build_community:\n            buildCommunity()\n            buildRabbitMQExport(versionCentos, \"community\")\n            copyCommunityFilesToReleaseDir(releaseDir, versionCentos, \"LINUX\")\n        buildEnterprise()\n        buildRabbitMQExport(versionCentos, \"ent\")\n        makeSHA256SUM(versionCentos,\"ent\")\n        copyFilesToReleaseDir(releaseDir, versionCentos, \"ent\")\n        packagePro(versionCentos)\n        makeSHA256SUM(versionCentos,\"pro\")\n        copyFilesToReleaseDir(releaseDir, versionCentos, \"pro\")\n        makeTrialLicense()\n        copyTrialLicenseToReleaseDir(releaseDir)\n        makeMavenJars()\n        copyMavenJarsToReleaseDir(releaseDir, versionCentos)\n\nexcept Exception as e:\n    print traceback.format_exc()\n    print \"Could not build LINUX kit. Exception: \" + str(e) + \", Type: \" + str(type(e))\n    build_errors=True\n\nrmNativeLibs()      # cleanup imported native libs so not picked up unexpectedly by other builds\n\nexit (build_errors)\n#archiveDir = os.path.join(os.getenv('HOME'), \"releases\", \"archive\", voltdbTreeish, versionCentos)\n#backupReleaseDir(releaseDir, archiveDir, versionCentos)", "original_line_number": 6929, "instruction_length": 640}
{"instruction": "This code defines several forms for user authentication and profile management in a Django application. To use these forms, import them from the corresponding modules and instantiate them as needed. The forms include AuthenticationForm, PasswordChangeForm, PasswordResetForm, SetPasswordForm, UserCreationForm, UserUpdateForm, and ProfileUpdateForm. The code also includes several views for user registration, confirmation, profile management, and account deletion. To use these views, include them in the URL configuration of the Django application.", "input": "", "output": "####################################################################################################\n#\n# GroupedPurchaseOrder - A Django Application.\n# Copyright (C) 2014 Fabrice Salvaire\n#\n#  This program is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU Affero General Public License as\n#  published by the Free Software Foundation, either version 3 of the\n#  License, or (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU Affero General Public License for more details.\n#\n#  You should have received a copy of the GNU Affero General Public License\n#  along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n####################################################################################################\n\n####################################################################################################\n\n# from django.forms.widgets import HiddenInput\nfrom django.contrib import messages\nfrom django.contrib.auth import forms\nfrom django.contrib.auth.decorators import login_required\nfrom django.contrib.auth.models import User\nfrom django.core.urlresolvers import reverse\nfrom django.forms import ModelForm\nfrom django.http import HttpResponseRedirect\nfrom django.shortcuts import get_object_or_404, render\nfrom django.utils import translation\nfrom django.utils.translation import ugettext as _\n\n####################################################################################################\n\nfrom GroupedPurchaseOrder.models import Profile\nfrom GroupedPurchaseOrder.utils import send_localized_mail\n\n####################################################################################################\n\nclass AuthenticationForm(forms.AuthenticationForm):\n\n    \"\"\"Override the default AuthenticationForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(AuthenticationForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['password'].widget.attrs['class'] = 'form-control'\n        self.fields['password'].widget.attrs['placeholder'] = _('Password')\n        self.fields['username'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['username'].widget.attrs['class'] = 'form-control'\n        self.fields['username'].widget.attrs['placeholder'] = _('Username')\n\n####################################################################################################\n\nclass PasswordChangeForm(forms.PasswordChangeForm):\n\n    \"\"\"Override the default PasswordChangeForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n\n        super(PasswordChangeForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['new_password1'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password1'].widget.attrs['placeholder'] = _('New password')\n        self.fields['new_password2'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password2'].widget.attrs['placeholder'] = _('New password')\n        self.fields['old_password'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['old_password'].widget.attrs['class'] = 'form-control'\n        self.fields['old_password'].widget.attrs['placeholder'] = _('Old password')\n\n####################################################################################################\n\nclass PasswordResetForm(forms.PasswordResetForm):\n\n    \"\"\"Override the default PasswordResetForm in order to add HTML5 attributes.  This is the only\n    change done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(PasswordResetForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['email'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['email'].widget.attrs['class'] = 'form-control'\n        self.fields['email'].widget.attrs['placeholder'] = _('email')\n\n####################################################################################################\n\nclass SetPasswordForm(forms.SetPasswordForm):\n\n    \"\"\"Override the default SetPasswordForm in order to add HTML5 attributes.  This is the only change\n    done and needed\n\n    \"\"\"\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(SetPasswordForm, self).__init__(*args, **kwargs)\n\n        # Add HTML5 attributes\n        self.fields['new_password1'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['new_password1'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password1'].widget.attrs['placeholder'] = _('New password')\n        self.fields['new_password2'].widget.attrs['class'] = 'form-control'\n        self.fields['new_password2'].widget.attrs['placeholder'] = _('New password')\n\n####################################################################################################\n\nclass UserCreationForm(forms.UserCreationForm):\n\n    \"\"\"Override the default UserCreationForm in order to add HTML5 attributes.\n\n    \"\"\"\n\n    ##############################################\n\n    class Meta:\n        model = User\n        fields = ('username', 'email', 'password1', 'password2', 'first_name', 'last_name')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(UserCreationForm, self).__init__(*args, **kwargs)\n\n        # email, first_name and last_name are required\n        self.fields['email'].required = True\n        self.fields['first_name'].required = True\n\n        # Add HTML5 attributes\n        self.fields['email'].widget.attrs['class'] = 'form-control'\n        self.fields['first_name'].widget.attrs['class'] = 'form-control'\n        self.fields['last_name'].widget.attrs['class'] = 'form-control'\n        self.fields['password1'].widget.attrs['class'] = 'form-control'\n        self.fields['password1'].widget.attrs['placeholder'] = _('Password')\n        self.fields['password2'].widget.attrs['class'] = 'form-control'\n        self.fields['password2'].widget.attrs['placeholder'] = _('Password')\n        self.fields['username'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['username'].widget.attrs['class'] = 'form-control'\n        self.fields['username'].widget.attrs['placeholder'] = _('Username')\n\n    ##############################################\n\n    def save(self, commit=True):\n\n        \"\"\"Create the new User and the associated Profile The User is not activated until the\n        register_confirm url has been visited\n\n        \"\"\"\n\n        if not commit:\n            raise NotImplementedError('Cannot create Profile and User without commit')\n        user = super(UserCreationForm, self).save(commit=False)\n        user.is_active = False\n        user.save()\n        profile = Profile(user=user)\n        profile.save()\n        return user\n\n####################################################################################################\n\nclass UserUpdateForm(ModelForm):\n\n    class Meta:\n        model = User\n        fields = ('first_name', 'last_name')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(UserUpdateForm, self).__init__(*args, **kwargs)\n\n        # first_name and last_name are required\n        self.fields['first_name'].required = True\n\n        self.fields['first_name'].widget.attrs['autofocus'] = 'autofocus'\n        self.fields['first_name'].widget.attrs['class'] = 'form-control'\n        self.fields['last_name'].widget.attrs['class'] = 'form-control'\n\n####################################################################################################\n\nclass ProfileUpdateForm(ModelForm):\n\n    class Meta:\n        model = Profile\n        fields = ('phone_number', 'language', 'timezone')\n\n    ##############################################\n\n    def __init__(self, *args, **kwargs):\n\n        super(ProfileUpdateForm, self).__init__(*args, **kwargs)\n\n        self.fields['language'].required = True\n        self.fields['timezone'].required = True\n\n        self.fields['language'].widget.attrs['class'] = 'form-control'\n        self.fields['phone_number'].widget.attrs['class'] = 'form-control'\n        self.fields['timezone'].widget.attrs['class'] = 'form-control'\n\n####################################################################################################\n\ndef register(request):\n\n    if request.method == 'POST':\n        user_form = UserCreationForm(request.POST)\n        if user_form.is_valid():\n            new_user = user_form.save()\n            send_localized_mail(new_user, _('Subscription to G.P.O.'),\n                                'GroupedPurchaseOrder/account/register_email.html',\n                                {'URL': request.build_absolute_uri(reverse('accounts.register.confirm',\n                                                                           args=[new_user.pk,\n                                                                                 new_user.profile.hash_id])),\n                                 'fullname': new_user.get_full_name()})\n            return render(request, 'GroupedPurchaseOrder/account/register_end.html')\n        else:\n            messages.error(request, _(\"Some information are missing or mistyped\"))\n    else:\n        user_form = UserCreationForm()\n\n    return render(request, 'GroupedPurchaseOrder/account/register.html', {'user_form': user_form})\n\n####################################################################################################\n\ndef register_confirm(request, user_id, user_hash):\n\n    \"\"\"Check that the User and the Hash are correct before activating the User\n\n    \"\"\"\n\n    user = get_object_or_404(User, pk=user_id, profile__hash_id=user_hash)\n    user.is_active = True\n    user.save()\n\n    return render(request, 'GroupedPurchaseOrder/account/confirm.html', {'user': user})\n\n####################################################################################################\n\n@login_required\ndef profile(request):\n\n    # Force the user to provide language and timezone\n    if not request.user.profile.language or request.user.profile.timezone == 'UTC':\n        messages.error(request, _(\"You should update your timezone. Without it G.P.O. will not work as expected.\"))\n        return HttpResponseRedirect(reverse('accounts.profile.update'))\n\n    return render(request, 'GroupedPurchaseOrder/account/profile.html')\n\n####################################################################################################\n\n@login_required\ndef update(request):\n\n    profile = get_object_or_404(Profile, user__pk=request.user.pk)\n\n    if request.method == 'POST':\n        user_form = UserUpdateForm(request.POST, instance=request.user)\n        profile_form = ProfileUpdateForm(request.POST, instance=profile)\n        if user_form.is_valid() and profile_form.is_valid():\n            user_form.save()\n            profile = profile_form.save()\n            # Update the language code and activate it for the message\n            if profile.language:\n                request.session['django_language'] = profile.language\n                translation.activate(profile.language)\n            # Update the timezone if needed\n            if profile.timezone:\n                request.session['django_timezone'] = profile.timezone\n            # Print the message\n            messages.success(request, _(\"Personnal information updated\"))\n            return HttpResponseRedirect(reverse('accounts.profile'))\n    else:\n        user_form = UserUpdateForm(instance=request.user)\n        profile_form = ProfileUpdateForm(instance=profile)\n\n        return render(request, 'GroupedPurchaseOrder/account/update.html',\n                      {'user_form': user_form, 'profile_form': profile_form})\n\n####################################################################################################\n\n@login_required\ndef password_change_done(request):\n\n    messages.success(request, _('Password changed successfully'))\n    return HttpResponseRedirect(reverse('accounts.profile'))\n\n####################################################################################################\n\ndef password_reset_done(request):\n\n    return render(request, 'GroupedPurchaseOrder/account/password_reset_done.html')\n\n####################################################################################################\n\n@login_required\ndef delete(request):\n\n    request.user.delete()\n    return HttpResponseRedirect(reverse('index'))\n\n####################################################################################################\n# \n# End\n# \n####################################################################################################", "original_line_number": 6955, "instruction_length": 550}
{"instruction": "To use this code, run the `main()` function. The code provides a command-line interface for building software using re-usable components. The available subcommands are `search`, `init`, `install`, `build`, `version`, `link`, `link-target`, `update`, `target`, `debug`, `test`, `start`, `publish`, `unpublish`, `login`, `logout`, `whoami`, `list`, `outdated`, `uninstall`, `remove`, `owners`, `licenses`, `clean`, `config`, and `shrinkwrap`. Use `yotta <subcommand> --help` for more detailed help on each subcommand. Options can be passed to the underlying build tool by passing them after `--`.", "input": "", "output": "# Copyright 2014-2015 ARM Limited\n#\n# Licensed under the Apache License, Version 2.0\n# See LICENSE file for details.\n\nfrom yotta.lib import lazyregex #pylint: disable=unused-import\nfrom yotta.lib import errors #pylint: disable=unused-import\n\n# NOTE: argcomplete must be first!\n# argcomplete, pip install argcomplete, tab-completion for argparse, Apache-2\nimport argcomplete\n\n# standard library modules, , ,\nimport argparse\nimport sys\nimport os\n\n# globalconf, share global arguments between modules, internal\nimport yotta.lib.globalconf as globalconf\n\n# hook to support coverage information when yotta runs itself during tests:\nif 'COVERAGE_PROCESS_START' in os.environ:\n    import coverage\n    coverage.process_startup()\n\n# set __version__ using the same file that's read by setup.py when installing:\nwith open(os.path.join(os.path.dirname(__file__), 'version.txt')) as _version_f:\n    __version__ = _version_f.read().strip()\n\ndef splitList(l, at_value):\n    r = [[]]\n    for x in l:\n        if x == at_value:\n            r.append(list())\n        else:\n            r[-1].append(x)\n    return r\n\ndef _handleUnhandledReqestExceptions(fn):\n    import functools\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):\n        # requests, apache2\n        import requests\n        try:\n            return fn(*args, **kwargs)\n        except requests.exceptions.RequestException as e:\n            import logging\n            if e.request is not None:\n                logging.critical('%s %s failed with status %s', e.request.method, e.request.url, e.response.status_code)\n                sys.exit(1)\n            else:\n                raise\n    return wrapped\n\ndef _exitSilentlyOnUnhandledPipeError(fn):\n    import functools\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):\n        try:\n            return fn(*args, **kwargs)\n        except IOError as e:\n            import errno\n            if e.errno == errno.EPIPE:\n                # unhandled pipe error -> exit silently, but with an error code\n                sys.exit(1)\n            else:\n                raise\n    return wrapped\n\n@_exitSilentlyOnUnhandledPipeError\n@_handleUnhandledReqestExceptions\ndef main():\n    # standard library modules, , ,\n    import logging\n    from functools import reduce\n\n    # logging setup, , setup the logging system, internal\n    from yotta.lib import logging_setup\n    # options, , common argument parser options, internal\n    import yotta.options as options\n\n    logging_setup.init(level=logging.INFO, enable_subsystems=None, plain=False)\n\n    # we override many argparse things to make options more re-usable across\n    # subcommands, and allow lazy loading of subcommand modules:\n    parser = options.parser.ArgumentParser(\n        formatter_class=argparse.RawTextHelpFormatter,\n        description='Build software using re-usable components.\\n'+\n        'For more detailed help on each subcommand, run: yotta <subcommand> --help'\n    )\n    subparser = parser.add_subparsers(dest='subcommand_name', metavar='<subcommand>')\n\n    parser.add_argument('--version', action='version', version=__version__,\n        help='display the version'\n    )\n\n    # add re-usable top-level options which subcommands may also accept\n    options.verbosity.addTo(parser)\n    options.debug.addTo(parser)\n    options.plain.addTo(parser)\n    options.noninteractive.addTo(parser)\n    options.registry.addTo(parser)\n    options.target.addTo(parser)\n    options.config.addTo(parser)\n\n    def addParser(name, module_name, description, help=None):\n        if help is None:\n            help = description\n        def onParserAdded(parser):\n            import importlib\n            module = importlib.import_module('.' + module_name, 'yotta')\n            module.addOptions(parser)\n            parser.set_defaults(command=module.execCommand)\n        subparser.add_parser_async(\n            name, description=description, help=help,\n            formatter_class=argparse.RawTextHelpFormatter,\n            callback=onParserAdded\n        )\n\n    addParser('search', 'search',\n        'Search for open-source modules and targets that have been published '+\n        'to the yotta registry (with yotta publish). See help for `yotta '+\n        'install` for installing modules, and for `yotta target` for '+\n        'switching targets.',\n        'Search for published modules and targets'\n    )\n    addParser('init', 'init', 'Create a new module.')\n    addParser('install', 'install',\n        'Add a specific module as a dependency, and download it, or install all '+\n        'dependencies for the current module. Use yotta install '+\n        'modulename@version to install a specific version.'\n    )\n    addParser('build', 'build',\n        'Build the current module. Options can be passed to the underlying '+\n        'build tool by passing them after --, e.g. to do a verbose build '+\n        'which will display each command as it is run, use:\\n'+\n        '  yotta build -- -v\\n\\n'+\n        'The programs or libraries to build can be specified (by default '+\n        'only the libraries needed by the current module and the current '+\n        \"module's own tests are built). For example, to build the tests of \"+\n        'all dependencies, run:\\n  yotta build all_tests\\n\\n',\n        'Build the current module.'\n    )\n    addParser('version', 'version', 'Bump the module version, or (with no arguments) display the current version.')\n    addParser('link', 'link',\n        'Symlink a module to be used into another module.\\n\\n'+\n        'Use: \"yotta link\" in a module to link it globally, then use \"yotta '+\n        'link <modulename>\" to link it into the module where you want to use '+\n        'it.\\n\\n'+\n        '\"yotta link ../path/to/module\" is also supported, which will create '+\n        'the global link and a link into the current module in a single step.',\n        'Symlink a module'\n    )\n    addParser('link-target', 'link_target',\n        'Symlink a target to be used into another module.\\n\\n'+\n        'Use: \"yotta link\" in a target to link it globally, then use \"yotta '+\n        'link-target <targetname>\" to link it into the module where you want to use '+\n        'it.\\n\\n'+\n        '\"yotta link ../path/to/target\" is also supported, which will create '+\n        'the global link and a link into the current module in a single step.',\n        'Symlink a target'\n    )\n    addParser('update', 'update', 'Update dependencies for the current module, or a specific module.')\n    addParser('target', 'target', 'Set or display the target device.')\n    addParser('debug', 'debug', 'Attach a debugger to the current target.  Requires target support.')\n    addParser('test', 'test_subcommand',\n        'Run the tests for the current module on the current target. A build '+\n        'will be run first, and options to the build subcommand are also '+\n        'accepted by test.\\nThis subcommand requires the target to provide a '+\n        '\"test\" script that will be used to run each test. Modules may also '+\n        'define a \"testReporter\" script, which will be piped the output from '+\n        'each test, and may produce a summary.',\n        'Run the tests for the current module on the current target. Requires target support for cross-compiling targets.'\n    )\n    addParser('start', 'start',\n        'Launch the compiled program (available for executable modules only). Requires target support for cross-compiling targets.'\n    )\n    addParser('publish', 'publish', 'Publish a module or target to the public registry.')\n    addParser('unpublish', 'unpublish', 'Un-publish a recently published module or target.')\n    addParser('login', 'login', 'Authorize for access to private github repositories and publishing to the yotta registry.')\n    addParser('logout', 'logout', 'Remove saved authorization token for the current user.')\n    addParser('whoami', 'whoami', 'Display who the currently logged in user is (if any).')\n    addParser('list', 'list', 'List the dependencies of the current module, or the inherited targets of the current target.')\n    addParser('outdated', 'outdated', 'Display information about dependencies which have newer versions available.')\n    addParser('uninstall', 'uninstall', 'Remove a specific dependency of the current module, both from module.json and from disk.')\n    addParser('remove', 'remove',\n        'Remove the downloaded version of a dependency module or target, or '+\n        'un-link a linked module or target (see yotta link --help for details '+\n        'of linking). This command does not modify your module.json file.',\n        'Remove or unlink a dependency without removing it from module.json.'\n    )\n    addParser('owners', 'owners', 'Add/remove/display the owners of a module or target.')\n    addParser('licenses', 'licenses', 'List the licenses of the current module and its dependencies.')\n    addParser('clean', 'clean', 'Remove files created by yotta and the build.')\n    addParser('config', 'config', 'Display the target configuration info.')\n    addParser('shrinkwrap', 'shrinkwrap', 'Create a yotta-shrinkwrap.json file to freeze dependency versions.')\n\n    # short synonyms, subparser.choices is a dictionary, so use update() to\n    # merge in the keys from another dictionary\n    short_commands = {\n                'up':subparser.choices['update'],\n                'in':subparser.choices['install'],\n                'ln':subparser.choices['link'],\n                 'v':subparser.choices['version'],\n                'ls':subparser.choices['list'],\n                'rm':subparser.choices['remove'],\n            'unlink':subparser.choices['remove'],\n     'unlink-target':subparser.choices['remove'],\n             'owner':subparser.choices['owners'],\n              'lics':subparser.choices['licenses'],\n               'who':subparser.choices['whoami'],\n               'run':subparser.choices['start']\n    }\n    subparser.choices.update(short_commands)\n\n    # split the args into those before and after any '--'\n    # argument - subcommands get raw access to arguments following '--', and\n    # may pass them on to (for example) the build tool being used\n    split_args = splitList(sys.argv, '--')\n    following_args = reduce(lambda x,y: x + ['--'] + y, split_args[1:], [])[1:]\n\n    # complete all the things :)\n    argcomplete.autocomplete(\n         parser,\n        exclude = list(short_commands.keys()) + ['-d', '--debug', '-v', '--verbose']\n    )\n\n    # when args are passed directly we need to strip off the program name\n    # (hence [:1])\n    args = parser.parse_args(split_args[0][1:])\n\n    # set global arguments that are shared everywhere and never change\n    globalconf.set('interactive', args.interactive)\n    globalconf.set('plain', args.plain)\n\n    # finally, do stuff!\n    if 'command' not in args:\n        parser.print_usage()\n        sys.exit(0)\n\n    try:\n        status = args.command(args, following_args)\n    except KeyboardInterrupt:\n        logging.warning('interrupted')\n        status = -1\n\n    sys.exit(status or 0)", "original_line_number": 6995, "instruction_length": 594}
{"instruction": "To use the given code, import the necessary libraries including psycopg2, io, os, datetime, time, logging, tornado, and urlparse. Then, create a class called listEnoseConfHandler that inherits from the RequestHandler class. Inside the class, define the initialize method that takes in database and IPs as parameters and sets them as instance variables. Define the get method that retrieves data from the database and renders an HTML page using the retrieved data. Create two more classes called inputEnoseConfigHandler and actionEnoseConfigHandler that inherit from the RequestHandler class. These classes also have the initialize method that takes in database and IPs as parameters and sets them as instance variables. The inputEnoseConfigHandler class has a get method that renders an HTML page. The actionEnoseConfigHandler class has a post method that retrieves data from a form and inserts it into the database.", "input": "", "output": "## database\nimport psycopg2\n\n## system libraries\nimport io, os\nimport datetime, time\nimport logging\n\n## web libraries\nimport tornado\nimport tornado.auth\nimport tornado.escape\nimport tornado.gen\nimport tornado.httpserver\nimport urlparse\nimport threading\nimport functools\nfrom tornado.ioloop import IOLoop\nfrom tornado.web import asynchronous, RequestHandler, Application\nfrom tornado.httpclient import AsyncHTTPClient\n\n## custom libraries\nimport faroreDB\n\nrootdir = os.path.dirname(__file__)\n\n\n\n\nclass listEnoseConfHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.db = database\n        self.IPs = IPs\n        return\n\n    def get(self):\n\n        if self.request.remote_ip[:11] in self.IPs :\n\n            miolo = '<div class=\"page-header\">' + \\\n                    '<table class=\"table table-striped\">' + \\\n                    '<thead><tr><th width=500px colspan=2>enose ID</th><th width=150px colspan=3>Location</th><th width=150px colspan=3>Date</th><th width=50px></th><th width=50px></th></tr></thead>'+ \\\n                    '<tbody>\\n'\n\n            # Retrieving data from inductions\n            db = self.db\n            listConfs = self.db.getEnoseConfs( )\n\n            for conf in listConfs:\n                miolo += \"<tr><td colspan=2>hal\" + str(conf[-1]) + \"k</td>\\n\"\n                miolo += \"<td colspan=3>\" + str(conf[-2]) + \"</td>\\n\"\n                miolo += \"<td colspan=5>\" + str(conf[1]) + \"</td>\"\n\n                miolo += \"</tr><tr>\"\n                for j in range(10):\n                    miolo += \"<td>\" + str(conf[2+j]) + \"</td>\"\n\n                miolo += \"</tr>\"\n\n            miolo += '</tbody></table></div>'\n            self.render(rootdir+'/pagess/index.html', title=\"Current list of ENoses\", miolo = miolo,\n                        top=file(rootdir+\"/pagess/top.html\").read(), bottom=file(rootdir+\"/pagess/bottom.html\").read())\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to list_inductions from outside IP list: ' + str(self.request.remote_ip) )\n\n        return\n\n\n\n\nclass inputEnoseConfigHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.IPs = IPs\n        return\n\n    def get(self):\n\n        if self.request.remote_ip[:-2] == self.IPs[0] or self.request.remote_ip[:7] == self.IPs[1]:\n            miolo = file(rootdir+'/pagess/input_enose_config.html').read()\n            self.render(rootdir+'/pagess/index.html', title=\"Farore's wind\", miolo = miolo,\n                        top=file(rootdir+\"/pagess/top.html\").read(), bottom=file(rootdir+\"/pagess/bottom.html\").read())\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to input_metadata from outside IP list: ' + str(self.request.remote_ip) )\n\n        return\n\n\n\n\n\n\nclass actionEnoseConfigHandler(tornado.web.RequestHandler):\n\n    def initialize(self, database, IPs):\n        self.db = database\n        self.IPs = IPs\n        return\n\n    def post(self):\n\n        if self.request.remote_ip[:11] in self.IPs :\n            self.render(rootdir+'/pagess/metadata_action.html')\n\n            date = self.get_argument('date', '')\n\n\n            S = []\n            for j in range(1,11):\n                S.append( self.get_argument('S'+str(j), '') )\n\n            T = []\n            for j in range(1,9):\n                T.append( self.get_argument('T'+str(j), '') )\n\n            location = self.get_argument('location', '')\n            enose =  self.get_argument('enose', '')\n\n            if len(enose) > 1:\n                enose = enose[3]\n\n            self.db.insertEnoseConf(enose, date, S, T, location)\n\n        ## If in this else, someone tried to access this\n        else:\n            logging.warning('Access to metadata_action from outside IP list: ' + str(self.request.remote_ip) )\n\n        return", "original_line_number": 7020, "instruction_length": 916}
{"instruction": "To run an MCMC simulation using emcee, follow these steps:\n\n1. Import the necessary libraries: emcee, pearce.emulator, pearce.mocks, numpy, and os.\n2. Define the training file path and fixed parameters.\n3. Create an emulator object using the OriginalRecipe class from pearce.emulator.\n4. Define the negative log-likelihood function, nll, which takes in a parameter vector and returns the negative log-likelihood of the emulator given the downsampled data.\n5. Define the prior function, lnprior, which takes in a parameter vector and returns the natural logarithm of the prior probability distribution.\n6. Define the log-posterior function, lnprob, which takes in a parameter vector and any additional arguments, and returns the natural logarithm of the posterior probability distribution.\n7. Set the initial parameter vector, p0, to a reasonable starting point.\n8. Set the number of walkers, nwalkers, and the number of steps, nsteps, for the MCMC simulation.\n9. Set the number of burn-in steps, nburn, to 0.\n10. Set the number of cores to use, ncores, for parallelization.\n11. Define the save directory and chain file name.\n12. Write the parameter names to the chain file.\n13. Initialize the EnsembleSampler object from emcee.\n14. Run the MCMC simulation using the sample() method of the EnsembleSampler object, passing in the initial position vector, pos0, and the number of iterations, nsteps.", "input": "", "output": "import emcee as mc\nfrom pearce.emulator import OriginalRecipe, ExtraCrispy\nfrom pearce.mocks import cat_dict\nimport numpy as np\nfrom os import path\ntraining_file = '/home/users/swmclau2/scratch/PearceRedMagicXiCosmo.hdf5'\na = 1.0\nz = 1./a-1.0\n\nfixed_params = {'z':z, 'r':24.06822623}\nn_leaves, n_overlap = 10, 2\nem_method = 'gp'\nemu = OriginalRecipe(training_file, method = em_method, fixed_params=fixed_params, downsample_factor = 0.1)\n\n# TODO downsample sampling?\ndef nll(p):\n    emu._emulator.set_parameter_vector(p)\n\n    ll = emu._emulator.lnlikelihood(emu.downsample_y, quiet=True)\n\n    return -ll if np.isfinite(ll) else 1e25\n\ndef lnprior(theta):\n    return -np.inf if np.any(np.logical_or(theta < -15, theta > 15)) else 0\n\ndef lnprob(theta, *args):\n    lp = lnprior(theta)\n    if not np.isfinite(lp):\n        return -np.inf\n\n    return lp - nll(theta, *args)\n\n#p0 = emu._emulator.get_parameter_vector()\n\n#p0 = np.array([  0.       ,  10.6161248,   1.8339794,   7.342365 ,  10.6371797,\n     #    2.2441632,  13.8155106,  11.3512804,   3.1795786,   4.6846614,\n     #             1.       ,   5.0188608,   3.7658774,  -1.5383083])\n\np0 = np.array([-12.0550382,   0.1054246,   0.2661017,   5.6407612,   0.2408568,   1.1295944,\n   0.3643993,  11.5649985,   4.9071932,   4.7031938,   1.,         11.7621938,\n     10.6279446,   0.,         10.6161248,   1.8339794,   7.342365   10.6371797,\n        2.2441632,  13.8155106,  11.3512804   3.1795786,   4.6846614   1.,\n           5.0188608,   3.7658774,  -1.5383083])\n\nnwalkers = 100 \nnsteps = 2000\nnburn = 0\nnum_params = p0.shape[0]#len(emu.get_param_names())+1\npos0 = p0+np.random.randn(nwalkers, num_params)\nncores = 16 \n\n\nsavedir = '/home/users/swmclau2/scratch/'\nchain_fname = path.join(savedir, '%d_walkers_%d_steps_cosmo_hyperparams.npy'%(nwalkers, nsteps))\n\npnames = ['amp']\npnames.extend(emu.get_param_names())\nwith open(chain_fname, 'w') as f:\n    f.write('#' + '\\t'.join(pnames)+'\\t'+ '\\t'.join(pnames)+'\\tamp'+'\\n')\n\nsampler = mc.EnsembleSampler(nwalkers, num_params, lnprob, threads=ncores)\nfor result in sampler.sample(pos0, iterations = nsteps, storechain=False):\n\n    with open(chain_fname, 'a') as f:\n        np.savetxt(f, result[0])", "original_line_number": 8053, "instruction_length": 1396}
{"instruction": "Create two functions, one to create a socket server and one to create a socket client. The server function should take in a host and port as arguments and return a socket object. The client function should take in a host and port as arguments and return a socket object. Additionally, create a function called forward that takes in a packet, an address, and a list of tuples containing addresses and ports. If the packet is not an error packet, iterate through the list of tuples and for each tuple, if the address is not equal to the current tuple's address, create a socket client and send the packet to the current tuple's address and port.", "input": "", "output": "import socket\n\n####### SOCKET\n\ndef create_socket_server(myHost, port):\n\ts = None\n\tfor res in socket.getaddrinfo(None, int(port), socket.AF_UNSPEC,socket.SOCK_STREAM, 0, socket.AI_PASSIVE):\n\t    af, socktype, proto, canonname, sa = res\n\t    try:\n\t    \ts = socket.socket(af, socktype, proto)\n\t    except socket.error as msg:\n\t    \ts = None\n\t    \tcontinue\n\t    try:\n\t    \ts.bind(sa)\n\t    \ts.listen(1000)\n\t    except socket.error as msg:\n\t    \ts.close()\n\t    \ts = None\n\t    \tcontinue\n\t    break\n\treturn s\n\ndef create_socket_client(myHost, port):\n\ts = None\n\tfor res in socket.getaddrinfo(myHost, int(port), socket.AF_UNSPEC, socket.SOCK_STREAM):\n\t    af, socktype, proto, canonname, sa = res\n\t    try:\n\t        s = socket.socket(af, socktype, proto)\n\t    except socket.error as msg:\n\t        s = None\n\t        continue\n\t    try:\n\t        s.connect(sa)\n\t    except socket.error as msg:\n\t        s.close()\n\t        s = None\n\t        continue\n\t    break\n\treturn s\n\ndef forward(pk, addr, l): \n\tif pk != bytes(const.ERROR_PKT, \"ascii\"):\n\t\tfor x in l:\n\t\t\tif addr != x[0]:\n\t\t\t\ts = func.create_socket_client(func.roll_the_dice(x[0]), x[1])\n\t\t\t\tif not(s is None):\n\t\t\t\t\ts.sendall(pk)\n\t\t\t\t\t#write_daemon_success(\"Daemon\", \"-\", \"Forward da \" + addr + \" a \" + x[0])\n\t\t\t\t\ts.close()", "original_line_number": 8088, "instruction_length": 643}
{"instruction": "To run pedestrian detection on a video file or camera feed, import the necessary packages by running the given code. Then, use argparse to add arguments for the video file and minimum area size. If no video file is specified, the camera feed will be used. Otherwise, the video file will be read. The code uses a HOG descriptor/person detector to detect people in the image and applies non-maxima suppression to the bounding boxes. The code also uses a Kalman filter to track the detected pedestrians. To run the code, execute the while loop and display the output images. The code will display the number of pedestrians found and the queue size. Press the ESC key to exit the program. At the end of the program, the elapsed time and approximate frames per second will be displayed, along with the tracked objects.", "input": "", "output": "# import the necessary packages\nfrom __future__ import print_function\nfrom non_max_suppression import non_max_suppression\nfrom myqueue import myqueue\nfrom frames import frames\nfrom object import Object\nimport numpy as np\nimport argparse\nimport datetime\nimport imutils\nimport cv2\nimport time\n\nap = argparse.ArgumentParser()\nap.add_argument(\"-v\", \"--video\", help=\"path to the video file\")\nap.add_argument(\"-a\", \"--min-area\", type=int, default=500, help=\"minimum area size\")\nargs = vars(ap.parse_args())\n\nif args.get(\"video\", None) is None:\n    camera = cv2.VideoCapture(0)\n# otherwise, we are reading from a video file\nelse:\n    print(\"[INFO] starting video file thread...\")\n    camera = myqueue(args[\"video\"]).start()\n    time.sleep(1.0)\n\ni = 0\ncenterX = 0\ncenterY = 0\nobjList = []\nmeas = []\npred = []\nmp = np.array((2, 1), np.float32)  # measurement\ntp = np.zeros((2, 1), np.float32)  # tracked / prediction\n\nkalman = cv2.KalmanFilter(4, 2)\nkalman.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)\nkalman.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)\nkalman.processNoiseCov = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32) * 0.03\n\n\ndef onPed(x, y):\n    global mp, meas\n    mp = np.array([[np.float32(x)], [np.float32(y)]])\n    meas.append((x, y))\n\n\ndef kalPredict(mp):\n    global tp, pred\n    kalman.correct(mp)\n    tp = kalman.predict()\n    pred.append((int(tp[0]), int(tp[1])))\n\n\ndef paint(tp, xA, yA, xB, yB):\n    global frame, pred\n    # cv2.circle(frame, ((tp[0]), (tp[1])), 3, (0, 0, 255), -1)\n    cv2.rectangle(frame, ((tp[0]) - ((xB - xA) / 2), (tp[1]) + (yB - yA) / 2),\n                  (((tp[0]) + ((xB - xA) / 2)), ((tp[1]) - (yB - yA) / 2)), (0, 0, 255), 2)\n\nfps = frames().start()\n# initialize the HOG descriptor/person detector\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n# loop over the image paths\nwhile camera.more():\n    frame = camera.read()\n    frame = imutils.resize(frame, width=600)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    # start = datetime.datetime.now()\n    # detect people in the image\n    (rects, weights) = hog.detectMultiScale(frame, winStride=(8, 8),\n                                            padding=(32, 32), scale=1.05)\n    # print(\"[INFO] detection took: {}\".format(\n    #(datetime.datetime.now() - start).total_seconds()))\n    # apply non-maxima suppression to the bounding boxes using a\n    # fairly large overlap threshold to try to maintain overlapping\n    # boxes that are still people\n    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n\n    # draw the final bounding boxes\n    for (xA, yA, xB, yB) in pick:\n        i = i+1\n        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n        centerX = (xB + xA) / 2\n        centerY = (yB + yA) / 2\n        obj = Object(centerX, centerY, i)\n        objList.append(obj)\n        onPed(centerX, centerY)\n        kalPredict(mp)\n        paint(tp, xA, yA, xB, yB)\n\n    cv2.putText(frame, \"Queue Size: {}\".format(camera.Q.qsize()),\n                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    peds_found = \"Found \" + str(len(pick)) + \" Pedestrians\"\n    cv2.putText(frame, peds_found, (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    # show the output images\n    cv2.imshow(\"HOG\", frame)\n    cv2.waitKey(1)\n    fps.update()\n    k = cv2.waitKey(27) & 0xff\n    if k == 27:\n        break\nfps.stop()\nfor objects in range(len(objList) - 1):\n    print(str(objList[objects]))\nprint(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\nprint(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\ncv2.destroyAllWindows()\ncamera.stop()", "original_line_number": 8301, "instruction_length": 813}
{"instruction": "This code defines a class called GoogleAPICaller that handles calling the Google API to compute itineraries. To use this class, create an instance of it by passing in a request object with from_x, from_y, to_x, and to_y attributes. Then, call the get_times() method to get the travel times for different modes of transportation, or call the get_itineraries() method to get the itineraries for different modes of transportation. The modes of transportation available are driving, walking, bicycling, and transit.", "input": "", "output": "import json\nimport re\nfrom bs4 import BeautifulSoup\nfrom model.Transport.Walk import Walk\nfrom model.Transport.PublicTransport import PublicTransport\nfrom model.Transport.Drive import Drive\nfrom model.Transport.Bicycle import Bicycle\nfrom model.Possibilities import Possibilities\nfrom webservice_caller.TransportAPICaller import TransportAPICaller\nfrom webservice_caller.call_url import call_url, APICallError\n\nclass GoogleAPICaller(TransportAPICaller):\n    '''\n    Class that handles calling google api to compute itiniraries\n    ''' \n    _url = 'https://maps.googleapis.com/maps/api/directions/json?'\n    _key = 'AIzaSyCqgwlzgUDYYF7xnePerJZaapgUWmyGYjc'\n\n    def __init__ (self, request):\n        '''\n        Create the different parameters that we will need for the API url\n        '''\n        self._origin = request.from_x, request.from_y\n        self._destination = request.to_x, request.to_y\n        self._modes = {'driving':Drive,'walking':Walk,'bicycling':Bicycle,'transit':PublicTransport}\n\n    @property\n    def modes(self):\n        return self._modes\n\n    def get_times(self):    \n        '''\n        Get the different times related to the travel modes and returns \n        a list of objects corresponding to each travel mode'\n        '''\n        times = {}\n    \n        for mode, mode_class in self._modes.items():\n            url_final = GoogleAPICaller._url + \"origin=\" + \",\".join(str (e) for e in self._origin) + \"&destination=\" + \",\".join(str(f) for f in self._destination) + \"&mode=\" + mode + \"&key=\" + GoogleAPICaller._key\n            response = call_url(url_final)\n            data = json.loads(response.content)  \n            try:\n                travel_time = data[\"routes\"][0][\"legs\"][0][\"duration\"][\"value\"]\n            except IndexError:\n                raise APICallError\n            except KeyError:\n                raise APICallError\n            times[mode] = travel_time\n        return times\n    \n\n    def get_itineraries(self):\n        '''\n        Get the different itineraries related to the travel modes\n        '''\n        itineraries = {}\n        for mode, mode_class in self._modes.items():\n            url_final = GoogleAPICaller._url + \"origin=\" + \",\".join(str (e) for e in self._origin) + \"&destination=\" + \",\".join(str(f) for f in self._destination) + \"&mode=\" + mode + \"&key=\" + GoogleAPICaller._key\n            response = call_url(url_final)\n            data = json.loads(response.content)  \n            try:\n                instruction = data[\"routes\"][0][\"legs\"][0][\"steps\"]\n            except IndexError:\n                raise APICallError\n            except KeyError:\n                raise APICallError    \n            itinerary = \"\"\n            for i in range(len(instruction)):\n                itinerary += instruction[i][\"html_instructions\"] + \", \"\n            clean_itinerary = BeautifulSoup(itinerary,\"html.parser\").text\n            itineraries[mode] = clean_itinerary  \n        return itineraries", "original_line_number": 8401, "instruction_length": 511}
{"instruction": "The given code is a Python module for reading and verifying configuration files for the OpenNSA network service. To use this module, import it into your Python code and call the `readConfig` function with the path to your configuration file as an argument. The function will return a `configparser.SafeConfigParser` object containing the configuration data. To verify the configuration, call the `readVerifyConfig` function with the `SafeConfigParser` object as an argument. The function will return a dictionary containing the verified configuration data.", "input": "", "output": "\"\"\"\nConfiguration reader and defaults.\n\nAuthor: Henrik Thostrup Jensen <htj@nordu.net>\nCopyright: NORDUnet (2011)\n\"\"\"\n\nimport os\nimport configparser\n\nfrom opennsa import constants as cnt\n\n\n\n# defaults\nDEFAULT_CONFIG_FILE     = '/etc/opennsa.conf'\nDEFAULT_LOG_FILE        = '/var/log/opennsa.log'\nDEFAULT_TLS             = 'true'\nDEFAULT_TOPOLOGY_FILE   = '/usr/local/share/nsi/topology.owl'\nDEFAULT_TCP_PORT        = 9080\nDEFAULT_TLS_PORT        = 9443\nDEFAULT_VERIFY          = True\nDEFAULT_CERTIFICATE_DIR = '/etc/ssl/certs' # This will work on most mordern linux distros\n\n\n# config blocks and options\nBLOCK_SERVICE    = 'service'\nBLOCK_DUD        = 'dud'\nBLOCK_JUNIPER_EX = 'juniperex'\nBLOCK_JUNIPER_VPLS = 'junipervpls'\nBLOCK_FORCE10    = 'force10'\nBLOCK_BROCADE    = 'brocade'\nBLOCK_NCSVPN     = 'ncsvpn'\nBLOCK_PICA8OVS   = 'pica8ovs'\nBLOCK_JUNOSMX    = 'junosmx'\nBLOCK_JUNOSEX    = 'junosex'\nBLOCK_JUNOSSPACE = 'junosspace'\nBLOCK_OESS       = 'oess'\nBLOCK_CUSTOM_BACKEND = 'custombackend'\n\n# service block\nDOMAIN           = 'domain'      # mandatory\nNETWORK_NAME     = 'network'     # legacy, used to be mandatory\nLOG_FILE         = 'logfile'\nHOST             = 'host'\nPORT             = 'port'\nTLS              = 'tls'\nREST             = 'rest'\nNRM_MAP_FILE     = 'nrmmap'\nPEERS            = 'peers'\nPOLICY           = 'policy'\nPLUGIN           = 'plugin'\nSERVICE_ID_START = 'serviceid_start'\n\n# database\nDATABASE                = 'database'    # mandatory\nDATABASE_USER           = 'dbuser'      # mandatory\nDATABASE_PASSWORD       = 'dbpassword'  # can be none (os auth)\nDATABASE_HOST           = 'dbhost'      # can be none (local db)\n\n# tls\nKEY                     = 'key'         # mandatory, if tls is set\nCERTIFICATE             = 'certificate' # mandatory, if tls is set\nCERTIFICATE_DIR         = 'certdir'     # mandatory (but dir can be empty)\nVERIFY_CERT             = 'verify'\nALLOWED_HOSTS           = 'allowedhosts' # comma seperated list\n\n# generic stuff\n_SSH_HOST               = 'host'\n_SSH_PORT               = 'port'\n_SSH_HOST_FINGERPRINT   = 'fingerprint'\n_SSH_USER               = 'user'\n_SSH_PASSWORD           = 'password'\n_SSH_PUBLIC_KEY         = 'publickey'\n_SSH_PRIVATE_KEY        = 'privatekey'\n\nAS_NUMBER              = 'asnumber'\n\n# TODO: Don't do backend specifics for everything, it causes confusion, and doesn't really solve anything\n\n# juniper block - same for mx / ex backends\nJUNIPER_HOST                = _SSH_HOST\nJUNIPER_PORT                = _SSH_PORT\nJUNIPER_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nJUNIPER_USER                = _SSH_USER\nJUNIPER_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nJUNIPER_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\n\n# force10 block\nFORCE10_HOST            = _SSH_HOST\nFORCE10_PORT            = _SSH_PORT\nFORCE10_USER            = _SSH_USER\nFORCE10_PASSWORD        = _SSH_PASSWORD\nFORCE10_HOST_FINGERPRINT = _SSH_HOST_FINGERPRINT\nFORCE10_SSH_PUBLIC_KEY  = _SSH_PUBLIC_KEY\nFORCE10_SSH_PRIVATE_KEY = _SSH_PRIVATE_KEY\n\n# Brocade block\nBROCADE_HOST              = _SSH_HOST\nBROCADE_PORT              = _SSH_PORT\nBROCADE_HOST_FINGERPRINT  = _SSH_HOST_FINGERPRINT\nBROCADE_USER              = _SSH_USER\nBROCADE_SSH_PUBLIC_KEY    = _SSH_PUBLIC_KEY\nBROCADE_SSH_PRIVATE_KEY   = _SSH_PRIVATE_KEY\nBROCADE_ENABLE_PASSWORD   = 'enablepassword'\n\n# Pica8 OVS\nPICA8OVS_HOST                = _SSH_HOST\nPICA8OVS_PORT                = _SSH_PORT\nPICA8OVS_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nPICA8OVS_USER                = _SSH_USER\nPICA8OVS_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nPICA8OVS_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\nPICA8OVS_DB_IP               = 'dbip'\n\n\n# NCS VPN Backend\nNCS_SERVICES_URL        = 'url'\nNCS_USER                = 'user'\nNCS_PASSWORD            = 'password'\n\n# JUNOS block\nJUNOS_HOST                = _SSH_HOST\nJUNOS_PORT                = _SSH_PORT\nJUNOS_HOST_FINGERPRINT    = _SSH_HOST_FINGERPRINT\nJUNOS_USER                = _SSH_USER\nJUNOS_SSH_PUBLIC_KEY      = _SSH_PUBLIC_KEY\nJUNOS_SSH_PRIVATE_KEY     = _SSH_PRIVATE_KEY\nJUNOS_ROUTERS             = 'routers'\n\n#Junosspace backend\nSPACE_USER              = 'space_user'\nSPACE_PASSWORD          = 'space_password'\nSPACE_API_URL           = 'space_api_url'\nSPACE_ROUTERS           = 'routers'\nSPACE_CONFIGLET_ACTIVATE_LOCAL = 'configlet_activate_local'  \nSPACE_CONFIGLET_ACTIVATE_REMOTE = 'configlet_activate_remote'\nSPACE_CONFIGLET_DEACTIVATE_LOCAL = 'configlet_deactivate_local'\nSPACE_CONFIGLET_DEACTIVATE_REMOTE = 'configlet_deactivate_remote'\n\n# OESS\nOESS_URL                = 'url'\nOESS_USER               = 'username'\nOESS_PASSWORD           = 'password'\nOESS_WORKGROUP          = 'workgroup'\n\n\nclass ConfigurationError(Exception):\n    \"\"\"\n    Raised in case of invalid/inconsistent configuration.\n    \"\"\"\n\n\nclass Peer(object):\n\n    def __init__(self, url, cost):\n        self.url = url\n        self.cost = cost\n\n\n\ndef readConfig(filename):\n\n    cfg = configparser.SafeConfigParser()\n\n    cfg.add_section(BLOCK_SERVICE)\n    cfg.read( [ filename ] )\n\n    return cfg\n\n\n\ndef readVerifyConfig(cfg):\n    \"\"\"\n    Read a config and verify that things are correct. Will also fill in\n    default values where applicable.\n\n    This is supposed to be used during application creation (before service\n    start) to ensure that simple configuration errors do not pop up efter\n    daemonization.\n\n    Returns a \"verified\" config, which is a dictionary.\n    \"\"\"\n\n    vc = {}\n\n    # Check for deprecated / old invalid stuff\n\n    try:\n        cfg.get(BLOCK_SERVICE, NRM_MAP_FILE)\n        raise ConfigurationError('NRM Map file should be specified under backend')\n    except configparser.NoOptionError:\n        pass\n\n    # check / extract\n\n    try:\n        vc[DOMAIN] = cfg.get(BLOCK_SERVICE, DOMAIN)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No domain name specified in configuration file (mandatory, see docs/migration)')\n\n    try:\n        cfg.get(BLOCK_SERVICE, NETWORK_NAME)\n        raise ConfigurationError('Network name no longer used, use domain (see docs/migration)')\n    except configparser.NoOptionError:\n        pass\n\n    try:\n        vc[LOG_FILE] = cfg.get(BLOCK_SERVICE, LOG_FILE)\n    except configparser.NoOptionError:\n        vc[LOG_FILE] = DEFAULT_LOG_FILE\n\n    try:\n        nrm_map_file = cfg.get(BLOCK_SERVICE, NRM_MAP_FILE)\n        if not os.path.exists(nrm_map_file):\n            raise ConfigurationError('Specified NRM mapping file does not exist (%s)' % nrm_map_file)\n        vc[NRM_MAP_FILE] = nrm_map_file\n    except configparser.NoOptionError:\n        vc[NRM_MAP_FILE] = None\n\n    try:\n        vc[REST] = cfg.getboolean(BLOCK_SERVICE, REST)\n    except configparser.NoOptionError:\n        vc[REST] = False\n\n    try:\n        peers_raw = cfg.get(BLOCK_SERVICE, PEERS)\n        vc[PEERS] = [ Peer(purl.strip(), 1) for purl in  peers_raw.split('\\n') ]\n    except configparser.NoOptionError:\n        vc[PEERS] = None\n\n    try:\n        vc[HOST] = cfg.get(BLOCK_SERVICE, HOST)\n    except configparser.NoOptionError:\n        vc[HOST] = None\n\n    try:\n        vc[TLS] = cfg.getboolean(BLOCK_SERVICE, TLS)\n    except configparser.NoOptionError:\n        vc[TLS] = DEFAULT_TLS\n\n    try:\n        vc[PORT] = cfg.getint(BLOCK_SERVICE, PORT)\n    except configparser.NoOptionError:\n        vc[PORT] = DEFAULT_TLS_PORT if vc[TLS] else DEFAULT_TCP_PORT\n\n    try:\n        policies = cfg.get(BLOCK_SERVICE, POLICY).split(',')\n        for policy in policies:\n            if not policy in (cnt.REQUIRE_USER, cnt.REQUIRE_TRACE, cnt.AGGREGATOR, cnt.ALLOW_HAIRPIN):\n                raise ConfigurationError('Invalid policy: %s' % policy)\n        vc[POLICY] = policies\n    except configparser.NoOptionError:\n        vc[POLICY] = []\n\n    try:\n        vc[PLUGIN] = cfg.get(BLOCK_SERVICE, PLUGIN)\n    except configparser.NoOptionError:\n        vc[PLUGIN] = None\n\n    # database\n    try:\n        vc[DATABASE] = cfg.get(BLOCK_SERVICE, DATABASE)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No database specified in configuration file (mandatory)')\n\n    try:\n        vc[DATABASE_USER] = cfg.get(BLOCK_SERVICE, DATABASE_USER)\n    except configparser.NoOptionError:\n        raise ConfigurationError('No database user specified in configuration file (mandatory)')\n\n    try:\n        vc[DATABASE_PASSWORD] = cfg.get(BLOCK_SERVICE, DATABASE_PASSWORD)\n    except configparser.NoOptionError:\n        vc[DATABASE_PASSWORD] = None\n\n    try:\n        vc[DATABASE_HOST] = cfg.get(BLOCK_SERVICE, DATABASE_HOST)\n    except configparser.NoOptionError:\n        vc[DATABASE_HOST] = None\n\n    try:\n        vc[SERVICE_ID_START] = cfg.get(BLOCK_SERVICE, SERVICE_ID_START)\n    except configparser.NoOptionError:\n        vc[SERVICE_ID_START] = None\n\n    # we always extract certdir and verify as we need that for performing https requests\n    try:\n        certdir = cfg.get(BLOCK_SERVICE, CERTIFICATE_DIR)\n        if not os.path.exists(certdir):\n            raise ConfigurationError('Specified certdir does not exist (%s)' % certdir)\n        vc[CERTIFICATE_DIR] = certdir\n    except configparser.NoOptionError:\n        vc[CERTIFICATE_DIR] = DEFAULT_CERTIFICATE_DIR\n    try:\n        vc[VERIFY_CERT] = cfg.getboolean(BLOCK_SERVICE, VERIFY_CERT)\n    except configparser.NoOptionError:\n        vc[VERIFY_CERT] = DEFAULT_VERIFY\n\n    # tls\n    if vc[TLS]:\n        try:\n            hostkey  = cfg.get(BLOCK_SERVICE, KEY)\n            hostcert = cfg.get(BLOCK_SERVICE, CERTIFICATE)\n\n            if not os.path.exists(hostkey):\n                raise ConfigurationError('Specified hostkey does not exist (%s)' % hostkey)\n            if not os.path.exists(hostcert):\n                raise ConfigurationError('Specified hostcert does not exist (%s)' % hostcert)\n\n            vc[KEY] = hostkey\n            vc[CERTIFICATE] = hostcert\n\n            try:\n                allowed_hosts_cfg = cfg.get(BLOCK_SERVICE, ALLOWED_HOSTS)\n                vc[ALLOWED_HOSTS] = allowed_hosts_cfg.split(',')\n            except:\n                pass\n\n        except configparser.NoOptionError as e:\n            # Not enough options for configuring tls context\n            raise ConfigurationError('Missing TLS option: %s' % str(e))\n\n\n    # backends\n    backends = {}\n\n    for section in cfg.sections():\n\n        if section == 'service':\n            continue\n\n        if ':' in section:\n            backend_type, name = section.split(':',2)\n        else:\n            backend_type = section\n            name = ''\n\n        if name in backends:\n            raise ConfigurationError('Can only have one backend named \"%s\"' % name)\n\n        if backend_type in (BLOCK_DUD, BLOCK_JUNIPER_EX, BLOCK_JUNIPER_VPLS, BLOCK_JUNOSMX, BLOCK_FORCE10, BLOCK_BROCADE,\n                            BLOCK_NCSVPN, BLOCK_PICA8OVS, BLOCK_OESS, BLOCK_JUNOSSPACE, BLOCK_JUNOSEX,\n                            BLOCK_CUSTOM_BACKEND, 'asyncfail'):\n            backend_conf = dict( cfg.items(section) )\n            backend_conf['_backend_type'] = backend_type\n            backends[name] = backend_conf\n\n    vc['backend'] = backends\n\n    return vc", "original_line_number": 8518, "instruction_length": 556}
{"instruction": "This code provides functions for management of installed APK lists. To use it, call the desired function with the appropriate arguments. The available functions are:\n- create_pkgdict(): creates a dictionary for fast path lookup from /data/system/packages.xml.\n- list_installed_pkgs(args): lists the members of a given category of packages.\n- list_installed_pkgs_nougat(args): uses Nougat's cmd command to query the package service (faster).\n- check_substratum(nougat): checks if the Substratum engine is installed.\n- exclude_overlays(aapt, pkgdict, pkgs): excludes Substratum overlays from the packages to extract.\n- exclude_arcus_variants(pkgs): excludes Arcus theme variants from the packages to extract.\n- check_already_extracted(pkgpath, md5sums): checks if an APK has already been extracted.", "input": "", "output": "\"\"\"Module with functions for management of installed APK lists.\"\"\"\n\nimport glob\nimport re\nimport subprocess\n\nimport apkutils    # needed for AndroidManifest.xml dump\nimport utils    # needed for sudo\n\n# Creates a APK/path dictionary to avoid the sluggish \"pm path\"\ndef create_pkgdict():\n    \"\"\"Creates a dict for fast path lookup from /data/system/packages.xml; returns dict.\"\"\"\n    \n    (out, err) = utils.sudo(\"cat /data/system/packages.xml\")\n    \n    if err: return False\n    \n    xml_dump = [i for i in out.decode(\"utf-8\").split(\"\\n\") if \"<package name=\" in i]\n    pkgdict = {}\n    \n    for i in xml_dump:\n        pkgname = re.findall(\"<package name=\\\"(.*?)\\\"\", i)[0]\n        pkgpath = re.findall(\"codePath=\\\"(.*?)\\\"\", i)[0]\n        \n        # Normalizes each entry\n        if not pkgpath.endswith(\".apk\"):\n            try:\n                pkgpath = glob.glob(pkgpath + \"/*.apk\")[0]\n            except:\n                continue\n        \n        pkgdict[pkgname] = pkgpath\n    \n    return pkgdict\n\ndef list_installed_pkgs(args):\n    \"\"\"Lists the members of a given category of packages; returns list.\"\"\"\n    \n    prefix = \"pm list packages\"\n    \n    if args.user:\n        suffix = \"-3\"\n    elif args.system:\n        suffix = \"-s\"\n    elif args.disabled:\n        suffix = \"-d\"\n    else:\n        suffix = \"\"\n    \n    pkgs = [i[8:] for i in subprocess.Popen(\"{0} {1}\".format(prefix, suffix), stdout = subprocess.PIPE, stderr = subprocess.PIPE, shell = True).communicate()[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    return pkgs\n\ndef list_installed_pkgs_nougat(args):\n    \"\"\"Uses Nougat's cmd command to query the package service (faster); returns list.\"\"\"\n    \n    prefix = \"cmd package list packages\"\n    \n    if args.user:\n        suffix = \"-3\"\n    elif args.system:\n        suffix = \"-s\"\n    elif args.disabled:\n        suffix = \"-d\"\n    else:\n        suffix = \"\"\n    \n    pkgs = [i[8:] for i in utils.sudo(\"{0} {1}\".format(prefix, suffix))[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    return pkgs\n\ndef check_substratum(nougat):\n    \"\"\"Checks if the Substratum engine is installed; returns bool.\"\"\"\n    \n    if nougat:\n        user_pkgs = [i[8:] for i in utils.sudo(\"cmd package list packages -3\")[0].decode(\"utf-8\").split(\"\\n\") if i]\n    else:\n        user_pkgs = [i[8:] for i in subprocess.Popen(\"pm list packages -3\", stdout = subprocess.PIPE, shell = True).communicate()[0].decode(\"utf-8\").split(\"\\n\") if i]\n    \n    substratum_installed = True if \"projekt.substratum\" in user_pkgs else False\n    \n    return substratum_installed\n\ndef exclude_overlays(aapt, pkgdict, pkgs):\n    \"\"\"Excludes Substratum overlays from the packages to extract; returns nothing.\"\"\"\n    \n    for i in pkgs:\n        pkgpath = pkgdict.get(i)\n        out = apkutils.get_pkgxml(aapt, pkgpath)[0].decode(\"utf-8\")\n        \n        if \"Substratum_Parent\" in out: pkgs.remove(i)\n\ndef exclude_arcus_variants(pkgs):\n    \"\"\"Excludes Arcus theme variants from the packages to extract; returns nothing.\"\"\"\n    \n    for i in pkgs:\n        if \"pixkart.arcus.user\" in i: pkgs.remove(i)\n\ndef check_already_extracted(pkgpath, md5sums):\n    \"\"\"Checks if an APK has already been extracted; returns bool, str.\"\"\"\n    \n    pkgsum = utils.compute_md5sum(pkgpath)\n    already_extracted = True if pkgsum in md5sums else False\n    \n    return already_extracted, pkgsum", "original_line_number": 8903, "instruction_length": 796}
{"instruction": "This code is an IBM Storwize V7000 autodiscovery script for Zabbix. To use it, run svc_perf_discovery_sender.py with the following options: --debug (to enable debug output), --clusters <svc1>[,<svc2>...] (comma-separated Storwize node list), --user <username> (Storwize V7000 user account with Administrator role), --password <pwd> (user password), and --discovery-types <type1>,[type2] (discovery types: 'volume-mdisk','volume','mdisk','pool'). The script sends volume/mdisk/pool LLD JSON data to LLD trapper items \"svc.discovery.<volume-mdisk|volume|mdisk|pool>\" and should be used with \"_Special_Storwize_Perf\" Zabbix template. See also http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery.", "input": "", "output": "#!/usr/bin/python\n# -*- coding: utf-8 -*- # coding: utf-8\n#\n# IBM Storwize V7000 autodiscovery script for Zabbix\n#\n# 2013 Matvey Marinin\n#\n# Sends volume/mdisk/pool LLD JSON data to LLD trapper items \"svc.discovery.<volume-mdisk|volume|mdisk|pool>\"\n# Use with \"_Special_Storwize_Perf\" Zabbix template\n#\n# See also http://www.zabbix.com/documentation/2.0/manual/discovery/low_level_discovery\n#\n# Usage:\n# svc_perf_discovery_sender.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd>\n#\n#   --debug    = Enable debug output\n#   --clusters = Comma-separated Storwize node list\n#   --user     = Storwize V7000 user account with Administrator role (it seems that Monitor role is not enough)\n#   --password = User password\n#\nimport pywbem\nimport getopt, sys\nfrom zbxsend import Metric, send_to_zabbix\nimport logging\n\ndef usage():\n  print >> sys.stderr, \"Usage: svc_perf_discovery_sender_zabbix.py [--debug] --clusters <svc1>[,<svc2>...] --user <username> --password <pwd> --discovery-types <type1>,[type2]\"\n  print >> sys.stderr, \"Discovery types: 'volume-mdisk','volume','mdisk','pool'\"\n\n\ntry:\n  opts, args = getopt.gnu_getopt(sys.argv[1:], \"-h\", [\"help\", \"clusters=\", \"user=\", \"password=\", \"debug\", \"discovery-types=\"])\nexcept getopt.GetoptError, err:\n  print >> sys.stderr, str(err)\n  usage()\n  sys.exit(2)\n\ndebug = False\nclusters = []\nDISCOVERY_TYPES = []\nuser = None\npassword = None\nfor o, a in opts:\n  if o == \"--clusters\" and not a.startswith('--'):\n    clusters.extend( a.split(','))\n  elif o == \"--user\" and not a.startswith('--'):\n    user = a\n  elif o == \"--password\" and not a.startswith('--'):\n    password = a\n  elif o == \"--debug\":\n    debug = True\n  elif o == \"--discovery-types\":\n    DISCOVERY_TYPES.extend( a.split(','))\n  elif o in (\"-h\", \"--help\"):\n    usage()\n    sys.exit()\n\nif not clusters:\n  print >> sys.stderr, '--clusters option must be set'\n  usage()\n  sys.exit(2)\n\nif not DISCOVERY_TYPES:\n  print >> sys.stderr, '--discovery-types option must be set'\n  usage()\n  sys.exit(2)\n\nif not user or not password:\n  print >> sys.stderr, '--user and --password options must be set'\n  usage()\n  sys.exit(2)\n\ndef debug_print(message):\n  if debug:\n    print message\n\nfor cluster in clusters:\n  debug_print('Connecting to: %s' % cluster)\n  conn = pywbem.WBEMConnection('https://'+cluster, (user, password), 'root/ibm')\n  conn.debug = True\n\n  for discovery in DISCOVERY_TYPES:\n    output = []\n\n    if discovery == 'volume-mdisk' or discovery == 'volume':\n      for vol in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_StorageVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('volume', vol.properties['ElementName'].value, vol.properties['DeviceID'].value) )\n\n    if discovery == 'volume-mdisk' or discovery == 'mdisk':\n      for mdisk in conn.ExecQuery('WQL', 'select DeviceID, ElementName from IBMTSSVC_BackendVolume'):\n        output.append( '{\"{#TYPE}\":\"%s\", \"{#NAME}\":\"%s\", \"{#ID}\":\"%s\"}' % ('mdisk', mdisk.properties['ElementName'].value, mdisk.properties['DeviceID'].value) )\n\n    if discovery == 'pool':\n      for pool in conn.ExecQuery('WQL', 'select PoolID, ElementName from IBMTSSVC_ConcreteStoragePool'):\n        output.append( '{\"{#TYPE}\":\"%s\",\"{#NAME}\":\"%s\",\"{#ID}\":\"%s\"}' % ('pool', pool.properties['ElementName'].value, pool.properties['PoolID'].value) )\n\n    json = []\n    json.append('{\"data\":[')\n\n    for i, v in enumerate( output ):\n      if i < len(output)-1:\n        json.append(v+',')\n      else:\n        json.append(v)\n    json.append(']}')\n\n    json_string = ''.join(json)\n    print(json_string)\n\n    trapper_key = 'svc.discovery.%s' % discovery\n    debug_print('Sending to host=%s, key=%s' % (cluster, trapper_key))\n\n    #send json to LLD trapper item with zbxsend module\n    if debug:\n      logging.basicConfig(level=logging.INFO)\n    else:\n      logging.basicConfig(level=logging.WARNING)\n    send_to_zabbix([Metric(cluster, trapper_key, json_string)], 'localhost', 10051)\n    debug_print('')", "original_line_number": 9553, "instruction_length": 717}
{"instruction": "Update the Django settings module to \"ms2ldaviz.settings_simon\". Create a feature set named \"binned_005\" if it does not exist. Get an experiment named \"massbank_binned_005\" and set its feature set to \"binned_005\". For features in \"massbank_binned_005\" without a feature set, set their feature set to \"binned_005\". Get all features tied to feature set \"binned_005\". Get another experiment specified by the command line argument and set its feature set to \"binned_005\". For features in the other experiment with the same name as features in \"massbank_binned_005\", replace them with the corresponding feature from \"massbank_binned_005\". For features in the other experiment with a name not in \"massbank_binned_005\", create a new feature with the same name and set its feature set to \"binned_005\". Delete features in the other experiment that have no feature instances, mass2motif instances, or feature maps.", "input": "", "output": "import os\nimport sys\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"ms2ldaviz.settings_simon\")\n\nimport django\ndjango.setup()\n\nfrom basicviz.models import *\n\nif __name__ == '__main__':\n\n\n\tother_experiment_name = sys.argv[1]\n\n\tfs,status = BVFeatureSet.objects.get_or_create(name = 'binned_005')\n\tif status:\n\t\tprint(\"Created feature set\")\n\telse:\n\t\tprint(\"Featureset already exists\")\n\n\tmbe = Experiment.objects.get(name = 'massbank_binned_005')\n\tprint(\"Got \" + str(mbe))\n\tif mbe.featureset == None:\n\t\tmbe.featureset = fs\n\t\tmbe.save()\n\n\tmbe_features = Feature.objects.filter(experiment = mbe)\n\tprint(\"Got {} features\".format(len(mbe_features)))\n\n\tmbe_features_sub = Feature.objects.filter(experiment = mbe,featureset = None)\n\tprint(\"{} have no featureset\".format(len(mbe_features_sub)))\n\tfor f in mbe_features_sub:\n\t\tf.featureset = fs\n\t\tf.save()\n\n\n\t# Now get the features as tied to the feature set\n\tmbe_features = Feature.objects.filter(featureset = fs)\n\tprint(\"Got {} features\".format(len(mbe_features)))\n\n\tfnames = set([f.name for f in mbe_features])\n\n\t# get another experiment\n\ti93 = Experiment.objects.get(name = other_experiment_name)\n\ti93.featureset = fs\n\ti93.save()\n\tprint(\"Got \" + str(i93))\n\n\ti93_features = Feature.objects.filter(experiment = i93)\n\n\tprint(\"Got {} features\".format(len(i93_features)))\n\tfor f in i93_features:\n\t\tif f.name in fnames:\n\t\t\t# Find all the instances\n\t\t\tfis = FeatureInstance.objects.filter(feature = f)\n\t\t\tgfeature = [g for g in mbe_features if g.name == f.name][0]\n\t\t\tfor fi in fis:\n\t\t\t\tfi.feature = gfeature\n\t\t\t\tfi.save()\n\t\t\tmis = Mass2MotifInstance.objects.filter(feature = f)\n\t\t\tfor ms in mis:\n\t\t\t\tms.feature = gfeature\n\t\t\t\tms.save()\n\n\n\t\telse:\n\t\t\tnew_feature = Feature.objects.create(name = f.name,featureset = fs,min_mz = f.min_mz,max_mz = f.max_mz)\n\t\t\tfis = FeatureInstance.objects.filter(feature = f)\n\t\t\tfor fi in fis:\n\t\t\t\tfi.feature = new_feature\n\t\t\t\tfi.save()\n\t\t\tmis = Mass2MotifInstance.objects.filter(feature = f)\n\t\t\tfor ms in mis:\n\t\t\t\tms.feature = new_feature\n\t\t\t\tms.save()\n\n\n\tfor f in i93_features:\n\t\tif len(f.featureinstance_set.all()) == 0 and len(f.mass2motifinstance_set.all()) == 0 and len(f.featuremap_set.all()) == 0:\n\t\t\tf.delete()\n\t\telse:\n\t\t\tprint(f)", "original_line_number": 9570, "instruction_length": 904}
{"instruction": "This code is a console application that allows searching for train stations and getting departure times. To use this application, you can run it in a Python environment and call the functions defined in the code. The main functions are \"do_stations\", \"do_departures\", and \"do_roadmap\". The \"do_stations\" function searches for train stations based on a given pattern. The \"do_departures\" function lists all departures for a given station. The \"do_roadmap\" function displays the roadmap to travel from a departure to an arrival station. To use the \"do_roadmap\" function, you can pass the departure and arrival stations as arguments and optionally specify the departure and arrival times using the \"--departure-time\" and \"--arrival-time\" options.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n# Copyright(C) 2010-2011 Romain Bignon, Julien Hébert\n#\n# This file is part of weboob.\n#\n# weboob is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# weboob is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with weboob. If not, see <http://www.gnu.org/licenses/>.\n\n\nimport sys\nfrom datetime import datetime\nimport logging\n\nfrom weboob.capabilities.travel import ICapTravel, RoadmapFilters\nfrom weboob.tools.application.repl import ReplApplication, defaultcount\n\n\n__all__ = ['Traveloob']\n\n\nclass Traveloob(ReplApplication):\n    APPNAME = 'traveloob'\n    VERSION = '0.h'\n    COPYRIGHT = 'Copyright(C) 2010-2011 Romain Bignon'\n    DESCRIPTION = \"Console application allowing to search for train stations and get departure times.\"\n    SHORT_DESCRIPTION = \"search for train stations and departures\"\n    CAPS = ICapTravel\n    DEFAULT_FORMATTER = 'table'\n\n    def add_application_options(self, group):\n        group.add_option('--departure-time')\n        group.add_option('--arrival-time')\n\n    @defaultcount(10)\n    def do_stations(self, pattern):\n        \"\"\"\n        stations PATTERN\n\n        Search stations.\n        \"\"\"\n        for backend, station in self.do('iter_station_search', pattern):\n            self.format(station)\n\n    @defaultcount(10)\n    def do_departures(self, line):\n        \"\"\"\n        departures STATION [ARRIVAL]\n\n        List all departures for a given station.\n        \"\"\"\n        station, arrival = self.parse_command_args(line, 2, 1)\n\n        station_id, backend_name = self.parse_id(station)\n        if arrival:\n            arrival_id, backend_name2 = self.parse_id(arrival)\n            if backend_name and backend_name2 and backend_name != backend_name2:\n                logging.error('Departure and arrival aren\\'t on the same backend')\n                return 1\n        else:\n            arrival_id = backend_name2 = None\n\n        if backend_name:\n            backends = [backend_name]\n        elif backend_name2:\n            backends = [backend_name2]\n        else:\n            backends = None\n\n        for backend, departure in self.do('iter_station_departures', station_id, arrival_id, backends=backends):\n            self.format(departure)\n\n    def do_roadmap(self, line):\n        \"\"\"\n        roadmap DEPARTURE ARRIVAL\n\n        Display the roadmap to travel from DEPARTURE to ARRIVAL.\n\n        Command-line parameters:\n           --departure-time TIME    requested departure time\n           --arrival-time TIME      requested arrival time\n\n        TIME might be in form \"yyyy-mm-dd HH:MM\" or \"HH:MM\".\n\n        Example:\n            > roadmap Puteaux Aulnay-sous-Bois --arrival-time 22:00\n        \"\"\"\n        departure, arrival = self.parse_command_args(line, 2, 2)\n\n        filters = RoadmapFilters()\n        try:\n            filters.departure_time = self.parse_datetime(self.options.departure_time)\n            filters.arrival_time = self.parse_datetime(self.options.arrival_time)\n        except ValueError as e:\n            print >>sys.stderr, 'Invalid datetime value: %s' % e\n            print >>sys.stderr, 'Please enter a datetime in form \"yyyy-mm-dd HH:MM\" or \"HH:MM\".'\n            return 1\n\n        for backend, route in self.do('iter_roadmap', departure, arrival, filters):\n            self.format(route)\n\n    def parse_datetime(self, text):\n        if text is None:\n            return None\n\n        try:\n            date = datetime.strptime(text, '%Y-%m-%d %H:%M')\n        except ValueError:\n            try:\n                date = datetime.strptime(text, '%H:%M')\n            except ValueError:\n                raise ValueError(text)\n            date = datetime.now().replace(hour=date.hour, minute=date.minute)\n\n        return date", "original_line_number": 9765, "instruction_length": 743}
{"instruction": "Create a PlayerThread class that extends Thread and takes in parameters GameThread, username, and ia. In the constructor, initialize the GameThread, username, ia, and joueur attributes. Increment the parties attribute of the barrierTours object of GameThread. Append the joueur attribute to the joueursAAdd list of GameThread. Increment the nbth attribute of GameThread. Implement the run method that executes an infinite loop. Inside the loop, wait for the barrierTours object of GameThread. Then, execute the executeIa method. Wait for the barrierEtape object of GameThread. Calculate the position of the player. Wait for the barrierEtape object of GameThread. Join the player. Wait for the barrierEtape object of GameThread. Acquire the lockmanger object of GameThread. Append the agraille variable to the aManger list of GameThread. Release the lockmanger object of GameThread. Wait for the barrierManger object of GameThread. If the poidTotal attribute of joueur is less than or equal to 0 and the end attribute of joueur is False, set the end attribute of joueur to True. Print a message indicating that the player has lost. Update the score of the user associated with the player. Implement the executeIa method that does nothing. Implement the calculePos method that updates the position and velocity of the spheres of the player. If the normeVitesse of a sphere is greater than the normeVitesseMax of the sphere, reduce the x and y components of the vectVitesse attribute of the sphere by 0.9. If the taille attribute of a sphere is greater than 50000 and a random integer between 1 and 300 is equal to 1, split the sphere and update the poidTotal attribute of the player. Implement the join method that joins the spheres of the player with the spheres of other players. Return a dictionary that maps the usernames of the other players to the spheres that were joined.", "input": "", "output": "from Server.Game import *\nfrom threading import Thread\nfrom CodIa.tuto.models import User\nfrom CodIa.tuto.app import db\nimport threading\nimport time\nimport random\n\nclass PlayerThread(Thread):\n\n    def __init__(self, GameThread, username, ia):\n        Thread.__init__(self)\n        self.GameThread = GameThread\n        self.username = username\n        GameThread.barrierTours._parties += 1\n        self.ia = ia\n        self.joueur = Player(ia,username,GameThread.game.gamesize)\n        #GameThread.game.joueurs[username]=Player(ia,username,GameThread.game.gamesize)\n        GameThread.joueursAAdd.append(self.joueur)\n\n        GameThread.nbth += 1\n\n\n    def run(self):\n        while True:\n            #attend le début du tours\n            # print(\"Barriere debut de tours \"+str(threading.current_thread().name))\n            # print(self.GameThread.barrierTours.parties)\n            self.GameThread.barrierTours.wait()\n            #execute le code de l'IA\n            self.executeIa()\n\n\n            #print(self.GameThread.barrierEtape.parties)\n            self.GameThread.barrierEtape.wait()\n\n            self.calculePos()\n\n            self.GameThread.barrierEtape.wait()\n\n            agraille = self.join()\n\n            #print(\"avant acquire\")\n            self.GameThread.barrierEtape.wait()\n            self.GameThread.lockmanger.acquire()\n            self.GameThread.aManger.append(agraille)\n            #print(\"pendant\")\n            self.GameThread.lockmanger.release()\n            #print(\"après release\")\n            self.GameThread.barrierManger.wait()\n            if self.joueur.poidTotal<=0 and not self.joueur.end:\n                self.joueur.end = True\n                print(\"\\033[91m Le Joueur \"+self.joueur.username +\" à perdu \\033[0m\")\n                user = User.query.filter_by(pseudo=self.joueur.username).first()\n                if user is not None:\n                    # print(\"\\033[91m Zbra \\033[0m\")\n\n                    user.score += self.joueur.score\n                    db.session.commit()\n            # time.sleep(1/60)\n        # self.GameThread.nbth-=1\n        # self.GameThread.barrierTours._parties -= 1\n\n\n\n\n\n    def executeIa(self):\n        pass\n\n    def calculePos(self):\n        # print(\"\\033[91m caca \\033[0m\")\n        # print(str(self.joueur.spheres[0].normeVitesse()) +\"    \"+ str(self.joueur.spheres[0].normeVitesseMax()))\n        res=0\n        for sphere in  self.joueur.spheres:\n            sphere.vectVitesse = sphere.vitesseNextTick()\n            if sphere.normeVitesse() > sphere.normeVitesseMax():\n                # print(\"\\033[91m caca \\033[0m\")\n                sphere.vectVitesse[0] *= 0.9\n                sphere.vectVitesse[1] *= 0.9\n            # else :\n            #     print(\"\\033[92m non caca \\033[0m\")\n            sphere.vectPos = sphere.posNextTick()\n\n            rand = random.randint(1,300)\n            if sphere.taille > 50000 and rand==1:\n                sphere.split(self.joueur)\n                somme = 0\n\n                #print(\"=======================================================\")\n\n                for joueur in self.GameThread.game.joueurs.values():\n                    somme += joueur.poidTotal\n                #print(\"somme2\"+str(somme))\n            #print(\"taille sphere max: \"+str((sphere.taille)))\n            #pass\n        self.joueur.updateScore()\n\n    def join(self):\n        try:\n            listjoueur = dict()\n            for sphere in  self.joueur.spheres:\n                for joueur2 in self.GameThread.game.joueurs.values():\n                    for sphere2 in joueur2.spheres:\n                        res = sphere.join(sphere2,joueur2)\n                        if(res != None):\n                            # if(not (listjoueur[res[0].username] in locals)):\n                            #     listjoueur[res[0].username] = []\n                            try:\n                                listjoueur[res[0].username].append(res[1])\n                            except KeyError:\n                                listjoueur[res[0].username] = []\n                                listjoueur[res[0].username].append(res[1])\n        except RuntimeError:\n             print(\"\\033[91m Nb de Thread :\"+str(self.GameThread.barrierManger._parties)+\", \"+str(self.GameThread.nbth)+\" \\033[0m\")\n\n        return listjoueur", "original_line_number": 10097, "instruction_length": 1877}
{"instruction": "Display a Classification banner for an Xwindows session by running the Python script \"classification-banner.py\". The banner displays the classification level and can be customized with various options such as foreground and background colors, font face and size, and opacity. The banner can also show the user and hostname and can be temporarily hidden by pressing the ESC key. The script can be configured with a global configuration file located at \"/etc/classification-banner\". To launch the banner, run the script and ensure that the DISPLAY environment variable is set.", "input": "", "output": "#!/usr/bin/python\n# Classification Banner\n#\n# Last update was 02 July 2018\n#\n# Script: classification-banner.py\n# Description: Displays a Classification for an Xwindows session\n# Copyright: Frank Caviggia, 2017\n# Author: Frank Caviggia <fcaviggia (at) gmail.com>\n# Version: 1.6.6\n# License: GPLv2\n\nimport sys,os,optparse,time\nfrom socket import gethostname\n\n# Check if DISPLAY variable is set\ntry:\n\tos.environ[\"DISPLAY\"]\n\timport pygtk,gtk\nexcept:\n\ttry:\n\t\timport Gtk\n\texcept:\n\t\tprint(\"Error: DISPLAY environment variable not set.\")\n\t\tsys.exit(1)\n\n# Global Configuration File\nCONF_FILE = \"/etc/classification-banner\"\n\n# Returns Username\ndef get_user():\n    try:\n        user = os.getlogin()\n    except:\n        user = ''\n        pass\n    return user\n\n# Returns Hostname\ndef get_host():\n    host = gethostname()\n    host = host.split('.')[0]\n    return host\n\n# Classification Banner Class\nclass Classification_Banner:\n    \"\"\"Class to create and refresh the actual banner.\"\"\"\n\n    def __init__(self, message=\"UNCLASSIFIED\", fgcolor=\"#000000\",\n                 bgcolor=\"#00CC00\", face=\"liberation-sans\", size=\"small\",\n                 weight=\"bold\", x=0, y=0, esc=True, opacity=0.75, sys_info=False):\n\n        \"\"\"Set up and display the main window\n\n        Keyword arguments:\n        message -- The classification level to display\n        fgcolor -- Foreground color of the text to display\n        bgcolor -- Background color of the banner the text is against\n        face    -- Font face to use for the displayed text\n        size    -- Size of font to use for text\n        weight  -- Bold or normal\n        hres    -- Horizontal Screen Resolution (int) [ requires vres ]\n        vres    -- Vertical Screen Resolution (int) [ requires hres ]\n        opacity -- Opacity of window (float) [0 .. 1, default 0.75]\n        \"\"\"\n        self.hres = x\n        self.vres = y\n\n        # Dynamic Resolution Scaling\n        self.monitor = gtk.gdk.Screen()\n        self.monitor.connect(\"size-changed\", self.resize)\n\n        # Newer versions of pygtk have this method\n        try:\n            self.monitor.connect(\"monitors-changed\", self.resize)\n        except:\n            pass\n\n        # Create Main Window\n        self.window = gtk.Window()\n        self.window.set_position(gtk.WIN_POS_CENTER)\n        self.window.connect(\"hide\", self.restore)\n        self.window.connect(\"key-press-event\", self.keypress)\n        self.window.modify_bg(gtk.STATE_NORMAL, gtk.gdk.color_parse(bgcolor))\n        self.window.set_property('skip-taskbar-hint', True)\n        self.window.set_property('skip-pager-hint', True)\n        self.window.set_property('destroy-with-parent', True)\n        self.window.stick()\n        self.window.set_decorated(False)\n        self.window.set_keep_above(True)\n        self.window.set_app_paintable(True)\n\n        try:\n            self.window.set_opacity(opacity)\n        except:\n            pass\n\n        # Set the default window size\n        self.window.set_default_size(int(self.hres), 5)\n\n        # Create Main Horizontal Box to Populate\n        self.hbox = gtk.HBox()\n\n        # Create the Center Vertical Box\n        self.vbox_center = gtk.VBox()\n        self.center_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, message))\n        self.center_label.set_use_markup(True)\n        self.center_label.set_justify(gtk.JUSTIFY_CENTER)\n        self.vbox_center.pack_start(self.center_label, True, True, 0)\n\n        # Create the Right-Justified Vertical Box to Populate for hostname\n        self.vbox_right = gtk.VBox()\n        self.host_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, get_host()))\n        self.host_label.set_use_markup(True)\n        self.host_label.set_justify(gtk.JUSTIFY_RIGHT)\n        self.host_label.set_width_chars(20)\n\n        # Create the Left-Justified Vertical Box to Populate for user\n        self.vbox_left = gtk.VBox()\n        self.user_label = gtk.Label(\n            \"<span font_family='%s' weight='%s' foreground='%s' size='%s'>%s</span>\" %\n            (face, weight, fgcolor, size, get_user()))\n        self.user_label.set_use_markup(True)\n        self.user_label.set_justify(gtk.JUSTIFY_LEFT)\n        self.user_label.set_width_chars(20)\n        \n        # Create the Right-Justified Vertical Box to Populate for ESC message\n        self.vbox_esc_right = gtk.VBox()\n        self.esc_label = gtk.Label(\n            \"<span font_family='liberation-sans' weight='normal' foreground='%s' size='xx-small'>  (ESC to hide temporarily)  </span>\" %\n            (fgcolor))\n        self.esc_label.set_use_markup(True)\n        self.esc_label.set_justify(gtk.JUSTIFY_RIGHT)\n        self.esc_label.set_width_chars(20)\n\n        # Empty Label for formatting purposes\n        self.vbox_empty = gtk.VBox()\n        self.empty_label = gtk.Label(\n            \"<span font_family='liberation-sans' weight='normal'>                 </span>\")\n        self.empty_label.set_use_markup(True)\n        self.empty_label.set_width_chars(20)\n\n        if not esc:\n            if not sys_info:\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n            else:\n                self.vbox_right.pack_start(self.host_label, True, True, 0)\n                self.vbox_left.pack_start(self.user_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_right, False, True, 20)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_left, False, True, 20)\n\n        else:\n            if esc and not sys_info:\n                self.empty_label.set_justify(gtk.JUSTIFY_LEFT)\n                self.vbox_empty.pack_start(self.empty_label, True, True, 0)\n                self.vbox_esc_right.pack_start(self.esc_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_esc_right, False, True, 0)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_empty, False, True, 0)\n\n        if sys_info:\n                self.vbox_right.pack_start(self.host_label, True, True, 0)\n                self.vbox_left.pack_start(self.user_label, True, True, 0)\n                self.hbox.pack_start(self.vbox_right, False, True, 20)\n                self.hbox.pack_start(self.vbox_center, True, True, 0)\n                self.hbox.pack_start(self.vbox_left, False, True, 20)\n\n        self.window.add(self.hbox)\n        self.window.show_all()\n        self.width, self.height = self.window.get_size()\n\n    # Restore Minimized Window\n    def restore(self, widget, data=None):\n        self.window.deiconify()\n        self.window.present()\n\n        return True\n\n    # Destroy Classification Banner Window on Resize (Display Banner Will Relaunch)\n    def resize(self, widget, data=None):\n        self.window.destroy()\n\n        return True\n\n    # Press ESC to hide window for 15 seconds\n    def keypress(self, widget, event=None):\n        if event.keyval == 65307:\n            if not gtk.events_pending():\n                self.window.iconify()\n                self.window.hide()\n                time.sleep(15)\n                self.window.show()\n                self.window.deiconify()\n                self.window.present()\n\n        return True\n\n\nclass Display_Banner:\n\n    \"\"\"Display Classification Banner Message\"\"\"\n    def __init__(self):\n        # Dynamic Resolution Scaling\n        self.monitor = gtk.gdk.Screen()\n        self.monitor.connect(\"size-changed\", self.resize)\n\n        # Newer versions of pygtk have this method\n        try:\n            self.monitor.connect(\"monitors-changed\", self.resize)\n        except:\n            pass\n\n        # Launch Banner\n        self.config, self.args = self.configure()\n        self.execute(self.config)\n\n    # Read Global configuration\n    def configure(self):\n        config = {}\n        try:\n            execfile(CONF_FILE, config)\n        except:\n            pass\n\n        defaults = {}\n        defaults[\"message\"] = config.get(\"message\", \"UNCLASSIFIED\")\n        defaults[\"fgcolor\"] = config.get(\"fgcolor\", \"#FFFFFF\")\n        defaults[\"bgcolor\"] = config.get(\"bgcolor\", \"#007A33\")\n        defaults[\"face\"] = config.get(\"face\", \"liberation-sans\")\n        defaults[\"size\"] = config.get(\"size\", \"small\")\n        defaults[\"weight\"] = config.get(\"weight\", \"bold\")\n        defaults[\"show_top\"] = config.get(\"show_top\", True)\n        defaults[\"show_bottom\"] = config.get(\"show_bottom\", True)\n        defaults[\"hres\"] = config.get(\"hres\", 0)\n        defaults[\"vres\"] = config.get(\"vres\", 0)\n        defaults[\"sys_info\"] = config.get(\"sys_info\", False)\n        defaults[\"opacity\"] = config.get(\"opacity\", 0.75)\n        defaults[\"esc\"] = config.get(\"esc\", True)\n        defaults[\"spanning\"] = config.get(\"spanning\", False)\n\n        # Use the global config to set defaults for command line options\n        parser = optparse.OptionParser()\n        parser.add_option(\"-m\", \"--message\", default=defaults[\"message\"],\n                          help=\"Set the Classification message\")\n        parser.add_option(\"-f\", \"--fgcolor\", default=defaults[\"fgcolor\"],\n                          help=\"Set the Foreground (text) color\")\n        parser.add_option(\"-b\", \"--bgcolor\", default=defaults[\"bgcolor\"],\n                          help=\"Set the Background color\")\n        parser.add_option(\"-x\", \"--hres\", default=defaults[\"hres\"], type=\"int\",\n                          help=\"Set the Horizontal Screen Resolution\")\n        parser.add_option(\"-y\", \"--vres\", default=defaults[\"vres\"], type=\"int\",\n                          help=\"Set the Vertical Screen Resolution\")\n        parser.add_option(\"-o\", \"--opacity\", default=defaults[\"opacity\"],\n                          type=\"float\", dest=\"opacity\",\n                          help=\"Set the window opacity for composted window managers\")\n        parser.add_option(\"--face\", default=defaults[\"face\"], help=\"Font face\")\n        parser.add_option(\"--size\", default=defaults[\"size\"], help=\"Font size\")\n        parser.add_option(\"--weight\", default=defaults[\"weight\"],\n                          help=\"Set the Font weight\")\n        parser.add_option(\"--disable-esc-msg\", default=defaults[\"esc\"],\n                          dest=\"esc\", action=\"store_false\",\n                          help=\"Disable the 'ESC to hide' message\")\n        parser.add_option(\"--hide-top\", default=defaults[\"show_top\"],\n                          dest=\"show_top\", action=\"store_false\",\n                          help=\"Disable the top banner\")\n        parser.add_option(\"--hide-bottom\", default=defaults[\"show_bottom\"],\n                          dest=\"show_bottom\", action=\"store_false\",\n                          help=\"Disable the bottom banner\")\n        parser.add_option(\"--system-info\", default=defaults[\"sys_info\"],\n                          dest=\"sys_info\", action=\"store_true\",\n                          help=\"Show user and hostname in the top banner\")\n        parser.add_option(\"--enable-spanning\", default=defaults[\"spanning\"],\n                          dest=\"spanning\", action=\"store_true\",\n                          help=\"Enable banner(s) to span across screens as a single banner\")\n\n        options, args = parser.parse_args()\n        return options, args\n\n    # Launch the Classification Banner Window(s)\n    def execute(self, options):\n        self.num_monitor = 0\n\n        if options.hres == 0 or options.vres == 0:\n            # Try Xrandr to determine primary monitor resolution\n            try:\n                self.screen = os.popen(\"xrandr | grep ' connected ' | awk '{ print $3 }'\").readlines()[0]\n                self.x = self.screen.split('x')[0]\n                self.y = self.screen.split('x')[1].split('+')[0]\n\n            except:\n                try:\n                    self.screen = os.popen(\"xrandr | grep ' current ' | awk '{ print $8$9$10+0 }'\").readlines()[0]\n                    self.x = self.screen.split('x')[0]\n                    self.y = self.screen.split('x')[1].split('+')[0]\n\n                except:\n                    self.screen = os.popen(\"xrandr | grep '^\\*0' | awk '{ print $2$3$4 }'\").readlines()[0]\n                    self.x = self.screen.split('x')[0]\n                    self.y = self.screen.split('x')[1].split('+')[0]\n\n                else:\n                    # Fail back to GTK method\n                    self.display = gtk.gdk.display_get_default()\n                    self.screen = self.display.get_default_screen()\n                    self.x = self.screen.get_width()\n                    self.y = self.screen.get_height()\n        else:\n            # Resoultion Set Staticly\n            self.x = options.hres\n            self.y = options.vres\n\n        if not options.spanning and self.num_monitor > 1:\n            for monitor in range(self.num_monitor):\n                mon_geo = self.screen.get_monitor_geometry(monitor)\n                self.x_location, self.y_location, self.x, self.y = mon_geo\n                self.banners(options)\n        else:\n            self.x_location = 0\n            self.y_location = 0\n            self.banners(options)\n\n    def banners(self, options):\n            if options.show_top:\n                top = Classification_Banner(\n                    options.message,\n                    options.fgcolor,\n                    options.bgcolor,\n                    options.face,\n                    options.size,\n                    options.weight,\n                    self.x,\n                    self.y,\n                    options.esc,\n                    options.opacity,\n                    options.sys_info)\n                top.window.move(self.x_location, self.y_location)\n\n            if options.show_bottom:\n                bottom = Classification_Banner(\n                    options.message,\n                    options.fgcolor,\n                    options.bgcolor,\n                    options.face,\n                    options.size,\n                    options.weight,\n                    self.x,\n                    self.y,\n                    options.esc,\n                    options.opacity)\n                bottom.window.move(self.x_location, int(bottom.vres))\n\n    # Relaunch the Classification Banner on Screen Resize\n    def resize(self, widget, data=None):\n        self.config, self.args = self.configure()\n        self.execute(self.config)\n        return True\n\n\n# Main Program Loop\nif __name__ == \"__main__\":\n    run = Display_Banner()\n    gtk.main()", "original_line_number": 10203, "instruction_length": 574}
{"instruction": "Given a string s and a dictionary of words dict, write a function `wordBreak` that returns all possible sentences where each word is a valid dictionary word. The function should take two arguments, a string `s` and a set of strings `wordDict`. The function should use DP and DFS to get rid of TLE. The DP algorithm should use backward DP. The DFS algorithm should start from the end of the string and recursively check if the remaining string is breakable. The function should return a list of strings.", "input": "", "output": "# 140. Word Break II  QuestionEditorial Solution  My Submissions\n# Total Accepted: 68516\n# Total Submissions: 321390\n# Difficulty: Hard\n# Given a string s and a dictionary of words dict, add spaces in s to construct a sentence where each word is a valid dictionary word.\n# \n# Return all such possible sentences.\n# \n# For example, given\n# s = \"catsanddog\",\n# dict = [\"cat\", \"cats\", \"and\", \"sand\", \"dog\"].\n# \n# A solution is [\"cats and dog\", \"cat sand dog\"].\n# \n# Subscribe to see which companies asked this question\n\n# Notes:\n# Forward DP or Backward DP?\n# WB1 is forward DP. DP[i] means s[:i+1] is breakable\n# WB2 is backward. DP[i] means s[i:] is breakable\n# Since DFS is to check remaining string is \n\n# 12.08.2016 Rewrite. DP + DFS\nclass Solution(object):\n    dp = []\n\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        n = len(s)\n        self.dp = [ False for x in xrange(len(s)+1) ]\n        self.dp[0] = True\n        \n        for i in xrange(n):\n            for j in xrange(i+1):\n                tmp = s[j:i+1]\n                if tmp in wordDict and self.dp[j]:\n                    self.dp[i+1] = True\n                    break\n        \n        if not self.dp[-1]: return []\n        res = []\n        self.dfs(res, \"\", s, n-1, wordDict)\n        return res\n    \n    def dfs(self, res, line, s, end, wordDict):\n        if end == -1:\n            res.append(line[:-1])\n            return\n        \n        for start in xrange(end, -1, -1):\n            tmp = s[start:end+1]\n            if tmp in wordDict and self.dp[start]:\n                self.dfs(res, tmp + \" \" + line, s, start - 1, wordDict)\n\n# DP + DFS can get rid of TLE\nclass Solution(object):\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        n = len(s)\n        print(s)\n        print(wordDict)\n        res = []\n\n        dp = [False for x in xrange(n+1)]\n        dp[n] = True\n        for i in xrange(n-1, -1, -1):\n            for j in xrange(n-1, i-1, -1):      # Better loop. i start index. j end index\n                if dp[j+1] and s[i:j+1] in wordDict:\n                    dp[i] = True\n                    break\n\n#        for i in xrange(n-1, -1, -1):\n#            for j in xrange(i, -1, -1):\n#                if dp[i+1] and s[j:i+1] in wordDict:\n#                    dp[j] = True\n#                    continue\n        \n        def dfs(start, line):\n            if not dp[start]:\n                return\n            \n            if start == len(s):\n                res.append(line[1:])\n                return\n    \n            for i in xrange(start, len(s)):\n                if dp[i+1] and s[start:i+1] in wordDict:\n                    dfs(i+1, line + \" \" + s[start:i+1])   \n\n        dfs(0, \"\")\n        return res\n        \nif __name__  == \"__main__\":\n#    s = \"catsanddog\"\n#    d = [\"cat\",\"cats\",\"and\",\"sand\",\"dog\"]\n#    Solution().wordBreak(s, d)\n#    s = \"goalspecial\"\n#    d = [\"go\",\"goal\",\"goals\",\"special\"]\n    s = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n    d = [\"a\",\"aa\",\"aaa\",\"aaaa\",\"aaaaa\",\"aaaaaa\",\"aaaaaaa\",\"aaaaaaaa\",\"aaaaaaaaa\",\"aaaaaaaaaa\"]\n    print(Solution().wordBreak(s, d))\n#    Solution().wordBreak(s, d)\n#    s1 = \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaabaabaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\"\n#    d1 = [\"aa\",\"aaa\",\"aaaa\",\"aaaaa\",\"aaaaaa\",\"aaaaaaa\",\"aaaaaaaa\",\"aaaaaaaaa\",\"aaaaaaaaaa\",\"ba\"]\n#    Solution().wordBreak(s1, d1)\n\nexit\n\n# If DFS only. get TLE \nclass Solution2(object):\n    def wordBreak(self, s, wordDict):\n        \"\"\"\n        :type s: str\n        :type wordDict: Set[str]\n        :rtype: List[str]\n        \"\"\"\n        res = []\n\n        # Precheck to get rid of TLE\n        set_s = set(s)\n        set_dict = set(\"\".join(wordDict))\n        for char in set_s:\n            if set_s not in set_dict:\n                return []\n\n        self.dfs(s, wordDict, res, \"\")\n        return res\n        \n    def dfs(self, s, wordDict, res, line):\n        if not s:\n            print(line)\n            res.append(line)\n            return\n        \n        for i in xrange(1, len(s)+1):\n            if s[:i] in wordDict:\n                if not line:\n                    self.dfs(s[i:], wordDict, res, s[:i])\n                else:\n                    self.dfs(s[i:], wordDict, res, line + \" \" + s[:i])", "original_line_number": 10368, "instruction_length": 502}
{"instruction": "The SimulationClient class is used to compute local geodesics using a modified BFGS method. To use this class, create an instance of it with the required arguments: simulation_client_id (str), server_host (str), server_port (int), authkey (str, optional), metric_server_addresses (list), configuration_file (str), logfile (str, optional), log_level (int, optional), and callback_delay (float). Then call the start_client() method to begin computing local geodesics. The SimulationClient will communicate with a running SimulationServer instance to receive tasks and return results. The SimulationClient will also communicate with running SimulationPotential instances to compute the metric values needed for the geodesics. If the SimulationClient is unable to communicate with the SimulationServer or SimulationPotential instances, it will shut down and send a signal to the other instances to shut down as well.", "input": "", "output": "from multiprocessing.connection import Client\nimport time\nimport logging\nimport socket\n\nimport numpy as np\n\nfrom SimulationUtilities import Configuration_Processing\nfrom SimulationUtilities.Communication_Codes import comm_code\nimport LinearAlgebra as la\nfrom CustomBFGS import find_geodesic_midpoint\nfrom MetricValues import shutdown_metric\n\n\nclass SimulationClient:\n    \"\"\"\n\n    The purpose of this object is to compute local geodesics using a modified BFGS method. The object receives a pair of\n    end points to compute the local geodesic between. The simulation client then returns a new position for the node.\n    The simulation client needs the value of the potential and it's gradient function, in order to achieve this it makes\n    calls to it's assigned SimulationPotential servers.\n\n    Attributes:\n      CONFIGURATION (dict) :\n          A dictionary containing the parsed values from the file in configuration_file.\n      CURVE_ADDRESS (str, int) :\n          A tuple containing a string representing the hostname/IP and an integer for the running SimulationServer.\n      AUTHKEY (str) :\n          A string containing the authorisation key for the listener method.\n      DELAY (float) :\n          The length of time the SimulationClient should wait if there is no new available jobs, before attempting to\n          contact the SimulationServer again.\n      METRIC_SERVERS :\n          A list containing tuples of addresses for SimulationPotential instances.\n      ID (str) :\n          A string that uniquely identifies the client amongst all other clients in the computation.\n      MASS_MATRIX (numpy.array) :\n          A NumPy matrix containing the mass matrix of the molecular system. Produced automatically from the Atomistic\n          Simulation Environment.\n\n    \"\"\"\n    def __init__(self, simulation_client_id, server_host, server_port, authkey, metric_server_addresses,\n                 configuration_file, logfile=None, log_level=logging.INFO, callback_delay=1.0):\n        \"\"\"The constructor for the SimulationClient class.\n\n        Note:\n          This class is intended to be used in conjunction with running SimulationServer and SimulationPotential\n          objects. It will cause a runtime exception if this condition isn't satisfied.\n\n        Args:\n          simulation_client_id (str) :\n              A string that uniquely identifies the client amongst all other clients in the computation.\n          server_host (str) :\n              The TCP/IP hostname of the running SimulationServer instance.\n          server_port (int) :\n              The port number that the SimulationServer instance is communicating on.\n          authkey (str, optional) :\n              Authentication key used to secure process communications. Default to None for local computations to\n              increase speed.\n          metric_server_addresses :\n              A list containing tuples of the type (str, int) containing the hostnames and ports for the running\n              SimulationPotential instances.\n          configuration_file (str) :\n              Directory and filename of the configuration file.\n          logfile (str, optional) :\n              Directory and filename of the log file. Is created if doesn't exist, overwritten if it does.\n          log_level (int, optional) :\n              Specify level of logging required as described in the logging package documentation.\n          callback_delay (float) :\n              The length of time the SimulationClient should wait if there is no new available jobs, before attempting\n               to contact the SimulationServer again.\n\n        \"\"\"\n        # Set the SimulationClient log output to write to logfile at prescribed log level if specified. Otherwise write\n        # to console output. Setting to DEBUG will cause poor performance and should only be used to debug.\n        if logfile is not None:\n            logging.basicConfig(filename=logfile, level=log_level, filemode='w')\n        else:\n            logging.basicConfig(level=logging.INFO)\n\n        # Read configuration from configuration_file and store in SimulationPotential's CONFIGURATION attribute.\n        self.CONFIGURATION = Configuration_Processing.read_configuration_file(configuration_file)\n\n        # Set ADDRESS and AUTHKEY attributes for Client object in the start_client method.\n        self.CURVE_ADDRESS = (server_host, server_port)\n        self.AUTHKEY = authkey\n\n        # Set the callback delay as described in the attributes.\n        self.DELAY = callback_delay\n\n        # Store the ADDRESS and AUTHKEY attributes for Client objects in the start_client method used to compute the\n        # metric values.\n        self.METRIC_SERVERS = metric_server_addresses\n\n        # Set the client's unique identifier.\n        self.ID = simulation_client_id\n\n        # Compute the mass matrix for the molecular system.\n        self.MASS_MATRIX = np.diag(np.dstack((self.CONFIGURATION['molecule'].get_masses(),) *\n                                             (self.CONFIGURATION['dimension'] /\n                                              len(self.CONFIGURATION['molecule'].get_masses()))).flatten())\n\n\n    def start_client(self):\n        \"\"\"Start the instance of SimulationClient and begin computing local geodesics.\n\n        \"\"\"\n\n        # Define a flag to indicate if contact with the SimulationServer instance is possible.\n        connection_made = False\n\n        # Create a response to send to the SimulationServer indicating that this is the first time this SimulationClient\n        # has attempted to get a task.\n        client_response = {'status_code': comm_code('CLIENT_FIRST_CONTACT'),\n                           'client_name': self.ID}\n\n        # Attempt to connect to the SimulationServer instance.\n        try:\n            # Create a Client object that communicates with the listener on CURVE_ADDRESS using password AUTHKEY.\n            server = Client(self.CURVE_ADDRESS, authkey=self.AUTHKEY)\n\n            # When a connection is made send the client message.\n            server.send(client_response)\n\n            # The client assumes the server will respond with a message, either a local geodesic to compute or a message\n            # asking the client to try again after DELAY seconds.\n            server_response = server.recv()\n\n            # Interpret the servers response by first extracting the status_code variable from the response.\n            server_response_code = server_response['status_code']\n\n            # Close the connection to the server at this point to allow other clients to communicate with the\n            # SimulationServer.\n            server.close()\n\n            # Store in the connection_made flag that it was possible to create a connection.\n            connection_made = True\n\n        # If it isn't possible to connect to the server than a socket.error exception is raised.\n        except socket.error:\n            # Write an error to the log for this client indicating that the connection couldn't be made.\n            logging.warning('Failed to Make Connection to SimulationServer. Shutting down client.')\n\n            # Send a signal to the running instances of SimulationPotential that the SimulationClient would have used\n            # indicating that they should also shutdown.\n            shutdown_metric(self.METRIC_SERVERS, self.AUTHKEY)\n\n        # This is the main loop of the SimulationClient - the program stops running when it is no longer possible to\n        # communicate with the SimulationServer. This is decided by the connection_made flag.\n        while connection_made:\n\n            # At this point in the code a new server_response should have been received. How the SimulationClient reacts\n            # depends on the communication code received.\n\n            # If the server has indicated it is giving the SimulationClient a new geodesic to compute then...\n            if server_response_code == comm_code('SERVER_GIVES_NEW_TASK'):\n\n                # Compute the rescaled tangent direction of the curve as store as a NumPy array.\n                tangent_direction = (1 / float(self.CONFIGURATION['local_number_of_nodes'] + 1)) * \\\n                    np.subtract(server_response['right_end_point'], server_response['left_end_point'], dtype='float64')\n\n                # Compute the local geodesic using the BFGS method and store the NumPy array in result\n                result = \\\n                    find_geodesic_midpoint(server_response['left_end_point'],\n                                                server_response['right_end_point'],\n                                                self.CONFIGURATION['local_number_of_nodes'],\n                                                la.orthonormal_tangent_basis(tangent_direction,\n                                                                          self.CONFIGURATION['dimension']),\n                                                tangent_direction, self.CONFIGURATION['codimension'],\n                                                self.METRIC_SERVERS,\n                                                self.MASS_MATRIX,\n                                                self.AUTHKEY)\n\n                # If the function find_geodesic_midpoint returned a None object then it couldn't contact it's\n                # SimulationPotential instances and should be restarted.\n                if result is None:\n                    # Tell the user via the log that the SimulationPotential instances couldn't be contacted.\n                    logging.warning('Failed to Make Connection to SimulationPotential. Shutting down client.')\n\n                    # Exit the main loop of the SimulationClient.\n                    break\n\n                # If there is a midpoint then construct a client response to tell the server which node has which new\n                # position.\n                client_response = {'status_code': comm_code('CLIENT_HAS_MIDPOINT_DATA'),\n                                   'node_number': server_response['node_number'],\n                                   'new_node_position': result,\n                                   'client_name': self.ID\n                                   }\n\n            # Otherwise if the server has asked the SimulationClient to try again later...\n            elif server_response_code == comm_code('SERVER_REQUEST_CALLBACK'):\n                # Make the SimulationClient wait for DELAY seconds\n                time.sleep(self.DELAY)\n\n                # Create a response to tell the SimulationServer that the SimulationClient would like a new job.\n                client_response = {'status_code': comm_code('CLIENT_HAS_NO_TASK'), 'client_name': self.ID}\n\n            # Attempt to connect to the SimulationServer instance.\n            try:\n                # Create a Client object that communicates with the listener on CURVE_ADDRESS using password AUTHKEY.\n                server = Client(self.CURVE_ADDRESS, authkey=self.AUTHKEY)\n\n                # When a connection is made send the client message.\n                server.send(client_response)\n\n                # The client assumes the server will respond with a message, either a local geodesic to compute or a\n                # message asking the client to try again after DELAY seconds.\n                server_response = server.recv()\n\n                # Interpret the servers response by first extracting the status_code variable from the response.\n                server_response_code = server_response['status_code']\n\n                # Close the connection to the server at this point to allow other clients to communicate with the\n                # SimulationServer.\n                server.close()\n\n            # If it isn't possible to connect to the server than a socket.error exception is raised.\n            except (socket.error, EOFError):\n                # Write an error to the log for this client indicating that the connection couldn't be made.\n                logging.warning('Failed to Make Connection to SimulationServer. Shutting down client.')\n\n                # Send a signal to the running instances of SimulationPotential that the SimulationClient would have\n                # used indicating that they should also shutdown.\n                shutdown_metric(self.METRIC_SERVERS, self.AUTHKEY)\n\n                # Exit the main loop of the SimulationClient.\n                break", "original_line_number": 10402, "instruction_length": 912}
{"instruction": "To build a soft clip position distribution in a BAM file, follow these steps:\n\n1. Install the pysam module.\n2. Use argparse to define the input BAM file, minimum mapping quality of read, bin size of position, minimum depth for output, and output soft clip distribution file name.\n3. Define the filterReadQuality function to filter reads based on mapping quality.\n4. Define the hasSoftClip function to check if a read has a soft clip.\n5. Read the input BAM file using pysam and loop through each read.\n6. Filter reads based on mapping quality, reference name length, and other criteria.\n7. Create a dictionary to store the soft clip position distribution for each chromosome.\n8. Calculate the position of each read and update the corresponding position in the dictionary with the number of soft clip and other reads.\n9. Write the soft clip position distribution to the output file.\n10. Done.", "input": "", "output": "import pysam\r\nimport argparse\r\nimport sys\r\nimport logging\r\nimport os\r\nfrom asyncore import read\r\n\r\nparser = argparse.ArgumentParser(description=\"Build soft clip position distribution in BAM file.\",\r\n                                 formatter_class=argparse.ArgumentDefaultsHelpFormatter)\r\n\r\nDEBUG=False\r\nNOT_DEBUG = not DEBUG\r\n\r\nparser.add_argument('-i', '--input', action='store', nargs='?', help='Input BAM file', required=NOT_DEBUG)\r\nparser.add_argument('--min-mapq', action='store', nargs='?', type=int, default=10, help=\"Minimum mapping quality of read\")\r\nparser.add_argument('--binsize', action='store', nargs='?', type=int, default=1000, help=\"Bin size of position\")\r\nparser.add_argument('--min-depth', action='store', nargs='?', type=int, default=100, help=\"Minimum depth for output\")\r\nparser.add_argument('-o', '--output', action='store', nargs='?', help=\"Output soft clip distribution file name\", required=NOT_DEBUG)\r\n\r\nif NOT_DEBUG and len(sys.argv)==1:\r\n  parser.print_help()\r\n  sys.exit(1)\r\n  \r\nargs = parser.parse_args()\r\n\r\nif DEBUG:\r\n  args.input = \"/scratch/cqs/shengq2/jennifer/20190906_lindsay_exomeseq_3772_hg38/softclip/P_175_06.indel.recal.TP53.bam\"\r\n  args.output = \"/scratch/cqs/shengq2/jennifer/20190906_lindsay_exomeseq_3772_hg38/softclip/P_175_06.softclip.position.tsv\"\r\n\r\nlogger = logging.getLogger('bamSoftClipPosition')\r\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)-8s - %(message)s')\r\n\r\ndef filterReadQuality(read, min_mapq):\r\n  return(read.is_unmapped or read.mapping_quality < min_mapq or read.is_secondary or read.is_qcfail or read.is_duplicate or read.is_supplementary)\r\n\r\ndef hasSoftClip(read):\r\n  return(\"S\" in read.cigarstring)\r\n\r\nchrPositionMap = {}\r\nprocessed = 0\r\nlogger.info(\"reading %s\" % args.input)\r\nwith pysam.Samfile(args.input, \"rb\") as samfile:\r\n  for read in samfile.fetch(until_eof=True):\r\n    processed += 1\r\n    if processed % 1000000 == 0:\r\n      logger.info(\"processed %d\" % processed)\r\n      #break\r\n      \r\n    if filterReadQuality(read, args.min_mapq):\r\n      continue\r\n    \r\n    if len(read.reference_name) > 5:\r\n      continue\r\n\r\n    if not read.reference_name in chrPositionMap:\r\n      chrPositionMap[read.reference_name] = {}\r\n    positionMap = chrPositionMap[read.reference_name]\r\n\r\n    position = int(read.reference_start / args.binsize)\r\n    if not position in positionMap:\r\n      positionMap[position] = [0, 0]\r\n    posvalues = positionMap[position]\r\n\r\n    if hasSoftClip(read):\r\n      posvalues[0] = posvalues[0] + 1\r\n    else:\r\n      posvalues[1] = posvalues[1] + 1\r\n\r\nwith open(args.output, \"wt\") as sw:  \r\n  sw.write(\"Chr\\tStartPosition\\tSoftClipRead\\tOtherRead\\tSoftClipPerc\\n\")\r\n  for chr in chrPositionMap.keys():\r\n    positionMap = chrPositionMap[chr]\r\n    positions = sorted(positionMap.keys())\r\n    for pos in positions:\r\n      posvalues = positionMap[pos]\r\n      sread = posvalues[0]\r\n      oread = posvalues[1]\r\n      allread = sread + oread\r\n      if allread >= args.min_depth:\r\n        sw.write(\"%s\\t%d\\t%d\\t%d\\t%.2f\\n\" % (chr, pos * args.binsize, sread, oread, sread * 1.0 / allread) )\r\n\r\nlogger.info(\"done.\")", "original_line_number": 10573, "instruction_length": 890}
{"instruction": "This code defines models for campaigns and coupons in a Shuup e-commerce platform. To use the code, create a new Django app and add this code to the models.py file. Then, run migrations to create the necessary database tables. The models include CatalogCampaign, BasketCampaign, Coupon, and CouponUsage. CatalogCampaign and BasketCampaign are subclasses of Campaign, which is an abstract base class. CatalogCampaign represents a campaign that applies to a set of products, while BasketCampaign represents a campaign that applies to the entire basket. Coupon represents a discount code that can be applied to a basket. CouponUsage represents the usage of a coupon in an order. The models have various fields and methods for managing campaigns and coupons, including checking if a campaign or coupon is available, attaching a coupon to a campaign, and checking if a coupon can be used by a customer. The models also include logging functionality through the use of the define_log_model function.", "input": "", "output": "# This file is part of Shuup.\n#\n# Copyright (c) 2012-2017, Shoop Commerce Ltd. All rights reserved.\n#\n# This source code is licensed under the OSL-3.0 license found in the\n# LICENSE file in the root directory of this source tree.\nimport random\nimport string\n\nfrom django.conf import settings\nfrom django.core.exceptions import ValidationError\nfrom django.db import models\nfrom django.db.models import Q\nfrom django.utils.encoding import force_text, python_2_unicode_compatible\nfrom django.utils.timezone import now\nfrom django.utils.translation import ugettext_lazy as _\nfrom enumfields import Enum\nfrom parler.models import TranslatableModel, TranslatedFields\n\nfrom shuup.campaigns.consts import (\n    CAMPAIGNS_CACHE_NAMESPACE, CATALOG_FILTER_CACHE_NAMESPACE,\n    CONTEXT_CONDITION_CACHE_NAMESPACE\n)\nfrom shuup.campaigns.models.basket_conditions import (\n    CategoryProductsBasketCondition, ProductsInBasketCondition\n)\nfrom shuup.campaigns.utils.campaigns import get_product_ids_and_quantities\nfrom shuup.campaigns.utils.matcher import get_matching_for_product\nfrom shuup.core import cache\nfrom shuup.core.fields import InternalIdentifierField\nfrom shuup.core.models import Category, Order, Shop\nfrom shuup.core.utils import context_cache\nfrom shuup.utils.analog import define_log_model\nfrom shuup.utils.properties import MoneyPropped\n\n\nclass CampaignType(Enum):\n    CATALOG = 1\n    BASKET = 2\n\n\nclass CampaignQueryset(models.QuerySet):\n    def available(self, shop=None):\n        query = Q(\n            Q(active=True) &\n            (Q(start_datetime__isnull=True) | Q(start_datetime__lte=now())) &\n            (Q(end_datetime__isnull=True) | Q(end_datetime__gte=now()))\n        )\n        if shop:\n            query &= Q(shop=shop)\n        return self.filter(query)\n\n\nclass Campaign(MoneyPropped, TranslatableModel):\n    admin_url_suffix = None\n\n    shop = models.ForeignKey(Shop, verbose_name=_(\"shop\"), help_text=_(\"The shop where the campaign is active.\"))\n    name = models.CharField(max_length=120, verbose_name=_(\"name\"), help_text=_(\"The name for this campaign.\"))\n\n    # translations in subclass\n    identifier = InternalIdentifierField(unique=True)\n\n    active = models.BooleanField(default=False, verbose_name=_(\"active\"), help_text=_(\n        \"Check this if the campaign is currently active. Please also set a start and end date.\"\n    ))\n    start_datetime = models.DateTimeField(null=True, blank=True, verbose_name=_(\"start date and time\"), help_text=_(\n        \"The date and time the campaign starts. This is only applicable if the campaign is marked as active.\"\n    ))\n    end_datetime = models.DateTimeField(null=True, blank=True, verbose_name=_(\"end date and time\"), help_text=_(\n        \"The date and time the campaign ends. This is only applicable if the campaign is marked as active.\"\n    ))\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    objects = CampaignQueryset.as_manager()\n\n    class Meta:\n        abstract = True\n        verbose_name = _('Campaign')\n        verbose_name_plural = _('Campaigns')\n\n    def save(self, *args, **kwargs):\n        super(Campaign, self).save(*args, **kwargs)\n        cache.bump_version(CAMPAIGNS_CACHE_NAMESPACE)\n        cache.bump_version(CONTEXT_CONDITION_CACHE_NAMESPACE)\n        cache.bump_version(CATALOG_FILTER_CACHE_NAMESPACE)\n\n    def is_available(self):\n        if not self.active:  # move to manager?\n            return False\n        if self.start_datetime and self.end_datetime:\n            if self.start_datetime <= now() <= self.end_datetime:\n                return True\n            return False\n        elif self.start_datetime and not self.end_datetime:\n            if self.start_datetime > now():\n                return False\n        elif not self.start_datetime and self.end_datetime:\n            if self.end_datetime < now():\n                return False\n        return True\n\n    @property\n    def type(self):\n        return CampaignType.BASKET if isinstance(self, BasketCampaign) else CampaignType.CATALOG\n\n\nclass CatalogCampaign(Campaign):\n    _queryset = None\n\n    admin_url_suffix = \"catalog_campaign\"\n    conditions = models.ManyToManyField('ContextCondition', blank=True, related_name='campaign')\n    filters = models.ManyToManyField('CatalogFilter', blank=True, related_name='campaign')\n\n    translations = TranslatedFields(public_name=models.CharField(max_length=120, blank=True, help_text=_(\n        \"The campaign name to show in the store front.\"\n    )))\n\n    def __str__(self):\n        return force_text(_(\"Catalog Campaign: %(name)s\" % dict(name=self.name)))\n\n    def save(self, *args, **kwargs):\n        super(CatalogCampaign, self).save(*args, **kwargs)\n        self.filters.update(active=self.active)\n        for f in self.filters.all():\n            for matching_product in f.get_matching_shop_products():\n                context_cache.bump_cache_for_shop_product(matching_product)\n        self.conditions.update(active=self.active)\n\n    def rules_match(self, context, shop_product, matching_catalog_filters, matching_context_conditions):\n        if not self.is_available():\n            return False\n\n        # If rule has filters, all of them has to match\n        for filter_pk in self.filters.values_list(\"pk\", flat=True):\n            if filter_pk not in matching_catalog_filters:\n                return False\n\n        # All filters match so let's check that also all the conditions match\n        for condition_pk in self.conditions.values_list(\"pk\", flat=True):\n            if condition_pk not in matching_context_conditions:\n                return False\n        return True\n\n    @classmethod\n    def get_for_product(cls, shop_product):\n        matching_filters = get_matching_for_product(shop_product, provide_category=\"campaign_catalog_filter\")\n        matching_conditions = get_matching_for_product(shop_product, provide_category=\"campaign_context_condition\")\n        query_filter = Q(Q(filters__in=matching_filters) | Q(conditions__in=matching_conditions))\n        return cls.objects.available(shop=shop_product.shop).filter(query_filter).distinct()\n\n    @classmethod\n    def get_matching(cls, context, shop_product):\n        prod_ctx_cache_elements = dict(\n            customer=context.customer.pk or 0,\n            shop=context.shop.pk,\n            product_id=shop_product.pk)\n        namespace = CAMPAIGNS_CACHE_NAMESPACE\n        key = \"%s:%s\" % (namespace, hash(frozenset(prod_ctx_cache_elements.items())))\n        cached_matching = cache.get(key, None)\n        if cached_matching is not None:\n            return cached_matching\n\n        from shuup.campaigns.models.matching import get_matching_context_conditions, get_matching_catalog_filters\n        matching_context_conditions = get_matching_context_conditions(context)\n        matching_catalog_filters = get_matching_catalog_filters(shop_product)\n\n        if not (matching_context_conditions or matching_catalog_filters):\n            return []\n\n        # Get all possible campaign id's for matching context_conditions\n        campaigns_based_on_conditions = set(\n            cls.objects.filter(\n                active=True,\n                shop=context.shop,\n                conditions__id__in=matching_context_conditions\n            ).values_list(\"pk\", flat=True)\n        )\n\n        campaigns_based_on_catalog_filters = set()\n        if hasattr(cls, \"filters\"):\n            # Get all possible campaigns for matching catalog_filters\n            campaigns_based_on_catalog_filters = set(\n                cls.objects.filter(\n                    active=True,\n                    shop=context.shop,\n                    filters__id__in=matching_catalog_filters\n                ).values_list(\"pk\", flat=True)\n            )\n\n        all_possible_campaigns_ids = (campaigns_based_on_conditions | campaigns_based_on_catalog_filters)\n        matching = []\n        for campaign in cls.objects.filter(id__in=all_possible_campaigns_ids):\n            if campaign.rules_match(context, shop_product, matching_catalog_filters, matching_context_conditions):\n                matching.append(campaign)\n        cache.set(key, matching, timeout=None)\n        return matching\n\n\nclass BasketCampaign(Campaign):\n    admin_url_suffix = \"basket_campaign\"\n\n    basket_line_text = models.CharField(\n        max_length=120, verbose_name=_(\"basket line text\"), help_text=_(\"This text will be shown in basket.\"))\n\n    conditions = models.ManyToManyField('BasketCondition', blank=True, related_name='campaign')\n    coupon = models.OneToOneField('Coupon', null=True, blank=True, related_name='campaign', verbose_name=_(\"coupon\"))\n\n    translations = TranslatedFields(\n        public_name=models.CharField(max_length=120, verbose_name=_(\"public name\"), help_text=_(\n            \"The campaign name to show in the store front.\"\n        ))\n    )\n\n    def __str__(self):\n        return force_text(_(\"Basket Campaign: %(name)s\" % dict(name=self.name)))\n\n    def save(self, *args, **kwargs):\n        if self.coupon:\n            code_count_for_shop = BasketCampaign.objects.filter(\n                active=True, shop_id=self.shop.id, coupon__code=self.coupon.code)\n            if not self.id and code_count_for_shop.exists():\n                raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n            if self.id and code_count_for_shop.exclude(coupon_id=self.coupon.id).exists():\n                raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n        super(BasketCampaign, self).save(*args, **kwargs)\n        self.conditions.update(active=self.active)\n\n    @classmethod\n    def get_for_product(cls, shop_product):\n        matching_conditions = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_condition\")\n        matching_effects = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_discount_effect_form\")\n        matching_line_effects = get_matching_for_product(\n            shop_product, provide_category=\"campaign_basket_line_effect_form\")\n        effects_q = Q(Q(line_effects__id__in=matching_line_effects) | Q(discount_effects__id__in=matching_effects))\n        matching_q = Q(Q(conditions__in=matching_conditions) | effects_q)\n        return cls.objects.available(shop=shop_product.shop).filter(matching_q).distinct()\n\n    @classmethod\n    def get_matching(cls, basket, lines):\n        matching = []\n        exclude_condition_ids = set()\n        product_id_to_qty = get_product_ids_and_quantities(basket)\n\n        # Get ProductsInBasketCondition's that can't match with the basket\n        products_in_basket_conditions_to_check = set(\n            ProductsInBasketCondition.objects.filter(\n                products__id__in=product_id_to_qty.keys()\n            ).values_list(\"id\", flat=True)\n        )\n        exclude_condition_ids |= set(\n            ProductsInBasketCondition.objects.exclude(\n                id__in=products_in_basket_conditions_to_check\n            ).values_list(\"id\", flat=True)\n        )\n\n        # Get CategoryProductsBasketCondition's that can't match with the basket\n        categories = set(Category.objects.filter(\n            shop_products__product_id__in=product_id_to_qty.keys()).values_list(\"id\", flat=True))\n        category_products_in_basket_to_check = set(\n            CategoryProductsBasketCondition.objects.filter(categories__in=categories).values_list(\"id\", flat=True)\n        )\n        exclude_condition_ids |= set(\n            CategoryProductsBasketCondition.objects.exclude(\n                id__in=category_products_in_basket_to_check\n            ).values_list(\"id\", flat=True)\n        )\n\n        queryset = cls.objects.filter(active=True, shop=basket.shop)\n        if exclude_condition_ids:\n            queryset = queryset.exclude(conditions__id__in=exclude_condition_ids)\n        for campaign in queryset.prefetch_related(\"conditions\"):\n            if campaign.rules_match(basket, lines):\n                matching.append(campaign)\n        return matching\n\n    def rules_match(self, basket, lines):\n        \"\"\"\n        Check if basket rules match.\n\n        They will not match if\n        1) The campaign is not active\n        2) The campaign has attached coupon\n           which doesn't match or is not active\n        3) Any of the attached rules doesn't match\n        \"\"\"\n        if not self.is_available():\n            return False\n\n        if self.coupon and not (self.coupon.active and self.coupon.code.upper() in [c.upper() for c in basket.codes]):\n            return False\n\n        for rule in self.conditions.all():\n            if not rule.matches(basket, lines):\n                return False\n        return True\n\n\nclass CouponUsage(models.Model):\n    coupon = models.ForeignKey('Coupon', related_name='usages')\n    order = models.ForeignKey(Order, related_name='coupon_usages')\n\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    @classmethod\n    def add_usage(cls, order, coupon):\n        return cls.objects.create(order=order, coupon=coupon)\n\n\n@python_2_unicode_compatible\nclass Coupon(models.Model):\n    admin_url_suffix = \"coupon\"\n    name_field = \"code\"  # TODO: Document me\n    search_fields = [\"code\"]    # used by Select2Multiple to know which fields use to search by\n\n    code = models.CharField(max_length=12)\n\n    usage_limit_customer = models.PositiveIntegerField(\n        blank=True, null=True,\n        verbose_name=_(\"usage limit per customer\"), help_text=_(\"Limit the amount of usages per a single customer.\"))\n    usage_limit = models.PositiveIntegerField(\n        blank=True, null=True,\n        verbose_name=_(\"usage limit\"),\n        help_text=_(\"Set the absolute limit of usages for this coupon. \"\n                    \"If the limit is zero (0) coupon cannot be used.\"))\n\n    active = models.BooleanField(default=False, verbose_name=_(\"is active\"))\n\n    created_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"created by\"))\n    modified_by = models.ForeignKey(\n        settings.AUTH_USER_MODEL, blank=True, null=True,\n        related_name=\"+\", on_delete=models.SET_NULL,\n        verbose_name=_(\"modified by\"))\n\n    created_on = models.DateTimeField(auto_now_add=True, editable=False, verbose_name=_(\"created on\"))\n    modified_on = models.DateTimeField(auto_now=True, editable=False, verbose_name=_(\"modified on\"))\n\n    def save(self, **kwargs):\n        campaign = BasketCampaign.objects.filter(active=True, coupon_id=self.id).first()\n        if campaign and BasketCampaign.objects.filter(\n                active=True, shop_id=campaign.shop.id, coupon__code=self.code).exclude(id=campaign.id).exists():\n            raise ValidationError(_(\"Can not have multiple active campaigns with same code.\"))\n\n        return super(Coupon, self).save(**kwargs)\n\n    @classmethod\n    def generate_code(cls, length=6):\n        if length > 12:\n            length = 12\n        return ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(length))\n\n    @property\n    def exhausted(self):\n        val = bool(self.usage_limit and self.usages.count() >= self.usage_limit)\n        return val\n\n    @property\n    def attached(self):\n        return BasketCampaign.objects.filter(coupon=self).exists()\n\n    def attach_to_campaign(self, campaign):\n        if not self.attached:\n            self.campaign = campaign\n\n    @classmethod\n    def is_usable(cls, code, customer):\n        try:\n            code = cls.objects.get(code__iexact=code, active=True)\n            return code.can_use_code(customer)\n        except cls.DoesNotExist:\n            return False\n\n    def can_use_code(self, customer):\n        \"\"\"\n        Check if customer can use the code\n\n        :param customer:\n        :type customer: `Contact` or None\n        :rtype: True|False\n        \"\"\"\n        if not self.active:\n            return False\n\n        if not self.attached:\n            return False\n\n        if self.usage_limit_customer:\n            if not customer or customer.is_anonymous:\n                return False\n            if (self.usages.filter(order__customer=customer, coupon=self).count() >= self.usage_limit_customer):\n                return False\n\n        return not self.exhausted\n\n    def use(self, order):\n        return CouponUsage.add_usage(order=order, coupon=self)\n\n    def increase_customer_usage_limit_by(self, amount):\n        if self.usage_limit_customer:\n            new_limit = self.usage_limit_customer + amount\n        else:\n            new_limit = self.usages.count() + amount\n        self.usage_limit_customer = new_limit\n\n    def increase_usage_limit_by(self, amount):\n        self.usage_limit = self.usage_limit + amount if self.usage_limit else (self.usages.count() + amount)\n\n    def has_been_used(self, usage_count=1):\n        \"\"\" See if code is used the times given \"\"\"\n        return CouponUsage.objects.filter(coupon=self).count() >= usage_count\n\n    def __str__(self):\n        return self.code\n\n\nCatalogCampaignLogEntry = define_log_model(CatalogCampaign)\nBasketCampaignLogEntry = define_log_model(BasketCampaign)\nCouponLogEntry = define_log_model(Coupon)\nCouponUsageLogEntry = define_log_model(CouponUsage)", "original_line_number": 11010, "instruction_length": 993}
{"instruction": "Create a neural network class with methods to play moves and reset the network. Use the `neurolab` library to create a feedforward neural network with 64 input neurons and 1 output neuron. The `play_move` method takes a game object and an optional epsilon value, generates possible moves, and returns the best move based on the network's predictions. The `reset` method trains the network on the collected data and resets the class variables. The `reset_without_train` method resets the class variables without training the network. The code also includes a main function that plays games against a systematic player and then against itself with a decreasing epsilon value, and outputs the win counts.", "input": "", "output": "import neurolab as nl\nimport game2\nimport othello\nimport ntuplesystematic as nts\nimport time\nimport random\n\nclass nn:\n\tdef __init__(self):\n\t\tself.x = [[-1,1] for x in range(64)]\n\t\tself.net = nl.net.newff(self.x,[1])\n\t\t#self.net.trainf = nl.train.train_gd\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\n\tdef play_move(self,game,epsilon = 0):\n\t\tmoves = game.generate_moves()\n\t\tnum = random.uniform(0,1)\n\t\tif(num <= epsilon):\n\t\t\ttemp = game.copy()\n\t\t\tif(game.player==-1):\n\t\t\t\tif(self.moveb == 0):\n\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\tv=[]\n\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\tv2 = [v]\n\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\tself.moveb = self.moveb+1\n\t\t\t\t\tself.last_vb = v\n\t\t\t\t\treturn (v1[0][0], move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 0\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv=[]\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 1\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (v1[0],move)\n\t\t\telse:\n\t\t\t\tif(self.movew == 0):\n\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\tv=[]\n\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\tv2 = [v]\n\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\tself.movew = self.movew+1\n\t\t\t\t\tself.last_vw = v\n\t\t\t\t\treturn (v1[0][0], move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 2\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tmove = random.choice(moves)\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv=[]\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 3\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (v1[0],move)\n\t\telse:\n\t\t\tif(game.player == -1):\n\t\t\t\tif(self.moveb==0):\n\t\t\t\t\tj=0\n\t\t\t\t\tmax1 = 0\n\t\t\t\t\tbest_v = 0\n\t\t\t\t\tbest_move = None\n\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t#print v\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\telif(v1[0][0]>max1):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\tj = j+1\n\t\t\t\t\tself.moveb = self.moveb+1\n\t\t\t\t\tself.last_vb = best_v\n\t\t\t\t\treturn (max1, best_move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 4\n\t\t\t\t\t\t#print self.last_vb\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tj=0\n\t\t\t\t\t\tmax1 = 0\n\t\t\t\t\t\tbest_v = 0\n\t\t\t\t\t\tbest_move = 0\n\t\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\t\tv = []\n\t\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t\t#print v\n\t\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\telif(v1[0][0]>max1):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\t\tj = j+1\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(best_move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv2 = [best_v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 5\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vb)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vb = best_v\n\t\t\t\t\t\treturn (max1,best_move)\n\t\t\t\t\n\t\t\telse:\n\t\t\t\tif(self.movew==0):\n\t\t\t\t\tj=0\n\t\t\t\t\tmax1 = 0\n\t\t\t\t\tbest_v = 0\n\t\t\t\t\tbest_move = 0\n\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t#print v\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\telif(v1[0][0]<max1):\n\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\tj = j+1\n\t\t\t\t\tself.movew = self.movew+1\n\t\t\t\t\tself.last_vw = best_v\n\t\t\t\t\treturn (max1,best_move)\n\t\t\t\telse:\n\t\t\t\t\tif(moves[0]==None):\n\t\t\t\t\t\tv = []\n\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\tv.append(game.get_color([k,l]))\n\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 6\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = v\n\t\t\t\t\t\treturn (0,None)\n\t\t\t\t\telse:\n\t\t\t\t\t\tj=0\n\t\t\t\t\t\tmax1 = 0\n\t\t\t\t\t\tbest_v = 0\n\t\t\t\t\t\tbest_move = 0\n\t\t\t\t\t\tfor move in moves:\n\t\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\t\ttemp.play_move(move)\n\t\t\t\t\t\t\tv = []\n\t\t\t\t\t\t\tfor k in range(8):\n\t\t\t\t\t\t\t\tfor l in range(8):\n\t\t\t\t\t\t\t\t\t#print temp.get_color([k,l])\n\t\t\t\t\t\t\t\t\tv.append(temp.get_color([k,l]))\n\t\t\t\t\t\t\t#print v\n\t\t\t\t\t\t\tv2 = [v]\n\t\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\t\tif(j==0):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_v = v\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\telif(v1[0][0]<max1):\n\t\t\t\t\t\t\t\tmax1 = v1[0][0]\n\t\t\t\t\t\t\t\tbest_move = move\n\t\t\t\t\t\t\t\tbest_v =v\n\t\t\t\t\t\t\tj = j+1\n\t\t\t\t\t\ttemp = game.copy()\n\t\t\t\t\t\treward = 0\n\t\t\t\t\t\ttemp.play_move(best_move)\n\t\t\t\t\t\tif(temp.terminal_test()):\n\t\t\t\t\t\t\tif(temp.score()>0):\n\t\t\t\t\t\t\t\treward=-1\n\t\t\t\t\t\t\telif(temp.score()<0):\n\t\t\t\t\t\t\t\treward = 1\n\t\t\t\t\t\tv2 = [best_v]\n\t\t\t\t\t\tv1 = self.net.sim(v2)\n\t\t\t\t\t\tv1 = v1[0][0]\n\t\t\t\t\t\tv1 = reward + v1\n\t\t\t\t\t\tv1 = [v1]\n\t\t\t\t\t\t#print 7\n\t\t\t\t\t\t#print self.last_vw\n\t\t\t\t\t\tself.fin_v.append(self.last_vw)\n\t\t\t\t\t\tself.fin_val.append(v1)\n\t\t\t\t\t\tself.last_vw = best_v\n\t\t\t\t\t\treturn (max1,best_move)\n\n\tdef reset(self):\n\t\t#print self.fin_v\n\t\t#print self.fin_val\n\t\terror = self.net.train(self.fin_v,self.fin_val,epochs=5,show=1)\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\n\tdef reset_without_train(self):\n\t\tself.moveb = 0\n\t\tself.movew = 0\n\t\tself.last_vb = 0\n\t\tself.last_vw = 0\n\t\tself.fin_v = []\n\t\tself.fin_val = []\n\nif __name__ == \"__main__\":\n\t\"\"\"\n\tCreates a main player\n\t\"\"\"\n\tplayernew = nn()\n\tnTuplesSystematicObject = nts.nTuplesSystematic()\n\tgame2.play(othello.game(), game2.player(lambda x: playernew.play_move(x)),game2.player(lambda x: nTuplesSystematicObject.play_next_move(x)), True)\n\tplayernew.reset_without_train()\n\ttime.sleep(5)\n\tk = 100\n\tfor i in range(k):\n\t\tprint(i)\n\t\tgame2.play(othello.game(), game2.player(lambda x: playernew.play_move(x,0.3)),game2.player(lambda x: playernew.play_move(x,0.3)), False)\n\t\tplayernew.reset()\n\n\twins = [0, 0]\n\tfor i in range(100):\n\t\twinner = game2.play(othello.game(), game2.player_epsilon(lambda x: playernew.play_move(x)),game2.player_epsilon(lambda x: nTuplesSystematicObject.play_next_move(x)), False)\n\t\tif winner == 1:\n\t\t\twins[0] += 1\n\t\telif winner == 2:\n\t\t\twins[1] += 1\n\t\twinner = game2.play(othello.game(),game2.player_epsilon(lambda x: nTuplesSystematicObject.play_next_move(x)), game2.player_epsilon(lambda x: playernew.play_move(x)), False)\n\t\tif winner == 2:\n\t\t\twins[0] += 1\n\t\telif winner == 1:\n\t\t\twins[1] += 1\n\n\tprint wins\n\tf = open('results','a')\n\tval = (k,0.001,'epsilon',wins)\n\tval = str(val)", "original_line_number": 11160, "instruction_length": 701}
{"instruction": "Instructions:\n1. Make sure you have the necessary dependencies installed, including utils, lemmatizer, igraph, numpy, and cPickle.\n2. Set the desired values for the following variables: rgx, min_length, min_occ, max_freq, min_size, max_size, data_dir, pickle_dir1, and pickle_dir2.\n3. Run the program by executing the code under the if __name__ == '__main__': statement.\n4. The program will tokenize the files in the specified data directory, filter them based on the specified criteria, and generate collocation matrices for qualifying files.\n5. The resulting token statistics and collocation matrices will be saved as pickle files in the specified directories.", "input": "", "output": "#\n# This file is part of DF.\n# \n# DF is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public\n# License as published by the Free Software Foundation;\n# either version 3 of the License, or (at your option) any\n# later version.\n#\n# Latassan is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied\n# warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR\n# PURPOSE.  See the GNU General Public License for more\n# details.\n#\n# You should have received a copy of the GNU General Public\n# License along with DF; see the file COPYING.  If not\n# see <http://www.gnu.org/licenses/>.\n# \n# Copyright (C) 2014-2019 Jimmy Dubuisson <jimmy.dubuisson@gmail.com>\n#\n\nfrom __future__ import division\nfrom utils import *\nfrom lemmatizer import *\nfrom igraph import Graph\nfrom numpy import dot\nimport cPickle as pickle\n\nif __name__ == '__main__':\n        rgx = '\\w+'\n        #punct = '\\',.!?'\n        min_length = 3\n        # min number of occurences\n        min_occ = 3\n        # max frequency (between 0 and 1)\n        max_freq = 1\n\t# min number of tokens\n        min_size = 100\n        # max number of tokens\n        max_size = 1000\n\t# folder path\n\tdata_dir = sys.argv[1]\n\tpickle_dir1 = sys.argv[2]\n\tpickle_dir2 = sys.argv[3]\n\t# collocation metrics instance to be used\n        #cmetrics = CollocationMetrics(CollocationMetrics.decreasing_exp,(1,1),CollocationMetrics.do_nothing,())\n        cmetrics = CollocationMetrics(CollocationMetrics.decreasing_exp,(1,1),CollocationMetrics.information,())\n\t# batch replace arrays\n\tvold = ['</?blog>','</?Blog>','</?post>','<date>.*</date>','nbsp','urlLink']\n\tvnew = ['','','','','','']\n\t\t\n\tfnames = FileUtils.get_files_list(data_dir)\n\n\tcounter = 1\n\tmax_count = 2000\n\tsuccess_count = 0\n\tfor p in fnames:\n\t\tif success_count == max_count:\n\t\t\tbreak\n\t\tprint counter, '- Tokenizing: ', p \n\t\tcounter += 1\n\t\ttxt = FileUtils.read_text_file(data_dir + '/' + p)\n\t\ttxt = FileUtils.batch_replace(txt,vold,vnew)\n        \tdoc = DocStats(txt, rgx, min_length, min_occ, max_freq, cmetrics)\n\t\tprint '# tokens: ', len(doc.token_set) \n        \tif len(doc.token_set) >= min_size and len(doc.token_set) <= max_size:\n\t\t\tmat = doc.get_collocation_mat()\n\t\t\tprint '# rows: ', mat.dim\n\t\t\tprint '# nnz entries: ', mat.vmat.nnz\n\t\t\tif mat:\n\t\t\t\tsuccess_count += 1\n\t\t\t\tpickle.dump(doc.token_stats, open(pickle_dir1 + '/' + p.replace('.xml','') + \".p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n\t\t\t\tpickle.dump(mat, open(pickle_dir2 + '/' + p.replace('.xml','') + \".p\", \"wb\"), pickle.HIGHEST_PROTOCOL)\n\t\tprint '---'", "original_line_number": 11357, "instruction_length": 662}
{"instruction": "Create a custom widget with a TableView and a toolbar that displays all `DataFrameModels` and enables the editing of this (edit data, adding/removing, rows/columns). The widget has five buttons: editButton, addColumnButton, addRowButton, removeColumnButton, and removeRowButton. The widget also has a DragTable class that inherits from `QtGui.QTableView` and allows starting a drag operation with a `PandasCellPayload` on a defined index. The `DataTableWidget` class has several methods that are also slots, including `enableEditing`, `uncheckButton`, `addColumn`, `showAddColumnDialog`, `addRow`, `removeRow`, `removeColumns`, `showRemoveColumnDialog`, `setViewModel`, `setModel`, `model`, `viewModel`, `view`, `updateDelegate`, `updateDelegates`, and `selectionModel`.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom pandasqt.compat import QtCore, QtGui, Qt, Slot, Signal\n\nfrom pandasqt.models.DataFrameModel import DataFrameModel\nfrom pandasqt.views.EditDialogs import AddAttributesDialog, RemoveAttributesDialog\nfrom pandasqt.views.CustomDelegates import createDelegate\nfrom pandasqt.models.mime import PandasCellPayload, MimeData\nfrom pandasqt.models.SupportedDtypes import SupportedDtypes\n\ntry:\n    _fromUtf8 = QtCore.QString.fromUtf8\nexcept AttributeError:\n    def _fromUtf8(s):\n        return s\n    \nclass DragTable(QtGui.QTableView):\n    \n    def __init__(self, parent=None):\n        \"\"\"create a table view with the ability to start drag operations\"\"\"\n        super(DragTable, self).__init__(parent)\n        self.setDragEnabled(True)\n\n    def startDrag(self, index):\n        \"\"\"start a drag operation with a PandasCellPayload on defined index.\n        \n        Args:\n            index (QModelIndex): model index you want to start the drag operation.\n        \"\"\"\n\n        if not index.isValid():\n            return\n        \n        dataFrame = self.model().dataFrame()\n\n        # get all infos from dataFrame\n        dfindex = dataFrame.iloc[[index.row()]].index\n        columnName = dataFrame.columns[index.column()]\n        dtype = dataFrame[columnName].dtype\n        value = dataFrame[columnName][dfindex]\n\n        # create the mime data\n        mimePayload = PandasCellPayload(\n            dfindex,\n            columnName,\n            value,\n            dtype,\n            hex(id(self.model()))\n        )\n        mimeData = MimeData()\n        mimeData.setData(mimePayload)\n                \n        # create the drag icon and start drag operation\n        drag = QtGui.QDrag(self)\n        drag.setMimeData(mimeData)\n        pixmap = QtGui.QPixmap(\":/icons/insert-table.png\")\n        drag.setHotSpot(QtCore.QPoint(pixmap.width()/3, pixmap.height()/3))\n        drag.setPixmap(pixmap)\n        result = drag.start(Qt.MoveAction)\n\n    def mouseMoveEvent(self, event):\n        super(DragTable, self).mouseMoveEvent(event)\n        self.startDrag(self.indexAt(event.pos()))\n\nclass DataTableWidget(QtGui.QWidget):\n    \"\"\"A Custom widget with a TableView and a toolbar.\n\n    This widget shall display all `DataFrameModels` and\n    enable the editing of this (edit data, adding/removing,\n    rows/columns).\n\n    \"\"\"\n    def __init__(self, parent=None, iconSize=QtCore.QSize(36, 36)):\n        \"\"\"Constructs the object with the given parent.\n\n        Args:\n            parent (QObject, optional): Causes the objected to be owned\n                by `parent` instead of Qt. Defaults to `None`.\n            iconSize (QSize, optional): Size of edit buttons. Defaults to QSize(36, 36).\n\n        \"\"\"\n        super(DataTableWidget, self).__init__(parent)\n        self._iconSize = iconSize\n        self.initUi()\n\n\n    def initUi(self):\n        \"\"\"Initalizes the Uuser Interface with all sub widgets.\n\n        \"\"\"\n        self.gridLayout = QtGui.QGridLayout(self)\n        self.gridLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.buttonFrame = QtGui.QFrame(self)\n        #self.buttonFrame.setMinimumSize(QtCore.QSize(250, 50))\n        #self.buttonFrame.setMaximumSize(QtCore.QSize(250, 50))\n        self.buttonFrame.setFrameShape(QtGui.QFrame.NoFrame)\n        spacerItemButton = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)\n\n        self.buttonFrameLayout = QtGui.QGridLayout(self.buttonFrame)\n        self.buttonFrameLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.editButton = QtGui.QToolButton(self.buttonFrame)\n        self.editButton.setObjectName('editbutton')\n        self.editButton.setText(self.tr(u'edit'))\n        self.editButton.setToolTip(self.tr(u'toggle editing mode'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/document-edit.png')))\n\n        self.editButton.setIcon(icon)\n\n        self.addColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.addColumnButton.setObjectName('addcolumnbutton')\n        self.addColumnButton.setText(self.tr(u'+col'))\n        self.addColumnButton.setToolTip(self.tr(u'add new column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-column-right.png')))\n\n        self.addColumnButton.setIcon(icon)\n\n        self.addRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.addRowButton.setObjectName('addrowbutton')\n        self.addRowButton.setText(self.tr(u'+row'))\n        self.addRowButton.setToolTip(self.tr(u'add new row'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-row-below.png')))\n\n        self.addRowButton.setIcon(icon)\n\n        self.removeColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeColumnButton.setObjectName('removecolumnbutton')\n        self.removeColumnButton.setText(self.tr(u'-col'))\n        self.removeColumnButton.setToolTip(self.tr(u'remove a column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-column.png')))\n\n        self.removeColumnButton.setIcon(icon)\n\n        self.removeRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeRowButton.setObjectName('removerowbutton')\n        self.removeRowButton.setText(self.tr(u'-row'))\n        self.removeRowButton.setToolTip(self.tr(u'remove selected rows'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-row.png')))\n\n        self.removeRowButton.setIcon(icon)\n\n        self.buttons = [self.editButton, self.addColumnButton, self.addRowButton, self.removeColumnButton, self.removeRowButton]\n\n        for index, button in enumerate(self.buttons):\n            button.setMinimumSize(self._iconSize)\n            button.setMaximumSize(self._iconSize)\n            button.setIconSize(self._iconSize)\n            button.setCheckable(True)\n            self.buttonFrameLayout.addWidget(button, 0, index, 1, 1)\n        self.buttonFrameLayout.addItem(spacerItemButton, 0, index+1, 1, 1)\n\n        for button in self.buttons[1:]:\n            button.setEnabled(False)\n\n        #self.tableView = QtGui.QTableView(self)\n        self.tableView = DragTable(self)\n        self.tableView.setAlternatingRowColors(True)\n        self.tableView.setSortingEnabled(True)\n        \n        self.gridLayout.addWidget(self.buttonFrame, 0, 0, 1, 1)\n        self.gridLayout.addWidget(self.tableView, 1, 0, 1, 1)\n\n        self.editButton.toggled.connect(self.enableEditing)\n        self.addColumnButton.toggled.connect(self.showAddColumnDialog)\n        self.addRowButton.toggled.connect(self.addRow)\n        self.removeRowButton.toggled.connect(self.removeRow)\n        self.removeColumnButton.toggled.connect(self.showRemoveColumnDialog)\n\n    def setButtonsVisible(self, visible):\n        \"\"\"hide/show the edit buttons\"\"\"\n        self.buttonFrame.setVisible(visible)\n        \n    @Slot(bool)\n    def enableEditing(self, enabled):\n        \"\"\"Enable the editing buttons to add/remove rows/columns and to edit the data.\n\n        This method is also a slot.\n        In addition, the data of model will be made editable,\n        if the `enabled` parameter is true.\n\n        Args:\n            enabled (bool): This flag indicates, if the buttons\n                shall be activated.\n\n        \"\"\"\n        for button in self.buttons[1:]:\n            button.setEnabled(enabled)\n            if button.isChecked():\n                button.setChecked(False)\n\n        model = self.tableView.model()\n\n        if model is not None:\n            model.enableEditing(enabled)\n\n    @Slot()\n    def uncheckButton(self):\n        \"\"\"Removes the checked stated of all buttons in this widget.\n\n        This method is also a slot.\n\n        \"\"\"\n        #for button in self.buttons[1:]:\n        for button in self.buttons:\n            # supress editButtons toggled event\n            button.blockSignals(True)\n            if button.isChecked():\n                button.setChecked(False)\n            button.blockSignals(False)\n\n    @Slot(str, object, object)\n    def addColumn(self, columnName, dtype, defaultValue):\n        \"\"\"Adds a column with the given parameters to the underlying model\n\n        This method is also a slot.\n        If no model is set, nothing happens.\n\n        Args:\n            columnName (str): The name of the new column.\n            dtype (numpy.dtype): The datatype of the new column.\n            defaultValue (object): Fill the column with this value.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.addDataFrameColumn(columnName, dtype, defaultValue)\n\n        self.addColumnButton.setChecked(False)\n\n    @Slot(bool)\n    def showAddColumnDialog(self, triggered):\n        \"\"\"Display the dialog to add a column to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the dialog will be created and shown.\n\n        \"\"\"\n        if triggered:\n            dialog = AddAttributesDialog(self)\n            dialog.accepted.connect(self.addColumn)\n            dialog.rejected.connect(self.uncheckButton)\n            dialog.show()\n\n    @Slot(bool)\n    def addRow(self, triggered):\n        \"\"\"Adds a row to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the row will be appended to the end.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            model.addDataFrameRows()\n            self.sender().setChecked(False)\n\n\n    @Slot(bool)\n    def removeRow(self, triggered):\n        \"\"\"Removes a row to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the selected row will be removed\n                from the model.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            selection = self.tableView.selectedIndexes()\n\n            rows = [index.row() for index in selection]\n            model.removeDataFrameRows(set(rows))\n            self.sender().setChecked(False)\n\n    @Slot(list)\n    def removeColumns(self, columnNames):\n        \"\"\"Removes one or multiple columns from the model.\n\n        This method is also a slot.\n\n        Args:\n            columnNames (list): A list of columns, which shall\n                be removed from the model.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.removeDataFrameColumns(columnNames)\n\n        self.removeColumnButton.setChecked(False)\n\n    @Slot(bool)\n    def showRemoveColumnDialog(self, triggered):\n        \"\"\"Display the dialog to remove column(s) from the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the dialog will be created and shown.\n\n        \"\"\"\n        if triggered:\n            model = self.tableView.model()\n            if model is not None:\n                columns = model.dataFrameColumns()\n                dialog = RemoveAttributesDialog(columns, self)\n                dialog.accepted.connect(self.removeColumns)\n                dialog.rejected.connect(self.uncheckButton)\n                dialog.show()\n\n    def setViewModel(self, model):\n        \"\"\"Sets the model for the enclosed TableView in this widget.\n\n        Args:\n            model (DataFrameModel): The model to be displayed by\n                the Table View.\n\n        \"\"\"\n        if isinstance(model, DataFrameModel):\n            self.enableEditing(False)\n            self.uncheckButton()\n            \n            selectionModel = self.tableView.selectionModel()\n            self.tableView.setModel(model)\n            model.dtypeChanged.connect(self.updateDelegate)\n            model.dataChanged.connect(self.updateDelegates)\n            del selectionModel\n            \n    def setModel(self, model):\n        \"\"\"Sets the model for the enclosed TableView in this widget.\n\n        Args:\n            model (DataFrameModel): The model to be displayed by\n                the Table View.\n\n        \"\"\"\n        self.setViewModel(model)\n\n    def model(self):\n        \"\"\"Gets the viewModel\"\"\"\n        return self.view().model()\n\n    def viewModel(self):\n        \"\"\"Gets the viewModel\"\"\"\n        return self.view().model()\n\n    def view(self):\n        \"\"\"Gets the enclosed TableView\n\n        Returns:\n            QtGui.QTableView: A Qt TableView object.\n\n        \"\"\"\n        return self.tableView\n\n    def updateDelegate(self, column, dtype):\n        \"\"\"update the delegates for a specific column\n        \n        Args:\n            column (int): column index.\n            dtype (str): data type of column.\n        \n        \"\"\"\n        # as documented in the setDelegatesFromDtype function\n        # we need to store all delegates, so going from\n        # type A -> type B -> type A\n        # would cause a segfault if not stored.\n        createDelegate(dtype, column, self.tableView)\n\n    def updateDelegates(self):\n        \"\"\"reset all delegates\"\"\"\n        for index, column in enumerate(self.tableView.model().dataFrame().columns):\n            dtype = self.tableView.model().dataFrame()[column].dtype\n            self.updateDelegate(index, dtype)\n            \n    def selectionModel(self):\n        \"\"\"return the table views selectionModel\"\"\"\n        return self.view().selectionModel()", "original_line_number": 11856, "instruction_length": 770}
{"instruction": "This code defines a class called PluginDiagnose that inherits from QDialog and Ui_Dialog. It has several methods such as initUi(), displayPage(), CancelButton_clicked(), NextButton_clicked(), and PathLineEdit_textChanged(). To use this code, create an instance of the PluginDiagnose class and call its methods as needed. The initUi() method sets up the user interface, while the displayPage() method updates the current page being displayed. The CancelButton_clicked() method closes the dialog when the Cancel button is clicked, while the NextButton_clicked() method attempts to install a plugin and closes the dialog if successful. The PathLineEdit_textChanged() method updates the plugin path when the user types in the path.", "input": "", "output": "# -*- coding: utf-8 -*-\n#\n# (c) Copyright 2001-2011 Hewlett-Packard Development Company, L.P.\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA\n#\n# Authors: Amarnath Chitumalla\n#\n\n\n# Local\nfrom base.g import *\nfrom base import device, utils, pkit\nfrom prnt import cups\nfrom base.codes import *\nfrom .ui_utils import *\nfrom installer import pluginhandler\nfrom base.sixext import  to_unicode\n\n# Qt\nfrom PyQt4.QtCore import *\nfrom PyQt4.QtGui import *\nimport signal\n\n# Ui\nfrom .plugindiagnose_base import Ui_Dialog\n\n\n\nclass PluginDiagnose(QDialog, Ui_Dialog):\n    def __init__(self, parent, install_mode=PLUGIN_NONE, plugin_reason=PLUGIN_REASON_NONE, upgrade=False):\n        QDialog.__init__(self, parent)\n        self.install_mode = install_mode\n        self.plugin_reason = plugin_reason\n        self.plugin_path = None\n        self.result = False\n        self.pluginObj = pluginhandler.PluginHandle()\n        self.setupUi(self, upgrade)\n\n        self.user_settings = UserSettings()\n        self.user_settings.load()\n        self.user_settings.debug()\n\n        self.initUi()\n\n\n\n\n    def initUi(self):\n        # connect signals/slots\n        self.connect(self.CancelButton, SIGNAL(\"clicked()\"), self.CancelButton_clicked)\n        self.connect(self.NextButton, SIGNAL(\"clicked()\"), self.NextButton_clicked)\n        signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n        # Application icon\n        self.setWindowIcon(QIcon(load_pixmap('hp_logo', '128x128')))\n\n\n    def PathLineEdit_textChanged(self, t):\n        self.plugin_path = to_unicode(t)\n        self.setPathIndicators()\n\n\n    #\n    # Misc\n    #\n\n    def displayPage(self, page):\n        self.updateStepText(page)\n        self.StackedWidget.setCurrentIndex(page)\n\n    def CancelButton_clicked(self):\n        self.close()\n\n\n    def NextButton_clicked(self):\n        self.NextButton.setEnabled(False)\n        self.CancelButton.setEnabled(False)\n        try:\n            plugin = PLUGIN_REQUIRED \n            plugin_reason = PLUGIN_REASON_NONE\n            ok, sudo_ok = pkit.run_plugin_command(plugin == PLUGIN_REQUIRED, plugin_reason)\n\n            if not ok or self.pluginObj.getStatus() != pluginhandler.PLUGIN_INSTALLED:\n                FailureUI(self, self.__tr(\"Failed to install Plug-in.\\nEither you have chosen to skip the Plug-in installation  or entered incorrect Password.\"))\n\n        finally:\n            endWaitCursor()\n        self.result = True\n        self.close()\n\n\n    def __tr(self,s,c = None):\n        return qApp.translate(\"PluginDialog\",s,c)", "original_line_number": 12404, "instruction_length": 727}
{"instruction": "This code performs performance tests based on tests/highlevel/high-in-low6-memory.py. The data overhead is at ~6.5 ms / MB. The database upload overhead is about the same (7 ms / MB) with a flatfile backend. Database download is almost free. A structured cell auth operation is about 10 ms. Map-list macro evaluation is cheap, 5 ms per (parallel) transformation. Re-translation is about 20 ms per transformation (on top of the macro). Expression evaluation is about 10 ms + 0.5 ms / MB (of input + output) per transformation (speculative relationship!). Non-linear scaling is observed between 100 and 1000 parallel transformations, a x4 slowdown is observed for the last three overheads above. Mixed-to-str conversion is expensive, don't do it!", "input": "", "output": "\"\"\"\nPerformance tests based on tests/highlevel/high-in-low6-memory.py\nSee also auth-*py.\n\n- Data overhead is now at ~6.5 ms / MB.\n  A lot of the data overhead comes from json.dumps. This is to build mixed cells.\n  The rest is from get_hash.\n  Pure Python version (with get_hash and dumps) is at 6.3 ms / MB,\n  so the rest of the data overhead is fine!\n\n- Database upload overhead is about the same (7 ms / MB) with a flatfile backend\n  Database download is almost free.\n\n- A structured cell auth operation is about 10 ms.\n\n- map-list macro evaluation is cheap, 5 ms per (parallel) transformation\n\n- re-translation is about 20 ms per transformation (on top of the macro)\n\n- expression evaluation is about 10 ms + 0.5 ms / MB (of input + output) per transformation\n  (speculative relationship!)\n\n- BUT: Non-linear scaling:\n  between 100 and 1000 parallel transformations, a x4 slowdown is observed for the last three overheads above.\n\n\nNOTE: Mixed-to-str conversion is expensive, don't do it!\n\n\n\n\"\"\"\n\nimport sys\nimport seamless\n\nimport seamless.core.execute\nseamless.core.execute.DIRECT_PRINT = True\n\nseamless.database_sink.connect()\nseamless.database_cache.connect()\n#seamless.set_ncores(2)\n#seamless.set_parallel_evaluations(5)\n\nseamless.set_ncores(8) ###\nseamless.set_parallel_evaluations(100)  ###\n\n# for the big testing, 20 evaluations\nseamless.set_parallel_evaluations(20)  ###\n\n\"\"\"\nimport logging\nlogging.basicConfig()\nlogging.getLogger(\"seamless\").setLevel(logging.DEBUG)\n\"\"\"\n\nfrom seamless.highlevel import Context, Cell, Macro\nfrom seamless.highlevel.library import LibraryContainer\n\nimport time\nimport cProfile\ncProfile.profiler = cProfile.Profile()\n\nmylib = LibraryContainer(\"mylib\")\nmylib.map_list_N = Context()\ndef constructor(ctx, libctx, context_graph, inp, result):\n    m = ctx.m = Macro()\n    m.graph = context_graph\n    m.pins.result = {\"io\": \"output\", \"celltype\": \"mixed\", \"hash_pattern\": {\"!\": \"#\"}}\n\n    ctx.inp = Context()\n    ctx.cs_inp = Context()\n    inp_prefix = \"INPUT_\"\n    m.inp_prefix = inp_prefix\n    for key in inp:\n        c = Cell()\n        ctx.inp[key] = c\n        c.hash_pattern = {\"!\": \"#\"}\n        inp[key].connect(c)\n        ctx.cs_inp[key] = Cell(\"checksum\")\n        ctx.cs_inp[key] = ctx.inp[key]\n        setattr(m, inp_prefix + key , ctx.cs_inp[key])\n\n    def map_list_N(ctx, inp_prefix, graph, **inp):\n        #print(\"INP\", inp)\n        first_k = list(inp.keys())[0]\n        length = len(inp[first_k])\n        first_k = first_k[len(inp_prefix):]\n        for k0 in inp:\n            k = k0[len(inp_prefix):]\n            if len(inp[k0]) != length:\n                err = \"all cells in inp must have the same length, but '{}' has length {} while '{}' has length {}\"\n                raise ValueError(err.format(k, len(inp[k0]), first_k, length))\n\n        print(\"LENGTH\", length)\n\n        from seamless.core import Cell as CoreCell\n        from seamless.core.unbound_context import UnboundContext\n        pseudo_connections = []\n        ctx.result = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n\n        ctx.sc_data = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n        ctx.sc_buffer = cell(\"mixed\", hash_pattern = {\"!\": \"#\"})\n        ctx.sc = StructuredCell(\n            data=ctx.sc_data,\n            buffer=ctx.sc_buffer,\n            inchannels=[(n,) for n in range(length)],\n            outchannels=[()],\n            hash_pattern = {\"!\": \"#\"}\n        )\n\n        for n in range(length):\n            #print(\"MACRO\", n+1)\n            hc = HighLevelContext(graph)\n\n            subctx = \"subctx%d\" % (n+1)\n            setattr(ctx, subctx, hc)\n\n            if not hasattr(hc, \"inp\"):\n                raise TypeError(\"map_list_N context must have a subcontext called 'inp'\")\n            hci = hc.inp\n            if not isinstance(hci, UnboundContext):\n                raise TypeError(\"map_list_N context must have an attribute 'inp' that is a context, not a {}\".format(type(hci)))\n\n            for k0 in inp:\n                k = k0[len(inp_prefix):]\n                if not hasattr(hci, k):\n                    raise TypeError(\"map_list_N context must have a cell called inp.'{}'\".format(k))\n                if isinstance(hci[k], StructuredCell):\n                    raise TypeError(\"map_list_N context has a cell called inp.'{}', but its celltype must be mixed, not structured\".format(k))\n                if not isinstance(hci[k], CoreCell):\n                    raise TypeError(\"map_list_N context must have an attribute inp.'{}' that is a cell, not a {}\".format(k, type(hci[k])))\n                if hci[k].celltype != \"mixed\":\n                    raise TypeError(\"map_list_N context has a cell called inp.'{}', but its celltype must be mixed, not {}\".format(k, hci[k].celltype))\n\n                con = [\"..\" + k], [\"ctx\", subctx, \"inp\", k]\n                pseudo_connections.append(con)\n                cs = inp[k0][n]\n                hci[k].set_checksum(cs)\n\n            resultname = \"result%d\" % (n+1)\n            setattr(ctx, resultname, cell(\"mixed\"))\n            c = getattr(ctx, resultname)\n            hc.result.connect(c)\n            c.connect(ctx.sc.inchannels[(n,)])\n            con = [\"ctx\", subctx, \"result\"], [\"..result\"]\n            pseudo_connections.append(con)\n\n        ctx.sc.outchannels[()].connect(ctx.result)\n        ctx._pseudo_connections = pseudo_connections\n        print(\"/MACRO\")\n\n        \"\"\"\n        import logging\n        logging.basicConfig()\n        logging.getLogger(\"seamless\").setLevel(logging.DEBUG)\n        \"\"\"\n\n        import cProfile\n        cProfile.profiler.enable()\n\n    m.code = map_list_N\n    ctx.result = Cell()\n    ctx.result.hash_pattern = {\"!\": \"#\"}\n    ctx.result = m.result\n    result.connect_from(ctx.result)\n\n\nmylib.map_list_N.constructor = constructor\nmylib.map_list_N.params = {\n    \"context_graph\": \"context\",\n    \"inp\": {\n        \"type\": \"celldict\",\n        \"io\": \"input\"\n    },\n    \"result\": {\n        \"type\": \"cell\",\n        \"io\": \"output\"\n    },\n}\n\nctx = Context()\nctx.adder = Context()\nsctx = ctx.adder\nsctx.inp = Context()\nsctx.inp.a = Cell(\"mixed\")\nsctx.inp.b = Cell(\"mixed\")\nsctx.a = Cell(\"mixed\")\nsctx.b = Cell(\"mixed\")\nsctx.a = sctx.inp.a\nsctx.b = sctx.inp.b\ndef add(a,b):\n    print(\"ADD\", a[:10])\n    return a+b\nsctx.add = add\nsctx.add.a = sctx.a\nsctx.add.b = sctx.b\nsctx.result = sctx.add\nsctx.result.celltype = \"mixed\"\nctx.compute()\n\nctx.data_a = Cell()\nctx.data_a.hash_pattern = {\"!\": \"#\"}\nctx.data_b = Cell()\nctx.data_b.hash_pattern = {\"!\": \"#\"}\nctx.compute()\n\n# Next section is 14.5 secs (if the database is filled), but can be elided to ~0.5s by setting checksum directly (if in flatfile cache).\n# Not having a DB at all is also 13 secs, so DB request communication (without upload) doesn't cost much.\n\nrepeat = int(10e6)\n#repeat = int(5)\n#for n in range(1000): # 2x10 GB\n#for n in range(100): # 2x1 GB\nfor n in range(1000):\n    a = \"A:%d:\" % n + str(n%10) * repeat\n    b = \"B:%d:\" % n + str(n%10) * repeat\n    ctx.data_a[n] = a\n    ctx.data_b[n] = b\n    if n % 20 == 0:\n        ctx.compute()\n    print(n+1)\n\nctx.compute()\nprint(ctx.data_a.checksum)\nprint(ctx.data_b.checksum)\n\n\"\"\"\nctx.data_a.set_checksum(\"d07050610c50de8c810aa1d1e322786ed8932cf6eafa0fbe1f132b2c881af9c2\")\nctx.data_b.set_checksum(\"374c02504f89ed0a760b03c3e1fd2258988576b919d763254709b66fc7bfb22e\")\nctx.compute()\n\n\"\"\"\n\n#\n### For repeat=10 million, 1000 items\n### ctx.data_a.set_checksum(\"fa4e6aa7e7edaa6feb036fd5e8c28ffc48575cefc332187552c5be4bf0511af8\")\n### ctx.data_b.set_checksum(\"2988c44780790e4ffceb1f97391e475f165e316f27a656957282a2998aee9d4f\")\n\n### For repeat=10 million, 200 items\n### ctx.data_a.set_checksum(\"d07050610c50de8c810aa1d1e322786ed8932cf6eafa0fbe1f132b2c881af9c2\")\n### ctx.data_b.set_checksum(\"374c02504f89ed0a760b03c3e1fd2258988576b919d763254709b66fc7bfb22e\")\n\n### For repeat=10 million\n### ctx.data_a.set_checksum(\"983730afb7ab41d524b72f1097daaf4a3c15b98943291f96e523730849cabe8c\")\n### ctx.data_b.set_checksum(\"46dabc02b59be44064a9e06dd50bc6841833578c2b6339fbc43f090cc17831fa\")\n\n### For repeat=5\n### ctx.data_a.set_checksum(\"9b4a551a6c1c5830d6070b9c22ae1788b9743e9637be47d56103bcda019a897c\")\n### ctx.data_b.set_checksum(\"9820f1ab795db7b0d195f21966ecb071d76e9ce2fd3a90845974a3905584eb3e\")\nctx.compute()\n\n\"\"\"\nIf there is no database (100 x repeat 10e6):\n- 13 secs up to here (6.5 ms per MB)\n- 0.5 secs to evaluate the macro\n- 2.3 secs (2.8 - 0.5) for re-translation (23 ms per transformer)\n- 32 secs total time, which leaves 32 - 13 - 0.5 = 18.5 secs for transformation and expression evaluation\n  Since 13 secs is required for calculate checksum and decoding, that means ~5.5 secs (55 ms per transformer) overhead\n  This is a supplement of 32 ms over just re-translation\n\nIf there is no database (100 x repeat 5):\n- 2.3 secs up to here (12 ms per auth operation)\n- Still 0.5 secs to evaluate the macro\n- Still 2.3 secs (2.8 - 0.5) for re-translation (23 ms per transformer, independent of data size!)\n- 6.2 secs total time, which leaves 6.2 - 2.3 - 0.5 = 3.5 secs for transformation and expression evaluation\n  This is an overhead of 35 ms per transformer, a supplement of just 12 ms over re-translation\n  The 20 ms reduction compared to above comes from not handling 2x10 MB of input and 20 MB of output,\n  so that's 0.5 ms/MB.\n\nIf there is no database (1000 x repeat 5):\n- 11.7 secs up to here (12 ms per auth operation). So scales linearly.\n- 6.5 secs to evaluate the macro, so scales about linearly\n- 98 secs (104.5 - 6.5) for re-translation, which is 4x slower than above  (98 ms)\n- 145 secs total time, which leaves 145 - 11.7 - 6.5 = 127 secs for transformation and expression evaluation\n  This is an overhead of 127 ms per transformer, which is 4x slower than above (127 ms).\n  => So in principle, 90 sconds slower than might be\n    - Some 45 secs is await-upon-connection-tasks, this could be optimized?\n    - 12 seconds from isinstance is probably unavoidable\n    - 9 seconds comes from validate deep structure, that may be unavoidable\n    - 5 seconds each from taskmanager.add_task (61k tasks) and asyncio.Task.done (119 million tasks). Avoidable?\n  => do maplist-inside-maplist\n\nIf the database has not been filled:\n- 27.5 secs up to here\nIf the database has been filled:\n- 14 secs up to here: to synthesize the data, and to verify that all is present\n  So the raw upload is 13.5 seconds (27.5 - 14); and communication with the DB delays only 1 sec.\n- 1.5 secs up to here, with the above elision.\n\nWith the database:\n- 1.5 secs to evaluate the macro (DB slows down!)\n- 5.5 secs for re-translation\n- 45.7 secs total time, which leaves 45.7 - 5.5 - 1.5 = 38.5 secs for transformation and expression evaluation\n  Compare this to the 18.5 secs w/o database, this is a loss of 20 secs.\n  But we have to count the download of the inputs and upload of the results.\n  When removing the tfr entries from the database, transformations will be repeated, but no buffers will be uploaded,\n  as the sink knows them already.\n  This brings total time down to 32 secs, the same as no database!\n  So all of the extra overhead is from upload, and download is almost free. (This could be hard disk caching, though)\n- 5.5 secs total time with pulling transformation results out of the DB. Again, download is almost free.\n\n\nBig test with the database (1000 x repeat 10e6):\n- Total time 940 secs. Data upload overhead should be ~120 secs, and Seamless data overhead should be ~140 secs.\n- 142 secs for re-translation + macro evaluation (142 ms / transformation), a factor 6 slowdown\n- 940 - 142 - 120 - 140 = ~540 secs for evaluation\n   I.e. 540 ms per transformation. If the same slowdown applies, it would have been 90.\n   But we would have expected 30. So a larger slowdown (fewer parallel expressions may have been a cause too)\n\"\"\"\n\nctx.result = Cell()\nctx.result.hash_pattern = {\"!\": \"#\"}\nctx.compute()\n\nctx.include(mylib.map_list_N)\nctx.inst = ctx.lib.map_list_N(\n    context_graph = ctx.adder,\n    inp = {\"a\": ctx.data_a, \"b\": ctx.data_b},\n    result = ctx.result\n)\nctx.compute()\n\nprint(\"Exception:\", ctx.inst.ctx.m.exception)\nprint(ctx.result.data)\n\nimport pstats\nsortby = 'tottime'\nps = pstats.Stats(cProfile.profiler).sort_stats(sortby)\nps.print_stats(40)\n\nt0 = time.time()\n\"\"\"\nprint(\"Re-set\")\ngraph = ctx.get_graph()\nctx_dummy = Context()\ndummy_graph = ctx_dummy.get_graph()\nctx.set_graph(dummy_graph)\nctx.translate(force=True)\nctx.compute()\nprint(time.time()-t0)\nprint(\"Re-eval\")\nctx.set_graph(graph)\n\"\"\"\n\"\"\"\nctx.translate(force=True)\nctx.compute()\nprint(time.time()-t0)\n\"\"\"", "original_line_number": 12610, "instruction_length": 744}
{"instruction": "Create a class called EPGSelection that inherits from EPGSelectionChannel and EPGServiceZap. The class should have an __init__ method that takes in a session, service, zapFunc, eventid, bouquetChangeCB, serviceChangeCB, EPGtype, StartBouquet, StartRef, and bouquets. If EPGtype is not \"similar\" or \"single\", print a warning message and continue in single EPG mode. The class should also have methods for various button presses, such as redButtonPressed, greenButtonPressed, yellowButtonPressed, and blueButtonPressed. Additionally, the class should have methods for refreshing the list, opening the timer list, and adding a timer from an event silently. Finally, the class should have a method for opening the event view.", "input": "", "output": "from __future__ import print_function\nfrom Screens.InfoBar import InfoBar\nfrom enigma import eServiceReference\nfrom Components.ActionMap import HelpableActionMap\nfrom Screens.EpgSelectionChannel import EPGSelectionChannel\nfrom Screens.EpgSelectionBase import EPGServiceZap\nfrom Screens.TimerEntry import addTimerFromEventSilent\n\n\n# Keep for backwards compatibility with plugins, including the parameter naming.\n# This class assumes that EPGSelection is only used in the SingleEPG sense.\nclass EPGSelection(EPGSelectionChannel, EPGServiceZap):\n\tdef __init__(self, session, service=None, zapFunc=None, eventid=None, bouquetChangeCB=None, serviceChangeCB=None, EPGtype=\"similar\", StartBouquet=None, StartRef=None, bouquets=None):\n\t\tif EPGtype not in (\"similar\", \"single\"):\n\t\t\tprint(\"[EPGSelection] Warning: EPGSelection does not support type '%s'\" % EPGtype)\n\t\t\tprint(\"               Attempting to continue in single EPG mode\")\n\t\tEPGSelectionChannel.__init__(self, session, eServiceReference(service))\n\t\tEPGServiceZap.__init__(self, zapFunc or InfoBar.instance.zapToService)\n\n\t\t# Rewrite the EPG actions to invoke the compatibility functions.\n\t\thelpDescription = _(\"EPG Commands\")\n\t\tself[\"epgactions\"] = HelpableActionMap(self, \"EPGSelectActions\", {\n\t\t\t\"info\": (self.Info, _(\"Show detailed event info\")),\n\t\t\t\"epg\": (self.epgButtonPressed, _(\"Show detailed event info\")),\n\t\t\t\"menu\": (self.createSetup, _(\"Setup menu\"))\n\t\t}, prio=-1, description=helpDescription)\n\t\tself[\"colouractions\"] = HelpableActionMap(self, \"ColorActions\", {\n\t\t\t\"red\": (self.redButtonPressed, _(\"IMDB search for current event\")),\n\t\t\t\"redlong\": (self.redButtonPressedLong, _(\"Sort EPG list\")),\n\t\t\t\"green\": (self.greenButtonPressed, _(\"Add/Remove timer for current event\")),\n\t\t\t\"greenlong\": (self.greenButtonPressedLong, _(\"Show timer list\")),\n\t\t\t\"yellow\": (self.yellowButtonPressed, _(\"Search for similar events\")),\n\t\t\t\"blue\": (self.blueButtonPressed, _(\"Add an autotimer for current event\")),\n\t\t\t\"bluelong\": (self.blueButtonPressedLong, _(\"Show autotimer list\"))\n\t\t}, prio=-1, description=helpDescription)\n\n\t# EPGSearch bypasses base class initialisation\n\t# try to limit the scope of its quirkyness by providing a limited\n\t# initialisation path\n\tdef EPGSearch_init(self, session):\n\t\tEPGServiceZap.__init__(self, InfoBar.instance.zapToService)\n\n\t# Backwards compatibility properties for plugins.\n\t@property\n\tdef ChoiceBoxDialog(self):\n\t\treturn self.choiceBoxDialog\n\n\t@ChoiceBoxDialog.setter\n\tdef ChoiceBoxDialog(self, value):\n\t\tself.choiceBoxDialog = value\n\n\t# Backwards compatibility functions for plugins.\n\t# Button names.\n\tdef redButtonPressed(self):\n\t\tself.openIMDb()\n\n\tdef redButtonPressedLong(self):\n\t\tself.sortEpg()\n\n\tdef greenButtonPressed(self):\n\t\tself.addEditTimer()\n\n\tdef greenButtonPressedLong(self):\n\t\tself.showTimerList()\n\n\tdef yellowButtonPressed(self):\n\t\tself.openEPGSearch()\n\n\tdef blueButtonPressed(self):\n\t\tself.addAutoTimer()\n\n\tdef blueButtonPressedLong(self):\n\t\tself.showAutoTimerList()\n\n\tdef Info(self):\n\t\tself.infoKeyPressed()\n\n\tdef InfoLong(self):\n\t\tself.OpenSingleEPG()\n\n\tdef infoKeyPressed(self):\n\t\tself.openEventView()\n\n\tdef eventSelected(self): # used by EPG Search plugin\n\t\tself.openEventView()\n\n\tdef epgButtonPressed(self):\n\t\tself.openEventView()\n\n\t# Actions\n\tdef showTimerList(self):\n\t\tself.openTimerList()\n\n\tdef showAutoTimerList(self):\n\t\tself.openAutoTimerList()\n\n\tdef OpenSingleEPG(self):\n\t\tself.openSingleEPG()\n\n\tdef sortEpg(self):\n\t\tself.sortEPG(self)\n\n\tdef timerAdd(self):\n\t\tself.addEditTimerMenu()\n\n\tdef doRecordTimer(self):\n\t\tself.doInstantTimer(0)\n\n\tdef doZapTimer(self):\n\t\tself.doInstantTimer(1)\n\n\tdef RecordTimerQuestion(self, manual=False):\n\t\tif manual:\n\t\t\tself.addEditTimer()\n\t\telse:\n\t\t\tself.addEditTimerMenu()\n\n\tdef doInstantTimer(self, zap=0):\n\t\tevent, service = self[\"list\"].getCurrent()[:2]\n\t\taddTimerFromEventSilent(self.session, self.refreshTimerActionButton, event, service, zap)\n\n\t# Things that need to be able to be overridden.\n\tdef refreshList(self):\n\t\ttry:\n\t\t\t# Allow plugins to override using the old all lowercase method name.\n\t\t\tself.refreshlist()\n\t\texcept AttributeError:\n\t\t\tEPGSelectionChannel.refreshList(self)", "original_line_number": 12804, "instruction_length": 721}
{"instruction": "This code defines a formaldehyde mm-line fitter based on RADEX models. To use the fitter, import the necessary modules and call the `h2co_mm_radex` function with the appropriate parameters. The `formaldehyde_mm` function generates a model Formaldehyde spectrum based on simple Gaussian parameters. The `formaldehyde_mm_model` class is a spectral model for the Formaldehyde spectrum. The `formaldehyde_mm_fitter` and `formaldehyde_mm_vheight_fitter` are fitters for the Formaldehyde spectrum. The `pmfFormaldehydeModel` and `pmfFormaldehydeModelVtau` classes are function models for the Formaldehyde spectrum.", "input": "", "output": "\"\"\"\n===========================\nFormaldehyde mm-line fitter\n===========================\n\nThis is a formaldehyde 3_03-2_02 / 3_22-221 and 3_03-2_02/3_21-2_20 fitter.  \nIt is based entirely on RADEX models.\n\nThis is the EWR fork of the fitter in pyspeckit.  \n\"\"\"\nimport numpy as np\nimport pyspeckit.spectrum.models.hyperfine as hyperfine\nfrom pyspeckit.spectrum.models import fitter,model#,modelgrid\ntry: # for model grid reading\n    import astropy.io.fits as pyfits\nexcept ImportError:\n    import pyfits\ntry:\n    import scipy.interpolate\n    import scipy.ndimage\n    scipyOK = True\nexcept ImportError:\n    scipyOK=False\n\nline_names = ['threeohthree','threetwotwo','threetwoone']\n\n# http://adsabs.harvard.edu/abs/1971ApJ...169..429T has the most accurate freqs\n# http://adsabs.harvard.edu/abs/1972ApJ...174..463T [twotwo]\ncentral_freq_dict = { \n        'threeohthree': 218.222192e9,\n        'threetwotwo': 218.475632e9,\n        'threetwoone': 218.760066e9,\n    }\nline_strength_dict={\n        'threeohthree': 1.,\n        'threetwotwo': 1.,\n        'threetwoone': 1.,\n        }\nrelative_strength_total_degeneracy={\n        'threeohthree': 1.,\n        'threetwotwo': 1.,\n        'threetwoone': 1.,\n        }\nfreq_dict = central_freq_dict \naval_dict = {\n        'threeohthree': 2.818e-4,\n        'threetwotwo': 1.571e-4,\n        'threetwoone': 1.577e-4,\n    }\n\nvoff_lines_dict = {\n        'threeohthree': 0.,\n        'threetwotwo': 0.,\n        'threetwoone': 0.,\n        }\n\n\nformaldehyde_mm_vtau = hyperfine.hyperfinemodel(line_names, voff_lines_dict,\n        freq_dict, line_strength_dict, relative_strength_total_degeneracy)\nformaldehyde_mm_vtau_fitter = formaldehyde_mm_vtau.fitter\nformaldehyde_mm_vtau_vheight_fitter = formaldehyde_mm_vtau.vheight_fitter\n\ndef h2co_mm_radex(xarr, \n        Temperature=25,\n        logColumn=13,\n        logDensity=4,\n        xoff_v=0.0,\n        width=1.0, \n        grid_vwidth=1.0,\n        gridbundle = None,\n        debug=False,\n        verbose=False,\n        **kwargs):\n\n    \"\"\"\n    Use a grid of RADEX-computed models to make a model line spectrum\n\n    The RADEX models have to be available somewhere.\n    OR they can be passed as arrays.  If as arrays, the form should be:\n    texgrid = ((minfreq1,maxfreq1,texgrid1),(minfreq2,maxfreq2,texgrid2))\n\n    xarr must be a SpectroscopicAxis instance\n    xoff_v, width are both in km/s\n\n\n    Parameters\n    ----------\n    grid_vwidth : float\n        the velocity assumed when computing the grid in km/s\n        this is important because tau = modeltau / width (see, e.g., \n        Draine 2011 textbook pgs 219-230)\n    density : float\n        Density!\n    \"\"\"\n\n    # Convert X-units to frequency in GHz\n    xarr = xarr.as_unit('Hz', quiet=True)\n    Tex303,Tex322,Tex321,tau303,tau322,tau321 = gridbundle\n\n\n    # if this gets too far different from 1, we are gonna have a Bad Time.\n    scalefac = grid_vwidth/width\n\n    tex = (Tex303(logColumn,logDensity,Temperature),\n           Tex322(logColumn,logDensity,Temperature),\n           Tex321(logColumn,logDensity,Temperature))\n    tau =  (tau303(logColumn,logDensity,Temperature)*scalefac,\n            tau322(logColumn,logDensity,Temperature)*scalefac,\n            tau321(logColumn,logDensity,Temperature)*scalefac)\n    if np.any(np.isnan(tex)) or np.any(np.isnan(tau)):\n        raise ValueError(\"Invalid column/density\")\n\n    if verbose:\n        for ta,tk in zip(tau,tex):\n            print \"density %20.12g temperature %20.12g column %20.12g: tau %20.12g tex %20.12g\" % (logDensity, Temperature, logColumn, ta, tk)\n\n    if debug:\n        import pdb; pdb.set_trace()\n\n# here there be physics\n    ckms = 2.99792458e5\n    freq_dict = { \n        '303': 218.222192e9,\n        '322': 218.475632e9,\n        '321': 218.760066e9,\n        }\n    Tbg = 2.73 #because it totally is\n\n\n    nu0 = np.array([ 218.222192e9, 218.475632e9,218.760066e9])\n    nuwidth = [width/ckms*nu for nu in nu0]\n    nuoff = [xoff_v/ckms*nu for nu in nu0]\n    minfreq = nu0/1e9 - 0.25\n    maxfreq = nu0/1e9 + 0.25\n#    spec2 = np.zeros(len(xarr))\n#    for ii in range(len(nu0)):\n#        taunu = tau[ii]*np.exp(-(xarr+nuoff[ii]-nu0[ii])**2/(2.0*nuwidth[ii]**2))\n#        spec2 = spec2 + (1-np.exp(-taunu))*tex[ii] + Tbg*(np.exp(-taunu)-1)  #second term assumes an ON-OFF \n\n    spec = np.sum([\n            (formaldehyde_mm_vtau(xarr, Tex=float(tex[ii]), tau=float(tau[ii]),\n                                  xoff_v=xoff_v, width=width, **kwargs)\n             * (xarr.as_unit('GHz')>minfreq[ii]) * (xarr.as_unit('GHz')<maxfreq[ii])) for ii in xrange(len(tex))],\n                  axis=0)\n#    import pdb \n#    pdb.set_trace()\n        \n\n    return spec\n\n\ndef formaldehyde_mm(xarr, amp=1.0, xoff_v=0.0, width=1.0, \n        return_components=False ):\n    \"\"\"\n    Generate a model Formaldehyde spectrum based on simple gaussian parameters\n\n    the \"amplitude\" is an essentially arbitrary parameter; we therefore define it to be Tex given tau=0.01 when\n    passing to the fitter\n    The final spectrum is then rescaled to that value\n\n    The components are independent, but with offsets set by frequency... in principle.\n\n    \"\"\"\n\n    mdl = formaldehyde_vtau(xarr, Tex=amp*0.01, tau=0.01, xoff_v=xoff_v,\n            width=width,\n            return_components=return_components)\n    if return_components:\n        mdlpeak = np.abs(mdl).squeeze().sum(axis=0).max()\n    else:\n        mdlpeak = np.abs(mdl).max()\n    if mdlpeak > 0:\n        mdl *= amp/mdlpeak\n\n    return mdl\n\n\nclass formaldehyde_mm_model(model.SpectralModel):\n    pass\n\nformaldehyde_mm_fitter = formaldehyde_mm_model(formaldehyde_mm, 3,\n        parnames=['amp','center','width'], \n        parlimited=[(False,False),(False,False), (True,False)], \n        parlimits=[(0,0), (0,0), (0,0)],\n        shortvarnames=(\"A\",\"v\",\"\\\\sigma\"), # specify the parameter names (TeX is OK)\n        fitunits='Hz' )\n\nformaldehyde_mm_vheight_fitter = formaldehyde_mm_model(fitter.vheightmodel(formaldehyde_mm), 4,\n        parnames=['height','amp','center','width'], \n        parlimited=[(False,False),(False,False),(False,False), (True,False)], \n        parlimits=[(0,0), (0,0), (0,0), (0,0)],\n        shortvarnames=(\"H\",\"A\",\"v\",\"\\\\sigma\"), # specify the parameter names (TeX is OK)\n        fitunits='Hz' )\n\n\ntry:\n    import pymodelfit\n\n    class pmfFormaldehydeModel(pymodelfit.FunctionModel1DAuto):\n        def f(self, x, amp0=1.0, xoff_v0=0.0,width0=1.0):\n            return formaldehyde(x,\n                    amp=amp0,\n                    xoff_v=xoff_v0,width=width0)\n\n    class pmfFormaldehydeModelVtau(pymodelfit.FunctionModel1DAuto):\n        def f(self, x, Tex0=1.0, tau0=0.01, xoff_v0=0.0, width0=1.0):\n            return formaldehyde_vtau(x,\n                    Tex=Tex0, tau=tau0,\n                    xoff_v=xoff_v0,width=width0)\nexcept ImportError:\n    pass", "original_line_number": 13216, "instruction_length": 608}
{"instruction": "This code tests BIP forks in Python 2. To run the test, follow these steps: \n\n1. Start a single node.\n2. Use regtest lock-in with 108/144 block signalling.\n3. Activate after a further 144 blocks.\n4. Mine 2 blocks and save coinbases for later use.\n5. Mine 141 blocks to transition from DEFINED to STARTED.\n6. Mine 100 blocks signalling readiness and 44 not in order to fail to change state this period.\n7. Mine 108 blocks signalling readiness and 36 blocks not signalling readiness (STARTED->LOCKED_IN).\n8. Mine a further 143 blocks (LOCKED_IN).\n9. Test that enforcement has not triggered (which triggers ACTIVE).\n10. Test that enforcement has triggered.", "input": "", "output": "#!/usr/bin/env python2\n# Copyright (c) 2015 The Deuscoin Core developers\n# Distributed under the MIT/X11 software license, see the accompanying\n# file COPYING or http://www.opensource.org/licenses/mit-license.php.\n#\n\nfrom test_framework.test_framework import ComparisonTestFramework\nfrom test_framework.util import *\nfrom test_framework.mininode import CTransaction, NetworkThread\nfrom test_framework.blocktools import create_coinbase, create_block\nfrom test_framework.comptool import TestInstance, TestManager\nfrom test_framework.script import CScript, OP_1NEGATE, OP_NOP3, OP_DROP\nfrom binascii import hexlify, unhexlify\nimport cStringIO\nimport time\nimport itertools\n\n'''\nThis test is meant to exercise BIP forks\nConnect to a single node.\nregtest lock-in with 108/144 block signalling\nactivation after a further 144 blocks\nmine 2 block and save coinbases for later use\nmine 141 blocks to transition from DEFINED to STARTED\nmine 100 blocks signalling readiness and 44 not in order to fail to change state this period\nmine 108 blocks signalling readiness and 36 blocks not signalling readiness (STARTED->LOCKED_IN)\nmine a further 143 blocks (LOCKED_IN)\ntest that enforcement has not triggered (which triggers ACTIVE)\ntest that enforcement has triggered\n'''\n\n\n\nclass BIP9SoftForksTest(ComparisonTestFramework):\n\n    def __init__(self):\n        self.num_nodes = 1\n\n    def setup_network(self):\n        self.nodes = start_nodes(1, self.options.tmpdir,\n                                 extra_args=[['-debug', '-whitelist=127.0.0.1']],\n                                 binary=[self.options.testbinary])\n\n    def run_test(self):\n        self.test = TestManager(self, self.options.tmpdir)\n        self.test.add_all_connections(self.nodes)\n        NetworkThread().start() # Start up network handling in another thread\n        self.test.run()\n\n    def create_transaction(self, node, coinbase, to_address, amount):\n        from_txid = node.getblock(coinbase)['tx'][0]\n        inputs = [{ \"txid\" : from_txid, \"vout\" : 0}]\n        outputs = { to_address : amount }\n        rawtx = node.createrawtransaction(inputs, outputs)\n        tx = CTransaction()\n        f = cStringIO.StringIO(unhexlify(rawtx))\n        tx.deserialize(f)\n        tx.nVersion = 2\n        return tx\n\n    def sign_transaction(self, node, tx):\n        signresult = node.signrawtransaction(hexlify(tx.serialize()))\n        tx = CTransaction()\n        f = cStringIO.StringIO(unhexlify(signresult['hex']))\n        tx.deserialize(f)\n        return tx\n\n    def generate_blocks(self, number, version, test_blocks = []):\n        for i in xrange(number):\n            block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n            block.nVersion = version\n            block.rehash()\n            block.solve()\n            test_blocks.append([block, True])\n            self.last_block_time += 1\n            self.tip = block.sha256\n            self.height += 1\n        return test_blocks\n\n    def get_bip9_status(self, key):\n        info = self.nodes[0].getblockchaininfo()\n        for row in info['bip9_softforks']:\n            if row['id'] == key:\n                return row\n        raise IndexError ('key:\"%s\" not found' % key)\n\n\n    def test_BIP(self, bipName, activated_version, invalidate, invalidatePostSignature):\n        # generate some coins for later\n        self.coinbase_blocks = self.nodes[0].generate(2)\n        self.height = 3  # height of the next block to build\n        self.tip = int (\"0x\" + self.nodes[0].getbestblockhash() + \"L\", 0)\n        self.nodeaddress = self.nodes[0].getnewaddress()\n        self.last_block_time = int(time.time())\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'defined')\n\n        # Test 1\n        # Advance from DEFINED to STARTED\n        test_blocks = self.generate_blocks(141, 4)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'started')\n\n        # Test 2\n        # Fail to achieve LOCKED_IN 100 out of 144 signal bit 1\n        # using a variety of bits to simulate multiple parallel softforks\n        test_blocks = self.generate_blocks(50, activated_version) # 0x20000001 (signalling ready)\n        test_blocks = self.generate_blocks(20, 4, test_blocks) # 0x00000004 (signalling not)\n        test_blocks = self.generate_blocks(50, activated_version, test_blocks) # 0x20000101 (signalling ready)\n        test_blocks = self.generate_blocks(24, 4, test_blocks) # 0x20010000 (signalling not)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'started')\n\n        # Test 3\n        # 108 out of 144 signal bit 1 to achieve LOCKED_IN\n        # using a variety of bits to simulate multiple parallel softforks\n        test_blocks = self.generate_blocks(58, activated_version) # 0x20000001 (signalling ready)\n        test_blocks = self.generate_blocks(26, 4, test_blocks) # 0x00000004 (signalling not)\n        test_blocks = self.generate_blocks(50, activated_version, test_blocks) # 0x20000101 (signalling ready)\n        test_blocks = self.generate_blocks(10, 4, test_blocks) # 0x20010000 (signalling not)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'locked_in')\n\n        # Test 4\n        # 143 more version 536870913 blocks (waiting period-1)\n        test_blocks = self.generate_blocks(143, 4)\n        yield TestInstance(test_blocks, sync_every_block=False)\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'locked_in')\n\n        # Test 5\n        # Check that the new rule is enforced\n        spendtx = self.create_transaction(self.nodes[0],\n                self.coinbase_blocks[0], self.nodeaddress, 1.0)\n        invalidate(spendtx)\n        spendtx = self.sign_transaction(self.nodes[0], spendtx)\n        spendtx.rehash()\n        invalidatePostSignature(spendtx)\n        spendtx.rehash()\n        block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n        block.nVersion = activated_version\n        block.vtx.append(spendtx)\n        block.hashMerkleRoot = block.calc_merkle_root()\n        block.rehash()\n        block.solve()\n\n        self.last_block_time += 1\n        self.tip = block.sha256\n        self.height += 1\n        yield TestInstance([[block, True]])\n\n        assert_equal(self.get_bip9_status(bipName)['status'], 'active')\n\n        # Test 6\n        # Check that the new sequence lock rules are enforced\n        spendtx = self.create_transaction(self.nodes[0],\n                self.coinbase_blocks[1], self.nodeaddress, 1.0)\n        invalidate(spendtx)\n        spendtx = self.sign_transaction(self.nodes[0], spendtx)\n        spendtx.rehash()\n        invalidatePostSignature(spendtx)\n        spendtx.rehash()\n\n        block = create_block(self.tip, create_coinbase(self.height), self.last_block_time + 1)\n        block.nVersion = 5\n        block.vtx.append(spendtx)\n        block.hashMerkleRoot = block.calc_merkle_root()\n        block.rehash()\n        block.solve()\n        self.last_block_time += 1\n        yield TestInstance([[block, False]])\n\n        # Restart all\n        stop_nodes(self.nodes)\n        wait_deuscoinds()\n        shutil.rmtree(self.options.tmpdir)\n        self.setup_chain()\n        self.setup_network()\n        self.test.clear_all_connections()\n        self.test.add_all_connections(self.nodes)\n        NetworkThread().start() # Start up network handling in another thread\n\n\n\n    def get_tests(self):\n        for test in itertools.chain(\n                self.test_BIP('csv', 536870913, self.sequence_lock_invalidate, self.donothing),\n                self.test_BIP('csv', 536870913, self.mtp_invalidate, self.donothing),\n                self.test_BIP('csv', 536870913, self.donothing, self.csv_invalidate)\n        ):\n            yield test\n\n    def donothing(self, tx):\n        return\n\n    def csv_invalidate(self, tx):\n        '''Modify the signature in vin 0 of the tx to fail CSV\n        Prepends -1 CSV DROP in the scriptSig itself.\n        '''\n        tx.vin[0].scriptSig = CScript([OP_1NEGATE, OP_NOP3, OP_DROP] +\n                                      list(CScript(tx.vin[0].scriptSig)))\n\n    def sequence_lock_invalidate(self, tx):\n        '''Modify the nSequence to make it fails once sequence lock rule is activated (high timespan)\n        '''\n        tx.vin[0].nSequence = 0x00FFFFFF\n        tx.nLockTime = 0\n\n    def mtp_invalidate(self, tx):\n        '''Modify the nLockTime to make it fails once MTP rule is activated\n        '''\n        # Disable Sequence lock, Activate nLockTime\n        tx.vin[0].nSequence = 0x90FFFFFF\n        tx.nLockTime = self.last_block_time\n\nif __name__ == '__main__':\n    BIP9SoftForksTest().main()", "original_line_number": 14044, "instruction_length": 653}
{"instruction": "Create a Python script that includes the following functions:\n- mean_accuracy(groundtruth, predictions): calculates the mean accuracy between the groundtruth and predictions using the confusion matrix from sklearn.metrics.\n- ensure_dir(address): checks if the directory at the given address exists, and creates it if it doesn't.\n- draw_bbox(img, bbox, color=100, width=2): draws a rectangle with the given color and width around the bounding box (bbox) on the given image (img).\n- get_rect(img, rect_info): returns the rectangular region of the image (img) defined by the coordinates (xmin, xmax, ymin, ymax) in rect_info.\n- get_rect_from_bbox(img, bbox): returns the rectangular region of the image (img) defined by the bounding box (bbox).\n- l2_feat_norm(feat): normalizes the feature vector (feat) using L2 normalization.", "input": "", "output": "import numpy\nimport sklearn.metrics\nimport os\nimport cv2\nimport numpy as np\n\n\ndef mean_accuracy(groundtruth, predictions):\n    groundtruth_cm = sklearn.metrics.confusion_matrix(groundtruth, groundtruth).astype(numpy.float32)\n    predictions_cm = sklearn.metrics.confusion_matrix(predictions, groundtruth).astype(numpy.float32)\n    return numpy.mean(numpy.diag(predictions_cm) / numpy.diag(groundtruth_cm))\n\n\ndef ensure_dir(address):\n    if not os.path.exists(address):\n        os.makedirs(address)\n\n\ndef draw_bbox(img, bbox, color=100, width=2):\n    try:\n        bx, by, bw, bh = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n    except:\n        bx, by, bw, bh = bbox\n    new_img = img.copy()\n    cv2.rectangle(new_img, (bx, by), (bx+bw, by+bh), color, width)\n    return new_img\n\n\ndef get_rect(img, rect_info):\n    xmin, xmax, ymin, ymax = rect_info\n    return img[xmin:xmax, ymin:ymax]\n\n\ndef get_rect_from_bbox(img, bbox):\n    by, bx, bw, bh = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n\n    return img[bx:bx+bh, by:by+bw]\n\n\ndef l2_feat_norm(feat):\n    row_norms = np.linalg.norm(feat, axis=1)\n    new_feat = feat / row_norms[:, np.newaxis]\n    return new_feat", "original_line_number": 14205, "instruction_length": 824}
{"instruction": "The given code is a Python class called `isoMeasurer` that inherits from another class called `Measurer`. It contains methods for making isophotal measurements on astronomical images, including getting the file path for the contours of the isophotes, making measurements on a map and writing them to a CSV file, making a visual panel to display the composite image and the isophotal measurements, and getting the contours from an image. The class also contains some helper methods for converting between different units and for reading and writing Python pickle files. To use this code, create an instance of the `isoMeasurer` class and call its methods with appropriate parameters.", "input": "", "output": "# isomeasurer.py \n# ALS 2017/06/01\nimport os\nimport astropy.units as u\nfrom astropy.io import fits\nimport numpy as np\nimport astropy.table as at\nimport pickle\nimport scipy.ndimage as simg\n\n\nfrom ..measurer import Measurer\nfrom ... import tabtools\nfrom . import polytools\nfrom . import plottools\n\nclass isoMeasurer(Measurer):\n\n\tdef __init__(self, **kwargs):\n\t\t\"\"\"\n\t\tchild of Measurer\n\t\tdo isophotal measurements\n\t\t\"\"\"\n\t\tsuper(isoMeasurer, self).__init__(**kwargs)\n\n\t\tself.msrtype = 'iso'\n\n\n\tdef get_fp_contours(self, imgtag='OIII5008_I', onlycenter=False, suffix=''):\n\t\t\"\"\" e.g., msr_iso-OIII5008_I{suffix}_contours.pkl \n\t\t\t\\or   msr_iso-OIII5008_I{suffix}_contours-ctr.pkl \n\t\t\"\"\"\n\t\tif onlycenter:\n\t\t\tctrtag = '-ctr'\n\t\telse:\n\t\t\tctrtag = ''\n\n\t\tfp_root = self.get_fp_msrtagroot(imgtag=imgtag, suffix=suffix)\n\t\treturn fp_root+'_contours{ctrtag}.pkl'.format(ctrtag=ctrtag)\n\n\n\tdef make_measurements(self, imgtag='OIII5008_I', isocut=3.e-15*u.Unit('erg / (arcsec2 cm2 s)'), minarea=5, onlycenter=True, centerradius=5.*u.arcsec, plotsuffix='', savecontours=False, plotmsr=False, msrsuffix='', overwrite=False, append=False):\n\t\t\"\"\"\n\t\tmake measurements on a map and write to msr_iso.csv. \n\t\t\tif imgtag='OIII5008_I' then measure 'stamp-OIII5008_I.fits'\n\n\t\tParams\n\t\t------\n\t\tself\n\t\timgtag='OIII5008_I'\n\t\toverwrite = False (bool)\n\n\t\tisocut=1.e-15*u.Unit('erg / (arcsec2 cm2 s)'):\n\t\t\tisophote cut\n\t\tminarea=0:\n\t\t\tconnected contour area (# pix) above the area is counted as part of the isophote measurement\n\t\tonlycenter=False:\n\t\t\twhether to consider only the center contours\n\t\tcenterradius=2.*u.arcsec\n\t\tplotsuffix = '':\n\t\t\tplotsuffix label to be attach to the end of the plot or contour file names. \n\t\tsavecontours=False\n\t\tplotmsr=False\n\t\tmsrsuffix=''\n\t\t\tplotsuffix label in the end of the measurement csv file: msr_iso_{msrsuffix}.csv.\n\t\toverwrite=False\n\t\tappend=False\n\n\t\tReturn\n\t\t------\n\t\tstatus (bool)\n\n\t\tWrite Output \n\t\t------------\n\t\te.g., msr_iso.csv\n\t\t\"\"\"\n\t\tfn = self.get_fp_msr(msrsuffix=msrsuffix)\n\n\t\tcondi = {'imgtag': imgtag, 'isocut': isocut, 'minarea': minarea, 'onlycenter': onlycenter, 'centerradius': centerradius}\n\n\t\tif append or overwrite or (not tabtools.fn_has_row(fn, condi)):\n\t\t\tprint(\"[isomeasurer] making measurement\")\n\n\t\t\timg = self.get_stamp_img(imgtag=imgtag, wunit=True)\n\t\t\txc, yc = self._get_xc_yc(img)\n\n\t\t\t# calc\n\t\t\tif np.all(~np.isnan(img)): \n\t\t\t\tcontours = self._get_contours_from_img(img=img, isocut=isocut, xc=xc, yc=yc, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius)\n\t\t\t\ttab_msr = self._get_tab_measurements_from_contours(contours=contours, xc=xc, yc=yc)\n\t\t\telse: \n\t\t\t\tcontours = []\n\t\t\t\ttab_msr = self._get_tab_measurements_nan()\n\n\t\t\ttab_params = self._get_tab_params(imgtag=imgtag, isocut=isocut, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius)\n\t\t\ttabout = at.hstack([tab_params, tab_msr])\n\n\t\t\t# output\n\t\t\ttabtools.write_row(fn=fn, row=tabout, condi=condi, overwrite=overwrite, append=append)\n\n\t\t\t# optional output\n\t\t\tif savecontours:\n\t\t\t\tfn_contours = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=plotsuffix)\n\t\t\t\twrite_pickle(contours, fn_contours, overwrite=overwrite)\n\n\t\t\tif plotmsr:\n\t\t\t\tfn_plot = self.get_fp_msrplot(imgtag=imgtag, suffix=plotsuffix)\n\t\t\t\tplottools.make_plot_img_w_contours(fn_plot=fn_plot, img=img, contours=contours)\n\n\t\telse:\n\t\t\tprint(\"[isomeasurer] skip making measurement as files exist\")\n\n\t\treturn os.path.isfile(fn)\n\n\n\tdef make_visualpanel(self, fn=None, compo_bands ='gri', imgtag='OIII5008_I', onlycenter=True, minarea=5, centerradius=5.*u.arcsec, tocolorbar=True, totitle=True, fontsize=12, overwrite=False):\n\t\t\"\"\" \n\t\tmake panel figure to visualize the composit and the iso measurements\n\t\tsaved to e.g., 'msr_iso-OIII5008_I_panel.pdf'\n\n\t\tParams\n\t\t------\n\t\tfn = None: default: msr_iso_{imgtag}_panel.pdf\n\t\tcompo_bands ='gri', imgtag='OIII5008_I', overwrite=False\n\n\t\tReturn\n\t\t------\n\t\tstatus\n\t\t\"\"\"\n\t\tif fn is None:\n\t\t\tfn = self.get_fp_msrplot(imgtag=imgtag, suffix='_panel')\n\t\telse: \n\t\t\tfn = self.dir_obj+fn\n\n\t\tif not os.path.isfile(fn) or overwrite:\n\t\t\tprint(\"[isomeasurer] making visual panel\")\n\n\t\t\t# get files ready \n\t\t\tself.make_colorimg(bands=compo_bands, img_type='stamp', overwrite=False)\n\n\t\t\t# access data\n\t\t\timg_compo = simg.imread(self.dir_obj+'color_stamp-{}.png'.format(compo_bands))\n\t\t\timg_map = self.get_stamp_img(imgtag=imgtag, wunit=False)\n\n\t\t\tsuffix = '_3e-15'\n\t\t\tisocut = 3.e-15*u.Unit('erg / (arcsec2 cm2 s)')\n\t\t\tfn_contours3 = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=suffix)\n\t\t\tif not os.path.isfile(fn_contours3):\n\t\t\t\tprint(\"[isomeasurer] re-doing measurements to make contours required for visual panel plots\")\n\t\t\t\tself.make_measurements(imgtag=imgtag, isocut=isocut, plotsuffix=suffix, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius, overwrite=True, savecontours=True, plotmsr=False), \n\n\t\t\tcontours3 = read_pickle(fn_contours3)\n\n\t\t\tsuffix = '_1e-15'\n\t\t\tisocut = 1.e-15*u.Unit('erg / (arcsec2 cm2 s)')\n\t\t\tfn_contours1 = self.get_fp_contours(imgtag=imgtag, onlycenter=onlycenter, suffix=suffix)\n\t\t\tif not os.path.isfile(fn_contours1):\n\t\t\t\tprint(\"[isomeasurer] re-doing measurements to make contours required for visual panel plots\")\n\t\t\t\tself.make_measurements(imgtag=imgtag, isocut=isocut, plotsuffix=suffix, minarea=minarea, onlycenter=onlycenter, centerradius=centerradius, overwrite=True, savecontours=True, plotmsr=False), \n\n\t\t\tcontours1 = read_pickle(fn_contours1)\n\n\t\t\tz = self.z\n\t\t\tpixsize = self.pixsize.to_value(u.arcsec)\n\t\t\tlegend_suffix = ' at 3'\n\t\t\tname = self.obj.name[4:]\n\n\t\t\ttitle_compo = '${}~{}~{}~$'.format(compo_bands[0], compo_bands[1], compo_bands[2])+'$\\mathrm{Composite}$'\n\t\t\ttitle_map = '$\\mathrm{[OIII]\\lambda 5007~Intensity}$'\n\t\t\tlabel_cbar = '$I~[10^{-15}~\\mathrm{erg~s^{-1}~cm^{-2}~arcsec^{-2}}]$'\n\n\t\t\tplottools.make_iso_visual_panel(fn, img_compo, img_map, contours1, contours3, z, pixsize, legend_suffix, name, title_compo, title_map, label_cbar, tocolorbar=tocolorbar, totitle=totitle, fontsize=fontsize)\n\n\t\telse:\n\t\t\tprint(\"[isomeasurer] skip making visual panel as files exist\")\n\n\t\treturn os.path.isfile(fn)\n\n\t\t\n\tdef _get_tab_params(self, imgtag, isocut, minarea, onlycenter, centerradius):\n\t\t\"\"\"\n\t\treturn a one row table of the measurement params \n\t\t\"\"\"\n\t\ttab = at.Table([[imgtag], [str(isocut)], [minarea], [onlycenter], [str(centerradius)], ], names=['imgtag', 'isocut', 'minarea', 'onlycenter', 'centerradius', ])\n\t\treturn tab\n\n\n\tdef _get_tab_measurements_from_contours(self, contours, xc, yc):\n\t\t\"\"\" \n\t\tcalculate iso measurements from contours, return a table like: \n\t\t\"\"\"\n\n\t\ttab = polytools.ShapeParamsTab_from_contours(contours, xc, yc)\n\n\t\t# unit conversion\n\t\tarea_ars = tab['area_pix'][0]*(self.pixsize/u.arcsec)**2\n\t\tdmax_ars = self._pix_to_theta(tab['dmax_pix'][0], wunit=False)\n\t\trmax_ars = self._pix_to_theta(tab['rmax_pix'][0], wunit=False)\n\t\tdper_ars = self._pix_to_theta(tab['dper_pix'][0], wunit=False)\n\n\t\tkpc_per_arcsec = np.array(self._get_kpc_proper_per_arcsec())\n\n\t\tarea_kpc = area_ars * kpc_per_arcsec**2\n\t\tdmax_kpc = dmax_ars * kpc_per_arcsec\n\t\trmax_kpc = rmax_ars * kpc_per_arcsec\n\t\tdper_kpc = dper_ars * kpc_per_arcsec\n\n\t\ttab_converted = at.Table(names=['area_kpc', 'dmax_kpc', 'rmax_kpc', 'dper_kpc', 'area_ars', 'dmax_ars', 'rmax_ars', 'dper_ars', ])\n\t\ttab_converted.add_row([area_kpc, dmax_kpc, rmax_kpc, dper_kpc, area_ars, dmax_ars, rmax_ars, dper_ars, ])\n\n\t\ttabout = at.hstack([tab_converted, tab])\n\n\t\treturn tabout\n\n\n\tdef _get_tab_measurements_nan(self):\n\t\t\"\"\" \n\t\treturn a tab measurement just like _get_tab_measurements_from_contours() but with entries all nan. \n\t\t\"\"\"\n\t\tnames = ['area_kpc', 'dmax_kpc', 'rmax_kpc', 'dper_kpc', 'area_ars', 'dmax_ars', 'rmax_ars', 'dper_ars', 'area_pix', 'dmax_pix', 'rmax_pix', 'dper_pix', 'theta_dmax', 'theta_rmax', 'theta_dper', 'aspectr']\n\n\t\ttabout = at.Table(names=names)\n\t\ttabout.add_row([np.nan for i in range(len(names))])\n\n\t\treturn tabout\n\n\n\tdef _get_contours_from_img(self, img, isocut, xc, yc, minarea=0., onlycenter=False, centerradius=2.*u.arcsec):\n\t\t\"\"\"\n\t\tmake contour at isocut of image as python pickle file (fn_contours)\n\t\talways overwrite\n\n\t\tParams\n\t\t------\n\t\tself\n\t\timg (array)\n\t\tisocut (float or quantity):\n\t\t\thas to be of the same type of unit as image\n\t\tminarea (float):\n\t\t\tminimum area (pix) to be considered as contour patch\n\t\tonlycenter (bool):\n\t\t\twhether to take only center patches as patches (they all have to pass minarea test as well)\n\t\tcenterradius (angular quantity):\n\t\t\tif onlycenter = True, then it sets the radius of the center area. only patches overlapping with that area will be considered. \n\n\t\t\"\"\"\n\n\t\t# prep\n\t\ttry: \n\t\t\timg.unit\n\t\texcept:\n\t\t\timg_nparr = img/isocut\n\t\telse:\n\t\t\timg_nparr = np.array((img/isocut).to(u.dimensionless_unscaled))\n\n\t\t# find contours -- satisfy minarea\n\t\tcontours = polytools.find_largecontours(img=img_nparr, threshold=1., minarea=minarea)\n\n\t\tif onlycenter:  # select only those at the center\n\t\t\tcenterradius_pix = self._theta_to_pix(centerradius)\n\t\t\tcontours = polytools.select_center_contours(contours, xc, yc, radius=centerradius_pix)\n\n\t\treturn contours\n\n\n\ndef read_pickle(fn):\n\twith open(fn, 'rb') as handle:\n\t\tresult = pickle.load(handle)\n\treturn result\n\n\ndef write_pickle(result, fn, overwrite=False):\n\n\tif not os.path.isfile(fn) or overwrite:\n\t\twith open(fn, 'wb') as handle:\n\t\t\tpickle.dump(result, handle)", "original_line_number": 14310, "instruction_length": 682}
{"instruction": "This code configures an MML file with datasource settings. To use it, you need to run the script with the following arguments: \n\n--host: the host name (default is 'localhost')\n--port: the port number (default is '5432')\n--dbname: the database name (default is 'abs')\n--user: the user name (default is 'abs')\n--password: the password (default is 'abs')\n--srs: the spatial reference system (default is '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0.0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over')\n--shapedir: the directory where the shapefiles are located (default is './layers/')\n--extent: the bounding box to restrict queries (default is '12570320.00,-5403474.50,17711958.00,-1636391.88')\n--mml: the name of the MML file to be configured (required)\n\nThe script will modify the MML file by updating the datasource settings for layers that use a postgis datasource and adding datasource settings for layers that use shapefiles. The modified MML file will be saved in the same location as the original file.", "input": "", "output": "#!/usr/bin/env python\n\n# Copyright (c) 2011, Development Seed, Inc.\n#               2011, Andrew Harvey <andrew.harvey4@gmail.com>\n#               All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in\n#       the documentation and/or other materials provided with the\n#       distribution.\n#     * Neither the name of the Development Seed, Inc. nor the names of\n#       its contributors may be used to endorse or promote products\n#       derived from this software without specific prior written\n#       permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n# IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n# TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n# PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER\n# OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nimport json\nfrom sys import path\nfrom os.path import join\nimport argparse\n\n#################################\n## argparse\n\nparser = argparse.ArgumentParser(description='Configure an MML file with datasource settings')\nparser.add_argument('--host', default='localhost')\nparser.add_argument('--port', default='5432')\nparser.add_argument('--dbname', default='abs')\nparser.add_argument('--user', default='abs')\nparser.add_argument('--password', default='abs')\nparser.add_argument('--srs', default='+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0.0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over')\n\nparser.add_argument('--shapedir', default='./layers/')\n\n# Increase performance if you are only rendering a particular area by\n# specifying a bounding box to restrict queries. Format is \"XMIN,YMIN,XMAX,YMAX\" in the\n# same units as the database (probably spherical mercator meters). The\n# whole world is \"-20037508.34,-20037508.34,20037508.34,20037508.34\".\n# Leave blank to let Mapnik estimate.\nparser.add_argument('--extent', default='12570320.00,-5403474.50,17711958.00,-1636391.88')\n\nparser.add_argument('--mml', required=True)\n\nargs = parser.parse_args()\n\n#################################\n## configure mml\n\nmml = join(path[0], args.mml + '/' + args.mml + '.mml')\n\nshoreline_300 = args.shapedir.rstrip('/') + '/shoreline_300.shp'\nprocessed_p = args.shapedir.rstrip('/') + '/processed_p.shp'\n\nwith open(mml, 'r') as f:\n  newf = json.loads(f.read())\nf.closed\n\nwith open(mml, 'w') as f:\n  for layer in newf[\"Layer\"]:\n    if \"Datasource\" in layer:\n      ds_type = layer[\"Datasource\"].get(\"type\")\n      if ds_type and ds_type == \"postgis\":\n        layer[\"Datasource\"][\"host\"] = args.host\n        layer[\"Datasource\"][\"port\"] = args.port\n        layer[\"Datasource\"][\"dbname\"] = args.dbname\n        layer[\"Datasource\"][\"user\"] = args.user\n        layer[\"Datasource\"][\"password\"] = args.password\n        layer[\"Datasource\"][\"extent\"] = args.extent\n        layer[\"srs\"] = args.srs\n    else:\n      if layer[\"id\"] == \"shoreline_300\":\n        layer[\"Datasource\"] = dict();\n        layer[\"Datasource\"][\"file\"] = shoreline_300\n        layer[\"Datasource\"][\"type\"] = 'shape'\n        layer[\"geometry\"] = 'polygon'\n        layer[\"srs\"] = '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over'\n      elif layer[\"id\"] == \"processed_p\":\n        layer[\"Datasource\"] = dict();\n        layer[\"Datasource\"][\"file\"] = processed_p\n        layer[\"Datasource\"][\"type\"] = 'shape'\n        layer[\"geometry\"] = 'polygon'\n        layer[\"srs\"] = '+proj=merc +a=6378137 +b=6378137 +lat_ts=0.0 +lon_0=0.0 +x_0=0.0 +y_0=0 +k=1.0 +units=m +nadgrids=@null +wktext +no_defs +over'\n  f.write(json.dumps(newf, indent=2))\nf.closed", "original_line_number": 14318, "instruction_length": 1041}
{"instruction": "This code defines a function called `parse_source` that takes a string as input and returns a `Channel` named tuple. The `Channel` tuple has three fields: `type`, `keywords`, and `branded`. The `type` field is an integer that represents the type of channel (1 for DIRECT, 2 for PPC, and 3 for SEO). The `keywords` field is a string that represents the keywords associated with the channel. The `branded` field is a boolean that indicates whether the keywords are branded or not. The function first checks if the input string starts with \"ppc\" or \"seo\" and sets the `type` field accordingly. If the input string does not start with \"ppc\" or \"seo\", the `type` field is set to 1 (DIRECT). The function then calls either `get_ppc_keywords` or `get_seo_keywords` to extract the keywords from the input string, depending on the value of the `type` field. Finally, the function calls `is_branded` to determine whether the keywords are branded or not and returns the `Channel` tuple.", "input": "", "output": "'''\n * Author: Lukasz Jachym\n * Date: 9/14/13\n * Time: 5:40 PM\n *\n * This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.\n * To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/.\n'''\n\nfrom collections import namedtuple\nfrom branding import is_branded\n\nDIRECT = 1\nPPC = 2\nSEO = 3\n\nChannel = namedtuple('Channel', ['type', 'keywords', 'branded'])\n\ndef get_ppc_keywords(ppc_str):\n    \"\"\"\n    :param ppc_str:\n    :return: keywords\n    \"\"\"\n\n    try:\n        keywords = ppc_str.split('_')[2]\n\n    except KeyError:\n        raise ValueError\n\n    return keywords\n\ndef get_seo_keywords(seo_query):\n    \"\"\"\n    :param ppc_str:\n    :return: keywords\n    \"\"\"\n\n    try:\n        keywords = seo_query.split(' :: ')[2]\n\n    except KeyError:\n        raise ValueError\n\n    return keywords\n\ndef parse_source(source_str):\n\n    if (source_str[0:3] == 'ppc'):\n        channel_type = PPC\n        keywords = get_ppc_keywords(source_str)\n        channel_keywords = keywords\n        channel_branded = is_branded(keywords)\n        channel = Channel(channel_type, channel_keywords, channel_branded)\n    else:\n        if source_str[0:3] == 'seo':\n            channel_type = SEO\n            keywords = get_seo_keywords(source_str)\n            channel_keywords = keywords\n            channel_branded = is_branded(keywords)\n            channel = Channel(channel_type, channel_keywords, channel_branded)\n        else:\n            channel_type = DIRECT\n            channel_keywords = None\n            channel_branded = False\n            channel = Channel(channel_type, channel_keywords, channel_branded)\n\n    return channel", "original_line_number": 14586, "instruction_length": 975}
{"instruction": "This code defines a Keyboard class that can simulate keyboard events on a Mac. To use it, create an instance of the Keyboard class and call its press_key and release_key methods with the name of the key you want to simulate. The special_key_assignment method sets up some common special keys to make them easier to use. The KeyboardEvent class sets up a listener for keyboard events and calls the appropriate methods on the Keyboard class when events occur. To use it, create an instance of the KeyboardEvent class and call its run method to start listening for events.", "input": "", "output": "#Copyright 2013 Paul Barton\n#\n#This program is free software: you can redistribute it and/or modify\n#it under the terms of the GNU General Public License as published by\n#the Free Software Foundation, either version 3 of the License, or\n#(at your option) any later version.\n#\n#This program is distributed in the hope that it will be useful,\n#but WITHOUT ANY WARRANTY; without even the implied warranty of\n#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#GNU General Public License for more details.\n#\n#You should have received a copy of the GNU General Public License\n#along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nimport time\nfrom Quartz import *\nfrom AppKit import NSEvent\nfrom .base import KeyboardMeta, KeyboardEventMeta\n\n# Taken from events.h\n# /System/Library/Frameworks/Carbon.framework/Versions/A/Frameworks/HIToolbox.framework/Versions/A/Headers/Events.h\ncharacter_translate_table = {\n    'a': 0x00,\n    's': 0x01,\n    'd': 0x02,\n    'f': 0x03,\n    'h': 0x04,\n    'g': 0x05,\n    'z': 0x06,\n    'x': 0x07,\n    'c': 0x08,\n    'v': 0x09,\n    'b': 0x0b,\n    'q': 0x0c,\n    'w': 0x0d,\n    'e': 0x0e,\n    'r': 0x0f,\n    'y': 0x10,\n    't': 0x11,\n    '1': 0x12,\n    '2': 0x13,\n    '3': 0x14,\n    '4': 0x15,\n    '6': 0x16,\n    '5': 0x17,\n    '=': 0x18,\n    '9': 0x19,\n    '7': 0x1a,\n    '-': 0x1b,\n    '8': 0x1c,\n    '0': 0x1d,\n    ']': 0x1e,\n    'o': 0x1f,\n    'u': 0x20,\n    '[': 0x21,\n    'i': 0x22,\n    'p': 0x23,\n    'l': 0x25,\n    'j': 0x26,\n    '\\'': 0x27,\n    'k': 0x28,\n    ';': 0x29,\n    '\\\\': 0x2a,\n    ',': 0x2b,\n    '/': 0x2c,\n    'n': 0x2d,\n    'm': 0x2e,\n    '.': 0x2f,\n    '`': 0x32,\n    ' ': 0x31,\n    '\\r': 0x24,\n    '\\t': 0x30,\n    'shift': 0x38\n}\n\n# Taken from ev_keymap.h\n# http://www.opensource.apple.com/source/IOHIDFamily/IOHIDFamily-86.1/IOHIDSystem/IOKit/hidsystem/ev_keymap.h\nspecial_key_translate_table = {\n    'KEYTYPE_SOUND_UP': 0,\n    'KEYTYPE_SOUND_DOWN': 1,\n    'KEYTYPE_BRIGHTNESS_UP': 2,\n    'KEYTYPE_BRIGHTNESS_DOWN': 3,\n    'KEYTYPE_CAPS_LOCK': 4,\n    'KEYTYPE_HELP': 5,\n    'POWER_KEY': 6,\n    'KEYTYPE_MUTE': 7,\n    'UP_ARROW_KEY': 8,\n    'DOWN_ARROW_KEY': 9,\n    'KEYTYPE_NUM_LOCK': 10,\n    'KEYTYPE_CONTRAST_UP': 11,\n    'KEYTYPE_CONTRAST_DOWN': 12,\n    'KEYTYPE_LAUNCH_PANEL': 13,\n    'KEYTYPE_EJECT': 14,\n    'KEYTYPE_VIDMIRROR': 15,\n    'KEYTYPE_PLAY': 16,\n    'KEYTYPE_NEXT': 17,\n    'KEYTYPE_PREVIOUS': 18,\n    'KEYTYPE_FAST': 19,\n    'KEYTYPE_REWIND': 20,\n    'KEYTYPE_ILLUMINATION_UP': 21,\n    'KEYTYPE_ILLUMINATION_DOWN': 22,\n    'KEYTYPE_ILLUMINATION_TOGGLE': 23\n}\n\nclass Keyboard(KeyboardMeta):\n    def press_key(self, key):\n        if key in special_key_translate_table:\n            self._press_special_key(key, True)\n        else:\n            self._press_normal_key(key, True)\n\n    def release_key(self, key):\n        if key in special_key_translate_table:\n            self._press_special_key(key, False)\n        else:\n            self._press_normal_key(key, False)\n\n    def special_key_assignment(self):\n        self.volume_mute_key = 'KEYTYPE_MUTE'\n        self.volume_down_key = 'KEYTYPE_SOUND_DOWN'\n        self.volume_up_key = 'KEYTYPE_SOUND_UP'\n        self.media_play_pause_key = 'KEYTYPE_PLAY'\n\n        # Doesn't work :(\n        # self.media_next_track_key = 'KEYTYPE_NEXT'\n        # self.media_prev_track_key = 'KEYTYPE_PREVIOUS'\n\n    def _press_normal_key(self, key, down):\n        try:\n            if self.is_char_shifted(key):\n                key_code = character_translate_table[key.lower()]\n\n                event = CGEventCreateKeyboardEvent(None, \n                            character_translate_table['shift'], down)\n                CGEventPost(kCGHIDEventTap, event)\n                # Tiny sleep to let OS X catch up on us pressing shift\n                time.sleep(.01)\n\n            else:\n                key_code = character_translate_table[key]\n\n\n            event = CGEventCreateKeyboardEvent(None, key_code, down)\n            CGEventPost(kCGHIDEventTap, event)\n\n\n        except KeyError:\n            raise RuntimeError(\"Key {} not implemented.\".format(key))\n\n    def _press_special_key(self, key, down):\n        \"\"\" Helper method for special keys. \n\n        Source: http://stackoverflow.com/questions/11045814/emulate-media-key-press-on-mac\n        \"\"\"\n        key_code = special_key_translate_table[key]\n\n        ev = NSEvent.otherEventWithType_location_modifierFlags_timestamp_windowNumber_context_subtype_data1_data2_(\n                NSSystemDefined, # type\n                (0,0), # location\n                0xa00 if down else 0xb00, # flags\n                0, # timestamp\n                0, # window\n                0, # ctx\n                8, # subtype\n                (key_code << 16) | ((0xa if down else 0xb) << 8), # data1\n                -1 # data2\n            )\n\n        CGEventPost(0, ev.CGEvent())\n\nclass KeyboardEvent(KeyboardEventMeta):\n    def run(self):\n        tap = CGEventTapCreate(\n            kCGSessionEventTap,\n            kCGHeadInsertEventTap,\n            kCGEventTapOptionDefault,\n            CGEventMaskBit(kCGEventKeyDown) |\n            CGEventMaskBit(kCGEventKeyUp),\n            self.handler,\n            None)\n\n        loopsource = CFMachPortCreateRunLoopSource(None, tap, 0)\n        loop = CFRunLoopGetCurrent()\n        CFRunLoopAddSource(loop, loopsource, kCFRunLoopDefaultMode)\n        CGEventTapEnable(tap, True)\n\n        while self.state:\n            CFRunLoopRunInMode(kCFRunLoopDefaultMode, 5, False)\n\n    def handler(self, proxy, type, event, refcon):\n        key = CGEventGetIntegerValueField(event, kCGKeyboardEventKeycode)\n        if type == kCGEventKeyDown:\n            self.key_press(key)\n        elif type == kCGEventKeyUp:\n            self.key_release(key)\n\n        if self.capture:\n            CGEventSetType(event, kCGEventNull)\n\n        return event", "original_line_number": 14922, "instruction_length": 569}
{"instruction": "This code generates connector models using CadQuery. To run the script, ensure that freecad (v1.5 and v1.6 have been tested) and cadquery FreeCAD plugin (v0.3.0 and v0.2.0 have been tested) are installed. To generate VRML/STEP files, use the export_conn_jst_xh script of the parent directory. The script generates models for the 4UCON 17809 series connectors. To generate a specific part, set the part_to_build variable to the desired part key. The generate_part function generates the pins, body, and contacts for the specified part key. The pins, body, and contacts can be displayed using the show function.", "input": "", "output": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# CadQuery script to generate connector models\n\n## requirements\n## freecad (v1.5 and v1.6 have been tested)\n## cadquery FreeCAD plugin (v0.3.0 and v0.2.0 have been tested)\n##   https://github.com/jmwright/cadquery-freecad-module\n\n## This script can be run from within the cadquery module of freecad.\n## To generate VRML/ STEP files for, use export_conn_jst_xh\n## script of the parent directory.\n\n#* This is a cadquery script for the generation of MCAD Models.             *\n#*                                                                          *\n#*   Copyright (c) 2016                                                     *\n#* Rene Poeschl https://github.com/poeschlr                                 *\n#* All trademarks within this guide belong to their legitimate owners.      *\n#*                                                                          *\n#*   This program is free software; you can redistribute it and/or modify   *\n#*   it under the terms of the GNU General Public License (GPL)             *\n#*   as published by the Free Software Foundation; either version 2 of      *\n#*   the License, or (at your option) any later version.                    *\n#*   for detail see the LICENCE text file.                                  *\n#*                                                                          *\n#*   This program is distributed in the hope that it will be useful,        *\n#*   but WITHOUT ANY WARRANTY; without even the implied warranty of         *\n#*   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the          *\n#*   GNU Library General Public License for more details.                   *\n#*                                                                          *\n#*   You should have received a copy of the GNU Library General Public      *\n#*   License along with this program; if not, write to the Free Software    *\n#*   Foundation, Inc.,                                                      *\n#*   51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA           *\n#*                                                                          *\n#* The models generated with this script add the following exception:       *\n#*   As a special exception, if you create a design which uses this symbol, *\n#*   and embed this symbol or unaltered portions of this symbol into the    *\n#*   design, this symbol does not by itself cause the resulting design to   *\n#*   be covered by the GNU General Public License. This exception does not  *\n#*   however invalidate any other reasons why the design itself might be    *\n#*   covered by the GNU General Public License. If you modify this symbol,  *\n#*   you may extend this exception to your version of the symbol, but you   *\n#*   are not obligated to do so. If you do not wish to do so, delete this   *\n#*   exception statement from your version.                                 *\n#****************************************************************************\n\n__title__ = \"model description for 4UCON 17809 series connectors\"\n__author__ = \"hackscribble\"\n__Comment__ = 'model description for 4UCON 17809 series connectors using cadquery'\n\n___ver___ = \"0.3 18/06/2020\"\n\n\nimport cadquery as cq\nfrom Helpers import show\nfrom collections import namedtuple\nimport FreeCAD\nfrom conn_4ucon_17809_params import *\n\nfrom ribbon import Ribbon\n\n\ndef generate_straight_pin(params, pin_1_side):\n    foot_height = seriesParams.foot_height\n    pin_width=seriesParams.pin_width\n    pin_depth=seriesParams.pin_depth\n    pin_height=seriesParams.pin_height\n    pin_inside_distance=seriesParams.pin_inside_distance\n    pin_thickness = seriesParams.pin_thickness\n    chamfer_long = seriesParams.pin_chamfer_long\n    chamfer_short = seriesParams.pin_chamfer_short\n    sign = 1 if pin_1_side else -1\n    pin=cq.Workplane(\"YZ\").workplane(offset=-pin_width/2.0)\\\n        .moveTo(0, foot_height)\\\n        .line(sign*pin_thickness/2,0)\\\n        .line(sign*1.27,-foot_height)\\\n        .line(0, -2.54)\\\n        .line(sign*-pin_thickness,0)\\\n        .line(0, 2.54)\\\n        .line(sign*-1.27, foot_height)\\\n        .line(0,1)\\\n        .close()\\\n        .extrude(pin_width).edges(\"|X\").fillet(0.07)\n    return pin\n\n\ndef generate_2_pin_group(params, pin_1_side):\n    pin_pitch=params.pin_pitch\n    pin_y_pitch=params.pin_y_pitch\n    num_pins=params.num_pins\n    pin_a = generate_straight_pin(params, pin_1_side).translate((0, -pin_y_pitch/2, 0))\n    pin_b = pin_a.translate((0, -2 * pin_y_pitch, 0))\n    pin_group = pin_a.union(pin_b)\n    return pin_group\n\n\ndef generate_pins(params):\n    pin_pitch=params.pin_pitch\n    num_pins=params.num_pins\n    pins = generate_2_pin_group(params, pin_1_side=True)\n    for i in range(1, num_pins // 2):\n        pins = pins.union(generate_2_pin_group(params, i % 2 == 0).translate((i*pin_pitch,0,0)))\n    return pins\n\n\ndef generate_2_contact_group(params):\n    pin_y_pitch=params.pin_y_pitch\n    foot_height = seriesParams.foot_height\n    pin_thickness = seriesParams.pin_thickness\n    pin_width=seriesParams.pin_width\n    y_offset = -(2*pin_y_pitch)\n    c_list = [\n        ('start', {'position': (pin_y_pitch, foot_height), 'direction': 90.0, 'width':pin_thickness}),\n        ('line', {'length': 4.5}),\n        ('arc', {'radius': 0.2, 'angle': 35.0}),\n        ('line', {'length': 3}),\n        ('arc', {'radius': 2.0, 'angle': -70.0}),\n        ('line', {'length': 2}),\n        ('arc', {'radius': 0.2, 'angle': 35.0}),\n        ('line', {'length': 2.8}),\n    ]\n    ribbon = Ribbon(cq.Workplane(\"YZ\").workplane(offset=-pin_width/2.0), c_list)\n    contact1 = ribbon.drawRibbon().extrude(pin_width)\n    contact2 = contact1.mirror(\"XZ\")\n    contact1 = contact1.union(contact2).translate((0,-3*pin_y_pitch/2.0,0))\n    return contact1\n\n\ndef generate_contacts(params):\n    num_pins=params.num_pins\n    pin_pitch=params.pin_pitch\n    pair = generate_2_contact_group(params)\n    contacts = pair\n    for i in range(0, num_pins // 2):\n        contacts = contacts.union(pair.translate((i*pin_pitch,0,0)))\n    return contacts\n\n\ndef generate_body(params, calc_dim):\n    pin_inside_distance = seriesParams.pin_inside_distance\n    pin_width = seriesParams.pin_width\n    num_pins = params.num_pins\n    pin_pitch = params.pin_pitch\n    pin_y_pitch=params.pin_y_pitch\n\n    body_length = calc_dim.length\n    body_width = seriesParams.body_width\n    body_height = seriesParams.body_height\n    body_fillet_radius = seriesParams.body_fillet_radius\n\n    marker_x_inside = seriesParams.marker_x_inside\n    marker_y_inside = seriesParams.marker_y_inside\n    marker_size = seriesParams.marker_size\n    marker_depth = seriesParams.marker_depth\n\n    foot_height = seriesParams.foot_height\n    foot_width = seriesParams.foot_width\n    foot_length = seriesParams.foot_length\n    foot_inside_distance = seriesParams.foot_inside_distance\n\n    slot_length = calc_dim.slot_length\n    slot_outside_pin = seriesParams.slot_outside_pin\n    slot_width = seriesParams.slot_width\n    slot_depth = seriesParams.slot_depth\n    slot_chamfer = seriesParams.slot_chamfer\n\n    hole_width = seriesParams.hole_width\n    hole_length = seriesParams.hole_length\n    hole_offset = seriesParams.hole_offset\n    hole_depth = seriesParams.hole_depth\n\n    top_void_depth = seriesParams.top_void_depth\n    top_void_width = seriesParams.top_void_width\n    bottom_void_width = calc_dim.bottom_void_width\n\n    recess_depth = seriesParams.recess_depth\n    recess_large_width = seriesParams.recess_large_width\n    recess_small_width = seriesParams.recess_small_width\n    recess_height = seriesParams.recess_height\n\n    x_offset = (((num_pins // 2) - 1)*pin_pitch)/2.0\n    y_offset = -(1.5*pin_y_pitch)\n\n    # body\n    body = cq.Workplane(\"XY\").workplane(offset=foot_height).moveTo(x_offset, y_offset)\\\n        .rect(body_length, body_width).extrude(body_height)\\\n        .edges(\"|Z\").fillet(body_fillet_radius).edges(\">Z\").fillet(body_fillet_radius)\n\n    # pin 1 marker\n    body = body.faces(\">Z\").workplane().moveTo(-(body_length/2)+marker_x_inside, (body_width/2)-marker_y_inside)\\\n        .line(-marker_size,-marker_size/2).line(0, marker_size).close().cutBlind(-marker_depth)\n\n    # foot\n    foot = cq.Workplane(\"YZ\").workplane(offset=(body_length/2)-foot_inside_distance)\\\n        .moveTo(y_offset - foot_length/2, 0)\\\n        .line(foot_length*0.2,0)\\\n        .line(0,foot_height/2)\\\n        .line(foot_length*0.6,0)\\\n        .line(0,-foot_height/2)\\\n        .line(foot_length*0.2,0)\\\n        .line(0,foot_height)\\\n        .line(-foot_length,0)\\\n        .close()\\\n        .extrude(-foot_width)\n\n    foot_mirror = foot.mirror(\"YZ\")\n\n    foot = foot.union(foot_mirror).translate((x_offset, 0, 0))\n\n    body = body.union(foot)\n\n    # slot\n    body = body.faces(\">Z\").workplane().rect(slot_length, slot_width).cutBlind(-slot_depth)\n\n    chamfer = cq.Workplane(\"XY\").workplane(offset=foot_height+body_height).moveTo(x_offset, y_offset) \\\n    .rect(slot_length+2*slot_chamfer, slot_width+2*slot_chamfer) \\\n    .workplane(offset=-slot_chamfer).rect(slot_length, slot_width) \\\n    .loft(combine=True)\n\n    body = body.cut(chamfer)\n\n    # contact holes\n    body = body.faces(\">Z\").workplane().center(0, hole_offset)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, hole_length)\\\n        .center(0, -2*hole_offset)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, hole_length)\\\n        .cutBlind(-2)\n\n    # internal void\n    body = body.faces(\">Z\").workplane(offset=-hole_depth)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, top_void_width)\\\n        .cutBlind(-(top_void_depth-hole_depth))\n\n    body = body.faces(\">Z\").workplane(offset=-top_void_depth)\\\n        .rarray(pin_pitch, 1, (num_pins//2), 1).rect(hole_width, bottom_void_width)\\\n        .cutBlind(-(body_height-top_void_depth))\n\n    # body end recesses\n    body = body.faces(\">Z\").workplane().center(body_length/2.0-recess_depth/2.0, 0)\\\n        .rect(recess_depth, recess_small_width).cutBlind(-recess_height)\n\n    recess = cq.Workplane(\"XY\").workplane(offset=foot_height+body_height).center(x_offset-body_length/2.0+recess_depth/2.0, y_offset)\\\n        .rect(recess_depth, recess_large_width).extrude(-recess_height).edges(\">X\").edges(\"|Z\").fillet(0.3)\n\n    body = body.cut(recess)\n\n    return body\n\n\ndef generate_part(part_key):\n    params = all_params[part_key]\n    calc_dim = dimensions(params)\n    pins = generate_pins(params)\n    body = generate_body(params, calc_dim)\n    contacts = generate_contacts(params)\n    return (pins, body, contacts)\n\n\n# opened from within freecad\nif \"module\" in __name__:\n    part_to_build = 'ucon_17809_02x10_1.27mm'\n\n    FreeCAD.Console.PrintMessage(\"Started from CadQuery: building \" +\n                                 part_to_build + \"\\n\")\n    (pins, body, contacts) = generate_part(part_to_build)\n\n    show(pins)\n    show(body)\n    show(contacts)", "original_line_number": 15085, "instruction_length": 609}
{"instruction": "Create a BlockStatistics class that generates a downsampled output raster by computing a statistical measure over non-overlapping square blocks of pixels in the input raster. The class should have a getParameterInfo method that returns information about the input raster, block size, statistical measure, and downsampling factor. The class should also have a getConfiguration method that sets the sampling factor, inherits properties, invalidates properties, sets the input mask, and sets the padding. The class should have an updateRasterInfo method that updates the output raster's cell size, pixel type, statistics, and histogram based on the downsampling factor and statistical measure. Finally, the class should have an updatePixels method that computes the statistical measure over each block of pixels in the input raster and generates the output raster and mask. The class should also have an updateKeyMetadata method that sets the datatype of the output raster to \"Processed\".", "input": "", "output": "import numpy as np\r\nfrom skimage.transform import resize\r\nfrom skimage.util import view_as_blocks\r\n\r\n\r\nclass BlockStatistics():\r\n\r\n    def __init__(self):\r\n        self.name = \"Block Statistics Function\"\r\n        self.description = (\"Generates a downsampled output raster by computing a statistical \"\r\n                            \"measure over non-overlapping square blocks of pixels in the input raster.\")\r\n        self.func = np.mean\r\n        self.padding = 0\r\n\r\n    def getParameterInfo(self):\r\n        return [\r\n            {\r\n                'name': 'raster',\r\n                'dataType': 'raster',\r\n                'value': None,\r\n                'required': True,\r\n                'displayName': \"Input Raster\",\r\n                'description': \"The primary input raster over which block statistics is computed.\"\r\n            },\r\n            {\r\n                'name': 'size',\r\n                'dataType': 'numeric',\r\n                'value': 1,\r\n                'required': False,\r\n                'displayName': \"Block Size\",\r\n                'description': (\"The number of pixels along each side of the square \"\r\n                                \"non-overlapping block.\")\r\n            },\r\n            {\r\n                'name': 'measure',\r\n                'dataType': 'string',\r\n                'value': 'Mean',\r\n                'required': False,\r\n                'displayName': \"Measure\",\r\n                'domain': ('Minimum', 'Maximum', 'Mean', 'Median', 'Sum', 'Nearest'),\r\n                'description': (\"The statistical measure computed over each \"\r\n                                \"block of pixels in the input raster.\")\r\n            },\r\n            {\r\n                'name': 'factor',\r\n                'dataType': 'numeric',\r\n                'value': 1,\r\n                'required': False,\r\n                'displayName': \"Downsampling Factor\",\r\n                'description': (\"The integer factor by which the output raster is \"\r\n                                \"downsampled relative to the input raster.\")\r\n            },\r\n        ]\r\n\r\n    def getConfiguration(self, **scalars):\r\n        s = scalars.get('size', None)\r\n        s = 3 if s is None else s\r\n        self.padding = int(s / 2)\r\n        return {\r\n            'samplingFactor': scalars.get('size', 1.0),\r\n            'inheritProperties': 4 | 8,             # inherit everything but the pixel type (1) and NoData (2)\r\n            'invalidateProperties': 2 | 4 | 8,      # invalidate histogram, statistics, and key metadata\r\n            'inputMask': True,\r\n            'resampling': False,\r\n            'padding': self.padding,\r\n        }\r\n\r\n    def updateRasterInfo(self, **kwargs):\r\n        f = kwargs.get('factor', 1.0)\r\n        kwargs['output_info']['cellSize'] = tuple(np.multiply(kwargs['raster_info']['cellSize'], f))\r\n        kwargs['output_info']['pixelType'] = 'f4'   # output pixels values are floating-point\r\n        kwargs['output_info']['statistics'] = ()\r\n        kwargs['output_info']['histogram'] = ()\r\n\r\n        m = kwargs.get('measure')\r\n        m = m.lower() if m is not None and len(m) else 'mean'\r\n\r\n        if m == 'minimum':\r\n            self.func = np.min\r\n        elif m == 'maximum':\r\n            self.func = np.max\r\n        elif m == 'mean':\r\n            self.func = np.mean\r\n        elif m == 'median':\r\n            self.func = np.median\r\n        elif m == 'sum':\r\n            self.func = np.sum\r\n        elif m == 'nearest':\r\n            self.func = None\r\n\r\n        return kwargs\r\n\r\n    def updatePixels(self, tlc, shape, props, **pixelBlocks):\r\n        p = pixelBlocks['raster_pixels']\r\n        m = pixelBlocks['raster_mask']\r\n\r\n        if self.func is None:\r\n            b = resize(p, shape, order=0, preserve_range=True)\r\n        else:\r\n            blockSizes = tuple(np.divide(p.shape, shape))\r\n            b = np.ma.masked_array(view_as_blocks(p, blockSizes),\r\n                                   view_as_blocks(~m.astype('b1'), blockSizes))\r\n            for i in range(len(b.shape) // 2):\r\n                b = self.func(b, axis=-1)\r\n            b = b.data\r\n\r\n        d = self.padding\r\n        pixelBlocks['output_pixels'] = b.astype(props['pixelType'], copy=False)\r\n        pixelBlocks['output_mask'] = resize(m, shape, order=0, preserve_range=True).astype('u1', copy=False)\r\n        return pixelBlocks\r\n\r\n    def updateKeyMetadata(self, names, bandIndex, **keyMetadata):\r\n        if bandIndex == -1:\r\n            keyMetadata['datatype'] = 'Processed'\r\n        return keyMetadata", "original_line_number": 15705, "instruction_length": 985}
{"instruction": "The given code is a Python class named FileSystem that provides an interface for file system operations. To use this class, create an instance of it and call its methods. The methods include abspath, realpath, path_to_module, expanduser, basename, chdir, copyfile, dirname, exists, dirs_under, files_under, getcwd, glob, isabs, isfile, getsize, isdir, join, listdir, mkdtemp, maybe_make_directory, move, mtime, normpath, open_binary_tempfile, open_binary_file_for_reading, read_binary_file, write_binary_file, open_text_file_for_reading, open_text_file_for_writing, open_stdin, read_text_file, write_text_file, sha1, relpath, remove, rmtree, copytree, split, splitext, and compare.", "input": "", "output": "# Copyright (C) 2010 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\"\"\"Wrapper object for the file system / source tree.\"\"\"\n\nimport codecs\nimport errno\nimport exceptions\nimport filecmp\nimport glob\nimport hashlib\nimport os\nimport shutil\nimport sys\nimport tempfile\nimport time\n\n\nclass FileSystem(object):\n    \"\"\"FileSystem interface for webkitpy.\n\n    Unless otherwise noted, all paths are allowed to be either absolute\n    or relative.\"\"\"\n    sep = os.sep\n    pardir = os.pardir\n\n    def abspath(self, path):\n        # FIXME: This gross hack is needed while we transition from Cygwin to native Windows, because we\n        # have some mixing of file conventions from different tools:\n        if sys.platform == 'cygwin':\n            path = os.path.normpath(path)\n            path_components = path.split(os.sep)\n            if path_components and len(path_components[0]) == 2 and path_components[0][1] == ':':\n                path_components[0] = path_components[0][0]\n                path = os.path.join('/', 'cygdrive', *path_components)\n\n        return os.path.abspath(path)\n\n    def realpath(self, path):\n        return os.path.realpath(path)\n\n    def path_to_module(self, module_name):\n        \"\"\"A wrapper for all calls to __file__ to allow easy unit testing.\"\"\"\n        # FIXME: This is the only use of sys in this file. It's possible this function should move elsewhere.\n        return sys.modules[module_name].__file__  # __file__ is always an absolute path.\n\n    def expanduser(self, path):\n        return os.path.expanduser(path)\n\n    def basename(self, path):\n        return os.path.basename(path)\n\n    def chdir(self, path):\n        return os.chdir(path)\n\n    def copyfile(self, source, destination):\n        shutil.copyfile(source, destination)\n\n    def dirname(self, path):\n        return os.path.dirname(path)\n\n    def exists(self, path):\n        return os.path.exists(path)\n\n    def dirs_under(self, path, dir_filter=None):\n        \"\"\"Return the list of all directories under the given path in topdown order.\n\n        Args:\n            dir_filter: if not None, the filter will be invoked\n                with the filesystem object and the path of each dirfound.\n                The dir is included in the result if the callback returns True.\n        \"\"\"\n        def filter_all(fs, dirpath):\n            return True\n        dir_filter = dir_filter or filter_all\n\n        dirs = []\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            if dir_filter(self, dirpath):\n                dirs.append(dirpath)\n        return dirs\n\n    def files_under(self, path, dirs_to_skip=[], file_filter=None):\n        \"\"\"Return the list of all files under the given path in topdown order.\n\n        Args:\n            dirs_to_skip: a list of directories to skip over during the\n                traversal (e.g., .svn, resources, etc.)\n            file_filter: if not None, the filter will be invoked\n                with the filesystem object and the dirname and basename of\n                each file found. The file is included in the result if the\n                callback returns True.\n        \"\"\"\n        def filter_all(fs, dirpath, basename):\n            return True\n\n        file_filter = file_filter or filter_all\n        files = []\n        if self.isfile(path):\n            if file_filter(self, self.dirname(path), self.basename(path)):\n                files.append(path)\n            return files\n\n        if self.basename(path) in dirs_to_skip:\n            return []\n\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            for d in dirs_to_skip:\n                if d in dirnames:\n                    dirnames.remove(d)\n\n            for filename in filenames:\n                if file_filter(self, dirpath, filename):\n                    files.append(self.join(dirpath, filename))\n        return files\n\n    def getcwd(self):\n        return os.getcwd()\n\n    def glob(self, path):\n        return glob.glob(path)\n\n    def isabs(self, path):\n        return os.path.isabs(path)\n\n    def isfile(self, path):\n        return os.path.isfile(path)\n\n    def getsize(self, path):\n        return os.path.getsize(path)\n\n    def isdir(self, path):\n        return os.path.isdir(path)\n\n    def join(self, *comps):\n        return os.path.join(*comps)\n\n    def listdir(self, path):\n        return os.listdir(path)\n\n    def mkdtemp(self, **kwargs):\n        \"\"\"Create and return a uniquely named directory.\n\n        This is like tempfile.mkdtemp, but if used in a with statement\n        the directory will self-delete at the end of the block (if the\n        directory is empty; non-empty directories raise errors). The\n        directory can be safely deleted inside the block as well, if so\n        desired.\n\n        Note that the object returned is not a string and does not support all of the string\n        methods. If you need a string, coerce the object to a string and go from there.\n        \"\"\"\n        class TemporaryDirectory(object):\n            def __init__(self, **kwargs):\n                self._kwargs = kwargs\n                self._directory_path = tempfile.mkdtemp(**self._kwargs)\n\n            def __str__(self):\n                return self._directory_path\n\n            def __enter__(self):\n                return self._directory_path\n\n            def __exit__(self, type, value, traceback):\n                # Only self-delete if necessary.\n\n                # FIXME: Should we delete non-empty directories?\n                if os.path.exists(self._directory_path):\n                    os.rmdir(self._directory_path)\n\n        return TemporaryDirectory(**kwargs)\n\n    def maybe_make_directory(self, *path):\n        \"\"\"Create the specified directory if it doesn't already exist.\"\"\"\n        try:\n            os.makedirs(self.join(*path))\n        except OSError, e:\n            if e.errno != errno.EEXIST:\n                raise\n\n    def move(self, source, destination):\n        shutil.move(source, destination)\n\n    def mtime(self, path):\n        return os.stat(path).st_mtime\n\n    def normpath(self, path):\n        return os.path.normpath(path)\n\n    def open_binary_tempfile(self, suffix):\n        \"\"\"Create, open, and return a binary temp file. Returns a tuple of the file and the name.\"\"\"\n        temp_fd, temp_name = tempfile.mkstemp(suffix)\n        f = os.fdopen(temp_fd, 'wb')\n        return f, temp_name\n\n    def open_binary_file_for_reading(self, path):\n        return codecs.open(path, 'rb')\n\n    def read_binary_file(self, path):\n        \"\"\"Return the contents of the file at the given path as a byte string.\"\"\"\n        with file(path, 'rb') as f:\n            return f.read()\n\n    def write_binary_file(self, path, contents):\n        with file(path, 'wb') as f:\n            f.write(contents)\n\n    def open_text_file_for_reading(self, path, errors='strict'):\n        # Note: There appears to be an issue with the returned file objects\n        # not being seekable. See http://stackoverflow.com/questions/1510188/can-seek-and-tell-work-with-utf-8-encoded-documents-in-python .\n        return codecs.open(path, 'r', 'utf8', errors)\n\n    def open_text_file_for_writing(self, path):\n        return codecs.open(path, 'w', 'utf8')\n\n    def open_stdin(self):\n        return codecs.StreamReaderWriter(sys.stdin,\n                                             codecs.getreader('utf8'),\n                                             codecs.getwriter('utf8'),\n                                             'replace')\n\n    def read_text_file(self, path):\n        \"\"\"Return the contents of the file at the given path as a Unicode string.\n\n        The file is read assuming it is a UTF-8 encoded file with no BOM.\"\"\"\n        with codecs.open(path, 'r', 'utf8') as f:\n            return f.read()\n\n    def write_text_file(self, path, contents):\n        \"\"\"Write the contents to the file at the given location.\n\n        The file is written encoded as UTF-8 with no BOM.\"\"\"\n        with codecs.open(path, 'w', 'utf-8') as f:\n            f.write(contents.decode('utf-8') if type(contents) == str else contents)\n\n    def sha1(self, path):\n        contents = self.read_binary_file(path)\n        return hashlib.sha1(contents).hexdigest()\n\n    def relpath(self, path, start='.'):\n        return os.path.relpath(path, start)\n\n    class _WindowsError(exceptions.OSError):\n        \"\"\"Fake exception for Linux and Mac.\"\"\"\n        pass\n\n    def remove(self, path, osremove=os.remove):\n        \"\"\"On Windows, if a process was recently killed and it held on to a\n        file, the OS will hold on to the file for a short while.  This makes\n        attempts to delete the file fail.  To work around that, this method\n        will retry for a few seconds until Windows is done with the file.\"\"\"\n        try:\n            exceptions.WindowsError\n        except AttributeError:\n            exceptions.WindowsError = FileSystem._WindowsError\n\n        retry_timeout_sec = 3.0\n        sleep_interval = 0.1\n        while True:\n            try:\n                osremove(path)\n                return True\n            except exceptions.WindowsError, e:\n                time.sleep(sleep_interval)\n                retry_timeout_sec -= sleep_interval\n                if retry_timeout_sec < 0:\n                    raise e\n\n    def rmtree(self, path):\n        \"\"\"Delete the directory rooted at path, whether empty or not.\"\"\"\n        shutil.rmtree(path, ignore_errors=True)\n\n    def copytree(self, source, destination):\n        shutil.copytree(source, destination)\n\n    def split(self, path):\n        \"\"\"Return (dirname, basename + '.' + ext)\"\"\"\n        return os.path.split(path)\n\n    def splitext(self, path):\n        \"\"\"Return (dirname + os.sep + basename, '.' + ext)\"\"\"\n        return os.path.splitext(path)\n\n    def compare(self, path1, path2):\n        return filecmp.cmp(path1, path2)", "original_line_number": 15892, "instruction_length": 681}
{"instruction": "Create a form for third-party to first-party OAuth 2.0 access token exchange with the given fields and methods. The form should inherit from OAuthForm and ScopeMixin. It should have CharField for access_token, client_id, and ScopeChoiceField for scope. The form should validate the access_token and client_id fields and raise an appropriate OAuthValidationError error if they are missing. It should also check if the client_id is a valid public client and if the backend is a supported provider. Finally, it should authenticate the user using the access_token and allow_inactive_user=True and raise an OAuthValidationError error if the access_token is not valid.", "input": "", "output": "\"\"\"\nForms to support third-party to first-party OAuth 2.0 access token exchange\n\"\"\"\nimport provider.constants\nfrom django.contrib.auth.models import User\nfrom django.forms import CharField\nfrom edx_oauth2_provider.constants import SCOPE_NAMES\nfrom oauth2_provider.models import Application\nfrom provider.forms import OAuthForm, OAuthValidationError\nfrom provider.oauth2.forms import ScopeChoiceField, ScopeMixin\nfrom provider.oauth2.models import Client\nfrom requests import HTTPError\nfrom social.backends import oauth as social_oauth\nfrom social.exceptions import AuthException\n\nfrom third_party_auth import pipeline\n\n\nclass AccessTokenExchangeForm(ScopeMixin, OAuthForm):\n    \"\"\"Form for access token exchange endpoint\"\"\"\n    access_token = CharField(required=False)\n    scope = ScopeChoiceField(choices=SCOPE_NAMES, required=False)\n    client_id = CharField(required=False)\n\n    def __init__(self, request, oauth2_adapter, *args, **kwargs):\n        super(AccessTokenExchangeForm, self).__init__(*args, **kwargs)\n        self.request = request\n        self.oauth2_adapter = oauth2_adapter\n\n    def _require_oauth_field(self, field_name):\n        \"\"\"\n        Raise an appropriate OAuthValidationError error if the field is missing\n        \"\"\"\n        field_val = self.cleaned_data.get(field_name)\n        if not field_val:\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_request\",\n                    \"error_description\": \"{} is required\".format(field_name),\n                }\n            )\n        return field_val\n\n    def clean_access_token(self):\n        \"\"\"\n        Validates and returns the \"access_token\" field.\n        \"\"\"\n        return self._require_oauth_field(\"access_token\")\n\n    def clean_client_id(self):\n        \"\"\"\n        Validates and returns the \"client_id\" field.\n        \"\"\"\n        return self._require_oauth_field(\"client_id\")\n\n    def clean(self):\n        if self._errors:\n            return {}\n\n        backend = self.request.backend\n        if not isinstance(backend, social_oauth.BaseOAuth2):\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_request\",\n                    \"error_description\": \"{} is not a supported provider\".format(backend.name),\n                }\n            )\n\n        self.request.session[pipeline.AUTH_ENTRY_KEY] = pipeline.AUTH_ENTRY_LOGIN_API\n\n        client_id = self.cleaned_data[\"client_id\"]\n        try:\n            client = self.oauth2_adapter.get_client(client_id=client_id)\n        except (Client.DoesNotExist, Application.DoesNotExist):\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_client\",\n                    \"error_description\": \"{} is not a valid client_id\".format(client_id),\n                }\n            )\n        if client.client_type not in [provider.constants.PUBLIC, Application.CLIENT_PUBLIC]:\n            raise OAuthValidationError(\n                {\n                    # invalid_client isn't really the right code, but this mirrors\n                    # https://github.com/edx/django-oauth2-provider/blob/edx/provider/oauth2/forms.py#L331\n                    \"error\": \"invalid_client\",\n                    \"error_description\": \"{} is not a public client\".format(client_id),\n                }\n            )\n        self.cleaned_data[\"client\"] = client\n\n        user = None\n        try:\n            user = backend.do_auth(self.cleaned_data.get(\"access_token\"), allow_inactive_user=True)\n        except (HTTPError, AuthException):\n            pass\n        if user and isinstance(user, User):\n            self.cleaned_data[\"user\"] = user\n        else:\n            # Ensure user does not re-enter the pipeline\n            self.request.social_strategy.clean_partial_pipeline()\n            raise OAuthValidationError(\n                {\n                    \"error\": \"invalid_grant\",\n                    \"error_description\": \"access_token is not valid\",\n                }\n            )\n\n        return self.cleaned_data", "original_line_number": 16393, "instruction_length": 662}
{"instruction": "Create a quiz application using Python's tkinter library. The application should have a start page with two buttons for selecting a subject (geography or mathematics). After selecting a subject, the application should display a series of random questions with multiple choice answers. The user should be able to select an answer and confirm it. After answering all questions, the application should display the user's score and a message based on their performance. The application should also play a sound effect when the user selects an answer or confirms their choice.", "input": "", "output": "from tkinter import *\nfrom tkinter import ttk\nimport random\n\nbutton_width = 17\nnumber_of_characters_per_row = 56\ndiff_for_answers = 8\ncolor = '#%02x%02x%02x' % (231, 231, 231)\n\nimport subprocess  # poskusile 5 razlicnih modulov: pyglet, mp3play, sound in se dva pa noben ni delal\n\n\n# pygame se nama zdi prevelika knjiznica za dodati za samo nekaj zvokov\ndef play_button_click():  # dela samo na OS X!\n    subprocess.call([\"afplay\", \"Sounds/button_click.mp3\"])\n    # dela prepočasi!! - ko to dela, ne dela nič drugo!\n\n\n# subprocess.call([\"afplay\", \"music.mp3\"]) # ce to igram, potem nic drugo ne dela dokler se glasba ne konca!\n\nimport gettext\n\n_ = gettext.gettext\n\n# noinspection PyBroadException\ntry:\n    en = gettext.translation('main', localedir='locale', languages=['en'])\n    en.install()\nexcept:\n    print(_(\"Prevedba v angleski jezik ni bila mogoca.\"))\n\n\nclass Quiz(Tk):\n    frames = {}\n    number_of_questions = 5\n    question_count = 0\n    number_of_all_questions = 20  # per subject in SUBJECTdata.txt\n    points = 0  # number of points user gets for answering the question correctly\n\n    def __init__(self, *args, **kwargs):\n        Tk.__init__(self, *args, **kwargs)\n        Tk.wm_title(self, _(\"Maturitetni kviz\"))\n\n        self.initialize_container_frame()\n        self.initialize_start_page()\n\n        self.set_images()\n\n    def initialize_container_frame(self):\n        self.container = ttk.Frame(self)  # to je frame, ki nima na sebi nič, na njega zlagama nove\n        self.container.pack_propagate(0)\n        self.container.pack(pady=10, padx=10)\n\n        self.container.grid_rowconfigure(0, weight=1)\n        # default weight je 0, kar pomeni da bo ta imel najvecji prostor ko spremenimo velikost - zaenkrat nima veze ker je sam\n        self.container.grid_columnconfigure(0, weight=1)\n\n    def initialize_start_page(self):\n        start_page = StartPage(self.container, self)\n        start_page.grid(row=0, column=0, sticky=\"nsew\")\n        self.frames[0] = start_page\n        self.show_frame()\n\n    def show_frame(self):\n        if self.question_count <= self.number_of_questions:\n            frame = self.frames.get(self.question_count, None)  # da slucajno ne pride do zrusitve programa\n            if frame is not None:\n                frame.tkraise()  # naloži nov frame - vprašanje\n            else:\n                print(_(\"Nekaj se je zalomilo. Vprasanja ni bilo mogoče naložiti\"))\n            self.question_count += 1\n        else:\n            self.show_result_frame()\n\n    def set_subject(self, subject):\n        self.create_random_questions(subject)\n        self.show_frame()\n        play_button_click()\n\n    def create_random_questions(self, subject):\n        random_question_numbers = []\n        table_of_possible_question_numbers = list(\n            range(1, self.number_of_all_questions + 1))  # iti more od 1 do vkljucno stevila\n\n        # tu samo dolocimo random stevilke vprasanj, stevilka pomeni vrstica v dokumentu:\n        while len(random_question_numbers) < self.number_of_questions:\n            rand_number = random.choice(table_of_possible_question_numbers)\n            random_question_numbers.append(rand_number)\n\n            if rand_number in table_of_possible_question_numbers:\n                table_of_possible_question_numbers.remove(rand_number)\n            else:\n                print(_(\"Pri določanju tvojih vprašanj se je zalomilo.\"))  # spet da slucajno ne pride do zrusitve\n\n        # nalozimo dejanska vprasanja, prikazemo zaenkrat se nobenega:\n        question_count = 1  # to ni lastnost metode self.question_count, ampak nova spremenljivka\n        for number in random_question_numbers:\n            question = Question(self.container, self, subject, number)\n            self.frames[question_count] = question\n            question_count += 1\n            question.grid(row=0, column=0, sticky=\"nsew\")\n\n    def show_result_frame(self):\n        result_page = ResultPage(self.container, self)\n        result_page.grid(row=0, column=0, sticky=\"nsew\")\n        result_page.tkraise()\n\n        # ponastavimo rezultate, ce bo slucajno igral ponovno:\n        self.question_count = 0\n        self.points = 0\n        self.destroy_previous_frames()  # da se nam spomin ne zabase\n\n    def destroy_previous_frames(self):\n        for frame in self.frames.values():\n            frame.destroy()\n        self.frames = {}\n\n    def increase_points(self):\n        self.points += 1\n\n    def set_images(self):\n        correct_photo = PhotoImage(file=\"Images/correct.gif\")\n        Label(self, image=correct_photo)\n        self.correct_photo = correct_photo\n        wrong_photo = wrong_photo = PhotoImage(file=\"Images/wrong.gif\")\n        Label(self, image=wrong_photo)\n        self.wrong_photo = wrong_photo\n\n\nclass StartPage(ttk.Frame):  # podeduje metode in lastnosti razreda\n    def __init__(self, parent, quiz_reference):  # self je container - vse se bo nalagalo na container\n        ttk.Frame.__init__(self, parent)\n        self.quiz_reference = quiz_reference\n\n        self.show_frame()\n\n    def show_frame(self):\n        text = _('''Pozdravljen bodoči maturant!\\nPred tabo je kratek kviz iz maturitetnih predmetov\\n''')\n        ttk.Label(self, text=text, justify=\"center\").pack(padx=10)\n\n        self.show_image()\n\n        ttk.Label(self, text=_(\"Izberi področje:\")).pack(pady=10, padx=10)\n\n        button_geo = ttk.Button(self, text=_(\"Geografija\"),\n                               command=lambda: self.quiz_reference.set_subject(\"GEO\"),\n                               width=button_width)\n        button_geo.pack(side=\"bottom\")\n        button_mat = ttk.Button(self, text=_(\"Matematika\"),\n                               command=lambda: self.quiz_reference.set_subject(\"MAT\"),\n                               width=button_width)\n        button_mat.pack(side=\"bottom\")\n        # lambda uporabimo, da lahko podamo parameter in ob tem ne sprožimo klica funkcije\n\n    def show_image(self):\n        photo = PhotoImage(file=\"Images/slika.gif\")\n        label = ttk.Label(self, image=photo)\n\n        self.start_page_image = photo  # treba je imeti se eno povezavo, zato da je avtomatsko ne izbrise\n        label.pack()\n\n\nclass Question(ttk.Frame):\n    question = \"\"\n    correct_answer = 0\n    possible_answers = {}\n    chosen_answer = \"\"\n    is_confirm_button_showing = False\n    radio_buttons = []\n\n    def __init__(self, parent, quiz_reference, subject, number):  # ko imama stevilko, poiscema vprasanje, odgovor in mozne odgovore iz datoteke\n        ttk.Frame.__init__(self, parent)\n\n        self.quiz_reference = quiz_reference\n        self.subject = subject\n        self.number = number\n        self.get_data()\n\n        self.show_frame_widgets()\n\n    def show_frame_widgets(self):\n        self.show_the_question()\n        self.show_possible_answers()\n\n    def show_the_question(self):\n        '''prikaze vprasanje na label widgetu'''\n        edited_text = self.check_if_text_too_long(self.question, number_of_characters_per_row)\n\n        ttk.Label(self, text=edited_text).pack(pady=15, padx=10, side=\"top\")\n\n    def check_if_text_too_long(self, unedited_text, allowed_number_of_chars):\n        '''vrne primerno preurejen text z novimi vrsticami, ce je trenutno predolg'''\n        if len(unedited_text) <= number_of_characters_per_row: return unedited_text  # je ze ok\n\n        text = ''''''  # vecvrsticni string\n        num_of_chars = 0  # in current row\n\n        for word in unedited_text.split(\" \"):\n            num_of_chars += len(word)\n            if num_of_chars < allowed_number_of_chars:\n                text += word + \" \"\n            else:\n                text = text + word + \"\\n\"\n                num_of_chars = 0\n        return text.strip(\"\\n\")\n\n    def show_possible_answers(self):\n        self.radio_buttons = {}\n        self.var = StringVar()\n        for possible_answer in self.possible_answers:\n            possible_answer = self.check_if_text_too_long(possible_answer,\n                                                          number_of_characters_per_row - diff_for_answers)\n            R = ttk.Radiobutton(self,\n                                compound=\"left\",\n                                text=possible_answer,\n                                variable=self.var,\n                                value=possible_answer,\n                                command=self.set_chosen_answer)\n            # Ko uporabnik izbere odgovor, se mu prikaze gumb za potrditev, ko stisne nanj se preveri pravilnost izbire\n            self.radio_buttons[possible_answer] = R\n            R.pack(anchor='w')\n\n    def set_chosen_answer(self):\n        if not self.is_confirm_button_showing: self.show_confirm_button()\n\n    def show_confirm_button(self):\n        self.confirm_button = ttk.Button(self, text=_(\"Potrdi izbiro\"),\n                                         command=self.check_the_answer,\n                                         width=button_width)\n        self.confirm_button.pack(pady=8, side=\"bottom\")\n        self.is_confirm_button_showing = True\n\n    def change_text_on_confirm_button(self):\n        self.confirm_button.destroy()\n        self.next_q_button = ttk.Button(self, text=_(\"Naprej\"),\n                                        command=self.confirm_button_pressed,\n                                        width=button_width)\n        self.next_q_button.pack(pady=8, side=\"bottom\")\n\n        # prepreci da stisne na gumbe:\n        for text, radio_button in self.radio_buttons.items():\n            radio_button.configure(state=DISABLED)\n            #if radio_button.text == self.chosen_answer: print(self.chosen_answer) # to ne dela! zato je narejeno z slovarjem\n            if text == self.chosen_answer:\n                appropriate_image = self.quiz_reference.correct_photo if self.chosen_answer == self.correct_answer \\\n                    else self.quiz_reference.wrong_photo\n                #print(appropriate_image.name)\n                #radio_button.configure(image=appropriate_image) # TU BI SE MORALA PRIKAZATI ZRAVEN PRIMERNA SLIKA\n\n    def confirm_button_pressed(self):\n        play_button_click()\n        self.quiz_reference.show_frame()\n\n    def check_the_answer(self):\n        self.chosen_answer = self.var.get()\n        if self.chosen_answer == self.correct_answer: self.quiz_reference.increase_points()\n        self.change_text_on_confirm_button()\n        play_button_click()\n\n    def get_data(self):\n        data = self.subject + \"data.txt\"\n\n        with open(data, \"r\") as file:\n            lines = [line.strip() for line in file]\n            currentLine = lines[self.number]\n            # zapisano v obliki Vprasanje;odg1:odg2:odg3;odgovorPravilen\n            data = currentLine.split(\";\")\n            self.question = data[0]\n            self.correct_answer = data[2]\n            self.possible_answers = data[1].split(\":\")\n\n\nclass ResultPage(ttk.Frame):\n    def __init__(self, parent, quiz_reference):  # ko imama stevilko, poiscema vprasanje, odgovor in mozne odgovore iz datoteke\n        ttk.Frame.__init__(self, parent)\n        self.quiz_reference = quiz_reference\n        self.show_frame_widgets()\n\n    def show_frame_widgets(self):\n        points = self.quiz_reference.points\n        all_points = self.quiz_reference.number_of_questions\n        ttk.Label(self, text=\"Tvoj rezultat je: {} od {} točk!\".\n                  format(points, all_points)).pack(pady=10, padx=10)\n\n        text_message = self.appropriate_message(points)\n        ttk.Label(self, text=text_message).pack(pady=10, padx=10)\n\n        appropriate_image = \"Images/failure.gif\" if points <= all_points // 2 else \"Images/bravo.gif\"\n\n        photo = PhotoImage(file=appropriate_image)\n        label = ttk.Label(self, image=photo)\n\n        self.congratulation_photo = photo\n        label.pack(pady=15)\n\n        ttk.Button(self, text=\"Igraj ponovno!\",\n                   command=self.quiz_reference.initialize_start_page,\n                   width=button_width).pack(side=\"bottom\")\n\n    def appropriate_message(self, user_points):\n        \"\"\"Prikaze sporocilo glede na rezultat\"\"\"\n        all_points = self.quiz_reference.number_of_questions\n        if user_points in range(all_points // 2 + 1):\n            message = \"Tvoje znanje je nezadostno!\"\n        elif user_points in range(all_points // 2 + 1, all_points // 4):\n            message = \"Tvoje znanje je zadovoljivo.\"\n        elif user_points in range(all_points // 4, all_points):\n            message = \"Čestitam, dosegel si skoraj vse točke!\"\n        else:\n            message = \"Bravo, tvoje znanje je izjemno!!!\"  # dosegel je vse točke\n        return message\n\n\napp = Quiz()\napp.geometry(\"500x250\")\napp.configure(bg=color)  # sicer bi bil rob beli\n\n# velikost okna - to ni resitev, hocem nastavit velikost vseh framov, ne samo okna, ker se zdaj čudno poravnava\napp.resizable(0, 0)  # v nobeno smer ni resizable\napp.mainloop()", "original_line_number": 16509, "instruction_length": 571}
{"instruction": "This code provides several functions for data analysis. To use these functions, import the necessary packages (numpy, networkx, scipy.spatial, and scipy.stats) and call the desired function with the appropriate inputs. The available functions are:\n\n- find_peaks(th,gc): finds peaks in a given dataset (gc) at the corresponding thresholds (th).\n- nodedistance(affine,vP,n1,n2): calculates the distance between two nodes in a graph, given their indices (n1, n2) and the affine transformation matrix (affine) and voxel positions (vP).\n- power_law_fit(x,y): fits a power law function to the given data (x, y) and returns the scaling exponent (a) and the fitted values (y_fit).\n- exp_fit(x,y): fits an exponential function to the given data (x, y) and returns the amplitude (A), characteristic length scale (x0), and fitted values (y_fit).\n- gaussian_fit(x,y): fits a Gaussian function to the given data (x, y) and returns the amplitude (A), center position (x0), standard deviation (s), and fitted values (y_fit).\n- window_correlation(x,y,w): calculates the Pearson correlation coefficient between two vectors (x, y) using a sliding window of size w.\n- find_th_jumps(bn,max_clus=2): finds the thresholds where a jump occurs in a given graph (bn), defined as the join of the biggest cluster with up to max_clus additional clusters.\n- nodelist2volumen(bn,nodelist,element): creates a 3D volume representation of a given set of nodes (nodelist) in a graph (bn), with the specified element value (element).", "input": "", "output": "# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed May 15 20:11:59 2013\n\n@author: andres\n\"\"\"\nimport numpy\nimport networkx\nfrom scipy import spatial\nfrom scipy import stats\n\ndef find_peaks(th,gc):\n  peaks=[]\n  for i in range(1,len(th)-1):\n    if gc[1][i-1]<gc[1][i] and gc[1][i]>gc[1][i+1]:\n      peaks.append((th[i],gc[1][i]))\n  return peaks\n\ndef nodedistance(affine,vP,n1,n2):\n  \"\"\"\n  node distance in cm. (en general)\n  \"\"\"\n  ind1=vP[n1]\n  ind2=vP[n2]\n  if len(ind1)==3:\n    ind1.append(1)\n  if len(ind2)==3:\n    ind2.append(1)\n  v1=numpy.dot(affine, numpy.transpose(ind1))[0:3]\n  v2=numpy.dot(affine, numpy.transpose(ind2))[0:3]\n  d=spatial.distance.euclidean(v1,v2)\n  return d\n\ndef power_law_fit(x,y):\n  pl = lambda A, d, x: A*x**d \n  a, b, r_value, p_value, std_err = stats.linregress(numpy.log(x),numpy.log(y))\n  y_fit=pl(numpy.exp(b),a,x)\n  return (a,y_fit)\n\ndef exp_fit(x,y):\n  exp_fun = lambda A, x0, x: A*numpy.exp(x/x0) \n  a, b, r_value, p_value, std_err = stats.linregress(x,numpy.log(y))\n  A=numpy.exp(b)  \n  x0=1.0/a  \n  y_fit=exp_fun(A,x0,x)\n  return (A,x0,y_fit)\n\ndef gaussian_fit(x,y):\n  pl = lambda A, x0, s, x: A*numpy.exp(((x-x0)**2)/s) \n  p = numpy.polyfit(x,numpy.log(y),2)\n  s=1./p[0]\n  x0=-p[1]/(2*p[0])\n  A=numpy.exp(p[2]+(p[1]**2)/(4*p[0]))\n  y_fit=pl(A,x0,s,x)\n  return ((A,x0,s),y_fit)\n\ndef window_correlation(x,y,w):\n  if len(x)!=len(y):\n    print 'vector x and y must be of the same size'\n    print 'len(x)='+str(len(x))\n    print 'len(y)='+str(len(y))\n    return\n  if len(x)<w:\n    print 'window mus be smaller than len(x)'\n    print 'len(x)='+str(len(x))+' w='+str(w)\n  N=len(x)-w\n  return [stats.pearsonr(x[i:i+w],y[i:i+w])[0] for i in range(N)]\n\ndef find_th_jumps(bn,max_clus=2):\n  \"\"\"\n  Returns the thresholds where a jump occurs. A jump is defined as the \n  join of the biggest cluster with, up to, the max_clus cluster.\n  \"\"\"\n  NON=bn.get_non()\n  node_list=[node for node,dat in NON.nodes(data=True) if dat['order']==0]\n  subNON=networkx.Graph()\n  for n1,n2 in NON.edges_iter(nbunch=node_list):\n    subNON.add_edge(n1,n2)\n  node_list=networkx.connected_components(subNON)[0]\n  subNON=NON.subgraph(node_list)\n  max_th=max([dat['th'] for n,dat in subNON.nodes(data=True)])\n  N=bn.number_of_nodes()\n  jumps=[]\n  first_cluster=(0,[])\n  for node,data in NON.nodes(data=True):\n    if NON.degree(node)>=3 and NON.node[node]['order']==0:\n      for node2 in NON.neighbors(node):\n        if 0<NON.node[node2]['order']<=max_clus:\n          if 20*len(NON.node[node2]['cc'])>len(NON.node[node]['cc']) or 200*len(NON.node[node2]['cc'])>N:\n            if NON.node[node2]['th']<max_th:\n              jumps.append((NON.node[node2]['th'],NON.node[node2]['cc']))\n              if NON.node[node2]['th']>first_cluster[0]:\n                for node3 in NON.neighbors(node):\n                  if NON.node[node3]['order']==0 and NON.node[node3]['th']==NON.node[node2]['th']:\n                    first_cluster=((NON.node[node3]['th'],NON.node[node3]['cc']))\n  jumps.append(first_cluster)\n  jumps=sorted(jumps,key=lambda x: x[0],reverse=True)\n  return jumps\n    \ndef nodelist2volumen(bn,nodelist,element):\n  node2voxel=bn.node2voxel\n  B=numpy.zeros(bn.volume_shape)   \n  for node in nodelist:\n    (i,j,k)=node2voxel[str(node)]\n    B[i,j,k]=element\n  return B", "original_line_number": 17099, "instruction_length": 1498}
{"instruction": "Create a class called \"GetHPUXData\" that retrieves system information from an HP-UX machine. The class should have an initializer that takes in the IP address, SSH port, timeout, username, password, use_key_file, key_file, get_serial_info, get_hardware_info, get_os_details, get_cpu_info, get_memory_info, ignore_domain, upload_ipv6, and debug. The class should have a main method that connects to the machine, checks if the user is root, and retrieves system information such as memory, hardware, UUID, serial number, name, OS version, CPU power, MAC addresses, IP addresses, CPU count, and disk information. The class should format the data and return it as a list.", "input": "", "output": "import paramiko\r\nimport math\r\nimport json\r\n\r\n\r\nclass GetHPUXData:\r\n    def __init__(self, ip, ssh_port, timeout, usr, pwd, use_key_file, key_file,\r\n                 get_serial_info, get_hardware_info, get_os_details,\r\n                 get_cpu_info, get_memory_info, ignore_domain, upload_ipv6, debug):\r\n        self.machine_name = ip\r\n        self.port = int(ssh_port)\r\n        self.timeout = timeout\r\n        self.username = usr\r\n        self.password = pwd\r\n        self.ssh = paramiko.SSHClient()\r\n        self.use_key_file = use_key_file\r\n        self.key_file = key_file\r\n        self.get_serial_info = get_serial_info\r\n        self.get_hardware_info = get_hardware_info\r\n        self.get_os_details = get_os_details\r\n        self.get_cpu_info = get_cpu_info\r\n        self.get_memory_info = get_memory_info\r\n        self.ignore_domain = ignore_domain\r\n        self.upload_ipv6 = upload_ipv6\r\n        self.debug = debug\r\n        self.ssh = paramiko.SSHClient()\r\n        self.conn = None\r\n        self.root = False\r\n        self.sysdata = {}\r\n        self.nic_data = {'nic_parts': {}}\r\n        self.ip_data = []\r\n        self.disk_data = {'hdd_parts':[]}\r\n        self.name = None\r\n        self.paths = {}\r\n        self.alldata = []\r\n        self.name = None\r\n\r\n        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r\n\r\n    def main(self):\r\n\r\n        self.connect()\r\n        self.are_u_root()\r\n        self.get_sys_1()\r\n        self.get_sys_2()\r\n        self.get_macs()\r\n        self.get_ips()\r\n\r\n\r\n        self.get_cpu_num()\r\n        self.get_disks()\r\n        self.format_data()\r\n        return self.alldata\r\n\r\n    def connect(self):\r\n        try:\r\n            if not self.use_key_file:\r\n                self.ssh.connect(str(self.machine_name), port=self.port,\r\n                                 username=self.username, password=self.password, timeout=self.timeout)\r\n            else:\r\n                self.ssh.connect(str(self.machine_name), port=self.port,\r\n                                 username=self.username, key_filename=self.key_file, timeout=self.timeout)\r\n        except paramiko.AuthenticationException:\r\n            print str(self.machine_name) + ': authentication failed'\r\n            return None\r\n        except Exception as err:\r\n            print str(self.machine_name) + ': ' + str(err)\r\n            return None\r\n\r\n    def execute(self, cmd, need_sudo=False):\r\n        if need_sudo and not self.root: # not working currently, maybe in the future\r\n            cmd_sudo = \"sudo -S -p '' %s\" % cmd\r\n            stdin, stdout, stderr = self.ssh.exec_command(cmd_sudo, timeout=30)\r\n            stdin.write('%s\\n' % self.password)\r\n            stdin.flush()\r\n        else:\r\n            stdin, stdout, stderr = self.ssh.exec_command(cmd, timeout=30)\r\n\r\n        data_err = stderr.readlines()\r\n        data_out = stdout.readlines()\r\n        return data_out, data_err\r\n\r\n    def are_u_root(self):\r\n        cmd = 'id -u'\r\n        data, err = self.execute(cmd)\r\n        if data[0].strip() == '0':\r\n            self.root = True\r\n        else:\r\n            self.root = False\r\n        if not self.root:\r\n            print '[!] You must be root to run HP-UX discovery!'\r\n            return\r\n\r\n    def format_data(self):\r\n        self.alldata.append(self.sysdata)\r\n        self.alldata.append(self.nic_data)\r\n        self.alldata.append(self.disk_data)\r\n\r\n\r\n    def get_sys_1(self):\r\n        cmd = '/usr/contrib/bin/machinfo'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec  in raw:\r\n                if rec.startswith('memory:'):\r\n                    ram = int(math.ceil(float(rec.split()[1])))\r\n                    self.sysdata.update({'memory':ram})\r\n                if rec.startswith('model:'):\r\n                    model = rec.split(':')[1].strip().strip('\"')\r\n                    self.sysdata.update({'hardware': model})\r\n                if rec.startswith('machine id number:'):\r\n                    uuid = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'uuid': uuid})\r\n                if rec.startswith('machine serial number'):\r\n                    serial = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'serial_no': serial})\r\n                if rec.startswith('nodename:'):\r\n                    name = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'name': name})\r\n                    self.name = name\r\n                if rec.startswith('release:'):\r\n                    os_version = rec.split(':')[1].strip()\r\n                    osver = ' '.join(os_version.split()[1:]).strip()\r\n                    self.sysdata.update({'os': 'hp-ux'})\r\n                    self.sysdata.update({'osver': osver if osver else 'D42_NULL'})\r\n        else:\r\n            print '[!] Error in get_sys_1(). Message was: %s' % data_err\r\n\r\n    def get_sys_2(self):\r\n        cmd = '/opt/ignite/bin/print_manifest'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec in raw:\r\n\r\n                if rec.startswith('model:'):\r\n                    model = rec.split(':')[1].strip()\r\n                    self.sysdata.update({'hardware': model})\r\n                if rec.startswith('main memory:'):\r\n                    m = rec.split(':')[1].split()[0]\r\n                    ram = int(math.ceil(float(m.strip())))\r\n                    self.sysdata.update({'memory': ram})\r\n                if 'speed:' in rec and 'mhz' in rec:\r\n                    cpu_speed= rec.split(':')[1].strip('mhz').strip()\r\n                    self.sysdata.update({'cpupower': cpu_speed})\r\n                if rec.startswith('hostname'):\r\n                    name = rec.split(':')[1].strip()\r\n                    self.name = name\r\n                    self.sysdata.update({'name': name})\r\n        else:\r\n            print '[!] Error in get_sys_2(). Message was: %s' % data_err\r\n\r\n\r\n    def get_macs(self):\r\n        cmd = 'lanscan'\r\n        data_out, data_err = self.execute(cmd, False)\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            for rec  in raw:\r\n                if rec.split()[3] == 'up':\r\n                    words = rec.split()\r\n                    nic_mac = words[1]\r\n                    nic_name = words[4]\r\n                    mac = ''.join(nic_mac.split('0x')[1:])\r\n                    n=2\r\n                    raw = [mac[i:i + n] for i in range(0, len(mac), n)]\r\n                    macaddress = ':'.join(raw)\r\n                    self.nic_data['nic_parts'].update({nic_name:{'serial_no':macaddress}})\r\n        else:\r\n            print '[!] Error in get_macs(). Message was: %s' % data_err\r\n\r\n    def get_ips(self):\r\n        ip_data = {}\r\n        mac_data = {}\r\n\r\n        for nic in self.nic_data['nic_parts']:\r\n            mac = self.nic_data['nic_parts'][nic]['serial_no']\r\n            ip_data.update({'device':self.name})\r\n            ip_data.update({'tag': nic})\r\n            mac_data.update({'device': self.name})\r\n            mac_data.update({'port_name': nic})\r\n            mac_data.update({'macaddress': mac})\r\n\r\n            ip_data.update({'macaddress': mac})\r\n            cmd = 'ifconfig %s | grep inet' % nic\r\n            data_out, data_err = self.execute(cmd, False)\r\n            if not data_err:\r\n                raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n                for rec in raw:\r\n                    ip = rec.split()[1].strip()\r\n                    self.nic_data['nic_parts'][nic].update({'ipaddress':ip})\r\n                    ip_data.update({'ipaddress': ip})\r\n            else:\r\n                print '[!] Error in get_ips(). Message was: %s' % data_err\r\n            self.alldata.append(ip_data)\r\n            self.alldata.append(mac_data)\r\n\r\n    def get_cpu_num(self):\r\n        cmd = 'ioscan -fnk|grep proc | wc -l'\r\n        data_out, data_err = self.execute(cmd, False)\r\n\r\n        if not data_err:\r\n            raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n            if raw:\r\n                cpu_num = raw[0]\r\n                self.sysdata.update({'cpucount': cpu_num})\r\n        else:\r\n            print '[!] Error in get_cpu_num(). Message was: %s' % data_err\r\n\r\n    def get_disks(self):\r\n        cmd = 'ls /dev/rdisk/'\r\n        data_out, data_err = self.execute(cmd, False)\r\n\r\n        if not data_err:\r\n            disks = list(set([x.strip().split('_')[0] for x in data_out if x]))\r\n            for disk in disks:\r\n                cmd = 'diskinfo /dev/rdisk/%s' % disk\r\n                data_out, data_err = self.execute(cmd, False)\r\n                if not data_err:\r\n                    raw = [x.strip().lower() for x in data_out if x not in ('', '\\n', None)]\r\n                    disk = {}\r\n                    for rec in raw:\r\n                        if 'describe of ' in rec:  # another disk\r\n                            if not len(disk) == 0:\r\n                                self.disk_data['hdd_parts'].append(disk)\r\n                                disk = {}\r\n                        else:\r\n                            if rec.startswith('product id'):\r\n                                product = rec.split(':')[1].strip()\r\n                                disk.update({'product': product})\r\n                            if rec.startswith('size'):\r\n                                size = int(math.ceil(float(rec.split(':')[1].split()[0].strip()) / 1024 / 1024))\r\n                                disk.update({'hdd_size': size})\r\n                                disk.update({'assignment': 'device'})\r\n                                if self.name:\r\n                                    disk.update({'device': self.name})\r\n                    self.disk_data['hdd_parts'].append(disk)\r\n\r\n        else:\r\n            print '[!] Error in get_disks(). Message was: %s' % data_err", "original_line_number": 17812, "instruction_length": 667}
{"instruction": "Create a PostProcessor class called PrefixSuffixReplacer that replaces prefixes and suffixes in Filth objects. The class should take in optional prefix and suffix strings, defaulting to '{{' and '}}' respectively. The process_filth method should iterate through a sequence of Filth objects and replace the replacement_string attribute with the uppercase type attribute surrounded by the prefix and suffix strings. If either the prefix or suffix is not provided, it should only add the provided one to the replacement_string.", "input": "", "output": "from typing import Optional, Sequence\n\nfrom ...filth import Filth\nfrom ..base import PostProcessor\n\n\nclass PrefixSuffixReplacer(PostProcessor):\n    name = 'prefix_suffix_replacer'  # type: str\n\n    def __init__(self, prefix: Optional[str] = '{{', suffix: Optional[str] = '}}', name: Optional[str] = None):\n        super(PrefixSuffixReplacer, self).__init__(name=name)\n\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def process_filth(self, filth_list: Sequence[Filth]) -> Sequence[Filth]:\n        for filth_item in filth_list:\n            if filth_item.replacement_string is None:\n                filth_item.replacement_string = filth_item.type.upper()\n\n            if self.prefix is not None and self.suffix is not None:\n                filth_item.replacement_string = self.prefix + filth_item.replacement_string + self.suffix\n            elif self.prefix is not None:\n                filth_item.replacement_string = self.prefix + filth_item.replacement_string\n            elif self.suffix is not None:\n                filth_item.replacement_string = filth_item.replacement_string + self.suffix\n\n        return filth_list\n\n\n__all__ = ['PrefixSuffixReplacer']", "original_line_number": 17946, "instruction_length": 524}
{"instruction": "This code provides functions for detecting and documenting TensorFlow models and their methods. To use it, import the necessary libraries and call the `detect_model_documentation` function, passing in the TensorFlow model as an argument. This will return a `ModelDocumentation` object containing information about the model, including its name, hyperparameters, overview, methods, and references. To write this information to disk, call the `write` function, passing in the path to the directory where the documentation should be saved and the `ModelDocumentation` object. To read the documentation from disk, call the `read` function, passing in the path to the directory where the documentation is saved and a dictionary of the model's signature definitions.", "input": "", "output": "import tfi.json\nimport tensorflow as tf\nimport os.path\n\nimport tfi.data\nimport tfi.doc\n\nfrom google.protobuf.json_format import ParseDict\nfrom tfi.parse.docstring import GoogleDocstring\n\ndef _detect_method_documentation(*, bibliographer, model, method_name, method, signature_def):\n    # NOTE(adamb) Since we don't want to be parsing rst here, we'll just rewrite\n    #     it to include detected citations. Expect that this rst will be parsed\n    #     for real when rendering HTML.\n    docstr = GoogleDocstring(obj=method).result()\n    docstr_sections = docstr['sections']\n    text_sections = [v for k, v in docstr_sections if k == 'text']\n    overview = \"\\n\".join([l for t in text_sections for l in t])\n\n    docstr['args'] = _enrich_docs_with_tensor_info(docstr['args'], signature_def.inputs)\n    docstr['returns'] = _enrich_docs_with_tensor_info(docstr['returns'], signature_def.outputs)\n\n    return tfi.doc.MethodDocumentation(\n        name=method_name,\n        overview=bibliographer.rewrite(overview),\n        inputs=docstr['args'],\n        outputs=docstr['returns'],\n        examples=[\n            tfi.doc.MethodDataDocumentation.generate(\n                method=getattr(model, method_name),\n                inputs={\n                    input_name: eval(\"\\n\".join(input_val_lines), {}, {'m': model, 'tfi': tfi})\n                    for input_name, _, input_val_lines in docstr['example args']\n                },\n            ),\n        ],\n    )\n\ndef detect_model_documentation(model):\n        source = tfi.doc.ModelSource.detect(model)\n\n        bibliographer = tfi.doc.Bibliographer()\n\n        def maybeattr(o, attr, default=None):\n            return getattr(o, attr) if o and hasattr(o, attr) else default\n\n        # NOTE(adamb) Since we don't want to be parsing rst here, we'll just rewrite\n        #     it to include detected citations. Expect that this rst will be parsed\n        #     for real when rendering HTML.\n        model_docstr = GoogleDocstring(obj=model).result()\n        model_docstr_sections = model_docstr['sections']\n        text_sections = [v for k, v in model_docstr_sections if k == 'text']\n        overview = \"\\n\".join([l for t in text_sections for l in t])\n\n        return tfi.doc.ModelDocumentation(\n            name=maybeattr(model, '__name__', type(model).__name__),\n            hyperparameters=maybeattr(model, '__tfi_hyperparameters__', []),\n            overview=bibliographer.rewrite(overview),\n            implementation_notes=[],\n            authors=[\n                *[\n                    {\n                        \"name\": author['name'],\n                        \"url\": author['url'],\n                        \"role_noun\": \"Commits\",\n                        \"role_url\": author['commits_url'],\n                    }\n                    for author in maybeattr(source, 'authors', [])\n                ],\n            ],\n            source=source,\n            facets_overview_proto=maybeattr(model, '__tfi_facets_overview_proto__'),\n            methods=[\n                _detect_method_documentation(\n                    model=model,\n                    bibliographer=bibliographer,\n                    method_name=method_name,\n                    method=getattr(model, method_name),\n                    signature_def=signature_def,\n                )\n                for method_name, signature_def in maybeattr(model, '__tfi_signature_defs__').items()\n            ],\n            references=bibliographer.references(),\n        )\n\ndef _tensor_info_str(tensor):\n    if tensor.shape.ndims is None:\n        return '%s ?' % tensor.dtype.name\n\n    return '%s <%s>' % (\n        tensor.dtype.name,\n        ', '.join(['?' if n is None else str(n) for n in tensor.shape.as_list()]),\n    )\n\ndef _enrich_docs_with_tensor_info(doc_fields, tensor_dict):\n    existing = {k: v for k, _, v in doc_fields}\n    return [\n        (name, _tensor_info_str(tensor), existing.get(name, ''))\n        for name, tensor in tensor_dict.items()\n    ]\n\nclass MethodDocumentationLayout(object):\n    def __init__(self, base_path, assets_extra_path):\n        self.assets_extra_path = assets_extra_path\n        self.metadata_path = os.path.join(base_path, 'metadata.json')\n        self._base_path = base_path\n\n    def file(self, subpath):\n        return os.path.join(self._base_path, subpath)\n\nclass ModelDocumentationLayout(object):\n    def __init__(self, model_dir):\n        self.basename = os.path.basename(model_dir)\n        self.assets_extra_path = os.path.join(model_dir, 'assets.extra')\n        self.doc_path = os.path.join(self.assets_extra_path, 'doc')\n        self.metadata_path = os.path.join(self.doc_path, 'metadata.json')\n        self.methods_path = os.path.join(self.doc_path, 'methods')\n\n    def method(self, method_name):\n        return MethodDocumentationLayout(\n            os.path.join(self.methods_path, method_name),\n            self.assets_extra_path,\n        )\n\ndef _read_json_else(path, default):\n    if not os.path.exists(path):\n      return default\n\n    with open(path) as f:\n      return tfi.json.load(f)\n\ndef _write_json(path, obj):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, 'w') as f:\n        tfi.json.dump(obj, f)\n\nclass MethodDataDocumentationCodec(object):\n    def __init__(self, method_layout):\n        self._layout = method_layout\n\n    def write(self, method_example):\n        _write_json(\n            self._layout.file('inputs.json'),\n            {\n                name: value\n                for name, value in method_example.inputs().items()\n            }\n        )\n\n        if method_example.outputs() is not None:\n            _write_json(\n                self._layout.file('outputs.json'),\n                {\n                    name: value\n                    for name, value in method_example.outputs().items()\n                }\n            )\n\n    def read(self, signature_def):\n        return tfi.doc.MethodDataDocumentation(\n            inputs=self._detect(\n                lambda: self._read_json_tf_example_from(\n                    signature_def.inputs,\n                    'inputs.pb.json',\n                ),\n                lambda: self._read_json_example_from(\n                    signature_def.inputs,\n                    'inputs.json',\n                ),\n            ),\n            outputs=self._detect(\n                lambda: self._read_json_tf_example_from(\n                    signature_def.outputs,\n                    'outputs.pb.json',\n                ),\n                lambda: self._read_json_example_from(\n                    signature_def.outputs,\n                    'outputs.json',\n                ),\n            ),\n            # 'example result': _detect(\n            #     lambda: _read_json_tf_example_from(\n            #         signature_def.outputs,\n            #         'outputs.pb.json',\n            #     ),\n            #     lambda: _read_json_example_from(\n            #         signature_def.outputs,\n            #         'outputs.json',\n            #     ),\n            # ),\n        )\n\n    def _detect(self, *fns):\n        for fn in fns:\n            result = fn()\n            if result is not None:\n                return result\n\n    def _feature_for_tensor_info(self, tensor_info):\n        tensor_shape = tensor_info.tensor_shape.dim[1:]\n        dtype = tf.DType(tensor_info.dtype)\n        if tensor_shape[-1].size != -1:\n            return tf.FixedLenFeature(dtype=dtype, shape=[dim.size for dim in tensor_shape])\n        return tf.VarLenFeature(dtype=dtype)\n\n    def _read_json_tf_example_from(self, tensor_infos, subpath):\n        path = self._layout.file(subpath)\n        if not os.path.exists(path):\n            return None\n\n        with open(path) as f:\n            example_dict = tfi.json.load(f)\n\n        with tf.Session(graph=tf.Graph()) as session:\n            example_features = {\n                name: self._feature_for_tensor_info(tensor_info)\n                for name, tensor_info in tensor_infos.items()\n            }\n            return session.run(\n                tf.parse_single_example(\n                    ParseDict(example_dict, tf.train.Example()).SerializeToString(),\n                    features=example_features))\n\n    def _read_json_example_from(self, tensor_infos, subpath):\n        path = self._layout.file(subpath)\n        if not os.path.exists(path):\n            return None\n\n        with open(path) as f:\n            return tfi.data.json(\n                f.read(),\n                assets_extra_root=self._layout.assets_extra_path)\n\nclass MethodDocumentationCodec(object):\n    def __init__(self, method_name, method_layout):\n        self._name = method_name\n        self._layout = method_layout\n\n    def write(self, method_doc):\n        metadata = {\n            'documentation': {\n                'inputs': {\n                    name: doc\n                    for name, tensor_info, doc in method_doc.inputs()\n                },\n                'outputs': {\n                    name: doc\n                    for name, tensor_info, doc in method_doc.outputs()\n                },\n            },\n        }\n        MethodDataDocumentationCodec(self._layout).write(method_doc.examples()[0]),\n        _write_json(self._layout.metadata_path, metadata)\n        \n    def read(self, signature_def):\n        metadata = _read_json_else(self._layout.metadata_path, {})\n        doc = metadata.get('documentation', {})\n        doc_inputs = doc.get('inputs', {})\n        doc_outputs = doc.get('outputs', {})\n\n        return tfi.doc.MethodDocumentation(\n            name=self._name,\n            overview=metadata.get('overview', None),\n            inputs=[\n                (name, self._tensor_info_str(ti), doc_inputs.get(name, ''))\n                for name, ti in signature_def.inputs.items()\n            ],\n            outputs=[\n                (name, self._tensor_info_str(ti), doc_outputs.get(name, ''))\n                for name, ti in signature_def.outputs.items()                    \n            ],\n            examples=[\n              MethodDataDocumentationCodec(self._layout).read(signature_def),\n            ],\n        )\n\n    def _tensor_info_str(self, tensor_info):\n        if tensor_info.tensor_shape.unknown_rank:\n            return '%s ?' % tf.as_dtype(tensor_info.dtype).name\n\n        return '%s <%s>' % (\n            tf.as_dtype(tensor_info.dtype).name,\n            ', '.join([\n                '?' if dim.size == -1 else str(dim.size)\n                for dim in tensor_info.tensor_shape.dim\n            ]),\n        )\n\nclass ModelDocumentationCodec(object):\n    def __init__(self, path):\n        self._layout = ModelDocumentationLayout(path)\n\n    def _method_codecs(self, method_names):\n        return [\n            (\n                method_name,\n                MethodDocumentationCodec(\n                    method_name,\n                    self._layout.method(method_name),\n                )\n            )\n            for method_name in method_names\n        ]\n\n    def write(self, model_doc):\n        metadata = {\n            'name': model_doc.name(),\n            'overview': model_doc.overview(),\n            'hyperparameters': [\n                (name, str(val_type), val, docs)\n                for name, val_type, val, docs in model_doc.hyperparameters()\n            ],\n            'authors': model_doc.authors(),\n            'references': model_doc.references(),\n            'implementation_notes': model_doc.implementation_notes(),\n            'source': model_doc.source(),\n            'facets_overview_proto': None, # model_doc.facets_overview_proto(),\n        }\n\n        methods = model_doc.methods()\n        for method_name, method_codec in self._method_codecs(methods.keys()):\n            method_codec.write(methods[method_name])\n\n        _write_json(self._layout.metadata_path, metadata)\n\n    def read(self, signature_defs):\n        metadata = _read_json_else(self._layout.metadata_path, {})\n        return tfi.doc.ModelDocumentation(\n            # TODO(adamb) Should be transformed to the below structure, with val_type_str -> val_type\n            # (name, val_type, val, docs)\n            hyperparameters=metadata.get('hyperparameters', []),\n            name=metadata.get('name', self._layout.basename),\n            overview=metadata.get('overview', None),\n            methods=[\n                method_codec.read(signature_defs[method_name])\n                for method_name, method_codec in self._method_codecs(signature_defs.keys())\n            ],\n            authors=metadata.get('authors', []),\n            references=metadata.get('references', {}),\n            implementation_notes=metadata.get('implementation_notes', []),\n            source=metadata.get('source', []),\n            facets_overview_proto=None,\n        )\n\ndef read(path, signature_defs):\n    return ModelDocumentationCodec(path).read(signature_defs)\n\ndef write(path, model_doc):\n    return ModelDocumentationCodec(path).write(model_doc)", "original_line_number": 18025, "instruction_length": 760}
{"instruction": "Download necessary NLTK packages and fuzzywuzzy library. Define a dictionary of US state abbreviations and their full names. Define functions to convert Penn Treebank tags to Wordnet tags, convert tagged words to synsets, fix state abbreviations in tokens, clean tokens by removing stop words and punctuations, and compute sentence similarity using Wordnet. Define a main function that compares the similarity of a focus sentence to a list of sentences using the sentence_similarity function. The main function also has an option to ignore integers in the comparison.", "input": "", "output": "'''\nnltk.download(['wordnet', 'stopwords', 'punkt']) if not already downloaded.\nShould add to wordnet if you want more words to compare as reference to.\n'''\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom fuzzywuzzy import fuzz, process\nstop_words = stopwords.words('english')\n\n\nstates = {\n    'ak': 'alaska',\n    'al': 'alabama',\n    'ar': 'arkansas',\n    'as': 'american samoa',\n    'az': 'arizona',\n    'ca': 'california',\n    'co': 'colorado',\n    'ct': 'connecticut',\n    'dc': 'district of columbia',\n    'de': 'delaware',\n    'fl': 'florida',\n    'ga': 'georgia',\n    'gu': 'guam',\n    'hi': 'hawaii',\n    'ia': 'iowa',\n    'id': 'idaho',\n    'il': 'illinois',\n    'in': 'indiana',\n    'ks': 'kansas',\n    'ky': 'kentucky',\n    'la': 'louisiana',\n    'ma': 'massachusetts',\n    'md': 'maryland',\n    'me': 'maine',\n    'mi': 'michigan',\n    'mn': 'minnesota',\n    'mo': 'missouri',\n    'mp': 'northern mariana islands',\n    'ms': 'mississippi',\n    'mt': 'montana',\n    'na': 'national',\n    'nc': 'north carolina',\n    'nd': 'north dakota',\n    'ne': 'nebraska',\n    'nh': 'new hampshire',\n    'nj': 'new jersey',\n    'nm': 'new mexico',\n    'nv': 'nevada',\n    'ny': 'new york',\n    'oh': 'ohio',\n    'ok': 'oklahoma',\n    'or': 'oregon',\n    'pa': 'pennsylvania',\n    'pr': 'puerto rico',\n    'ri': 'rhode island',\n    'sc': 'south carolina',\n    'sd': 'south dakota',\n    'tn': 'tennessee',\n    'tx': 'texas',\n    'ut': 'utah',\n    'va': 'virginia',\n    'vi': 'virgin islands',\n    'vt': 'vermont',\n    'wa': 'washington',\n    'wi': 'wisconsin',\n    'wv': 'west virginia',\n    'wy': 'wyoming'\n}\n\ndef penn_to_wn(tag):\n    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n    if tag.startswith('N'):\n        return 'n'\n\n    if tag.startswith('V'):\n        return 'v'\n\n    if tag.startswith('J'):\n        return 'a'\n\n    if tag.startswith('R'):\n        return 'r'\n\n    return None\n\ndef tagged_to_synset(word, tag):\n    wn_tag = penn_to_wn(tag)\n    # wn_tag is None if no definition is found\n    if wn_tag is None:\n        return word\n    # try:\n        # most probable english word\n    return wn.synsets(word, wn_tag)[0]\n    # except:\n    #     return word\n\ndef fix_state_abbrev(tokens):\n    token = [\n        states[token] if states.get(token) else token\n        for token in tokens\n    ]\n    return token\n\ndef clean_tokens(tokens, ignore_integers=False):\n    punctuations = ['(',')',';',':','[',']',',','.','/']\n    keywords = [\n        word for word in tokens\n        if not word in stop_words and not word in punctuations\n    ]\n    keywords = fix_state_abbrev(keywords)\n    if ignore_integers:\n        keywords = [word for word in keywords if not is_possible_integer(word)]\n    return keywords\n\ndef clean(word):\n    word = str(word).lower().strip()\n    punctuations = ['(',')',';',':','[',']',',','.','/']\n    for punctuation in punctuations:\n        word = word.replace(punctuation, '')\n    return word\n\ndef is_possible_integer(word):\n    try:\n        int(word)\n        return True\n    except:\n        return False\n\ndef sentence_similarity(sentence1, sentence2, ignore_integers=False):\n    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n    # Tokenize and tag\n    sentence1 = ' '.join([clean(word) for word in sentence1.split()])\n    sentence2 = ' '.join([clean(word) for word in sentence2.split()])\n    tokens1 = word_tokenize(sentence1)\n    tokens2 = word_tokenize(sentence2)\n    tokens1 = clean_tokens(tokens1, ignore_integers)\n    tokens2 = clean_tokens(tokens2, ignore_integers)\n\n    # tag\n    sentence1 = pos_tag(tokens1)\n    sentence2 = pos_tag(tokens2)\n\n    # Get the synsets for the tagged words\n    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n    print(synsets1)\n    print(synsets2)\n    # Filter out the Nones\n    synsets1 = [ss for ss in synsets1 if ss]\n    synsets2 = [ss for ss in synsets2 if ss]\n\n    score, count = 0.0, 0.0\n\n    # For each word in the first sentence\n    for synset1 in synsets1:\n        # Get the similarity value of the most similar word in the other sentence\n        best_score=[\n            wn.path_similarity(synset1, synset2)\n            if not isinstance(synset1, str) and not isinstance(synset2, str)\n            # just in case there are scientific words wordnet does not have\n            else fuzz.ratio(str(synset1), str(synset2)) / 100\n            for synset2 in synsets2\n        ]\n        best_score=[s if s else 0 for s in best_score]\n        # print(synsets1, synsets2)\n        # Check that the similarity could have been computed\n        if best_score:\n            score += max(best_score)\n            count += 1\n\n    # Average the values\n    if count > 0:\n        score /= count\n    else:\n        score = 0\n\n    return score\n\ndef get_tokenized_sentence(sentence):\n    # Tokenize and tag\n    sentence = pos_tag(word_tokenize(sentence))\n    # Get the synsets for the tagged words\n    synsets = []\n    for tagged_word in sentence:\n        synset = tagged_to_synset(*tagged_word)\n        if synset:\n            synsets.append(synset)\n        else:\n            synsets.append(tagged_word[0])\n    return synsets # str(sorted(synsets))\n\ndef main():\n    sentences = [\n        \"life is good in pa 92092\",\n        \"life is good in pa\",\n        \"life is good within pa 92092/2\",\n        \"life is good pa 92092/2\",\n        \"life is good in pa 92092/2\",\n        \"testing for difference\"\n    ]\n    focus_sentence = \"life is good in pennsylvania\"\n\n    for sentence in sentences:\n        # print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n        print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence, ignore_integers=True)))\n\n    # print(sentence_similarity(focus_sentence, sentences[2], ignore_integers=True))\n\nif __name__ == '__main__':\n    main()", "original_line_number": 18231, "instruction_length": 567}
{"instruction": "The code reads an input file and performs calculations on the input values based on equations provided in the file. To use the code, provide the input file path and output file path as command line arguments. The input file should contain the equations to be evaluated, with each equation on a separate line. The equations can be of the form y = x + z, y = x * z, or y = x. The input file should also specify the initial values of the variables in the x set. The output file will contain the final values of the variables in the y set.", "input": "", "output": "#!/usr/bin/python\nimport sys\nimport re\nimport string\n\nvariables = {}\nouts = {}\nmonomial = \"([a-zA-z]+\\d+)\"\nmn = re.compile(monomial)\n\ndef extractValues(strin):\n        xAddr1 = strin[2].strip()\n        xAddr2 = strin[4].strip()\n        if xAddr1 in variables:\n            x1 = variables[xAddr1]\n        else:\n            raise Exception(\"equationReader: variable \" + xAddr1 + \" not found\")\n        if mn.match(xAddr2):\n            if xAddr2 in variables:\n                x2 = variables[xAddr2]\n            else:\n                raise Exception(\"equationReader: variable \" + xAddr2 + \" not found\")\n        else:\n            x2 = bool(int(xAddr2))\n        return {'x1':x1, 'x2':x2}\n\nif len(sys.argv) != 3:\n    raise Exception(\"Usage: equationReader <input file> <output file>\")\n\nfin = open(sys.argv[1], \"r\")\n\n\nlines = fin.readlines()\ninputs = re.compile(\"\\d+ inputs\")\noutputs = re.compile(\"\\d+ outputs\")\nfor index, line in enumerate(lines):\n    if inputs.match(line):\n        #Creation of the x set\n        xLine = lines[index + 1]\n        xValueLine = lines[index + 2]\n        ins = string.split(xLine)\n        insValues = string.split(xValueLine)\n        if len(ins) != len(insValues):\n            print(line + xLine + xValueLine)\n            raise Exception(\"equationReader: you need to provide a starting value for each x inserted\")\n        for i in range(len(ins)):\n            x = ins[i].strip()\n            variables[x] = bool(int(insValues[i]))\n    else:\n        if outputs.match(line):\n            #Creation of the y set\n            yLine = lines[index + 1]\n            ins = string.split(yLine, \" \")\n            for y in ins:\n                y.strip()\n                outs[y] = None\n            else:\n                if line == \"begin\":\n                    #When the equations start we get to the next cicle which performs the calculations\n                    break\n\n#y = x + z\nequation_XOR = re.compile(monomial + \" = \" + monomial + \" \\+ (\" + monomial + \"|(0|1))\")\n#y = x * z\nequation_AND = re.compile(monomial + \" = \" + monomial + \" \\* (\" + monomial + \"|(0|1))\")\n#y = x\nequation_ASSIGNEMENT = re.compile(monomial + \" = (\" + monomial + \"|(0|1))\")\n\nfor index, line in enumerate(lines):\n    tmp = string.split(line, \" \")\n    print(line)\n    if equation_XOR.match(line):\n        xdict = extractValues(tmp)\n        yAddr = tmp[0]\n        y = xdict['x1'] ^ xdict['x2']\n        variables[yAddr] = y\n        if yAddr in outs:\n            outs[yAddr] = y\n    else:\n        if equation_AND.match(line):\n            xdict = extractValues(tmp)\n            yAddr = tmp[0]\n            y = xdict['x1'] & xdict['x2']\n            variables[yAddr] = y\n            if yAddr in outs:\n                outs[yAddr] = y\n        else:\n            if equation_ASSIGNEMENT.match(line):\n                yAddr = tmp[0].strip()\n                xAddr = tmp[2].strip()\n                if mn.match(xAddr):\n                    if xAddr in variables:\n                        x = variables[xAddr]\n                    else:\n                        raise Exception(\"equationReader: variable \" + xAddr + \" not found\")\n                else:\n                    x = bool(xAddr)\n                y = x\n                variables[yAddr] = y\n                if yAddr in outs:\n                    outs[yAddr] = y\n            else:\n                print(\"Skipping malformed equation:\" + line)\n\n#Printing out the results\nfin.close()\nfout = open(sys.argv[2], \"w\")\nfor key, value in outs.items():\n    fout.write(key + \" = {}\\n\".format(int(value)))\nfout.close()", "original_line_number": 18350, "instruction_length": 535}
{"instruction": "Create a Django web application that allows users to create and manage MungerBuilders, which are objects that define how to transform CSV data. The application should have the following features:\n\n1. Import necessary libraries: os, re, csv, json, time, django.shortcuts, django.http, django.contrib, django.conf, guardian.shortcuts, and models from the current directory.\n\n2. Define a view function called \"munger_builder_index\" that takes a request object and returns a rendered HTML template. The template should display a list of MungerBuilders that the current user has permission to modify. If the user has no MungerBuilders, the function should create a sample MungerBuilder and return the list.\n\n3. Define a function called \"get_user_or_create_anon\" that takes a request object and returns the current user or creates an anonymous user if there is no current user. The function should assign permissions to the anonymous user and add them to the \"Global Sample\" group.\n\n4. Define a function called \"add_sample_munger\" that takes a user object and creates a sample MungerBuilder for that user. The MungerBuilder should have a name that includes the user's username and should have several DataFields with different PivotFields. The function should return a list of MungerBuilders that the user has permission to modify.\n\n5. Define a view function called \"new_munger_builder\" that takes a request object and creates a new MungerBuilder for the current user. The function should redirect to a view function called \"pivot_builder\" with the MungerBuilder's ID as a parameter.\n\n6. Define a view function called \"pivot_builder\" that takes a request object and a MungerBuilder ID and returns a rendered HTML template. The template should display a form for creating and modifying DataFields and PivotFields for the MungerBuilder.\n\n7. Define a view function called \"download_munger\" that takes a request object and a MungerBuilder ID and returns a response object that downloads a CSV file of the MungerBuilder's transformed data.\n\n8. Define a view function called \"download_test_data\" that takes a request object and a MungerBuilder ID and returns a response object that downloads a CSV file of test data for the MungerBuilder.\n\n9. Define a view function called \"poll_for_download\" that takes a request object and returns a response object that polls an asynchronous task for a file download. The function should check if the task is ready and return the filename of the downloaded file.\n\n10. Define several helper functions that parse text fields, check if the current user is anonymous, and display warning messages if necessary.", "input": "", "output": "import os\nimport re\nimport csv\nimport json\nimport time\n\nfrom django.shortcuts import render, render_to_response\nfrom django.http import HttpResponse, HttpResponseRedirect\nfrom django.contrib import messages\nfrom django.contrib.auth.models import User, Group\nfrom django.contrib.auth import authenticate, login\nfrom django.conf import settings\n\nfrom guardian.shortcuts import assign_perm, get_objects_for_user\n\nfrom .models import DataField, FieldType, CSVDocument, MungerBuilder, PivotField\nfrom .tasks import download_munger_async, download_test_data_async\n\nINDEX_REDIRECT = HttpResponseRedirect('/script_builder/munger_builder_index')\n\ndef munger_builder_index(request):\n\n    user = get_user_or_create_anon(request)\n\n    anon_check(request)\n\n    munger_builder_list = get_objects_for_user(user, 'script_builder.change_mungerbuilder')\n    if len(munger_builder_list) == 0:\n        munger_builder_list = add_sample_munger(user)\n\n    context = {'munger_builder_list': munger_builder_list}\n    return render(request, 'script_builder/munger_builder_index.html', context)\n\ndef get_user_or_create_anon(request):\n    if not request.user.id:\n        timestamp = int(time.time())\n        credentials = {\n            'username': 'anon_{0}'.format(timestamp),\n            'password': timestamp,\n        }\n        user = User.objects.create_user(**credentials)\n        user.save()\n        assign_perm('script_builder.add_mungerbuilder', user)\n        assign_perm('script_builder.add_fieldtype', user)\n        assign_perm('script_builder.add_datafield', user)\n        assign_perm('script_builder.add_pivotfield', user)\n        group = Group.objects.get(name='Global Sample')\n        user.groups.add(group)\n        user.save()\n        anon_user = authenticate(**credentials)\n        login(request, anon_user)\n    else:\n        user = request.user\n    return user\n\ndef add_sample_munger(user):\n\n    mb = MungerBuilder.objects.create(\n        munger_name='Sample for {0}'.format(user.username),\n        input_path='test_data.csv',\n        is_sample=True,\n    )\n    mb.save()\n    mb.assign_perms(user)\n\n    sample_field_dict = {\n        'order_num': ['count'],\n        'product': None,\n        'sales_name': ['index'],\n        'region': ['column'],\n        'revenue': ['mean', 'sum'],\n        'shipping': ['median'],\n    }\n\n    for field_name, field_types in sample_field_dict.items():\n        data_field = DataField.objects.create(munger_builder=mb, current_name=field_name)\n        data_field.save()\n        data_field.assign_perms(user)\n        if field_types:\n            for type_name in field_types:\n                field_type = FieldType.objects.get(type_name=type_name)\n                PivotField.objects.create(data_field=data_field, field_type=field_type).save()\n\n    return get_objects_for_user(user, 'script_builder.change_mungerbuilder')\n\ndef new_munger_builder(request):\n    user = get_user_or_create_anon(request)\n    mb = MungerBuilder.objects.create(munger_name='New Munger - {0}'.format(user.username))\n    mb.save()\n    mb.assign_perms(user)\n    return HttpResponseRedirect('/script_builder/pivot_builder/{0}'.format(mb.id))\n\ndef pivot_builder(request, munger_builder_id):\n\n    anon_check(request)\n\n    mb = MungerBuilder.objects.get(pk=munger_builder_id)\n\n    if not mb.user_is_authorized():\n        return INDEX_REDIRECT\n\n    return render(request, 'script_builder/pivot_builder_react.html', context={'mb': mb})\n\ndef download_munger(request, munger_builder_id):\n    task = download_munger_async.delay(munger_builder_id)\n    return render_to_response('script_builder/poll_for_download.html',\n                              {'task_id': task.id, 'mb_id': munger_builder_id})\n\ndef download_test_data(request, munger_builder_id):\n    task = download_test_data_async.delay(munger_builder_id)\n    return render_to_response('script_builder/poll_for_download.html',\n                              {'task_id': task.id, 'mb_id': munger_builder_id})\n\ndef poll_for_download(request):\n\n    task_id = request.GET.get(\"task_id\")\n    filename = request.GET.get(\"filename\")\n\n    if filename == 'test_data.csv':\n        async_func = download_test_data_async\n        file_path = os.path.join(settings.STATIC_ROOT, filename)\n    else:\n        async_func = download_munger_async\n        file_path = os.path.join(settings.MEDIA_ROOT, 'user_munger_scripts', '{0}'.format(filename))\n\n    if request.is_ajax():\n        result = async_func.AsyncResult(task_id)\n        if result.ready():\n            return HttpResponse(json.dumps({\"filename\": result.get()}))\n        return HttpResponse(json.dumps({\"filename\": None}))\n\n    with open(file_path, 'r') as f:\n        response = HttpResponse(f, content_type='application/octet-stream')\n        response['Content-Disposition'] = 'attachment; filename={0}'.format(filename)\n        return response\n\n# Helper Functions\n\ndef parse_text_fields(form, request, input_type):\n    if input_type == 'text':\n        return re.split('[,\\t\\n]', form.cleaned_data['fields_paste'])\n\n    if input_type == 'csv':\n        new_csv = CSVDocument(csv_file=request.FILES['csv_file'])\n        new_csv.save()\n        reader = csv.DictReader(request.FILES['csv_file'])\n        return reader.fieldnames\n\ndef anon_check(request):\n    if 'anon_' in request.user.username:\n        anon_message = \"\"\"You are logged in as an anonymous user.\n                       You may not be able to transfer any mungers to a permanent account in the future.\n                       Register to save mungers.\"\"\"\n        messages.warning(request, anon_message)", "original_line_number": 18613, "instruction_length": 2630}
{"instruction": "The given code is a module for communication between devices. To use it, follow these instructions:\n\n1. Import the necessary modules: `traceback`, `threading`, `multiprocessing`, `Queue`, `sleep`, and `Logger` from `kivy.logger`.\n2. Define the `STAY_ALIVE_TIMEOUT` and `COMMAND_CLOSE` and `COMMAND_KEEP_ALIVE` constants.\n3. Define the `connection_process_message_reader()` function that reads messages from the connection and puts them in the receive queue.\n4. Define the `connection_process_message_writer()` function that writes messages from the transmit queue to the connection.\n5. Define the `connection_message_process()` function that sets up the connection, starts the reader and writer threads, and waits for commands to close the connection.\n6. Define the `Comms()` class with methods to start the connection process, get available devices, check if the connection is open, open the connection, keep the connection alive, close the connection, read messages, and write messages.\n7. To use the `Comms()` class, create an instance with the device and connection parameters, call the `open()` method to start the connection process, and then use the `read_message()` and `write_message()` methods to communicate with the device. Call the `keep_alive()` method periodically to keep the connection alive, and call the `close()` method to close the connection when finished.", "input": "", "output": "import traceback\nimport threading\nimport multiprocessing\nfrom Queue import Empty\nfrom time import sleep\nfrom kivy.logger import Logger\nfrom autosportlabs.comms.commscommon import PortNotOpenException\n\nSTAY_ALIVE_TIMEOUT = 4\nCOMMAND_CLOSE = 'CLOSE'\nCOMMAND_KEEP_ALIVE = 'PING'\n\ndef connection_process_message_reader(rx_queue, connection, should_run):\n    Logger.debug('Comms: connection process message reader started')\n\n    while should_run.is_set():\n        try:\n            msg = connection.read_line()\n            if msg:\n                rx_queue.put(msg)\n        except:\n            Logger.error('Comms: Exception in connection_process_message_reader')\n            Logger.debug(traceback.format_exc())\n            should_run.clear()\n            sleep(0.5)\n    Logger.debug('Comms: connection process message reader exited')\n\ndef connection_process_message_writer(tx_queue, connection, should_run):\n    Logger.debug('Comms: connection process message writer started')\n    while should_run.is_set():\n        try:\n            message = tx_queue.get(True, 1.0)\n            if message:\n                connection.write(message)\n        except Empty:\n            pass\n        except Exception as e:\n            Logger.error('Comms: Exception in connection_process_message_writer ' + str(e))\n            Logger.debug(traceback.format_exc())\n            should_run.clear()\n            sleep(0.5)\n    Logger.debug('Comms: connection process message writer exited')\n\ndef connection_message_process(connection, device, rx_queue, tx_queue, command_queue):\n    Logger.debug('Comms: connection process starting')\n\n    try:\n        connection.open(device)\n        connection.flushInput()\n        connection.flushOutput()\n\n        reader_writer_should_run = threading.Event()\n        reader_writer_should_run.set()\n\n        reader_thread = threading.Thread(target=connection_process_message_reader, args=(rx_queue, connection, reader_writer_should_run))\n        reader_thread.start()\n\n        writer_thread = threading.Thread(target=connection_process_message_writer, args=(tx_queue, connection, reader_writer_should_run))\n        writer_thread.start()\n\n\n        while reader_writer_should_run.is_set():\n            try:\n                command = command_queue.get(True, STAY_ALIVE_TIMEOUT)\n                if command == COMMAND_CLOSE:\n                    Logger.debug('Comms: connection process: got close command')\n                    reader_writer_should_run.clear()\n            except Empty:\n                Logger.debug('Comms: keep alive timeout')\n                reader_writer_should_run.clear()\n        Logger.debug('Comms: connection worker exiting')\n\n        reader_thread.join()\n        writer_thread.join()\n\n        try:\n            connection.close()\n        except:\n            Logger.debug('Comms: Exception closing connection worker connection')\n            Logger.debug(traceback.format_exc())\n    except Exception as e:\n        Logger.debug('Comms: Exception setting up connection process: ' + str(type(e)) + str(e))\n        Logger.trace(traceback.format_exc())\n\n    Logger.debug('Comms: connection worker exited')\n\n\nclass Comms():\n    CONNECT_TIMEOUT = 1.0\n    DEFAULT_TIMEOUT = 1.0\n    QUEUE_FULL_TIMEOUT = 1.0\n    _timeout = DEFAULT_TIMEOUT\n    device = None\n    _connection = None\n    _connection_process = None\n    _rx_queue = None\n    _tx_queue = None\n    _command_queue = None\n\n    def __init__(self, device, connection):\n        self.device = device\n        self._connection = connection\n        self.supports_streaming = False\n\n    def start_connection_process(self):\n        rx_queue = multiprocessing.Queue()\n        tx_queue = multiprocessing.Queue(5)\n        command_queue = multiprocessing.Queue()\n        connection_process = multiprocessing.Process(target=connection_message_process, args=(self._connection, self.device, rx_queue, tx_queue, command_queue))\n        connection_process.start()\n        self._rx_queue = rx_queue\n        self._tx_queue = tx_queue\n        self._command_queue = command_queue\n        self._connection_process = connection_process\n\n    def get_available_devices(self):\n        return self._connection.get_available_devices()\n\n    def isOpen(self):\n        return self._connection_process != None and self._connection_process.is_alive()\n\n    def open(self):\n        connection = self._connection\n        Logger.debug('Comms: Opening connection ' + str(self.device))\n        self.start_connection_process()\n\n    def keep_alive(self):\n        try:\n            self._command_queue.put_nowait(COMMAND_KEEP_ALIVE)\n        except:\n            pass\n\n    def close(self):\n        Logger.debug('Comms: comms.close()')\n        if self.isOpen():\n            try:\n                Logger.debug('Comms: closing connection process')\n                self._command_queue.put_nowait(COMMAND_CLOSE)\n                self._connection_process.join(self._timeout * 2)\n                Logger.debug('Comms: connection process joined')\n            except:\n                Logger.error('Comms: Timeout joining connection process')\n\n    def read_message(self):\n        if not self.isOpen():\n            raise PortNotOpenException('Port Closed')\n        try:\n            return self._rx_queue.get(True, self._timeout)\n        except:  # returns Empty object if timeout is hit\n            return None\n\n    def write_message(self, message):\n        if not self.isOpen(): raise PortNotOpenException('Port Closed')\n        self._tx_queue.put(message, True, Comms.QUEUE_FULL_TIMEOUT)", "original_line_number": 19068, "instruction_length": 1378}
{"instruction": "This code contains functions to generate questions and grade answers for music theory exercises. The \"degree\" function asks for the degree of a given note in a given scale, the \"grade_degree\" function generates a random key and asks the user to identify the degree of a note in that key and scale, the \"triad\" function asks for the notes in a triad built on a given note in a given scale, and the \"grade_triad\" function generates a random key and asks the user to identify the notes in a triad built on a given note in that key and scale. To use these functions, call the appropriate function with the necessary arguments and input the requested information when prompted.", "input": "", "output": "'''\nQuestions generation functions\n'''\n\nimport random\n\n\ndef degree(note, scale, degree):\n    '''\n    What is the <Number> of <Note> <Scale>?\n    '''\n    try:\n        answer = raw_input('What is the {} of {} {}: '.format(str(degree + 1), note, scale.capitalize()))\n        return answer, degree\n    except KeyboardInterrupt:\n        print '\\nQUITTER!'\n        raise SystemExit\n\n\ndef grade_degree(key, note, scale):\n    deg = random.randint(0, 6)\n    answer = key[deg]\n    correct = False\n    while not correct:\n        my_answer, my_degree = degree(note, scale, deg)\n        if my_answer == answer:\n            print 'You Done got it Right!'\n            correct = True\n        else:\n            continue\n\n\ndef triad(note, scale):\n    '''\n    What are the notes in a <NOTE> <Scale> triad?\n    '''\n    try:\n        answer = raw_input('What notes are in a {} {} triad: '.format(note, scale.capitalize()))\n        return answer\n    except KeyboardInterrupt:\n        print '\\nQUITTER!'\n        raise SystemExit\n\n\ndef grade_triad(key, note, scale):\n    correct = False\n    answer_triad = [key[0], key[2], key[4]]\n    my_triad = []\n    while not correct:\n        answer = triad(note, scale)\n        if ',' in answer:\n            my_triad = answer.split(', ')\n            print my_triad\n            if len(my_triad) != 3:\n                my_triad = answer.split(',')\n        else:\n            my_triad = answer.split(' ')\n        if len(my_triad) != 3:\n            print 'Answer with commas or spaces between notes'\n            raise SystemExit\n        validation = [i for i, x in zip(answer_triad, my_triad) if i == x]\n        if len(validation) == 3:\n            print 'You Done got it Right!  '\n            correct = True\n        else:\n            continue", "original_line_number": 19487, "instruction_length": 672}
{"instruction": "The code defines a function called \"partitionBlocDefaut\" that partitions a block by an ellipse generator, an elliptical torus, and a planar crack face. The function takes in the block, the generator, the torus, and the crack face as parameters and returns the partitioned block, the sub-shapes resulting from the partition (the block minus the torus, the generator, the torus, and the crack face), and other relevant information. To use the function, call \"partitionBlocDefaut\" with the appropriate parameters.", "input": "", "output": "# -*- coding: utf-8 -*-\n\nimport logging\nfrom geomsmesh import geompy\n\n# -----------------------------------------------------------------------------\n# --- partition du bloc defaut par generatrice, tore et plan fissure\n\ndef partitionBlocDefaut(volDefaut, facesDefaut, gener, pipe,\n                        facefis, ellipsoide):\n  \"\"\"\n  Partition du bloc defaut par la generatrice de l'ellipse,\n  le tore elliptique, la face plane de fissure\n  @param volDefaut : le bloc defaut\n  @param gener : la generatrice de l'ellipse, positionnée dans l'espace\n  @param pipe : le tore partitionné par le plan de fissure, positionné\n  dans l'espace\n  @param facefis : la face plane de la fissure, positionnée dans l'espace\n  @return (volDefautPart, blocp, tore, faceFissure), le bloc partitionné,\n  les sous shapes issues de la partition\n  (le bloc moins le tore, la generatrice, le tore, la face de fissure)\n  \"\"\"\n  logging.info(\"start\")\n\n  volDefautPart = geompy.MakePartition([volDefaut], [pipe, facefis, ellipsoide], [], [], geompy.ShapeType[\"SOLID\"], 0, [], 1)\n  blocp = geompy.GetInPlaceByHistory(volDefautPart, volDefaut)\n  #gencnt = geompy.GetInPlaceByHistory(volDefautPart, gener)\n  tore = geompy.GetInPlaceByHistory(volDefautPart, pipe)\n  faceFissure = geompy.GetInPlaceByHistory(volDefautPart, facefis)\n  #ellipsoidep =geompy.GetInPlaceByHistory(volDefautPart, ellipsoide)\n\n  geompy.addToStudy( volDefautPart, 'volDefautPart' )\n  geompy.addToStudyInFather( volDefautPart, tore, 'tore' )\n  geompy.addToStudyInFather( volDefautPart, faceFissure, 'FACE1' )\n  #geompy.addToStudyInFather( volDefautPart, gencnt, 'generatrice' )\n\n  solids = geompy.ExtractShapes(blocp, geompy.ShapeType[\"SOLID\"], True)\n  vols = []\n  for i in range(len(solids)):\n    props = geompy.BasicProperties(solids[i])\n    vols.append(props[2])\n  maxvol = max(vols)\n  imaxvol = vols.index(maxvol)\n  blocp = solids[imaxvol]\n  vols[imaxvol] = 0\n  maxvol = max(vols)\n  imaxvol = vols.index(maxvol)\n  ellipsoidep = solids[imaxvol]\n\n  geompy.addToStudyInFather( volDefautPart, blocp, 'bloc' )\n  geompy.addToStudyInFather( volDefautPart, ellipsoidep, 'ellipsoide' )\n\n  sharedFaces = geompy.GetSharedShapesMulti([blocp, ellipsoidep], geompy.ShapeType[\"FACE\"])\n  for i in range(len(sharedFaces)):\n    name = \"faceCommuneEllipsoideBloc_%d\"%i\n    geompy.addToStudyInFather(blocp, sharedFaces[i], name)\n\n  #sharedEdges = geompy.GetSharedShapesMulti([blocp, ellipsoidep], geompy.ShapeType[\"EDGE\"])\n  allSharedEdges = geompy.GetSharedShapesMulti([blocp, ellipsoidep], geompy.ShapeType[\"EDGE\"])\n  sharedEdges = []\n  for i in range(len(allSharedEdges)):\n    if geompy.NbShapes(allSharedEdges[i], geompy.ShapeType[\"VERTEX\"]) > 1: # edge non degeneree\n      sharedEdges.append(allSharedEdges[i])\n  for i in range(len(sharedEdges)):\n    name = \"edgeCommuneEllipsoideBloc_%d\"%i\n    geompy.addToStudyInFather(blocp, sharedEdges[i], name)\n\n  facesExternes = []\n  facesExtBloc = []\n  facesExtElli = []\n  faces = geompy.ExtractShapes(facesDefaut, geompy.ShapeType[\"FACE\"], True)\n  if len(faces) == 0:\n    faces = [facesDefaut]\n  for i in range(len(faces)):\n    faceExt = geompy.GetInPlace(ellipsoidep, faces[i])\n    if faceExt is not None:\n      name = \"faceExterne_e%d\"%i\n      geompy.addToStudyInFather(ellipsoidep, faceExt, name)\n      facesExternes.append(faceExt)\n      facesExtElli.append(faceExt)\n\n    faceExt = geompy.GetInPlace(blocp, faces[i])\n    if faceExt is not None:\n      name = \"faceExterne_b%d\"%i\n      geompy.addToStudyInFather(blocp, faceExt, name)\n      facesExternes.append(faceExt)\n      facesExtBloc.append(faceExt)\n    else:\n      logging.info(\"  recherche faces externes par GetShapesOnShape\")\n      vertex = geompy.MakeVertexOnSurface(faces[i], 0.5, 0.5)\n      normal = geompy.GetNormal(faces[i], vertex)\n      extrusionFace = geompy.MakePrismVecH(faces[i], normal, 1)\n      #extrusionFace = geompy.MakePrismVecH2Ways(faces[i], normal, 0.1)\n      #extrusionFace = geompy.MakeScaleTransform(extrusionFace, vertex, 1.01)\n      name = \"extrusionFace_b%d\"%i\n      geompy.addToStudyInFather(blocp, extrusionFace, name)\n      #facesExt = geompy.GetShapesOnShape(extrusionFace, blocp, geompy.ShapeType[\"FACE\"], GEOM.ST_ONIN)\n      facesExt = geompy.GetShapesOnShape(extrusionFace, blocp, geompy.ShapeType[\"FACE\"], GEOM.ST_ON)\n      for j in range(len(facesExt)):\n        name = \"faceExterne_b%d_%d\"%(i,j)\n        geompy.addToStudyInFather(blocp, facesExt[j], name)\n        facesExternes.append(facesExt[j])\n        facesExtBloc.append(facesExt[j])\n\n  if len(facesExtBloc) < len(faces): # toutes les faces externes du bloc n'ont pas été trouvées. TODO eliminer les detections  multiples\n    logging.info(\"  recherche faces externes par aretes partagees avec faces externes ellipsoide\")\n    facesBloc = geompy.ExtractShapes(blocp, geompy.ShapeType[\"FACE\"], True)\n    for i in range(len(facesBloc)):\n      notOnEllipsoide = True\n      for j in range(len(sharedFaces)): # eliminer les faces communes avec l'ellipsoide\n        if facesBloc[i].IsSame(sharedFaces[j]):\n          notOnEllipsoide = False\n          break\n      if notOnEllipsoide:\n        for j in range(len(facesExtElli)): # les faces recherchees ont une ou plusieurs edge communes avec la ou les faces externes de l'ellipsoide\n          allSharedEdges = []\n          try:\n            allSharedEdges += geompy.GetSharedShapesMulti([facesBloc[i], facesExtElli[j]], geompy.ShapeType[\"EDGE\"])\n          except:\n            pass\n          if len(allSharedEdges) > 0:\n            name = \"faceExterne_b%d_%d\"%(i,j)\n            geompy.addToStudyInFather(blocp, facesBloc[i], name)\n            facesExternes.append(facesBloc[i])\n            facesExtBloc.append(facesBloc[i])\n\n  aretesInternes = []\n  for i in range(len(facesExternes)):\n    for j in range(i+1,len(facesExternes)):\n      shared = []\n      try:\n        shared += geompy.GetSharedShapesMulti([facesExternes[i], facesExternes[j]], geompy.ShapeType[\"EDGE\"])\n      except:\n        logging.info(\"no shared edges in %s,%s\",i,j)\n      else:\n        aretesInternes += shared\n  for i in range(len(aretesInternes)):\n    name = \"aretesInternes_%d\"%i\n    geompy.addToStudyInFather(blocp, aretesInternes[i], name)\n\n  edgesBords = []\n  for faceExtB in facesExtBloc:\n    edges = geompy.ExtractShapes(faceExtB, geompy.ShapeType[\"EDGE\"], True)\n    for i in range(len(edges)):\n      isInterne = False\n      for j in range(len(aretesInternes)):\n        if edges[i].IsSame(aretesInternes[j]):\n          isInterne = True\n          break\n      if not isInterne:\n        edgesBords.append(edges[i])\n        name = \"edgeBord%d\"%i\n        geompy.addToStudyInFather(blocp,edges[i] , name)\n  group = None\n  if len(edgesBords) > 0:\n    group = geompy.CreateGroup(blocp, geompy.ShapeType[\"EDGE\"])\n    geompy.UnionList(group, edgesBords)\n  edgesBords = group\n\n  return volDefautPart, blocp, tore, faceFissure, facesExternes, facesExtBloc, facesExtElli, aretesInternes, ellipsoidep, sharedFaces, sharedEdges, edgesBords", "original_line_number": 19832, "instruction_length": 510}
{"instruction": "To use the Django-mptt package, follow these steps:\n\n1. Import os, INSTALL_SCHEMES, and setup from distutils.command.install.\n2. Define a function called fullsplit that splits a pathname into components in a platform-neutral way.\n3. Modify the INSTALL_SCHEMES to put the data_files in platform-specific installation locations.\n4. Compile the list of packages available and the data files.\n5. Call the setup function with the appropriate parameters, including the name, description, version, author, author_email, url, packages, data_files, and classifiers.", "input": "", "output": "\"\"\"\nBased entirely on Django's own ``setup.py``.\n\"\"\"\nimport os\nfrom distutils.command.install import INSTALL_SCHEMES\nfrom distutils.core import setup\n\ndef fullsplit(path, result=None):\n    \"\"\"\n    Split a pathname into components (the opposite of os.path.join) in a\n    platform-neutral way.\n    \"\"\"\n    if result is None:\n        result = []\n    head, tail = os.path.split(path)\n    if head == '':\n        return [tail] + result\n    if head == path:\n        return result\n    return fullsplit(head, [tail] + result)\n\n# Tell distutils to put the data_files in platform-specific installation\n# locations. See here for an explanation:\n# http://groups.google.com/group/comp.lang.python/browse_thread/thread/35ec7b2fed36eaec/2105ee4d9e8042cb\nfor scheme in INSTALL_SCHEMES.values():\n    scheme['data'] = scheme['purelib']\n\n# Compile the list of packages available, because distutils doesn't have\n# an easy way to do this.\npackages, data_files = [], []\nroot_dir = os.path.dirname(__file__)\nmptt_dir = os.path.join(root_dir, 'mptt')\npieces = fullsplit(root_dir)\nif pieces[-1] == '':\n    len_root_dir = len(pieces) - 1\nelse:\n    len_root_dir = len(pieces)\n\nfor dirpath, dirnames, filenames in os.walk(mptt_dir):\n    # Ignore dirnames that start with '.'\n    for i, dirname in enumerate(dirnames):\n        if dirname.startswith('.'): del dirnames[i]\n    if '__init__.py' in filenames:\n        packages.append('.'.join(fullsplit(dirpath)[len_root_dir:]))\n    elif filenames:\n        data_files.append([dirpath, [os.path.join(dirpath, f) for f in filenames]])\n\nsetup(\n    name = 'django-mptt',\n    description = 'Utilities for implementing Modified Preorder Tree Traversal with your Django Models and working with trees of Model instances',\n    version = '0.3_pre',\n    author = 'Jonathan Buchanan',\n    author_email = 'jonathan.buchanan@gmail.com',\n    url = 'http://code.google.com/p/django-mptt/',\n    packages = packages,\n    data_files = data_files,\n    classifiers = ['Development Status :: 4 - Beta',\n                   'Environment :: Web Environment',\n                   'Framework :: Django',\n                   'Intended Audience :: Developers',\n                   'License :: OSI Approved :: BSD License',\n                   'Operating System :: OS Independent',\n                   'Programming Language :: Python',\n                   'Topic :: Utilities'],\n)", "original_line_number": 19866, "instruction_length": 556}
{"instruction": "Create two spiders named \"cw-all-candidates\" and \"cw-trends\" using Scrapy. The \"cw-all-candidates\" spider should make requests to scrape election results from either the URL \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/ConstituencywiseS03%s.htm?ac=%s\" or \"http://eciresults.nic.in/ConstituencywiseS05%s.htm?ac=%s\" for each of the 40 constituencies, depending on the value of the \"endpoint\" variable. The spider should then parse the response to extract the state, constituency, candidate, party, votes, and status and yield the results. The \"cw-trends\" spider should make requests to scrape election trends from either the URL \"https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03%s.htm\" or \"http://eciresults.nic.in/StatewiseS05%s.htm\" for each of the 3 states, depending on the value of the \"endpoint\" variable. The spider should then parse the response to extract the constituency, constituency number, leading candidate, leading party, trailing candidate, trailing party, margin, and status and yield the results.", "input": "", "output": "import scrapy\nfrom scrapy import Request\n\nclass CWACResultsSpider(scrapy.Spider):\n    name = \"cw-all-candidates\"\n    def start_requests(self):\n        for i in range(40):\n            if self.endpoint == 'archive':            \n                yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/ConstituencywiseS03%s.htm?ac=%s' % (i+1,i+1), callback=self.parse)\n            else:\n                yield Request('http://eciresults.nic.in/ConstituencywiseS05%s.htm?ac=%s' % (i+1,i+1), callback=self.parse)              \n    def parse(self, response):\n\tresults = response.css('#div1 > table > tr')\n        for result in results[3:len(results)-1]:\n            yield {\n                'state': results[0].css('td::text').extract_first().split(' - ')[0],\n\t\t'constituency': results[0].css('td::text').extract_first().split(' - ')[1],\n\t\t'candidate': result.css('td::text')[0].extract(),\n                'party': result.css('td::text')[1].extract(),\n                'votes': result.css('td::text')[2].extract(),\n\t\t'status': results[1].css('td::text').extract_first(),\n            }\n\nclass CWTrendsSpider(scrapy.Spider):\n    name = \"cw-trends\"\n    def start_requests(self):\n        if self.endpoint == 'archive':\n            yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03.htm', callback=self.parse)\n        else:\n            yield Request('http://eciresults.nic.in/StatewiseS05.htm', callback=self.parse)\n        for i in range(3):\n            if self.endpoint == 'archive':\n                yield Request('https://web.archive.org/web/20160823114553/http://eciresults.nic.in/StatewiseS03%s.htm' % (i+1), callback=self.parse)\n            else:  \n                yield Request('http://eciresults.nic.in/StatewiseS05%s.htm' % (i+1), callback=self.parse)\n    \n    def parse(self, response):\n        results = response.css('#divACList > table > tr')\n        for result in results[4:len(results)-1]:\n            yield {\n                'constituency': result.css('td::text')[0].extract(),\n                'const. no.': result.css('td::text')[1].extract(),\n                'leading candidate': result.css('td::text')[2].extract(),\n                'leading party': result.css('td::text')[3].extract(),\n\t\t'trailing candidate': result.css('td::text')[4].extract(),\n\t\t'trailing party': result.css('td::text')[5].extract(),\n\t\t'margin': result.css('td::text')[6].extract(),\n\t\t'status': result.css('td::text')[7].extract()\n            }", "original_line_number": 20129, "instruction_length": 1069}
{"instruction": "This code is a script to filter out isoforms from peptide files in FASTA ENSEMBL or NCBI format. It can also add an easier to read species label to each sequence within the file. To use this script, you need to specify the input file, input file type, and output file. For ENSEMBL files, use the following command: python isoform_filter.py -i [input_fasta_file] -t ens -l [species_label] -o [output_filename]. For NCBI files, use the following command: python isoform_filter.py -i [input_fasta_file] -t ncbi -g [toplevel_gff_file] -l [species_label] -o [output_filename]. To just do the relabeling, set -f 0. You shouldn't need a gff file for the NCBI file in this case. For NCBI relabeling, the gene ID is also moved to the front of the title line.", "input": "", "output": "#!/usr/bin/python\n########################################################################################\n#Script to filter out isoforms from peptide files in FASTA ENSEMBL or NCBI format. This\n#script can also add an easier to read species label to each sequence within the file.\n#\n#Sample ENSEMBL usage: python isoform_filter.py -i [input_fasta_file] -t ens -l [species_label] -o [output_filename]\n#\n#Sample NCBI usage: python isoform_filter.py -i [input_fasta_file] -t ncbi -g [toplevel_gff_file] -l [species_label] -o [output_filename]\n#\n#To just do the relabeling, set -f 0. You shouldn't need a gff file for the NCBI file in\n#this case. For NCBI relabeling, the gene ID is also moved to the front of the title line.\n#\n#Written by: Gregg Thomas, Summer 2014\n#\n#NCBI filter command kindly provided by the folks at NCBI.\n#\n########################################################################################\n\nimport sys, re, os, argparse\nsys.path.append(os.path.dirname(os.path.realpath(__file__)) + \"/corelib/\")\nimport core\n\n############################################\n#Function definitions.\n\ndef optParse():\n#This function handles the command line options.\n\n\tparser = argparse.ArgumentParser()\n\n\tparser.add_argument(\"-i\", dest=\"input_file\", help=\"An input file containing peptides from a species in FASTA format\");\n\tparser.add_argument(\"-t\", dest=\"file_type\", help=\"Currently supported file types are ENSEMBL and NCBI peptide files. Enter as 'ens' or 'ncbi' here. Note: If file type is NCBI you will also need to specify the top level gff file with -g\")\n\tparser.add_argument(\"-g\", dest=\"gff_file\", help=\"If file type is NCBI, the top level gff file is also needed and should be specified here.\");\n\tparser.add_argument(\"-l\", dest=\"spec_label\", help=\"A species label to add to the gene ID of each sequence.\", default=\"\");\n\tparser.add_argument(\"-o\", dest=\"output_file\", help=\"The desired name of the output file. If none is specified the default is [input_filename]_isofiltered.fa or [input_filename]_isofiltered_relabel.fa\");\n\n\targs = parser.parse_args();\n\n\tif None in [args.input_file, args.file_type, args.output_file]:\n\t\tsys.exit(core.errorOut(1, \"An input file (-i), input file type (-t), and an output file (-o) must all be specified\"));\n\n\tif args.file_type not in ['ens', 'ncbi']:\n\t\tsys.exit(core.errorOut(2, \"File type (-t) must be one of either 'ens' (Ensembl) or 'ncbi'\"));\n\n\tif args.file_type == \"ens\" and args.gff_file != None:\n\t\tsys.exit(core.errorOut(3, \"A gff file (-g) should not be specified with file type ens\"));\n\n\tif args.file_type == \"ncbi\" and args.gff_file == None:\n\t\tsys.exit(core.errorOut(4, \"A gff file (-g) must be specified with file type ncbi\"));\n\n\treturn args.input_file, args.file_type, args.gff_file, args.spec_label, args.output_file;\n\n############################################\ndef ensFilter(inseqs, spec_label, outfilename):\n\n\tprint \"Indexing\", len(inseqs), \"sequences to be filtered.\";\n\tprint \"Parsing identifiers...\";\n\n\tfor title in inseqs:\n\t\tgeneid = title[title.index(\"gene:\") + 5:title.index(\"gene:\") + 23];\n\n\t\tif geneid in identDict:\n\t\t\tidentDict[geneid].append((title, inseqs[title]));\n\t\telse:\n\t\t\tidentDict[geneid] = [];\n\t\t\tidentDict[geneid].append((title, inseqs[title]));\n\tsys.stderr.write('\\b');\n\n\tprint \"Filtering and writing sequences...\";\n\tnumbars, donepercent, i = 0,[],0;\n\n\tfor key in identDict:\n\t\tnumbars, donepercent = core.loadingBar(i, len(identDict), donepercent, numbars);\n\n\t\tif len(identDict[key]) == 1:\n\t\t\tlong_title, long_seq = identDict[key][0];\n\n\t\telse:\n\t\t\ttitlelist = [];\n\t\t\tseqlist = [];\n\n\t\t\tfor tup in identDict[key]:\n\t\t\t\tcur_itle, cur_seq = tup;\n\t\t\t\ttitlelist.append(cur_itle);\n\t\t\t\tseqlist.append(cur_seq);\n\n\t\t\tlong_seq = max(seqlist, key=len)\n\t\t\tlong_title = titlelist[seqlist.index(long_seq)];\n\n\t\tnew_title = \">\" + spec_label + \"_\" + long_title[1:];\n\t\tcore.writeSeq(outfilename, long_seq, new_title);\n\t\ti += 1;\n\n\tpstring = \"100.0% complete.\";\n\tsys.stderr.write('\\b' * len(pstring) + pstring);\n\tprint \"\\nDone!\";\n\tprint i, \"sequences written.\";\n\tprint len(inseqs) - i, \"sequences filtered.\";\n\n############################################\ndef ncbiFilter(inseqs, gff_file, spec_label, outfilename):\n\n\tnumbars, donepercent, i = 0, [], 0;\n\n\n\tprint \"Obtaining longest isoforms from .gff file...\";\n\n\tcmd = \"zcat \" + gff_file + \" | awk \\'BEGIN{FS=\\\"\t\\\";OFS=\\\"|\\\"}$3==\\\"CDS\\\"{if($4<$5){print $5-$4+1,$9}else{print $4-$5+1,$9}}\\' | grep \\\"[NX]P[_]\\\" | sed \\'s/\\([0-9]*\\).*GeneID:\\([0-9]*\\).*\\([NX]P[_][0-9]*\\.[0-9]*\\).*/\\\\1|\\\\2|\\\\3/\\' | awk \\'BEGIN{FS=\\\"|\\\";OFS=\\\"\\t\\\";gene=\\\"\\\";acc=\\\"\\\";len=0}{if(acc!=$3){print gene,acc,len/3-1;gene=$2;acc=$3;len=$1}else{len=len+$1}}END{print gene,acc,len/3-1}\\' | sort -k1,1n -k3,3nr -k2,2 | awk \\'BEGIN{FS=\\\"\t\\\";OFS=\\\"\t\\\";gene=\\\"\\\";acc=\\\"\\\";len=0}{if(gene!=$1){print $1,$2,$3};gene=$1;acc=$2;len=$3}\\' > ncbi_isoform_filter_tmp11567.txt\"\n\tos.system(cmd);\n\n\ttmpFile = open(\"ncbi_isoform_filter_tmp11567.txt\", \"r\");\n\ttmpLines = tmpFile.readlines();\n\ttmpFile.close();\n\tos.system(\"rm ncbi_isoform_filter_tmp11567.txt\");\n\n\tlongest_isos = [];\n\n\tfor each in tmpLines:\n\t\tlongest_isos.append(each.split(\"\\t\")[1]);\n\tlongest_isos = filter(None, longest_isos);\n\n\tprint \"Writing longest isoforms to output file...\";\n\n\tcount = 0;\n\n\tfor title in inseqs:\n\t\tnumbars, donepercent = core.loadingBar(i, len(inseqs), donepercent, numbars);\n\t\ti += 1;\n\n\t\tfound = 0;\n\n\t\tfor gid in longest_isos:\n\t\t\tif gid in title:\n\t\t\t\tgid = title[title.index(\"P_\")-1:title.index(\"|\",title.index(\"P_\"))]\n\t\t\t\tnew_title = \">\" + spec_label + \"_\" + gid + \" |\" + title[1:title.index(\"P_\")-1] + title[title.index(\"|\",title.index(\"P_\"))+1:];\n\t\t\t\tcore.writeSeq(outfilename, inseqs[title], new_title);\n\t\t\t\tcount += 1;\n\t\t\t\tbreak;\n\n\tpstring = \"100.0% complete.\";\n\tsys.stderr.write('\\b' * len(pstring) + pstring);\n\tprint \"\\nDone!\";\n\tprint count, \"sequences written.\";\n\tprint len(inseqs) - count, \"sequences filtered.\";\n\n############################################\n#Main Block\n############################################\n\ninfilename, in_type, gff_file, label, outfilename = optParse();\n\npad = 50;\nprint \"=======================================================================\";\nprint \"\\t\\t\\t\" + core.getDateTime();\nprint core.spacedOut(\"Filtering isoforms from:\", pad), infilename;\nif in_type == \"ens\":\n\tprint core.spacedOut(\"File type:\", pad), \"Ensembl\";\nif in_type == \"ncbi\":\n\tprint core.spacedOut(\"File type:\", pad), \"NCBI\";\n\tprint core.spacedOut(\"Using GFF file:\", pad), gff_file;\nif in_type == \"crow\":\n\tprint core.spacedOut(\"File type:\", pad), \"Crow\";\nif label != \"\":\n\tprint core.spacedOut(\"Adding label to beginning of FASTA headers:\", pad), label;\nprint core.spacedOut(\"Writing output to:\", pad), outfilename;\ncore.filePrep(outfilename);\nprint \"--------------------------\";\n\nidentDict = {};\nins, skip_flag = core.fastaReader(infilename);\n\nif in_type == \"ens\":\n\tensFilter(ins, label, outfilename);\nelif in_type == \"ncbi\":\n\tncbiFilter(ins, gff_file, label, outfilename);\n\nprint \"=======================================================================\";\n\n\n\n\n\n\n\n\n\n## DEFUNCT FILTER FOR THE CROW FILES\n# elif in_type == \"crow\":\n# \tcrowFilter(ins, label, outfilename);\n\n# def crowFilter(inSeqs, filterflag, speclabel, outFilename):\n# \trotator = 0;\n# \tnumbars = 0;\n# \tdonepercent = [];\n# \ti = 0;\n\n# \tif filterflag == 1:\n# \t\tprint \"Indexing\", len(inSeqs), \"sequences to be filtered.\";\n# \t\tprint \"Parsing identifiers...\";\n\n# \t\tfor each in inSeqs:\n\n# \t\t\trotator = core.loadingRotator(i, rotator, 100)\n\n# \t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, each[0], each[1], each[2], each[3]);\n\n# \t\t\tif \"gene=\" not in curTitle:\n# \t\t\t\tprint curTitle;\n# \t\t\t\tcontinue;\n\n# \t\t\tgeneid = curTitle[curTitle.index(\"gene=\") + 5:].strip();\n\n# \t\t\tif geneid in identDict:\n# \t\t\t\tidentDict[geneid].append(each);\n\n# \t\t\telse:\n# \t\t\t\tidentDict[geneid] = [];\n# \t\t\t\tidentDict[geneid].append(each);\n\n# \t\t\ti = i + 1;\n\n# \t\tsys.stderr.write('\\b');\n\n# \t\tprint \"Filtering and writing sequences...\";\n\n# \t\ti = 0;\n# \t\tcount = 0;\n\n# \t\tfor key in identDict:\n\n# \t\t\tnumbars, donepercent = core.loadingBar(i, len(identDict), donepercent, numbars);\n\n# \t\t\tif len(identDict[key]) == 1:\n# \t\t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, identDict[key][0][0], identDict[key][0][1], identDict[key][0][2], identDict[key][0][3]);\n\n# \t\t\t\tif speclabel != \"\":\n# \t\t\t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n# \t\t\t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n# \t\t\t\telse:\n# \t\t\t\t\tcore.writeSeq(outFilename, curSeq, curTitle);\n\n# \t\t\t\tcount = count + 1;\n\n# \t\t\telse:\n# \t\t\t\ttitlelist = [];\n# \t\t\t\tseqlist = [];\n\n# \t\t\t\tfor inds in identDict[key]:\n# \t\t\t\t\taTitle, aSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\ttitlelist.append(aTitle);\n# \t\t\t\t\tseqlist.append(aSeq);\n\n# \t\t\t\tlongseq = max(seqlist, key=len)\n\n# \t\t\t\tfor inds in identDict[key]:\n# \t\t\t\t\taTitle, aSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\tif aSeq == longseq:\n# \t\t\t\t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, inds[0], inds[1], inds[2], inds[3]);\n\n# \t\t\t\t\t\tif speclabel != \"\":\n# \t\t\t\t\t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n# \t\t\t\t\t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n# \t\t\t\t\t\telse:\n# \t\t\t\t\t\t\tcore.writeSeq(outFilename, curSeq, curTitle);\n\n# \t\t\t\t\t\tcount = count + 1;\n# \t\t\t\t\t\tbreak;\n\n# \t\t\ti = i + 1;\n\n# \t\tpstring = \"100.0% complete.\";\n# \t\tsys.stderr.write('\\b' * len(pstring) + pstring);\n# \t\tprint \"\\nDone!\";\n# \t\tprint count, \"out of\", len(identDict), \"identifiers written.\";\n# \t\tprint len(inSeqs) - count, \"sequences filtered.\";\n\n# \telse:\n# \t\tprint \"Relabeling...\";\n# \t\tfor seq in inSeqs:\n\t\n# \t\t\tnumbars, donepercent = core.loadingBar(i, len(inSeqs), donepercent, numbars);\n# \t\t\ti = i + 1;\n\n# \t\t\tcurTitle, curSeq = core.getFastafromInd(inFilename, seq[0], seq[1], seq[2], seq[3]);\n\n# \t\t\tnewTitle = \">\" + speclabel + \"_\" + curTitle[1:];\n\n# \t\t\tcore.writeSeq(outFilename, curSeq, newTitle);\n\n\n# \t\tpstring = \"100.0% complete.\";\n# \t\tsys.stderr.write('\\b' * len(pstring) + pstring);\n# \t\tprint \"\\nDone!\";", "original_line_number": 20290, "instruction_length": 749}
{"instruction": "This code defines a Python function called `dd_hyperbolic_single` that runs a Bayesian analysis of the Delay Discounting Task using the Hyperbolic Model. The function takes in a data set in the form of a Pandas DataFrame, a file path, or the string \"example\", and returns an `hbayesdm.TaskModel` instance with various components, including the fitted model and raw data. The function has several optional arguments that allow for customization of the analysis, such as the number of iterations, number of chains, and initial values. To use the function, call `dd_hyperbolic_single()` with the desired arguments.", "input": "", "output": "from typing import Sequence, Union, Any\nfrom collections import OrderedDict\n\nfrom numpy import Inf, exp\nimport pandas as pd\n\nfrom hbayesdm.base import TaskModel\nfrom hbayesdm.preprocess_funcs import dd_single_preprocess_func\n\n__all__ = ['dd_hyperbolic_single']\n\n\nclass DdHyperbolicSingle(TaskModel):\n    def __init__(self, **kwargs):\n        super().__init__(\n            task_name='dd',\n            model_name='hyperbolic',\n            model_type='single',\n            data_columns=(\n                'subjID',\n                'delay_later',\n                'amount_later',\n                'delay_sooner',\n                'amount_sooner',\n                'choice',\n            ),\n            parameters=OrderedDict([\n                ('k', (0, 0.1, 1)),\n                ('beta', (0, 1, 5)),\n            ]),\n            regressors=OrderedDict([\n                \n            ]),\n            postpreds=['y_pred'],\n            parameters_desc=OrderedDict([\n                ('k', 'discounting rate'),\n                ('beta', 'inverse temperature'),\n            ]),\n            additional_args_desc=OrderedDict([\n                \n            ]),\n            **kwargs,\n        )\n\n    _preprocess_func = dd_single_preprocess_func\n\n\ndef dd_hyperbolic_single(\n        data: Union[pd.DataFrame, str, None] = None,\n        niter: int = 4000,\n        nwarmup: int = 1000,\n        nchain: int = 4,\n        ncore: int = 1,\n        nthin: int = 1,\n        inits: Union[str, Sequence[float]] = 'vb',\n        ind_pars: str = 'mean',\n        model_regressor: bool = False,\n        vb: bool = False,\n        inc_postpred: bool = False,\n        adapt_delta: float = 0.95,\n        stepsize: float = 1,\n        max_treedepth: int = 10,\n        **additional_args: Any) -> TaskModel:\n    \"\"\"Delay Discounting Task - Hyperbolic Model\n\n    Individual Bayesian Modeling of the Delay Discounting Task \n    using Hyperbolic Model [Mazur1987]_ with the following parameters:\n    \"k\" (discounting rate), \"beta\" (inverse temperature).\n\n    \n\n    \n    .. [Mazur1987] Mazur, J. E. (1987). An adjustment procedure for studying delayed reinforcement.\n\n    \n\n    User data should contain the behavioral data-set of all subjects of interest for\n    the current analysis. When loading from a file, the datafile should be a\n    **tab-delimited** text file, whose rows represent trial-by-trial observations\n    and columns represent variables.\n\n    For the Delay Discounting Task, there should be 6 columns of data\n    with the labels \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\". It is not necessary for the columns to be\n    in this particular order; however, it is necessary that they be labeled\n    correctly and contain the information below:\n\n    - \"subjID\": A unique identifier for each subject in the data-set.\n    - \"delay_later\": An integer representing the delayed days for the later option (e.g. 1, 6, 28).\n    - \"amount_later\": A floating point number representing the amount for the later option (e.g. 10.5, 13.4, 30.9).\n    - \"delay_sooner\": An integer representing the delayed days for the sooner option (e.g. 0).\n    - \"amount_sooner\": A floating point number representing the amount for the sooner option (e.g. 10).\n    - \"choice\": If amount_later was selected, choice == 1; else if amount_sooner was selected, choice == 0.\n\n    .. note::\n        User data may contain other columns of data (e.g. ``ReactionTime``,\n        ``trial_number``, etc.), but only the data within the column names listed\n        above will be used during the modeling. As long as the necessary columns\n        mentioned above are present and labeled correctly, there is no need to\n        remove other miscellaneous data columns.\n\n    .. note::\n\n        ``adapt_delta``, ``stepsize``, and ``max_treedepth`` are advanced options that\n        give the user more control over Stan's MCMC sampler. It is recommended that\n        only advanced users change the default values, as alterations can profoundly\n        change the sampler's behavior. See [Hoffman2014]_ for more information on the\n        sampler control parameters. One can also refer to 'Section 34.2. HMC Algorithm\n        Parameters' of the `Stan User's Guide and Reference Manual`__.\n\n        .. [Hoffman2014]\n            Hoffman, M. D., & Gelman, A. (2014).\n            The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo.\n            Journal of Machine Learning Research, 15(1), 1593-1623.\n\n        __ https://mc-stan.org/users/documentation/\n\n    Parameters\n    ----------\n    data\n        Data to be modeled. It should be given as a Pandas DataFrame object,\n        a filepath for a data file, or ``\"example\"`` for example data.\n        Data columns should be labeled as: \"subjID\", \"delay_later\", \"amount_later\", \"delay_sooner\", \"amount_sooner\", \"choice\".\n    niter\n        Number of iterations, including warm-up. Defaults to 4000.\n    nwarmup\n        Number of iterations used for warm-up only. Defaults to 1000.\n\n        ``nwarmup`` is a numerical value that specifies how many MCMC samples\n        should not be stored upon the beginning of each chain. For those\n        familiar with Bayesian methods, this is equivalent to burn-in samples.\n        Due to the nature of the MCMC algorithm, initial values (i.e., where the\n        sampling chains begin) can have a heavy influence on the generated\n        posterior distributions. The ``nwarmup`` argument can be set to a\n        higher number in order to curb the effects that initial values have on\n        the resulting posteriors.\n    nchain\n        Number of Markov chains to run. Defaults to 4.\n\n        ``nchain`` is a numerical value that specifies how many chains (i.e.,\n        independent sampling sequences) should be used to draw samples from\n        the posterior distribution. Since the posteriors are generated from a\n        sampling process, it is good practice to run multiple chains to ensure\n        that a reasonably representative posterior is attained. When the\n        sampling is complete, it is possible to check the multiple chains for\n        convergence by running the following line of code:\n\n        .. code:: python\n\n            output.plot(type='trace')\n    ncore\n        Number of CPUs to be used for running. Defaults to 1.\n    nthin\n        Every ``nthin``-th sample will be used to generate the posterior\n        distribution. Defaults to 1. A higher number can be used when\n        auto-correlation within the MCMC sampling is high.\n\n        ``nthin`` is a numerical value that specifies the \"skipping\" behavior\n        of the MCMC sampler. That is, only every ``nthin``-th sample is used to\n        generate posterior distributions. By default, ``nthin`` is equal to 1,\n        meaning that every sample is used to generate the posterior.\n    inits\n        String or list specifying how the initial values should be generated.\n        Options are ``'fixed'`` or ``'random'``, or your own initial values.\n    ind_pars\n        String specifying how to summarize the individual parameters.\n        Current options are: ``'mean'``, ``'median'``, or ``'mode'``.\n    model_regressor\n        Whether to export model-based regressors. Currently not available for this model.\n    vb\n        Whether to use variational inference to approximately draw from a\n        posterior distribution. Defaults to ``False``.\n    inc_postpred\n        Include trial-level posterior predictive simulations in\n        model output (may greatly increase file size). Defaults to ``False``.\n    adapt_delta\n        Floating point value representing the target acceptance probability of a new\n        sample in the MCMC chain. Must be between 0 and 1. See note below.\n    stepsize\n        Integer value specifying the size of each leapfrog step that the MCMC sampler\n        can take on each new iteration. See note below.\n    max_treedepth\n        Integer value specifying how many leapfrog steps the MCMC sampler can take\n        on each new iteration. See note below.\n    **additional_args\n        Not used for this model.\n\n    Returns\n    -------\n    model_data\n        An ``hbayesdm.TaskModel`` instance with the following components:\n\n        - ``model``: String value that is the name of the model ('dd_hyperbolic_single').\n        - ``all_ind_pars``: Pandas DataFrame containing the summarized parameter values\n          (as specified by ``ind_pars``) for each subject.\n        - ``par_vals``: OrderedDict holding the posterior samples over different parameters.\n        - ``fit``: A PyStan StanFit object that contains the fitted Stan model.\n        - ``raw_data``: Pandas DataFrame containing the raw data used to fit the model,\n          as specified by the user.\n        \n\n    Examples\n    --------\n\n    .. code:: python\n\n        from hbayesdm import rhat, print_fit\n        from hbayesdm.models import dd_hyperbolic_single\n\n        # Run the model and store results in \"output\"\n        output = dd_hyperbolic_single(data='example', niter=2000, nwarmup=1000, nchain=4, ncore=4)\n\n        # Visually check convergence of the sampling chains (should look like \"hairy caterpillars\")\n        output.plot(type='trace')\n\n        # Plot posterior distributions of the hyper-parameters (distributions should be unimodal)\n        output.plot()\n\n        # Check Rhat values (all Rhat values should be less than or equal to 1.1)\n        rhat(output, less=1.1)\n\n        # Show the LOOIC and WAIC model fit estimates\n        print_fit(output)\n    \"\"\"\n    return DdHyperbolicSingle(\n        data=data,\n        niter=niter,\n        nwarmup=nwarmup,\n        nchain=nchain,\n        ncore=ncore,\n        nthin=nthin,\n        inits=inits,\n        ind_pars=ind_pars,\n        model_regressor=model_regressor,\n        vb=vb,\n        inc_postpred=inc_postpred,\n        adapt_delta=adapt_delta,\n        stepsize=stepsize,\n        max_treedepth=max_treedepth,\n        **additional_args)", "original_line_number": 20340, "instruction_length": 611}
{"instruction": "This is a Python code for creating user registration form with additional fields. To use this code, import the necessary modules and create a UserForm, UserProfileForm, and UserRegistrationForm classes. The UserRegistrationForm class extends the UserCreationForm class and adds email2, timezone2, and skype fields. The clean_email2 method checks if the email already exists in the database and raises a validation error if it does. The save method saves the user's email, timezone, and skype fields to the UserProfile model.", "input": "", "output": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Copyright (C) 2011 Adriano Monteiro Marques\n#\n# Author: Amit Pal <amix.pal@gmail.com>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU Affero General Public License as\n# published by the Free Software Foundation, either version 3 of the\n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Affero General Public License for more details.\n#\n# You should have received a copy of the GNU Affero General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom django import forms\nfrom django.contrib.auth.forms import UserCreationForm\nfrom django.contrib.auth.forms import AdminPasswordChangeForm\nfrom django.contrib.auth.models import User\nfrom django.utils.translation import ugettext as _\nfrom datetime import datetime\nimport pytz\nfrom pytz import timezone\nimport pdb\n\nfrom netadmin.users.models import UserProfile\n\n\nclass UserForm(forms.ModelForm):\n    class Meta:\n        model = User\n        fields = ('first_name', 'last_name', 'email')\n\nclass UserProfileForm(forms.ModelForm):\n    class Meta:\n        model = UserProfile\n        fields = ('is_public', 'in_search')\n        \nclass UserRegistrationForm(UserCreationForm):\n    email2 = forms.EmailField(label=_(\"E-mail\"))\n    timezone2 = forms.ChoiceField(choices=[(x, x) for x in pytz.common_timezones], label = _(\"TimeZone\"))\n    skype = forms.CharField(max_length=20)\n    \n    def clean_email2(self):\n        email2 = self.cleaned_data['email2']\n        try:\n            user = User.objects.get(email=email2)\n        except User.DoesNotExist:\n            return email2\n        raise forms.ValidationError(_(\"Account with this e-mail address already exists.\"))\n    \n    def save(self, commit=True):\n        user = super(UserRegistrationForm, self).save(commit=False)\n        user.email = self.cleaned_data[\"email2\"]\n        user.is_active = False\n        if commit:\n            user.save()\n            user_profile = user.get_profile()\n            user_profile.timezone = self.cleaned_data[\"timezone2\"]\n            user_profile.skype = self.cleaned_data[\"skype\"]\n            user_profile.save()\n        return user", "original_line_number": 20432, "instruction_length": 524}
{"instruction": "This code is for an asynchronous queue producer. To use it, first import the aioamqp library. Then, create an instance of the AsyncQueueProducer class and pass in the app object. Call the connect method to establish a connection to the RabbitMQ server. Use the create_queue method to create a new queue, passing in the queue name if desired. Use the delete_queue method to delete a queue. To add a message to a queue, use the add_msg_to_queue method, passing in the queue name and message. To get a response from a message, use the add_msg_to_queue_with_response method, passing in the queue name and message.", "input": "", "output": "# coding: utf-8\n\nimport aioamqp\nimport uuid\nimport logging\nfrom core.utils import async_wait_for\n\n\nlog = logging.getLogger(__name__)\n\n\nclass AsyncQueueProducer(object):\n    messages = {}\n    connection = None\n    channel = None\n    consumer_tag = None\n    responses_queue = None\n    commands_queue = None\n\n    def __init__(self, app):\n        self.app = app\n\n    async def connect(self):\n        params = {\n            'loop': self.app.loop,\n            'login': self.app.cfg.RABBITMQ_USER,\n            'password': self.app.cfg.RABBITMQ_PASSWORD,\n            'host': self.app.cfg.RABBITMQ_HOST,\n            'port': self.app.cfg.RABBITMQ_PORT\n        }\n        self.connection = await self.make_connection(params)\n        self.channel = await self.connection.channel()\n        self.responses_queue, self.consumer_tag = await self.create_queue_and_consume()\n        self.commands_queue = await self.create_queue(self.app.cfg.RABBITMQ_COMMAND_QUEUE)\n\n    async def create_queue(self, queue_name=None):\n        if not queue_name:\n            result = await self.channel.queue_declare(exclusive=True)\n        else:\n            result = await self.channel.queue_declare(queue_name=queue_name)\n        queue, messages, consumers = result.get('queue'), result.get('message_count'), result.get('consumer_count')\n        log.info(\"Queue %s was declared(messages: %s, consumers: %s)\" % (queue, messages, consumers))\n        return queue\n\n    async def delete_queue(self, queue_name):\n        await self.channel.queue_delete(queue_name)\n        log.info('Queue %s was deleted' % queue_name)\n\n    async def create_queue_and_consume(self, queue_name=None):\n        if not queue_name:\n            queue_name = await self.create_queue()\n        else:\n            await self.create_queue(queue_name)\n        consumer_tag = await self.queue_consume(queue_name)\n        return queue_name, consumer_tag\n\n    @staticmethod\n    async def make_connection(params):\n        transport, connection = await aioamqp.connect(**params)\n        return connection\n\n    async def queue_consume(self, queue_name):\n        log.info(\"Start consuming for queue %s\" % queue_name)\n        await self.channel.basic_consume(\n            callback=self.on_message, queue_name=queue_name, no_ack=False\n        )\n\n    async def on_message(self, channel, body, envelope, properties):\n        log.debug(\"Got new message %s\" % body)\n        for correlation_id in list(self.messages.keys()):\n            if correlation_id == properties.correlation_id:\n                log.info(\"Response with corr_id %s from queue %s: %s\" % (correlation_id, self.responses_queue, body))\n                self.messages[correlation_id][\"response\"] = body\n                channel.basic_client_ack(delivery_tag=envelope.delivery_tag)\n\n    async def add_msg_to_queue(self, queue_name, msg):\n        correlation_id = str(uuid.uuid4())\n        await self.channel.basic_publish(\n            payload=str(msg),\n            exchange_name='',\n            routing_key=queue_name,\n            properties={\n                \"reply_to\": self.responses_queue,\n                \"correlation_id\": correlation_id\n            })\n        log.info(\"Message(id:%s body: %s) was published to %s\" % (correlation_id, msg, queue_name))\n        self.messages[correlation_id] = {\"request\": msg, \"response\": None}\n        return correlation_id\n\n    async def get_message_from_queue(self, correlation_id):\n        log.info(\"Waiting response for message with id: %s\" % correlation_id)\n        response = await async_wait_for(\n            lambda: self.messages.get(correlation_id).get(\"response\"),\n            self.app.loop,\n            timeout=self.app.cfg.BACKEND_REQUEST_TIMEOUT\n        )\n        del self.messages[correlation_id]\n        log.info(\"Got response %s for message with id: %s\" % (response, correlation_id))\n        return response\n\n    async def add_msg_to_queue_with_response(self, queue_name, msg):\n        correlation_id = await self.add_msg_to_queue(queue_name, msg)\n        response = await self.get_message_from_queue(correlation_id)\n        return response", "original_line_number": 20442, "instruction_length": 609}
{"instruction": "This code defines a Python class called PcaPlotVector that inherits from BaseCClass and has several methods for accessing and manipulating data. To use this code, import the necessary modules from ert.cwrap, ert.enkf, and ert.util. Then, create an instance of the PcaPlotVector class with the required arguments: an integer component, a Matrix object for the principal component matrix, and a Matrix object for the observation principal component matrix. The resulting object can be used to access the data stored in the matrices, including the size, individual values, observation value, and singular value. When finished using the object, call the free() method to release the memory allocated for it.", "input": "", "output": "from ert.cwrap import BaseCClass, CWrapper\nfrom ert.enkf import ENKF_LIB\nfrom ert.util import Matrix\n\n\nclass PcaPlotVector(BaseCClass):\n\n    def __init__(self, component, principal_component_matrix, observation_principal_component_matrix):\n        assert isinstance(component, int)\n        assert isinstance(principal_component_matrix, Matrix)\n        assert isinstance(observation_principal_component_matrix, Matrix)\n\n        c_pointer = PcaPlotVector.cNamespace().alloc(component, principal_component_matrix, observation_principal_component_matrix)\n        super(PcaPlotVector, self).__init__(c_pointer)\n\n\n    def __len__(self):\n        \"\"\" @rtype: int \"\"\"\n        return PcaPlotVector.cNamespace().size(self)\n\n\n    def __getitem__(self, index):\n        \"\"\"\n        @type index: int\n        @rtype: float \n        \"\"\"\n        assert isinstance(index, int)\n        return PcaPlotVector.cNamespace().get(self, index)\n\n    def __iter__(self):\n        cur = 0\n        while cur < len(self):\n            yield self[cur]\n            cur += 1\n\n    def getObservation(self):\n        \"\"\" @rtype: float \"\"\"\n        return PcaPlotVector.cNamespace().get_obs(self)\n\n    def getSingularValue(self):\n        \"\"\" @rtype: float \"\"\"\n        return PcaPlotVector.cNamespace().get_singular_value(self)\n        \n\n    def free(self):\n        PcaPlotVector.cNamespace().free(self)\n\n\n\ncwrapper = CWrapper(ENKF_LIB)\ncwrapper.registerType(\"pca_plot_vector\", PcaPlotVector)\ncwrapper.registerType(\"pca_plot_vector_obj\", PcaPlotVector.createPythonObject)\ncwrapper.registerType(\"pca_plot_vector_ref\", PcaPlotVector.createCReference)\n\nPcaPlotVector.cNamespace().alloc   = cwrapper.prototype(\"c_void_p pca_plot_vector_alloc(int, matrix, matrix)\")\nPcaPlotVector.cNamespace().free    = cwrapper.prototype(\"void pca_plot_vector_free(pca_plot_vector)\")\nPcaPlotVector.cNamespace().size    = cwrapper.prototype(\"int pca_plot_vector_get_size(pca_plot_vector)\")\nPcaPlotVector.cNamespace().get     = cwrapper.prototype(\"double pca_plot_vector_iget_sim_value(pca_plot_vector, int)\")\nPcaPlotVector.cNamespace().get_obs = cwrapper.prototype(\"double pca_plot_vector_get_obs_value(pca_plot_vector)\")\nPcaPlotVector.cNamespace().get_singular_value = cwrapper.prototype(\"double pca_plot_vector_get_singular_value(pca_plot_vector)\")", "original_line_number": 21442, "instruction_length": 703}
{"instruction": "The code is an implementation of the Expectation-Maximization algorithm for Gaussian Mixture Models. To use the code, first import the necessary libraries: sframe, numpy, matplotlib.pyplot, scipy.stats, copy, PIL, io, and matplotlib.mlab. Then, define the function generate_MoG_data to generate data from a Gaussian mixture model with given means, covariances, and weights. Next, define the function log_sum_exp to compute the log of the sum of exponentials of an array. Then, define the function loglikelihood to compute the loglikelihood of a given dataset for a Gaussian mixture model with given parameters. Next, define the function EM to run the Expectation-Maximization algorithm on a given dataset with given initial parameters. Finally, use the functions plot_contours, plot_responsibilities_in_RB, get_top_images, and save_images to visualize the results of the EM algorithm on a given dataset.", "input": "", "output": "# Imports\nimport sframe\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nimport copy\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.mlab as mlab\nimport colorsys\n\n\ndef generate_MoG_data(num_data, means, covariances, weights):\n    data = []\n    for i in range(num_data):\n        #  Use np.random.choice and weights to pick a cluster id greater than or equal to 0 and less than num_clusters.\n        k = np.random.choice(len(weights), 1, p=weights)[0]\n        # Use np.random.multivariate_normal to create data from this cluster\n        x = np.random.multivariate_normal(means[k], covariances[k])\n        data.append(x)\n    return data\n\n\ndef log_sum_exp(Z):\n    \"\"\" Compute log(\\sum_i exp(Z_i)) for some array Z.\"\"\"\n    return np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))\n\n\ndef loglikelihood(data, weights, means, covs):\n    \"\"\" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. \"\"\"\n    num_clusters = len(means)\n    num_dim = len(data[0])\n    \n    ll = 0\n    for d in data:\n        \n        Z = np.zeros(num_clusters)\n        for k in range(num_clusters):\n            \n            # Compute (x-mu)^T * Sigma^{-1} * (x-mu)\n            delta = np.array(d) - means[k]\n            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))\n            \n            # Compute loglikelihood contribution for this data point and this cluster\n            Z[k] += np.log(weights[k])\n            Z[k] -= 1/2. * (num_dim * np.log(2*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)\n            \n        # Increment loglikelihood contribution of this data point across all clusters\n        ll += log_sum_exp(Z)\n        \n    return ll\n\n\ndef EM(data, init_means, init_covariances, init_weights, maxiter=1000, thresh=1e-4):\n    # Make copies of initial parameters, which we will update during each iteration\n    means = copy.deepcopy(init_means)\n    covariances = copy.deepcopy(init_covariances)\n    weights = copy.deepcopy(init_weights)\n    \n    # Infer dimensions of dataset and the number of clusters\n    num_data = len(data)\n    num_dim = len(data[0])\n    num_clusters = len(means)\n    \n    # Initialize some useful variables\n    resp = np.zeros((num_data, num_clusters))\n    ll = loglikelihood(data, weights, means, covariances)\n    ll_trace = [ll]\n    \n    for i in range(maxiter):\n        if i % 5 == 0:\n            print(\"Iteration %s\" % i)\n        \n        # E-step: compute responsibilities\n        # Update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j.\n        # Hint: To compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf.\n        for j in range(num_data):\n            for k in range(num_clusters):\n                # YOUR CODE HERE\n                resp[j, k] = weights[k] * multivariate_normal.pdf(x=data[j],\n                                                                  mean=means[k],\n                                                                  cov=covariances[k])\n        row_sums = resp.sum(axis=1)[:, np.newaxis]\n        resp = resp / row_sums # normalize over all possible cluster assignments\n\n        # M-step\n        # Compute the total responsibility assigned to each cluster, which will be useful when \n        # implementing M-steps below. In the lectures this is called N^{soft}\n        counts = np.sum(resp, axis=0)\n        \n        for k in range(num_clusters):\n            \n            # Update the weight for cluster k using the M-step update rule for the cluster weight, \\hat{\\pi}_k.\n            # YOUR CODE HERE\n            Nsoft_k = counts[k]\n            weights[k] = float(Nsoft_k)/float(num_data)\n            \n            # Update means for cluster k using the M-step update rule for the mean variables.\n            # This will assign the variable means[k] to be our estimate for \\hat{\\mu}_k.\n            weighted_sum = 0\n            for j in range(num_data):\n                # YOUR CODE HERE\n                weighted_sum += resp[j, k] * data[j]\n            # YOUR CODE HERE\n            means[k] = weighted_sum/Nsoft_k\n            \n            # Update covariances for cluster k using the M-step update rule for covariance variables.\n            # This will assign the variable covariances[k] to be the estimate for \\hat{Sigma}_k.\n            weighted_sum = np.zeros((num_dim, num_dim))\n            for j in range(num_data):\n                # YOUR CODE HERE (Hint: Use np.outer on the data[j] and this cluster's mean)\n                weighted_sum += resp[j, k] * np.outer(data[j] - means[k], data[j] - means[k])\n            # YOUR CODE HERE\n            covariances[k] = weighted_sum/Nsoft_k\n          \n        \n        # Compute the loglikelihood at this iteration\n        # YOUR CODE HERE\n        ll_latest = loglikelihood(data, weights, means, covariances)\n        ll_trace.append(ll_latest)\n        \n        # Check for convergence in log-likelihood and store\n        if (ll_latest - ll) < thresh and ll_latest > -np.inf:\n            break\n        ll = ll_latest\n    \n    if i % 5 != 0:\n        print(\"Iteration %s\" % i)\n    \n    out = {'weights': weights, 'means': means, 'covs': covariances, 'loglik': ll_trace, 'resp': resp}\n\n    return out\n\n\ndef plot_contours(data, means, covs, title):\n    plt.figure()\n    plt.plot([x[0] for x in data], [y[1] for y in data],'ko') # data\n\n    delta = 0.025\n    k = len(means)\n    x = np.arange(-2.0, 7.0, delta)\n    y = np.arange(-2.0, 7.0, delta)\n    X, Y = np.meshgrid(x, y)\n    col = ['green', 'red', 'indigo']\n    for i in range(k):\n        mean = means[i]\n        cov = covs[i]\n        sigmax = np.sqrt(cov[0][0])\n        sigmay = np.sqrt(cov[1][1])\n        sigmaxy = cov[0][1]/(sigmax*sigmay)\n        Z = mlab.bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n        plt.contour(X, Y, Z, colors = col[i])\n        plt.title(title)\n    plt.rcParams.update({'font.size':16})\n    plt.tight_layout()\n\n\ndef plot_responsibilities_in_RB(img, resp, title):\n    N, K = resp.shape\n    \n    HSV_tuples = [(x*1.0/K, 0.5, 0.9) for x in range(K)]\n    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n    \n    R = img['red']\n    B = img['blue']\n    resp_by_img_int = [[resp[n][k] for k in range(K)] for n in range(N)]\n    cols = [tuple(np.dot(resp_by_img_int[n], np.array(RGB_tuples))) for n in range(N)]\n\n    plt.figure()\n    for n in range(len(R)):\n        plt.plot(R[n], B[n], 'o', c=cols[n])\n    plt.title(title)\n    plt.xlabel('R value')\n    plt.ylabel('B value')\n    plt.rcParams.update({'font.size':16})\n    plt.tight_layout()\n\n\ndef get_top_images(assignments, cluster, k=5):\n    # YOUR CODE HERE\n    images_in_cluster = assignments[assignments['assignments']==cluster]\n    print images_in_cluster\n    top_images = images_in_cluster.topk('probs', k)\n    return top_images['image']\n\n\ndef save_images(images, prefix):\n    for i, image in enumerate(images):\n        Image.open(BytesIO(image._image_data)).save(prefix % i)\n\n\n# Model parameters\ninit_means = [\n    [5, 0], # mean of cluster 1\n    [1, 1], # mean of cluster 2\n    [0, 5]  # mean of cluster 3\n]\ninit_covariances = [\n    [[.5, 0.], [0, .5]], # covariance of cluster 1\n    [[.92, .38], [.38, .91]], # covariance of cluster 2\n    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n]\ninit_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n\n# Generate data\nnp.random.seed(4)\ndata = generate_MoG_data(100, init_means, init_covariances, init_weights)\n\n# Plot clusters\nplt.figure()\nd = np.vstack(data)\nplt.plot(d[:,0], d[:,1],'ko')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Test EM algorithm\nnp.random.seed(4)\n\n# Initialization of parameters\nchosen = np.random.choice(len(data), 3, replace=False)\ninitial_means = [data[x] for x in chosen]\ninitial_covs = [np.cov(data, rowvar=0)] * 3\ninitial_weights = [1/3.] * 3\n\n# Run EM \nresults = EM(data, initial_means, initial_covs, initial_weights)\n\n# Parameters after initialization\nplot_contours(data, initial_means, initial_covs, 'Initial clusters')\n\n# Parameters after 12 iterations\nresults = EM(data, initial_means, initial_covs, initial_weights, maxiter=12)\nplot_contours(data, results['means'], results['covs'], 'Clusters after 12 iterations')\n\n# Parameters after running EM to convergence\nresults = EM(data, initial_means, initial_covs, initial_weights)\nplot_contours(data, results['means'], results['covs'], 'Final clusters')\n\n# Log-likelihood plot\nloglikelihoods = results['loglik']\nplt.plot(range(len(loglikelihoods)), loglikelihoods, linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Load image data\nimages = sframe.SFrame('../data/Week04/images.sf/')\nimages['rgb'] = images.pack_columns(['red', 'green', 'blue'])['X4']\n\n# Run EM on image data\nnp.random.seed(1)\n\n# Initalize parameters\ninit_means = [images['rgb'][x] for x in np.random.choice(len(images), 4, replace=False)]\ncov = np.diag([images['red'].var(), images['green'].var(), images['blue'].var()])\ninit_covariances = [cov, cov, cov, cov]\ninit_weights = [1/4., 1/4., 1/4., 1/4.]\n\n# Convert rgb data to numpy arrays\nimg_data = [np.array(i) for i in images['rgb']]  \n\n# Run our EM algorithm on the image data using the above initializations. \n# This should converge in about 125 iterations\nout = EM(img_data, init_means, init_covariances, init_weights)\n\n# Log-likelihood plot\nll = out['loglik']\nplt.plot(range(len(ll)),ll,linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\nplt.figure()\nplt.plot(range(10,len(ll)),ll[10:],linewidth=4)\nplt.xlabel('Iteration')\nplt.ylabel('Log-likelihood')\nplt.rcParams.update({'font.size':16})\nplt.tight_layout()\n\n# Visualize evolution of responsibility\nN, K = out['resp'].shape\nrandom_resp = np.random.dirichlet(np.ones(K), N)\nplot_responsibilities_in_RB(images, random_resp, 'Random responsibilities')\n\nout = EM(img_data, init_means, init_covariances, init_weights, maxiter=1)\nplot_responsibilities_in_RB(images, out['resp'], 'After 1 iteration')\n\nout = EM(img_data, init_means, init_covariances, init_weights, maxiter=20)\nplot_responsibilities_in_RB(images, out['resp'], 'After 20 iterations')\n\n# Interpreting clusters\nweights = out['weights']\nmeans = out['means']\ncovariances = out['covs']\nrgb = images['rgb']\nN = len(images) # number of images\nK = len(means) # number of clusters\n\nassignments = [0]*N\nprobs = [0]*N\n\nfor i in range(N):\n    # Compute the score of data point i under each Gaussian component:\n    p = np.zeros(K)\n    for k in range(K):\n        p[k] = weights[k]*multivariate_normal.pdf(rgb[i], mean=means[k], cov=covariances[k])\n        \n    # Compute assignments of each data point to a given cluster based on the above scores:\n    assignments[i] = np.argmax(p)\n    \n    # For data point i, store the corresponding score under this cluster assignment:\n    probs[i] = np.max(p)\n\nassignments = sframe.SFrame({'assignments':assignments, 'probs':probs, 'image': images['image']})\n\nfor idx in range(4):\n    get_top_images(assignments, idx)\n\nfor component_id in range(4):\n    print 'Component {0:d}'.format(component_id)\n    images = get_top_images(assignments, component_id)\n    save_images(images, 'component_{0:d}_%d.jpg'.format(component_id))\n    print '\\n'", "original_line_number": 21452, "instruction_length": 903}
{"instruction": "Create a class XbmcContextUI that extends AbstractContextUI and has several methods for creating UI elements and interacting with the user. The class has a constructor that takes in an xbmc_addon and a context. The class has methods for creating progress dialogs, setting and getting view modes, getting the skin ID, handling keyboard input, numeric input, yes/no input, and selecting from a list of items. The class also has methods for showing notifications, opening settings, refreshing the container, and setting, getting, and clearing properties of the home window.", "input": "", "output": "__author__ = 'bromix'\n\nfrom six import string_types\n\nimport xbmc\nimport xbmcgui\n\nfrom ..abstract_context_ui import AbstractContextUI\nfrom .xbmc_progress_dialog import XbmcProgressDialog\nfrom .xbmc_progress_dialog_bg import XbmcProgressDialogBG\nfrom ... import constants\nfrom ... import utils\n\n\nclass XbmcContextUI(AbstractContextUI):\n    def __init__(self, xbmc_addon, context):\n        AbstractContextUI.__init__(self)\n\n        self._xbmc_addon = xbmc_addon\n\n        self._context = context\n        self._view_mode = None\n\n    def create_progress_dialog(self, heading, text=None, background=False):\n        if background and self._context.get_system_version().get_version() > (12, 3):\n            return XbmcProgressDialogBG(heading, text)\n\n        return XbmcProgressDialog(heading, text)\n\n    def set_view_mode(self, view_mode):\n        if isinstance(view_mode, string_types):\n            view_mode = self._context.get_settings().get_int(constants.setting.VIEW_X % view_mode, 50)\n\n        self._view_mode = view_mode\n\n    def get_view_mode(self):\n        if self._view_mode is not None:\n            return self._view_mode\n\n        return self._context.get_settings().get_int(constants.setting.VIEW_DEFAULT, 50)\n\n    def get_skin_id(self):\n        return xbmc.getSkinDir()\n\n    def on_keyboard_input(self, title, default='', hidden=False):\n        # fallback for Frodo\n        if self._context.get_system_version().get_version() <= (12, 3):\n            keyboard = xbmc.Keyboard(default, title, hidden)\n            keyboard.doModal()\n            if keyboard.isConfirmed() and keyboard.getText():\n                text = utils.to_unicode(keyboard.getText())\n                return True, text\n            else:\n                return False, u''\n\n        # Starting with Gotham (13.X > ...)\n        dialog = xbmcgui.Dialog()\n        result = dialog.input(title, utils.to_unicode(default), type=xbmcgui.INPUT_ALPHANUM)\n        if result:\n            text = utils.to_unicode(result)\n            return True, text\n\n        return False, u''\n\n    def on_numeric_input(self, title, default=''):\n        dialog = xbmcgui.Dialog()\n        result = dialog.input(title, str(default), type=xbmcgui.INPUT_NUMERIC)\n        if result:\n            return True, int(result)\n\n        return False, None\n\n    def on_yes_no_input(self, title, text, nolabel='', yeslabel=''):\n        dialog = xbmcgui.Dialog()\n        return dialog.yesno(title, text, nolabel=nolabel, yeslabel=yeslabel)\n\n    def on_ok(self, title, text):\n        dialog = xbmcgui.Dialog()\n        return dialog.ok(title, text)\n\n    def on_remove_content(self, content_name):\n        text = self._context.localize(constants.localize.REMOVE_CONTENT) % utils.to_unicode(content_name)\n        return self.on_yes_no_input(self._context.localize(constants.localize.CONFIRM_REMOVE), text)\n\n    def on_delete_content(self, content_name):\n        text = self._context.localize(constants.localize.DELETE_CONTENT) % utils.to_unicode(content_name)\n        return self.on_yes_no_input(self._context.localize(constants.localize.CONFIRM_DELETE), text)\n\n    def on_select(self, title, items=[]):\n        _dict = {}\n        _items = []\n        i = 0\n        for item in items:\n            if isinstance(item, tuple):\n                _dict[i] = item[1]\n                _items.append(item[0])\n            else:\n                _dict[i] = i\n                _items.append(item)\n\n            i += 1\n\n        dialog = xbmcgui.Dialog()\n        result = dialog.select(title, _items)\n        return _dict.get(result, -1)\n\n    def show_notification(self, message, header='', image_uri='', time_milliseconds=5000):\n        _header = header\n        if not _header:\n            _header = self._context.get_name()\n        _header = utils.to_utf8(_header)\n\n        _image = image_uri\n        if not _image:\n            _image = self._context.get_icon()\n\n        _message = utils.to_utf8(message)\n        try:\n            _message = _message.replace(',', ' ')\n            _message = _message.replace('\\n', ' ')\n        except TypeError:\n            _message = _message.replace(b',', b' ')\n            _message = _message.replace(b'\\n', b' ')\n            _message = utils.to_unicode(_message)\n            _header = utils.to_unicode(_header)\n\n        xbmc.executebuiltin(\"Notification(%s, %s, %d, %s)\" % (_header, _message, time_milliseconds, _image))\n\n    def open_settings(self):\n        self._xbmc_addon.openSettings()\n\n    @staticmethod\n    def refresh_container():\n        xbmc.executebuiltin(\"Container.Refresh\")\n\n    @staticmethod\n    def set_home_window_property(property_id, value):\n        property_id = 'plugin.video.youtube-' + property_id\n        xbmcgui.Window(10000).setProperty(property_id, value)\n\n    @staticmethod\n    def get_home_window_property(property_id):\n        property_id = 'plugin.video.youtube-' + property_id\n        return xbmcgui.Window(10000).getProperty(property_id) or None\n\n    @staticmethod\n    def clear_home_window_property(property_id):\n        property_id = 'plugin.video.youtube-' + property_id\n        xbmcgui.Window(10000).clearProperty(property_id)", "original_line_number": 21697, "instruction_length": 570}
{"instruction": "Use this code to create a caching template loader that allows disk-based templates to be used. The loader should cache compiled templates for performance. To use the loader, create an instance of the Loader class and pass in a list of sources and a parser. The sources should be instances of either the MemorySource or DirectorySource classes, and the parser should be an instance of a template parser. Once the loader is initialized, you can use its load and render methods to load and render templates. On Windows, use forward slashes as path separators.", "input": "", "output": "\"\"\"A caching template loader that allows disk-based templates to be used.\"\"\"\n\n\nimport os, sys\nfrom abc import ABCMeta, abstractmethod\n\n\nclass TemplateDoesNotExist(Exception):\n    \n    \"\"\"A named template could not be found.\"\"\"\n\n\nclass Source(metaclass=ABCMeta):\n    \n    \"\"\"A source of template data.\"\"\"\n    \n    __slots__ = ()\n    \n    @abstractmethod\n    def load_source(self, template_name):\n        \"\"\"\n        Loads the template source code for the template of the given name.\n        \n        If no source code can be found, returns None.\n        \"\"\"\n        \n        \nclass MemorySource(Source):\n\n    \"\"\"A template loader that loads from memory.\"\"\"\n    \n    __slots__ = (\"templates\",)\n    \n    def __init__(self, templates):\n        \"\"\"Initializes the MemorySource from a dict of template source strings.\"\"\"\n        self.templates = templates\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from the memory template dict.\"\"\"\n        return self.templates.get(template_name)\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return \"<memory>\"\n        \n        \nclass DirectorySource(Source):\n    \n    \"\"\"A template loader that loads from a directory on disk.\"\"\"\n    \n    __slots__ = (\"dirname\")\n    \n    def __init__(self, dirname):\n        \"\"\"\n        Initializes the DirectorySource.\n        \n        On windows, the dirname should be specified using forward-slashes.\n        \"\"\"\n        self.dirname = dirname\n        \n    def load_source(self, template_name):\n        \"\"\"Loads the source from disk.\"\"\"\n        template_path = os.path.normpath(os.path.join(self.dirname, template_name))\n        if os.path.exists(template_path):\n            with open(template_path, \"r\") as template_file:\n                return template_file.read()\n        return None\n        \n    def __str__(self):\n        \"\"\"Returns a string representation.\"\"\"\n        return self.dirname\n\n\nclass DebugLoader:\n\n    \"\"\"\n    A template loader that doesn't cache compiled templates.\n    \n    Terrible performance, but great for debugging.\n    \"\"\"\n    \n    __slots__ = (\"_sources\", \"_parser\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"\n        Initializes the Loader.\n        \n        When specifying template_dirs on Windows,the forward slash '/' should be used as a path separator.\n        \"\"\"\n        self._sources = list(reversed(sources))\n        self._parser = parser\n    \n    def compile(self, template, name=\"__string__\", params=None, meta=None):\n        \"\"\"Compiles the given template source.\"\"\"\n        default_meta = {\n            \"__loader__\": self\n        }\n        default_meta.update(meta or {})\n        return self._parser.compile(template, name, params, default_meta)\n    \n    def _load_all(self, template_name):\n        \"\"\"Loads and returns all the named templates from the sources.\"\"\"\n        # Load from all the template sources.\n        templates = []\n        for source in self._sources:\n            template_src = source.load_source(template_name)\n            if template_src is not None:\n                meta = {\n                    \"__super__\": templates and templates[-1] or None,\n                }\n                templates.append(self.compile(template_src, template_name, {}, meta))\n        return templates\n    \n    def load(self, *template_names):        \n        \"\"\"\n        Loads and returns the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        if not template_names:\n            raise ValueError(\"You must specify at least one template name.\")\n        for template_name in template_names:\n            templates = self._load_all(template_name)\n            if templates:\n                return templates[-1]\n        # Raise an error.\n        template_name_str = \", \".join(repr(template_name) for template_name in template_names)\n        source_name_str = \", \".join(str(source) for source in self._sources)\n        raise TemplateDoesNotExist(\"Could not find a template named {} in any of {}.\".format(template_name_str, source_name_str))\n        \n    def render(self, *template_names, **params):\n        \"\"\"\n        Loads and renders the named template.\n        \n        If more than one template name is given, then the first template that exists will be used.\n        \n        On Windows, the forward slash '/' should be used as a path separator.\n        \"\"\"\n        return self.load(*template_names).render(**params)\n\n\nclass Loader(DebugLoader):\n    \n    \"\"\"\n    A template loader.\n    \n    Compiled templates are cached for performance.\n    \"\"\"\n    \n    __slots__ = (\"_cache\",)\n    \n    def __init__(self, sources, parser):\n        \"\"\"Initializes the loader.\"\"\"\n        super(Loader, self).__init__(sources, parser)\n        self._cache = {}\n    \n    def clear_cache(self, ):\n        \"\"\"Clears the template cache.\"\"\"\n        self._cache.clear()\n        \n    def _load_all(self, template_name):\n        \"\"\"A caching version of the debug loader's load method.\"\"\"\n        if template_name in self._cache:\n            return self._cache[template_name]\n        template = super(Loader, self)._load_all(template_name)\n        self._cache[template_name] = template\n        return template", "original_line_number": 21976, "instruction_length": 556}
{"instruction": "This code extracts new miRNAs from a report and can generate a JSON dump. To use it, create a Python script and import the necessary modules (argparse, json, os, and date). Define the function \"extract_new_mirnas_from_report\" with the input \"report_tsv\" and an optional argument \"type\" with a default value of \"new\". This function reads the report file, checks if the candidate miRNA is a new family, skips families requiring review, and adds the new miRNAs to a dictionary. Define the function \"extract_rfam_family_accessions\" with the input \"report_file\". This function reads the report file, checks if the overlap is less than or equal to 100.0, and adds the miRNA accessions to a dictionary. Define the function \"parse_arguments\" with no input. This function creates an ArgumentParser object and adds the necessary arguments. In the main block, parse the arguments using the ArgumentParser object and call the appropriate function based on the arguments. If the \"--create-dump\" argument is provided, generate a JSON dump with the appropriate filename.", "input": "", "output": "import argparse\nimport json\nimport os\nfrom datetime import date\n\n\n# -------------------------------------------------------------------------------\n\n\ndef extract_new_mirnas_from_report(report_tsv, type='new'):\n    \"\"\"\n    \"\"\"\n\n    new_mirnas = {}\n\n    fp = open(report_tsv, 'r')\n    count = 0\n    for line in fp:\n        line = line.strip().split('\\t')\n        # check if candidate mirna is a new family\n        if line[6].lower() == \"new family\":\n            # skip families requiring review\n            if line[1] != '?' and line[1] != '' and line[2] != \"1_SEED\":\n                if line[0] not in new_mirnas:\n                    print line\n                    new_mirnas[line[0]] = line[1]\n        elif line[6].lower() == 'done':\n            count += 1\n    fp.close()\n\n    return new_mirnas\n\n\n# -------------------------------------------------------------------------------\n\ndef extract_rfam_family_accessions(report_file):\n\n    fp = open(report_file, 'r')\n\n    accession_map = {}\n\n    for line in fp:\n        line = line.strip().split('\\t')\n\n        overlap = float(line[4])\n        if overlap <= 100.0:\n            # converts to upper to ensure labels match the constant\n            if line[6].strip().upper() == \"UPDATE SEED\":\n\n                rfam_acc = line[3].strip()\n\n                rfam_acc_list = []\n                if rfam_acc.find(',') == -1:\n                    rfam_acc_list = rfam_acc.split(',')\n                else:\n                    rfam_acc_list = [rfam_acc]\n\n                threshold = 0.0\n                if line[1] != '':\n                    threshold = float(line[1])\n\n                # trim any whitespace characters\n                mirbase_id = line[0].strip()\n\n\n                accession_map[mirbase_id] = {\"rfam_acc\": rfam_acc_list,\n                                              \"threshold\": threshold,\n                                              \"overlap\": float(line[4])}\n                \n    fp.close()\n\n    return accession_map\n\n\n# -------------------------------------------------------------------------------\n\ndef parse_arguments():\n    \"\"\"\n    \"\"\"\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--report\", help=\"miRNA report in .tsv format\", action=\"store\")\n    parser.add_argument(\"--dest-dir\", help=\"Desctination directory\", action=\"store\", default=os.getcwd())\n    parser.add_argument(\"--old-rfam\", help=\"Fetches old Rfam miRNA accessions to be updated\",\n                        action=\"store_true\", default=False)\n    parser.add_argument(\"--create-dump\", help=\"Generates a JSON (.json) dump in destination directory\",\n                        action=\"store_true\", default=False)\n\n    return parser\n\n\n# -------------------------------------------------------------------------------\n\nif __name__ == '__main__':\n\n    parser = parse_arguments()\n    args = parser.parse_args()\n\n    accessions = None\n    if not args.old_rfam:\n        new_mirnas = extract_new_mirnas_from_report(args.report, type='new')\n        accessions = new_mirnas\n\n    else:\n        accessions = extract_rfam_family_accessions(args.report)\n\n    if args.create_dump:\n        filename = \"new_mirnas_\"\n\n        if args.old_rfam:\n            filename = \"mirna_families_to_update_\"\n        fp_out = open(os.path.join(args.dest_dir, filename + str(date.today()) + \".json\"), 'w')\n        json.dump(accessions, fp_out)\n        fp_out.close()", "original_line_number": 22219, "instruction_length": 1055}
{"instruction": "Create a class called \"Character\" that represents a player character with stats, equipment, and leveling information. The class should have an __init__ method that initializes the character's level, experience_into_level, name, equipment_classes, stats, and leveling_formula. It should also have a method called \"load_character_file\" that loads a character JSON file and sets the character's name, equipment classes, and stats. Finally, the class should have a __str__ method that returns a string representation of the character's attributes. The necessary imports are json, formulas from caladbolg.agents, and Stats and EquipmentStats from caladbolg.agents.stats.", "input": "", "output": "import json\r\nfrom caladbolg.agents import formulas\r\nfrom caladbolg.agents.stats import Stats, EquipmentStats\r\n\r\nclass Character:\r\n    \"\"\"\r\n    Represents a player character.\r\n\r\n    Characters have stats, equipment, and leveling information.\r\n    \"\"\"\r\n\r\n    def __init__(self, character_file):\r\n        self.level = 1\r\n        self.experience_into_level = 0\r\n        self.name = None\r\n        self.equipment_classes = None\r\n        self.stats = None\r\n        self.leveling_formula = None\r\n        self.load_character_file(character_file)\r\n\r\n    def load_character_file(self, character_file):\r\n        \"\"\"\r\n        Loads a character JSON file, setting the name, equipment classes, and stats.\r\n\r\n        The ``name`` is naturally the character's name as it should appear in-game. While there's no hard limit, it\r\n        should be kept short to prevent it from being truncated or overlapping.\r\n\r\n        The ``equipment_classes`` is a set of string \"classes\" corresponding to which classes of equipment can\r\n        be equipped. Each piece of equipment has one or more classes. If there's at least one class in common, we can\r\n        equip the piece of equipment. The classes are not mentioned in-game, as the naming is intended to be internal.\r\n\r\n        The ``stats`` is the character's ``Stats`` instance. The equipment stats are not initialized yet as they are\r\n        not known at this time.\r\n\r\n        :param character_file: The file to load the character data from.\r\n        \"\"\"\r\n        character_json = json.load(character_file)\r\n        self.name = character_json['name']\r\n        self.equipment_classes = character_json['equipment_classes']\r\n        self.stats = Stats(character_json['base_stats']['health'],\r\n                           character_json['base_stats']['stamina'],\r\n                           character_json['base_stats']['strength'],\r\n                           character_json['base_stats']['magic'],\r\n                           character_json['base_stats']['endurance'],\r\n                           character_json['base_stats']['agility'])\r\n        try:\r\n            self.leveling_formula = formulas.leveling_formulas[character_json['leveling_formula']]\r\n        except KeyError:\r\n            raise ValueError('No leveling formula named \"{0}\" ({1})'.format(character_json['leveling_formula'],\r\n                             character_file.name)) from None\r\n\r\n    def __str__(self):\r\n        return str(self.__dict__)", "original_line_number": 22256, "instruction_length": 665}
{"instruction": "The code defines a class that runs the `abundance_estimates_to_matrix.pl` script on genes or isoforms counts tables produced by `align_and_estimate_abundance.pl`. To use the class, pass either `genes.results` or `isoforms.results` files in the following slots: `sample_data[<sample>][\"genes.results\"]` or `sample_data[<sample>][\"isoforms.results\"]`. The class creates the following files in the following slots: `project.counts.matrix` in `self.sample_data[\"project_data\"][\"counts.matrix\"]`, `project.not_cross_norm.fpkm.tmp` in `self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp\"]`, `project.not_cross_norm.fpkm.tmp.TMM_info.txt` in `self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp.TMM_info.txt\"]`, and `project.TMM.fpkm.matrix` in `self.sample_data[\"project_data\"][\"TMM.fpkm.matrix\"]`. The class requires the `abundance_estimates_to_matrix.pl` script and the `--gene_trans_map` parameter.", "input": "", "output": "# -*- coding: UTF-8 -*-\n\"\"\" \n``trinity_statistics``\n-----------------------------------------------------------------\n:Authors: Menachem Sklarz\n:Affiliation: Bioinformatics core facility\n:Organization: National Institute of Biotechnology in the Negev, Ben Gurion University.\n\nA class that defines a module for running ``abundance_estimates_to_matrix.pl`` on genes or isoforms counts tables produced by ``align_and_estimate_abundance.pl``\n\nSee the script documentation `here <https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification#building-expression-matrices>`_.\n\nThis conversion makes sense at the project level - combining all sample matrices into a single, normalized, comparison table. However, for completeness, we included a sample scope option for running the script in each sample separately.\n\n.. Note:: ``scope`` is not defined for this module. It only makes sense to run ``abundance_estimates_to_matrix`` when comparing many samples against a single assembly\n\nRequires\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    \n* Either ``genes.results`` or ``isoforms.results`` files in the following slots:\n        \n        * ``sample_data[<sample>][\"genes.results\"]``\n        * ``sample_data[<sample>][\"isoforms.results\"]``\n\n    \nOutput:\n~~~~~~~~~~~~~\n\n* Creates the following files in the following slots:\n    \n        * ``<project>.counts.matrix``                        in ``self.sample_data[\"project_data\"][\"counts.matrix\"]``\n        * ``<project>.not_cross_norm.fpkm.tmp``              in ``self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp\"]``\n        * ``<project>.not_cross_norm.fpkm.tmp.TMM_info.txt`` in ``self.sample_data[\"project_data\"][\"not_cross_norm.fpkm.tmp.TMM_info.txt\"]``\n        * ``<project>.TMM.fpkm.matrix``                      in ``self.sample_data[\"project_data\"][\"TMM.fpkm.matrix\"]``\n\n        \nParameters that can be set        \n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. csv-table:: \n    :header: \"Parameter\", \"Values\", \"Comments\"\n\n    \"use_genes\", \"\", \"Use 'genes.results' matrix. If not passed, use 'isoforms.results'\"\n    \"redirects: --gene_trans_map\", \"path or 'none'\", \"If path, use path as gene_trans_map for all samples. If 'none', does not produce gene level estimates. **In order to use an internal gene_trans_map, do not pass this parameter!**\"\n    \n    \nLines for parameter file\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\n\n    trin_map_stats:\n        module:             trinity_statistics\n        base:               trin_map1\n        script_path:        /path/to/abundance_estimates_to_matrix.pl\n        use_genes:       \n        redirects:\n            --est_method:   RSEM\n\nReferences\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGrabherr, M.G., Haas, B.J., Yassour, M., Levin, J.Z., Thompson, D.A., Amit, I., Adiconis, X., Fan, L., Raychowdhury, R., Zeng, Q. and Chen, Z., 2011. **Trinity: reconstructing a full-length transcriptome without a genome from RNA-Seq data**. *Nature biotechnology*, 29(7), p.644.\n\n\"\"\"\n\n\n\nimport os\nimport sys\nimport re\nfrom neatseq_flow.PLC_step import Step,AssertionExcept\n\n\n__author__ = \"Menachem Sklarz\"\n__version__ = \"1.6.0\"\n\n\nclass Step_trinity_statistics(Step):\n    \n    def step_specific_init(self):\n        self.shell = \"bash\"      # Can be set to \"bash\" by inheriting instances\n        self.file_tag = \"trin_stats\"\n        \n        if \"use_genes\" not in self.params:\n            self.write_warning(\"'use_genes' not passed. Using 'isoforms.results' matrix\")\n\n        \n    def step_sample_initiation(self):\n        \"\"\" A place to do initiation stages following setting of sample_data\n            Here you should do testing for dependency output. These will NOT exist at initiation of this instance. They are set only following sample_data updating\n        \"\"\"\n\n        # In new version, --gene_trans_map is compulsory! Adding\n        # If not passed:\n            # If one exists, use it. \n            # Otherwise, specify \"none\"\n        # If passed:\n            # If with value, use the value and set project \"gene_trans_map\" to value\n            # Otherwise, use existing\n        if \"--gene_trans_map\" not in self.params[\"redir_params\"]:\n            if \"gene_trans_map\" in self.sample_data[\"project_data\"]:\n                self.params[\"redir_params\"][\"--gene_trans_map\"] = self.sample_data[\"project_data\"][\"gene_trans_map\"]\n                self.use_gene_trans_map = True\n            else:\n                self.params[\"redir_params\"][\"--gene_trans_map\"] = \"none\"\n                self.use_gene_trans_map = False\n                    \n        else:  # --gene_trans_map is defined in redir_params\n            if self.params[\"redir_params\"][\"--gene_trans_map\"] == None:\n                raise AssertionExcept(\"You passed --gene_trans_map with no value. Please specify path or 'none'\")\n            elif self.params[\"redir_params\"][\"--gene_trans_map\"] == \"none\":\n                self.use_gene_trans_map = False\n            else:\n                self.sample_data[\"project_data\"][\"gene_trans_map\"] = self.params[\"redir_params\"][\"--gene_trans_map\"]\n                self.use_gene_trans_map = True\n            \n                \n            \n        \n        \n        \n    def create_spec_wrapping_up_script(self):\n        \"\"\" Add stuff to check and agglomerate the output data\n        \"\"\"\n        \n\n    def build_scripts(self):\n        \n\n        # Name of specific script:\n        self.spec_script_name = self.set_spec_script_name()\n\n        self.script = \"\"\n\n        # This line should be left before every new script. It sees to local issues.\n        # Use the dir it returns as the base_dir for this step.\n        use_dir = self.local_start(self.base_dir)\n\n        \n        prefix = self.sample_data[\"Title\"]\n\n        self.script += self.get_script_const()\n\n        self.script += \"--out_prefix %s \\\\\\n\\t\" % os.sep.join([use_dir, prefix])\n        # type2use is 'genes.results' or 'isoforms.results'. This is used to then select the correct slot from \"mapping\"\n        type2use = \"genes.results\" if \"use_genes\" in list(self.params.keys()) else \"isoforms.results\"\n        \n        for sample in self.sample_data[\"samples\"]:\n            try:\n                self.script += \"%s \\\\\\n\\t\" % self.sample_data[sample][type2use] \n            except:\n                raise AssertionExcept(\"file type %s does not exist for sample.\" % type2use, sample)\n        \n        self.script = self.script.rstrip(\"\\\\\\n\\t\")\n        self.script += \"\\n\\n\"\n        \n        \n        if not \"version\" in self.params or self.params[\"version\"].lower() == \"new\":\n\n        # Storing all output files even though probably not very useful downstream...\n            self.sample_data[\"project_data\"][\"isoform.raw_counts\"] = os.sep.join([self.base_dir,  \"%s.isoform.counts.matrix\" % prefix])\n            self.sample_data[\"project_data\"][\"isoform.norm_counts\"] = os.sep.join([self.base_dir, \"%s.isoform.TPM.not_cross_norm\" % prefix])\n            \n            self.stamp_file(self.sample_data[\"project_data\"][\"isoform.raw_counts\"] )\n            self.stamp_file(self.sample_data[\"project_data\"][\"isoform.norm_counts\"])\n\n            if(self.use_gene_trans_map):  # True when --gene_trans_map is not \"none\"\n                self.sample_data[\"project_data\"][\"gene.raw_counts\"] = os.sep.join([self.base_dir,  \"%s.gene.counts.matrix\" % prefix])\n                self.sample_data[\"project_data\"][\"gene.norm_counts\"] = os.sep.join([self.base_dir, \"%s.gene.TPM.not_cross_norm\" % prefix])\n                self.stamp_file(self.sample_data[\"project_data\"][\"gene.raw_counts\"] )\n                self.stamp_file(self.sample_data[\"project_data\"][\"gene.norm_counts\"])\n\n        else:\n            self.write_warning(\"Not storing output files for old version of trinity. \"\n                               \"If required, load the appropriate files with a 'manage_types' module\")\n\n       \n        # Move all files from temporary local dir to permanent base_dir\n        # Sees to copying local files to final destination (and other stuff)\n        self.local_finish(use_dir,self.base_dir)\n\n        self.create_low_level_script()", "original_line_number": 22274, "instruction_length": 908}
{"instruction": "Create a TextInterface class with methods for getting user input and displaying search results. The getUserInput method prompts the user to specify a direction of travel and a station at which they want to find trains. The showApproachingTrains method takes 2 arguments, the station at which the user is looking for trains and a list of trains currently approaching that station. The showStationSearchResults method takes 1 argument, a list of possible station results. The againOrQuit method asks the user whether they want to perform a new search or quit the program.", "input": "", "output": "import sys\n\nclass TextInterface:\n    \"\"\"Text-based interface that contains methods for getting input from the user and displaying search\n    results.\n    \"\"\"\n    \n    def __init__(self, trains_list):\n        \"\"\"Initializes the TextInterface object with a list of all train objects in the simulation.\n        \"\"\"\n        self.trains = trains_list\n    \n    def getUserInput(self):\n        \"\"\"This method prompts the user to specify a direction of travel and a station at which\n        they want to find trains. If a user types a direction other than n or s, the program will\n        ask them to provide a valid input before continuing.\n        \"\"\"\n        print \"\\nWelcome! This program lets you search for New York City subway trains running on the 1, 2, 3, 4, 5, 6, or S lines.\"\n        print \"Note that for S trains, northbound is eastbound in real life and southbound is westbound in real life.\"\n        print \"\\nFirst, choose a direction - northbound or southbound. Type n for northbound or s for southbound.\"\n        # valid_input will remain False until either n or s is typed\n        valid_input = False\n        while valid_input == False:\n            direction = raw_input()\n            if direction == \"n\":\n                valid_input = True\n                direction = 'northbound'\n            elif direction == 's':\n                valid_input = True\n                direction = 'southbound'\n            # If you really don't like our program, you can quit by typing q\n            elif direction == 'q':\n                sys.exit()\n            else:\n                print \"We didn't understand that. Please try again.\"\n        print \"\\nNow, search for the station you want trains from.\"\n        station = raw_input()\n        return direction, station\n    \n    def showApproachingTrains(self, station, list_of_trains):\n        \"\"\"Takes 2 arguments, the station at which the user is looking for trains and a list of\n        trains currently approaching that station, where each item in the list is formatted\n        [train_index, stop_number, arrival_time]. If the list is empty, it informs the user that\n        no trains are near the station. Otherwise, it looks up information about each train in\n        the list and displays it to the user.\n        \"\"\"\n        print \"...\"\n        if len(list_of_trains) == 0:\n            print \"Sorry, there aren't any trains currently approaching\", station\n        else:\n            print \"Here is a list of trains arriving at or departing from\", station, \"in the next 30 minutes:\\n\"\n            for train_list in list_of_trains:\n                train_number = train_list[0] # Used to look up train object in the master list of trains\n                stop_number = train_list[1]\n                if int(self.trains[train_number].getArrivalTime(stop_number)) <= 30:\n                    self.trains[train_number].showInfo(stop_number)\n        print \"\"\n    \n    def showStationSearchResults(self, results_list):\n        \"\"\"Takes 1 argument, a list of possible station results. If there is only one possible\n        result, this function will never be called, so it only has to handle list of length 0 or >1.\n        If the length of the list is 0, the program will ask the user whether they want to do\n        another search or quit. Otherwise, all possible results will be displayed next to a unique\n        integer, and the user will be asked to type in an integer to choose the station they want.\n        \"\"\"\n        print \"...\"\n        if len(results_list) == 0:\n            print \"Sorry, we couldn't find a station with that name.\\n\"\n            self.againOrQuit()\n        else:\n            print \"We found several stations with that name. Please choose one from the list below.\"\n            for i in range(len(results_list)):\n                print (i+1), ': ', results_list[i]\n            choice = int(raw_input(\"Type the number of the station you want: \"))\n            return results_list[choice-1]\n    \n    def againOrQuit(self):\n        \"\"\"Asks the user whether they want to perform a new search or quit the program.\n        \"\"\"\n        print \"Type n to do a new search or q to exit the program.\"\n        choice = raw_input()\n        if choice == \"n\":\n            return True\n        if choice == \"q\":\n            return False\n        else:\n            print \"We didn't understand that. Please try again.\"\n            return self.againOrQuit()", "original_line_number": 22307, "instruction_length": 569}
{"instruction": "This code is an implementation of an image printing program based on haftoning. To use it, first import the necessary libraries: sys, numpy, scipy, matplotlib.pyplot, and DFT from Proj04-01. Then define the following functions: en_padding, de_padding, shift, sqdistance, lowpass_mask, and highpass_mask. Finally, call the main function to run the program. The main function reads in an image file, applies padding and shifting, performs a DFT, applies highpass filtering with different cut-off frequencies, and saves the resulting images.", "input": "", "output": "#!/bin/python\n# *-* encoding=utf-8 *-*\n'''\nImage Priting Program Based on Haftoning\n'''\n\nimport sys\nimport numpy, scipy\nfrom scipy import ndimage\nfrom scipy import misc\nimport scipy.fftpack as fftpack\nimport matplotlib.pyplot as plt\n\nsys.path.append('../Proj04-01')\n\nfrom DFT import DFT_2D, IDFT_2D\n\ndef en_padding(img):\n    M, N = img.shape\n    P, Q = 2 * M, 2 * N\n    _img = numpy.zeros(P*Q).reshape((P, Q))\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y]\n    return _img\n\ndef de_padding(img):\n    P, Q = img.shape\n    M, N = P/2, Q/2\n    _img = numpy.zeros(M*N).reshape((M, N))\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y]\n    return _img\n\ndef shift(img):\n    M, N = img.shape\n    _img = img.copy()\n    for x in range(M):\n        for y in range(N):\n            _img[x][y] = img[x][y] * numpy.power(-1.0, (x+y))\n    return _img\n\ndef sqdistance(p1, p2):\n    return ((p1[0]-p2[0])*(p1[0]-p2[0])) + \\\n           ((p1[1]-p2[1])*(p1[1]-p2[1]))\n\ndef lowpass_mask(P, Q, cuf_off_frequency):\n    center = (P/2.0, Q/2.0)\n    mask = numpy.zeros(P * Q).reshape(P, Q)\n    for u in range(P):\n        for v in range(Q):\n            mask[u][v] = numpy.exp(-1*sqdistance(center, (u, v)) / (2*(cuf_off_frequency*cuf_off_frequency)))\n    return mask\n\ndef highpass_mask(P, Q, cuf_off_frequency):\n    return 1.0 - lowpass_mask(P, Q, cuf_off_frequency)\n#    center = (P/2.0, Q/2.0)\n#    mask = numpy.zeros(P * Q).reshape(P, Q)\n #  for u in range(P):\n#        for v in range(Q):\n#            mask[u][v] = 1.0-numpy.exp(-1*sqdistance(center, (u, v)) / (2*(cuf_off_frequency*cuf_off_frequency)))\n#    return mask\n\ndef main():\n    img_file = 'Fig0441(a)(characters_test_pattern).tif'\n    img = misc.imread(img_file)\n\n    padding_img = en_padding(img)\n\n    padding_img = shift(padding_img)\n\n    dft_img = DFT_2D(padding_img)\n\n    for cut_off_frequency in [30, 60, 160]:\n        print cut_off_frequency\n        hp_mask = highpass_mask(dft_img.shape[0], dft_img.shape[1], cut_off_frequency)\n        \n        misc.imsave('%s_hpmask_%d.tif' % (img_file, cut_off_frequency), 255 * hp_mask)\n        hp_img  = numpy.multiply(dft_img, hp_mask)\n\n        misc.imsave('%s_fft_%d.tif' % (img_file, cut_off_frequency), numpy.log(1+numpy.abs(hp_img)))\n     \n        hp_idtft_img = shift(IDFT_2D(hp_img).real)\n        hp_idtft_img = de_padding(hp_idtft_img)\n        print hp_idtft_img.shape\n        misc.imsave('%s_hp_%d.tif' % (img_file, cut_off_frequency), hp_idtft_img)\n\nif __name__ == '__main__':\n    main()", "original_line_number": 22419, "instruction_length": 538}
{"instruction": "Create a MovieDataEditor class that inherits from wx.Dialog. The class should have methods to initialize controls, set and get data, and handle events. The controls should include a scrolled window, text controls, a multiline text control, and several FieldDataList controls. The class should also have a toolbar with a button to get metadata from IMDB. When the button is clicked, the class should start a new thread to get the metadata and update the controls with the results. The class should also have a Save button that checks if the Title field is empty before closing the dialog.", "input": "", "output": "#\n#\tportable-movie-organizer\n#\t\n#\tCopyright (c) 2010 Ali Aafee\n#\t\n#\tThis file is part of portable-movie-organizer.\n#\n#\tportable-movie-organizer is free software: you can redistribute it \n#\tand/or modify it under the terms of the GNU General Public License \n#\tas published by the Free Software Foundation, either version 3 of \n#\tthe License, or (at your option) any later version.\n#\n#\tportable-movie-organizer is distributed in the hope that it will \n#\tbe useful, but WITHOUT ANY WARRANTY; without even the implied \n#\twarranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  \n#\tSee the GNU General Public License for more details.\n#\n#\tYou should have received a copy of the GNU General Public License\n#\talong with portable-movie-organizer.  \n#\tIf not, see <http://www.gnu.org/licenses/>.\n\nimport wx\nimport FieldDataList\nimport ImdbAPI\nimport os.path\nimport thread\n\n\ndirName = os.path.dirname(os.path.abspath(__file__))\ndirName, fileName = os.path.split(dirName)\nresDir = os.path.join(dirName, 'res')\n\nclass MovieDataEditor(wx.Dialog):\n\tdef __init__(self, parent, postersPath, catchPath, title='Edit Movie Metadata'):\n\t\tself.title = title\n\t\tself.postersPath = postersPath\n\t\tself.catchPath = catchPath\n\t\t\n\t\tself._init_ctrls(parent)\n\t\t\n\t\n\tdef _init_ctrls(self, parent):\n\t\twx.Dialog.__init__(self, name='MovieEditor', parent=parent,\n\t\t\tstyle=wx.DEFAULT_DIALOG_STYLE | wx.RESIZE_BORDER,\n\t\t\ttitle=self.title, size=wx.Size(640,480))\n\t\t\t\n\t\tself.fieldWindow = wx.ScrolledWindow(self, size=wx.Size(200,200), style=wx.HSCROLL)\n\t\tself.fieldWindow.SetScrollbars(0,10,0,65)\n\t\t\n\t\tgridSizer = wx.FlexGridSizer(7,4,10,10)\n\t\tgridSizer.AddGrowableCol(1,1)\n\t\tgridSizer.AddGrowableCol(3,1)\n\t\t\n\t\tlabelWidth = -1\n\t\t\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\tgridSizer.AddSpacer(5)\n\t\t\n\t\tself.lblTitle = wx.StaticText(self.fieldWindow, label='Title', size=wx.Size(labelWidth,-1))\n\t\tself.txtTitle = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblTitle, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtTitle, 1, wx.EXPAND)\n\t\t\n\t\tself.lblSort = wx.StaticText(self.fieldWindow, label='Sort', size=wx.Size(labelWidth,-1))\n\t\tself.txtSort = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblSort, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtSort, 1, wx.EXPAND)\n\t\t\n\t\tself.lblImage = wx.StaticText(self.fieldWindow, label='Poster', size=wx.Size(labelWidth,-1))\n\t\tself.txtImage = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblImage, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtImage, 1, wx.EXPAND)\n\t\t\t\n\t\tself.lblReleased = wx.StaticText(self.fieldWindow, label='Released', size=wx.Size(labelWidth,-1))\n\t\tself.txtReleased = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblReleased, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtReleased, 1, wx.EXPAND)\n\t\t\n\t\tself.lblRuntime = wx.StaticText(self.fieldWindow, label='Runtime', size=wx.Size(labelWidth,-1))\n\t\tself.txtRuntime = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblRuntime, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtRuntime, 1, wx.EXPAND)\n\t\t\n\t\tself.lblRated = wx.StaticText(self.fieldWindow, label='Rated', size=wx.Size(labelWidth,-1))\n\t\tself.txtRated = wx.TextCtrl(self.fieldWindow)\n\t\tgridSizer.Add(self.lblRated, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtRated, 1, wx.EXPAND)\n\t\t\n\t\tself.lblSummary = wx.StaticText(self.fieldWindow, label='Summary', size=wx.Size(labelWidth,-1))\n\t\tself.txtSummary = wx.TextCtrl(self.fieldWindow, style=wx.TE_MULTILINE, size=wx.Size(-1,80))\n\t\tgridSizer.Add(self.lblSummary, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.txtSummary, 1, wx.EXPAND)\n\t\t\n\t\tself.lblGenres = wx.StaticText(self.fieldWindow, label='Genres', size=wx.Size(labelWidth,-1))\n\t\tself.lstGenres = FieldDataList.FieldDataList(self.fieldWindow, size=wx.Size(-1,100))\n\t\tgridSizer.Add(self.lblGenres, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstGenres, 1, wx.EXPAND)\n\t\t\n\t\tself.lblActors = wx.StaticText(self.fieldWindow, label='Actors', size=wx.Size(labelWidth,-1))\n\t\tself.lstActors = FieldDataList.FieldDataList(self.fieldWindow, size=wx.Size(-1,100))\n\t\tgridSizer.Add(self.lblActors, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstActors, 1, wx.EXPAND)\n\t\t\n\t\tself.lblDirectors = wx.StaticText(self.fieldWindow, label='Directors', size=wx.Size(labelWidth,-1))\n\t\tself.lstDirectors = FieldDataList.FieldDataList(self.fieldWindow)\n\t\tgridSizer.Add(self.lblDirectors, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstDirectors, 1, wx.EXPAND)\n\t\t\n\t\tself.lblFiles = wx.StaticText(self.fieldWindow, label='Files', size=wx.Size(labelWidth,-1))\n\t\tself.lstFiles = FieldDataList.FieldDataList(self.fieldWindow)\n\t\tgridSizer.Add(self.lblFiles, 1, wx.ALIGN_RIGHT | wx.ALIGN_CENTER_VERTICAL)\n\t\tgridSizer.Add(self.lstFiles, 1, wx.EXPAND)\n\t\t\n\t\tgridSizer.Add(wx.StaticText(self.fieldWindow, label=''))\n\t\t\n\t\tself.fieldWindow.SetSizer(gridSizer)\n\t\tself.fieldWindow.Layout()\n\t\t\n\t\tself.btnSizer = self.CreateButtonSizer(wx.CANCEL)\n\t\t\n\t\tself.btnSave = wx.Button(self, label=\"Save\")\n\t\tself.btnSave.Bind(wx.EVT_BUTTON, self.OnSave)\n\t\tself.btnSizer.Add(self.btnSave)\n\t\t\n\t\tself.mainTb = self._create_main_tb(self)\n\t\t\n\t\tvbox = wx.BoxSizer(wx.VERTICAL)\n\t\tvbox.Add(self.mainTb, 0, wx.ALL | wx.ALIGN_LEFT | wx.EXPAND, 0 )\n\t\tvbox.Add(self.fieldWindow, 1, wx.EXPAND)\n\t\tvbox.Add(wx.StaticText(self,label=\"\"))\n\t\tvbox.Add(self.btnSizer, 0, wx.ALIGN_CENTER)\n\t\t\n\t\tself.SetSizer(vbox)\n\t\tself.Layout()\n\t\n\t\t\n\tdef _create_main_tb(self, parent):\n\t\ttb = wx.ToolBar(parent, style=wx.TB_TEXT|wx.TB_NODIVIDER|wx.TB_HORIZONTAL|wx.TB_FLAT)\n\t\ttb.SetToolBitmapSize((21, 21))\n\t\t\n\t\tself.tb_search = wx.NewId()\n\t\ttb.DoAddTool(\n\t\t\tbitmap=wx.Bitmap(os.path.join(resDir,'web.png'), wx.BITMAP_TYPE_PNG),\n\t\t\t#bitmap=wx.ArtProvider.GetBitmap(wx.ART_FIND),\n\t\t\tbmpDisabled=wx.NullBitmap, \n\t\t\tid=self.tb_search,\n\t\t\tkind=wx.ITEM_NORMAL, \n\t\t\tlabel='', \n\t\t\tlongHelp='', \n\t\t\tshortHelp='Get Metadata from IMDB')\n\t\tself.Bind(wx.EVT_TOOL, self.OnGetMetadata,\n\t\t\tid=self.tb_search)\n\t\t\t\n\t\tself.statusText = wx.StaticText(tb, label=\"\")\n\t\t\n\t\ttb.AddControl(self.statusText)\n\n\t\ttb.Realize()\n\t\t\n\t\treturn tb\n\t\t\n\t\t\n\tdef SetData(self, data):\n\t\tself.txtTitle.SetValue(data['title'])\n\t\tself.txtSort.SetValue(data['sort'])\n\t\tself.txtImage.SetValue(data['image'])\n\t\tself.txtReleased.SetValue(data['released'])\n\t\tself.txtRuntime.SetValue(data['runtime'])\n\t\tself.txtRated.SetValue(data['rated'])\n\t\tself.txtSummary.SetValue(data['summary'])\n\n\t\tself.lstGenres.DeleteAllItems()\n\t\tself.lstGenres.AddValues(data['genres'])\n\n\t\tself.lstActors.DeleteAllItems()\n\t\tself.lstActors.AddValues(data['actors'])\n\n\t\tself.lstDirectors.DeleteAllItems()\n\t\tself.lstDirectors.AddValues(data['directors'])\n\n\t\tself.lstFiles.DeleteAllItems()\n\t\tself.lstFiles.AddValues(data['files'])\n\t\t \n\t\t\n\t\t\n\tdef GetData(self):\n\t\tdata = {}\n\t\tdata['title'] = self.txtTitle.GetValue()\n\t\tdata['sort'] = self.txtSort.GetValue()\n\t\tdata['image'] = self.txtImage.GetValue()\n\t\tdata['released'] = self.txtReleased.GetValue()\n\t\tdata['runtime'] = self.txtRuntime.GetValue()\n\t\tdata['rated'] = self.txtRated.GetValue()\n\t\tdata['summary'] = self.txtSummary.GetValue()\n\t\t\n\t\tdata['genres'] = self.lstGenres.GetValues()\n\t\tdata['actors'] = self.lstActors.GetValues()\n\t\tdata['directors'] = self.lstDirectors.GetValues()\n\t\tdata['files'] = self.lstFiles.GetValues()\n\t\t\n\t\treturn data\n\t\t\n\t\t\n\tdef OnSave(self, event):\n\t\tif self.txtTitle.GetValue() == '':\n\t\t\tmsg = wx.MessageDialog(self, \n\t\t\t\t'Movie metadata cannot be saved without a Title. Cannot continue', \n\t\t\t\t'Movie Title Missing', wx.OK|wx.ICON_INFORMATION)\n\t\t\tmsg.ShowModal()\n\t\t\tmsg.Destroy()\n\t\telse:\n\t\t\tself.EndModal(wx.ID_OK)\n\t\t\n\t\t\n\tdef OnGetMetadata(self, event):\n\t\ttitle = self.txtTitle.GetValue()\n\t\tyear = self.txtReleased.GetValue()\n\t\t\n\t\tif title=='':\n\t\t\tdlg = wx.MessageDialog(self,\n\t\t\t\t\"Enter the title of the movie. Optionally enter the year(approximate).\", \n\t\t\t\t\"Get metadata from IMDB\",\n\t\t\t\twx.OK|wx.ICON_INFORMATION)\n\t\t\tresult = dlg.ShowModal()\n\t\t\tdlg.Destroy()\n\t\t\treturn\n\t\t\n\t\tself.mainTb.EnableTool(self.tb_search, False)\n\t\t\n\t\tself.statusText.SetLabel(\"Getting metadata from IMDB...\")\n\t\t\n\t\tthread.start_new_thread(self._get_metadata, (title, year, self.postersPath, self.catchPath))\n\t\t\n\t\n\tdef _get_metadata(self, title, year, postersPath, catchPath):\n\t\ttry:\n\t\t\tmetadata = ImdbAPI.GetMetadata(title, year, postersPath, catchPath)\n\t\t\n\t\t\twx.CallAfter(self._done_get_metadata, metadata)\n\t\texcept wx._core.PyDeadObjectError, e:\n\t\t\tprint \"dialog closed before thread could complete\"\n\t\t\n\t\n\tdef _done_get_metadata(self, metadata):\n\t\tself.statusText.SetLabel(\"\")\n\t\t\n\t\tif metadata != None:\n\t\t\tprint \"Success\"\n\t\t\t\n\t\t\tself.txtTitle.SetValue(metadata['title'])\n\t\t\tself.txtImage.SetValue(metadata['image'])\n\t\t\tself.txtReleased.SetValue(metadata['released'])\n\t\t\tself.txtRuntime.SetValue(metadata['runtime'])\n\t\t\tself.txtRated.SetValue(metadata['rated'])\n\t\t\tself.txtSummary.SetValue(metadata['summary'])\n\t\t\n\t\t\tprint \"Genres\"\n\t\t\tself.lstGenres.DeleteAllItems()\n\t\t\tself.lstGenres.AddValuesSimple(metadata['genres'])\n\n\t\t\tprint \"Actors\"\n\t\t\tself.lstActors.DeleteAllItems()\n\t\t\tself.lstActors.AddValuesSimple(metadata['actors'])\n\n\t\t\tprint \"Directors\"\n\t\t\tself.lstDirectors.DeleteAllItems()\n\t\t\tself.lstDirectors.AddValuesSimple(metadata['directors'])\n\t\telse:\n\t\t\tdlg = wx.MessageDialog(self,\n\t\t\t\t\"No results were found for the given title and year. (this may be due to a network error)\", \n\t\t\t\t\"Get metadata from IMDB\",\n\t\t\t\twx.OK|wx.ICON_INFORMATION)\n\t\t\tresult = dlg.ShowModal()\n\t\t\tdlg.Destroy()\n\t\t\n\t\tself.mainTb.EnableTool(self.tb_search, True)", "original_line_number": 22533, "instruction_length": 587}
{"instruction": "This code defines an Enum class called RollState with five possible states, and a default state of No_Roll. It also defines a dictionary of explanations for each state, and several functions for working with RollState objects, including checking if a state is a forced roll state, explaining a RollState object as a string, and determining the allowable RollState transitions based on the current state and a priced position. To use this code, import the necessary modules and create RollState objects as needed, then call the provided functions to work with them.", "input": "", "output": "from enum import Enum\nfrom syscore.objects import named_object\n\nRollState = Enum(\"RollState\",(\n    \"No_Roll\",\n    \"Passive\",\n    \"Force\",\n    \"Force_Outright\",\n    \"Roll_Adjusted\"))\n\ndefault_state = RollState.No_Roll\nroll_adj_state = RollState.Roll_Adjusted\n\n\nroll_explanations = {\n    RollState.No_Roll:\"No rolling happens. Will only trade priced contract.\",\n    RollState.Passive:\"Allow the contract to roll naturally (closing trades in priced contract, opening trades in forward contract)\",\n    RollState.Force:\"Force the contract to roll ASAP using spread order\",\n    RollState.Force_Outright:\"Force the contract to roll ASAP using two outright orders\",\n    RollState.Roll_Adjusted:\"Roll adjusted prices from existing priced to new forward contract (after adjusted prices have been changed, will automatically move state to no roll\"}\n\ndef is_forced_roll_state(roll_state: RollState):\n    if roll_state == RollState.Force or roll_state == RollState.Force_Outright:\n        return True\n    else:\n        return False\n\ndef is_type_of_active_rolling_roll_state(roll_state: RollState):\n    if is_forced_roll_state(roll_state) or roll_state == RollState.Roll_Adjusted:\n        return True\n    else:\n        return False\n\n\ndef explain_roll_state_str(roll_state: RollState):\n    return roll_explanations[RollState[roll_state]]\n\ndef name_of_roll_state(roll_state: RollState):\n    return roll_state.name\n\ndef complete_roll_state(roll_state: RollState, priced_position):\n    if priced_position == 0:\n        flag_position_in_priced = 0\n    else:\n        flag_position_in_priced = 1\n\n    return \"%s%s\" % (name_of_roll_state(roll_state), flag_position_in_priced)\n\n\ndef allowable_roll_state_from_current_and_position(\n        current_roll_state: RollState, priced_position:int):\n    # Transition matrix: First option is recommended\n    # A 0 suffix indicates we have no position in the priced contract\n    # A 1 suffix indicates we do have a position in the priced contract\n    allowed_transition = dict(\n        No_Roll0=[\"Roll_Adjusted\", \"Passive\", \"No_Roll\"],\n        No_Roll1=[\"Passive\", \"Force\", \"Force_Outright\", \"No_Roll\"],\n        Passive0=[\"Roll_Adjusted\", \"Passive\", \"No_Roll\"],\n        Passive1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Force0=[\"Roll_Adjusted\", \"Passive\"],\n        Force1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Force_Outright0=[\"Roll_Adjusted\", \"Passive\"],\n        Force_Outright1=[\"Force\", \"Force_Outright\", \"Passive\", \"No_Roll\"],\n        Roll_Adjusted0=[\"No_Roll\"],\n        Roll_Adjusted1=[\"Roll_Adjusted\"],\n    )\n\n    status_plus_position = complete_roll_state(\n        current_roll_state, priced_position)\n    try:\n        allowable_states = allowed_transition[status_plus_position]\n    except KeyError:\n        raise Exception(\n            \"State plus position %s not recognised\" %\n            status_plus_position)\n\n    return allowable_states", "original_line_number": 23495, "instruction_length": 564}
{"instruction": "To run the code, follow these steps:\n1. Import the necessary libraries: `from __future__ import division`, `from __future__ import print_function`, `import os`, `import sys`, `import functools`, `import numpy as np`, `import pandas as pd`, `import multiprocessing`, `from pdb import set_trace`, `from Simulator import simulate`, `from Utils.PlotsUtils import line, line2`, `from Utils.RandomUtil import Random`, `from Utils.MisclUtils import TimeUtil`.\n2. Update the path: `root = os.path.join(os.getcwd().split('proj1')[0], 'proj1')`, and append it to the system path if it's not already there: `if root not in sys.path: sys.path.append(root)`.\n3. Set the seed for the random number generator: `rand.set_seed(seed_val=12458)`.\n4. Define a function to calculate the customer loss rate: `def customer_loss_rate(customers): ...`.\n5. Define a function to plot the runtime: `def plot_runtime(x=None, y=None): ...`.\n6. Define a function to plot the runtime vs. average: `def plot_runtime_vs_avg(x, y, y_1): ...`.\n7. Define a function to perform task 5: `def task_5(): ...`.\n8. Define a function to plot the results of task 5: `def task5_plot(): ...`.\n9. Define a function to compare the results of task 5 with the expected values: `def compare_plot(): ...`.\n10. Call the functions in the following order: `task_5()`, `task5_plot()`, `compare_plot()`.", "input": "", "output": "from __future__ import division\nfrom __future__ import print_function\nimport os\nimport sys\nimport functools\n\n# Update path\nroot = os.path.join(os.getcwd().split('proj1')[0], 'proj1')\nif root not in sys.path:\n    sys.path.append(root)\n\nimport numpy as np\nimport pandas as pd\nimport multiprocessing\nfrom pdb import set_trace\nfrom Simulator import simulate\nfrom Utils.PlotsUtils import line, line2\nfrom Utils.RandomUtil import Random\nfrom Utils.MisclUtils import TimeUtil\n\nrand = Random()\ntimer = TimeUtil()\n\n# Set seed\nrand.set_seed(seed_val=12458)\n\ndef customer_loss_rate(customers):\n    served = np.sum([customer.serviced for customer in customers])\n    total = len(customers)\n    return served / total\n\ndef plot_runtime(x=None, y=None):\n    line(x, y, x_label=r\"$\\rho$\", y_label=r\"Run Times\", the_title=r\"$\\mathrm{Run\\ Times\\ in\\ }\\mu\\mathrm{s\\ vs.\\ }\\rho$\")\n\n\ndef plot_runtime_vs_avg(x, y, y_1):\n    line2(x, y, x, y_1, label_1=\"Actual Runtimes\", label_2=\"Expected value of $\\rho$\", x_label=r\"$\\rho$\", y_label=r\"Run Times\", the_title=r\"$\\mathrm{Run\\ Times\\ in\\ }\\mu\\mathrm{s\\ vs.\\ }\\rho$\")\n\n\ndef task_5():\n    rho_list = np.arange(0.05, 1, 0.1)\n    C = 1e5\n    elapsed = []\n    for rho in rho_list:\n        start_time = timer.current_time()\n        serviced = simulate(l = rho, server_lim = 40, max_serviced=C, L=1, verbose=False)\n        end_time = timer.current_time()\n        elapsed.append(end_time-start_time)\n\n    data = pd.DataFrame([[a,b] for a, b in zip(rho_list, elapsed)], columns=[\"Rho\", \"Seconds\"])\n    data.to_csv(os.path.abspath(os.path.join(root,\"tasks/task5.csv\")))\n\ndef task5_plot():\n    data = pd.read_csv(os.path.abspath(\"tasks/task5.csv\"))\n    plot_runtime(data[\"Rho\"], data[\"Seconds\"])\n    set_trace()\n\n\ndef compare_plot():\n    rho_list = np.arange(0.05, 1, 0.1)\n    average_rho = [np.mean([rand.exponential(lam=p) for _ in xrange(10000)]) for p in rho_list]\n    data = pd.read_csv(os.path.abspath(\"tasks/task5.csv\"))\n    plot_runtime(data[\"Rho\"], average_rho)\n\n\nif __name__ == \"__main__\":\n    task_5()\n    task5_plot()\n    compare_plot()", "original_line_number": 24829, "instruction_length": 1345}
{"instruction": "To generate a collage of images from Instagram, Tumblr, and Google based on a location, follow these steps:\n1. Import the necessary libraries: requests, os, time, random, Flask, request, redirect, session, url_for, render_template, jsonify, dumps, loads, OAuth2Session, json, urllib2, mechanize, BeautifulSoup, urlparse, and build.\n2. Set the environment variable 'OAUTHLIB_INSECURE_TRANSPORT' to '1'.\n3. Define the template directory as the directory containing the HTML templates.\n4. Create a Flask app with the name 'app' and set the DEBUG and PROPAGATE_EXCEPTIONS configurations to True.\n5. Set the secret key for the app.\n6. Set the Instagram client ID, client secret, authorization base URL, token URL, and image search URL.\n7. Define a route for the Instagram authorization page.\n8. If the user has already authorized the app, redirect to the home page.\n9. Create an OAuth2Session object with the Instagram client ID and redirect URI.\n10. Get the authorization URL and state from the OAuth2Session object.\n11. Save the state in the session.\n12. Redirect the user to the authorization URL.\n13. Define a route for the Instagram token retrieval page.\n14. Create an OAuth2Session object with the Instagram client ID, client secret, and redirect URI, and the state from the session.\n15. Fetch the token using the OAuth2Session object and the code from the request arguments.\n16. Save the access token in the session.\n17. Redirect the user to the home page.\n18. Define a route for the home page.\n19. If the user has not authorized the app, redirect to the Instagram authorization page.\n20. Get information about the user using the access token.\n21. Render the home page template with the user data.\n22. Define a route for the collage page.\n23. If the request method is POST, get the number of photos and the place from the request data.\n24. Use Google Geocoding to convert the place to latitude and longitude coordinates.\n25. Make an API call to Instagram to get the models.\n26. Choose the desired number of images from the models.\n27. Get images from Tumblr and Google using the place as a tag or query.\n28. Get the airport, lowest price, and average savings for the place using Jetblue API.\n29. Render the collage page template with the images, airport, lowest price, and average savings.", "input": "", "output": "import requests\nimport os \nimport time\nimport random\nfrom flask import Flask, request, redirect, session, url_for, render_template\nfrom flask.json import jsonify, dumps, loads\nfrom requests_oauthlib import OAuth2Session\nimport requests\nimport json\nimport urllib2\nimport mechanize\nfrom bs4 import BeautifulSoup\nfrom urlparse import urlparse\nfrom apiclient.discovery import build\n\n\n\nos.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\ntmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')\n\napp = Flask(__name__, tmpl_dir)\napp.config['DEBUG'] = True\napp.config['PROPAGATE_EXCEPTIONS'] = True\napp.secret_key = 'A0Zr98j/3yX R~XHH!jmN]LWX/,?RT'\n\ninstagram_client_id = \"115a6c0fd0a64bccbf213e4eafec554a\"\ninstagram_client_secret = \"72f3282930444d9e826e5f083ede32d1\"\ninstagram_authorization_base_url = \"https://api.instagram.com/oauth/authorize\"\ninstagram_token_url = \"https://api.instagram.com/oauth/access_token\"\ninstagram_image_search_url = \"https://api.instagram.com/v1/media/search\"\n\ngoogle_api_key = \"AIzaSyCLehiRvLWhFXbwkI6zojampXcICC0-rMU\"\ngoogle_geocoding_url = \"https://maps.googleapis.com/maps/api/geocode/json?address=%s\"\n\n\n@app.route('/')\ndef instagram_authorization():\n\tif(session.get(\"instagram_access_key\" != None)):\n\t\treturn redirect(\"/home\")\n\n\toauth = OAuth2Session(instagram_client_id, redirect_uri=\"http://127.0.0.1:5000/callback\")\n\tauthorization_url, state = oauth.authorization_url(instagram_authorization_base_url)\n\n\tsession['oauth_state'] = state\n\treturn redirect(authorization_url)\n\n@app.route('/callback', methods=[\"GET\"])\ndef instagram_token_retrieval():\n\toauth = OAuth2Session(instagram_client_id, redirect_uri=\"http://127.0.0.1:5000/callback\", state=session['oauth_state'])\n\t# When grabbing the token Instagram requires you give the code you got in the authorization step in the token step, along with client_id + secret -_-\n\t# This fetch token call might not be right for other APIs, it all demands on their requirements\n\tmy_token = oauth.fetch_token(instagram_token_url, code=request.args.get('code'), client_secret=instagram_client_secret, client_id=instagram_client_id, authorization_url=request.url)\n\tsession['instagram_access_key'] = my_token['access_token']\n\treturn redirect(\"/home\")\n\n'''\n\tRoute representing the home page\n'''\n@app.route('/home')\ndef home():\n\t#TODO: Put in Flickr APi for the home page. \n\tif(session.get('instagram_access_key') == None):\n\t\treturn redirect(\"/\")\n\t#Lets get info on myself the access_token holder\n\taccess_token = session['instagram_access_key']\n\tr = requests.request(\"GET\",'https://api.instagram.com/v1/users/self/?access_token=%s' % access_token)\n\treturn render_template('home.html', user_data=r.json())\n\n\n'''\n\tThe main route for the collage page\n'''\n#after user hits submit button. \n@app.route('/location/<place>', methods=[\"POST\", \"GET\"])\ndef get_collage(place):\n\t#headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n\t#payload = {'num_photos': 3, 'place': place}\n\turl = 'http://127.0.0.1:5000/location/instagram/'+place\n\t#import urlparse\n\t#url = 'http://127.0.0.1:5000/location/instagram/holder'\n\t#parts = urlparse.urlparse(url)\n\t#parts = parts._replace(path=)\n\t#parts.geturl()\n\t#print payload\n\tresponse = get_instagram_photos(place)\n\n\tresponse2= json.loads(get_google_images(place))\n\tresponse3= json.loads(get_google_images2(place))\n\tresponse4 = json.loads(get_tumblr_images(place)) \n \n\tprint \"RECIEVES\"\n\tprint response\n\tprint \"GOOGLE\"\n\tprint response2\n\tprint \"TUMBLR\" \n\tprint response4\n        place = place.replace(\"+\", \" \")\n        airport = get_airport(place)\n        price = \"Packages for Jetblue start as low as \" + str(get_lowest_price(place)) + \". \"\n        average_savings = \"And save up to \" + str(get_savings_percentage(place)) + \" compared to Expedia! Wow Jetblue is so awesome!\"\n    \treturn render_template('collage.html', place=place, photos_display=response, photos_google= response2, photos_tumblr= response4, photos_google2 = response3, lowest_price=price, average_savings=average_savings, airport=airport)\n\ndef get_airport(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[1]\n\n\ndef get_savings_percentage(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[5][:-1]\n\ndef get_lowest_price(place):\n    f = open('./jetblue/jetblueresults', 'r')\n    for line in f:\n        lineList = line.split(',')\n        destination = lineList[2].lower()\n        if (destination == place.lower()):\n            return lineList[4]\n        \n'''\n\tWill return a list of image URLs from instagram given the name of a location\n'''\n\ndef get_instagram_photos(place):\n\tprint \"hell\"\n\tprint place\n\tif(session.get('instagram_access_key') == None):\n\t\tprint \"REDIRECT\"\n\t\treturn redirect(\"/\")\n    #http://127.0.0.1:5000/location/instagram/Chicago/3\n    #place, num_photos, \n\t# Use Google Geocoding to convert place to lat and long coordinates\n\tnum_photos = 25;\n\tprint place\n\tlocation = requests.get(google_geocoding_url % place)\n\tlocation = location.json()\n\tprint location\n\tlat_coord = location.get(\"results\")[0].get(\"geometry\").get(\"location\").get(\"lat\")\n\tlong_coord = location.get(\"results\")[0].get(\"geometry\").get(\"location\").get(\"lng\")\n\tprint lat_coord\n\tprint long_coord\n\t# Make the API call to get the Models\n\tquerys = {\"lat\": lat_coord, \"lng\" : long_coord, \"min_timestamp\": \"1262304000\", \"max_timestamp\":\"1446940800\", \"distance\" : \"10000\" , \"access_token\": session.get('instagram_access_key')}\n\tinstagram_models = requests.get(instagram_image_search_url, params=querys)\n\tchosen_images = []\n\n\tjson_object = loads(instagram_models.text) \n\tprint json_object\n\tif len(json_object[\"data\"]) > num_photos:\n\t\tfor i in range(0, num_photos): \n\t\t\tchosen_images.append(json_object[\"data\"][i][\"images\"])\n\telse:\n\t\tfor i in range(0, len(json_object[\"data\"])):\n\t\t\tchosen_images.append(json_object[\"data\"][i][\"images\"])\n\t\tprint len(json_object[\"data\"])\n\t\tprint num_photos\n\n\tprint chosen_images\n\n\n\treturn chosen_images\t\n\ndef get_tumblr_images(place):\n\tprint \"GETTING TUMBLR\" \n\turl = ('https://api.tumblr.com/v2/tagged?tag='+urllib2.quote(place)+\"&api_key=YaGrzj5NUOlMDxQyTtkSBz1KEAnVyUYcCRKWT74VzNUJwRbtH4\")\n\tprint url \n\treq = urllib2.Request(url, headers={'accept': '*/*'})\n\tresponse = urllib2.urlopen(req)\n\tprint \"TUMBLR\"\n\tret = response.read() \n\tprint ret \n\treturn ret\n\ndef get_google_images(place):\n    print \"MOVING ON TO GOOGLE\"\n    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0&q='+ urllib2.quote(place) + \"&rsz=8\")\n    print url\n    req = urllib2.Request(url, headers={'accept': '*/*'})\n    response = urllib2.urlopen(req)\n    print \"GOOGLE RESPONSE\"\n    print type(response)\n    print \"TYPE OF RESPONSE.READ\"    \n    ret = response.read()\n    print len(ret)\n    print \"RET\"\n    print ret\n    return ret\n\n\n\ndef get_google_images2(place):\n    print \"MOVING ON TO GOOGLE\"\n    url = ('https://ajax.googleapis.com/ajax/services/search/images?' + 'v=1.0&q='+ urllib2.quote(place) +'&rsz=8&start=9')\n    print url\n    req = urllib2.Request(url, headers={'accept': '*/*'})\n    response = urllib2.urlopen(req)\n    print \"GOOGLE RESPONSE\"\n    print type(response)\n    print \"TYPE OF RESPONSE.READ\"    \n    ret = response.read()\n    print len(ret)\n    print \"RET\"\n    print ret\n    return ret\n\nif __name__ == '__main__':\n\tapp.run()", "original_line_number": 25010, "instruction_length": 2291}
{"instruction": "This code retrieves the status of various devices from Verisure and updates the corresponding devices in Domoticz. To use this code, you need to have a Verisure account and a Domoticz installation. Make sure that the necessary libraries are installed and that the paths to the configuration files are correct. Once the code is executed, it will log in to Verisure, retrieve the status of the alarm, smartplugs, and climate devices, and update the corresponding devices in Domoticz. The code will run every `frequency` seconds, as specified in the configuration file.", "input": "", "output": "#!/usr/bin/python\n# -*- coding: utf-8 -*-\nimport domoticz as d\nimport sys\nimport time\nsys.path.insert(0, '/opt/python-verisure/')\nimport verisure\nimport pickle\nimport pytz\nimport urllib3\nimport certifi\nfrom datetime import datetime\nfrom tzlocal import get_localzone\n\ndebug = False\n\ntry:\n    execfile(\"/etc/domoticz/scripts.conf\")\nexcept:\n    exec(open(\"/etc/domoticz/scripts.conf\").read())\n\nd.log(\"Getting status from Verisure...\")\n\nif int(time.time()) % frequency < 60 :\n\n\t#Login\n\ttry:\n\t\tf = open(mypagesSession, 'rb')\n\t\tmyPages = pickle.load(f)\n\t\tf.close()\n\texcept:\n\t\tmyPages = verisure.Session(email, verisurepass)\n\t\tmyPages.login()\n\t\tf = open(mypagesSession, 'wb')\n\t\tpickle.dump(myPages, f)\n\t\tf.close()\n\t\tif debug:\n\t\t\td.log(\"Loading file failed.\")\n\n\t#Get overview\n\ttry:\n\t\toverview = myPages.get_overview()\n\texcept:\n\t\tmyPages = verisure.Session(email, verisurepass)\n\t\tmyPages.login()\n\t\tf = open(mypagesSession, 'wb')\n\t\tpickle.dump(myPages, f)\n\t\tf.close()\n\t\toverview = myPages.get_overview()\n\t\tif debug:\n\t\t\td.log(\"Session was timed out\")\n\n\t#Alarm\n\tstatus = overview['armState']['statusType']\n\tif debug:\n\t\td.log(\"Verisure Alarm status: \", status )\n\tdevice = d.devices[atHome]\n\tif status == \"DISARMED\" or status == \"ARMED_HOME\":\n\t\tdevice.on()\n\telse:\n\t\tdevice.off()\n\n\t#Smartplugs\n\tfor i in overview['controlPlugs']:\n\t\tif debug:\n\t\t\td.log(\"Verisure Smartplug status for \" + i['area'].encode(\"utf-8\",\"ignore\") + \": \", i['currentState'] )\n\t\tdevice = d.devices[i['area'].encode(\"utf-8\",\"ignore\")]\n\t\tif i['currentState'] == \"ON\":\n\t\t\tdevice.on()\n\t\telse:\n\t\t\tdevice.off()\n\n\t#Climate\n\tfor i in overview['climateValues']:\n\t\tdevice = d.devices[i['deviceArea'].encode(\"utf-8\",\"ignore\")]\n\t\tdomlastupdate = datetime.strptime(device.last_update_string, '%Y-%m-%d %H:%M:%S')\n\t\tverilastupdate = datetime.strptime(i['time'][:-5], '%Y-%m-%dT%H:%M:%S')\n\t\tverilastupdate = verilastupdate.replace(tzinfo=pytz.UTC)\n\t\tverilastupdate = verilastupdate.astimezone(get_localzone())\n\t\tverilastupdate = verilastupdate.replace(tzinfo=None)\n\t\tif debug:\n\t\t\td.log(\"Domoticz last update of \" + device.name + \": \" + str(domlastupdate))\n\t\t\td.log(\"Verisure last update of \" + device.name + \": \" + str(verilastupdate))\n\n\t\tif verilastupdate > domlastupdate:\n\t\t\tif debug:\n\t\t\t\td.log(\"update domoticz climate device \" + device.name)\n\t\t\tif debug:\n\t\t\t\td.log(\"time: \" + i['time'] )\n\t\t\t\td.log(\"location: \" + i['deviceArea'].encode(\"utf-8\",\"ignore\") )\n\t\t\t\td.log(\"serial: \" + i['deviceLabel'] )\n\t\t\t\td.log(\"temperature: \" + str(i['temperature']))\n\t\t\tif 'humidity' in i:\n\t\t\t\tif debug:\n\t\t\t\t\td.log(\"humidity: \" + str(i['humidity']))\n\t\t\t\tif i['humidity'] < 20:\n\t\t\t\t\tcomf = 2\n\t\t\t\tif i['humidity'] >= 20 and i['humidity'] <= 35:\n\t\t\t\t\tcomf = 0\n\t\t\t\tif i['humidity'] > 35 and i['humidity'] <= 75:\n\t\t\t\t\tcomf = 1\n\t\t\t\tif i['humidity'] > 75:\n\t\t\t\t\tcomf = 3\n\t\t\t\turl = baseurl + \"type=command&param=udevice&idx=\" + climate[i['deviceArea'].encode(\"utf-8\",\"ignore\")] + \"&nvalue=0&svalue=\" + str(i['temperature']) + \";\" + str(i['humidity']) + \";\" + str(comf)\n\t\t\telse:\n\t\t\t\turl = baseurl + \"type=command&param=udevice&idx=\" + climate[i['deviceArea'].encode(\"utf-8\",\"ignore\")] + \"&nvalue=0&svalue=\" + str(i['temperature'])\n\n\t\t\tif debug:\n\t\t\t\td.log('URL: ' + url)\n\n\t\t\thttp = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n\t\t\tr = http.request('GET', url, timeout=2.5)\n\t\t\tif debug:\n\t\t\t\td.log(\"Status code: \" + str(r.status) + \"\\n\" + r.data)\n\t\t\tif r.status != 200:\n\t\t\t\td.log(\"Error updating temp in Domoticz. HTTP code: \" + str(r.status) + \" \" + r.data)\n\nelse:\n\tif debug:\n\t\td.log(\"Only runs every \" + str(frequency/60) + \" min.\")\n\nd.log(\"done getting status from Verisure\")", "original_line_number": 25028, "instruction_length": 566}
{"instruction": "Create a Django model called \"FileBrowserFile\" with the following fields: \"file_type\" (a character field with a max length of 3 and choices of \"img\" or \"doc\"), \"uploaded_file\" (a file field that uploads to a function called \"content_file_name\" and has a verbose name of \"File / Image\" and a max length of 300), \"create_date\" (a date time field that auto populates with the current date and time and has a verbose name of \"Create date\"), and \"user_id\" (an integer field that is nullable, blankable, and has a verbose name of \"Who does this file belong to?\"). The \"__unicode__\" method should return the name of the uploaded file.", "input": "", "output": "import datetime\n\nfrom django.db import models\nfrom django.utils.translation import ugettext as _\n\nfrom mce_filebrowser.conf import   LOCAL_MCE_FILEBROWSER_UPLOADDIR,LOCAL_MCE_FILEBROWSER_PERUSER\n\ndef content_file_name(instance, filename):\n    if LOCAL_MCE_FILEBROWSER_PERUSER == True:\n        return \"%s/%s/%s/%s\" %(LOCAL_MCE_FILEBROWSER_UPLOADDIR,'user-%s' % str(instance.user_id), datetime.datetime.now().strftime(\"%Y/%m/%d\"), filename)\n    else:\n        return \"%s/%s/%s\" %(LOCAL_MCE_FILEBROWSER_UPLOADDIR, datetime.datetime.now().strftime(\"%Y/%m/%d\"), filename)\n\n\n\nclass FileBrowserFile(models.Model):\n    \"\"\" Uploaded file model \"\"\"\n    FILE_TYPES = (\n        ('img', _('Image')),\n        ('doc', _('Document')),\n    )\n    \n    file_type = models.CharField(max_length=3, choices=FILE_TYPES)\n    uploaded_file = models.FileField(\n        upload_to=content_file_name,\n        verbose_name = _('File / Image'),\n        max_length=300,\n    )\n    create_date = models.DateTimeField(\n        auto_now_add=True, verbose_name=_('Create date')\n    )\n\n    user_id = models.IntegerField(null=True, blank=True, verbose_name=_('Who does this file belong to?'))\n\n    def __unicode__(self):\n        return u'%s' % self.uploaded_file.name", "original_line_number": 25539, "instruction_length": 627}
{"instruction": "This code defines a Django model for handling errors in a project. To use this code, create a Django app and add this code to the models.py file. Then, run the migrations to create the necessary database tables. To create a new error, use the ErrorManager.create() method, passing in the computer, project, and description as arguments. To get a queryset of all errors, use the DomainErrorManager.get_queryset() method. To get a queryset of unchecked errors, use the UncheckedManager.get_queryset() method. To get a queryset of errors scoped to a specific user, use the DomainErrorManager.scope() method, passing in the user as an argument. To get the count of unchecked errors, use the Error.unchecked_count() method. To get a dictionary of unchecked errors grouped by project and platform, use the Error.unchecked_by_project() method, passing in the user as an argument. To get a dictionary of errors grouped by project and computer status, use the Error.status_by_project() method, passing in the user as an argument. To mark an error as checked, use the checked_ok() method on an Error instance.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2015-2021 Jose Antonio Chavarría <jachavar@gmail.com>\n# Copyright (c) 2015-2021 Alberto Gacías <alberto@migasfree.org>\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program. If not, see <http://www.gnu.org/licenses/>.\n\nfrom django.db import models\nfrom django.db.models.aggregates import Count\nfrom django.utils.translation import gettext_lazy as _\n\nfrom ...core.models import Project\n\nfrom .computer import Computer\nfrom .event import Event\n\n\nclass DomainErrorManager(models.Manager):\n    def get_queryset(self):\n        return super().get_queryset().select_related(\n            'project',\n            'computer',\n            'computer__project',\n            'computer__sync_user',\n        )\n\n    def scope(self, user):\n        qs = self.get_queryset()\n        if not user.is_view_all():\n            qs = qs.filter(\n                project_id__in=user.get_projects(),\n                computer_id__in=user.get_computers()\n            )\n\n        return qs\n\n\nclass UncheckedManager(DomainErrorManager):\n    def get_queryset(self):\n        return super().get_queryset().filter(checked=0)\n\n    def scope(self, user):\n        return super().scope(user).filter(checked=0)\n\n\nclass ErrorManager(DomainErrorManager):\n    def create(self, computer, project, description):\n        obj = Error()\n        obj.computer = computer\n        obj.project = project\n        obj.description = description\n        obj.save()\n\n        return obj\n\n\nclass Error(Event):\n    description = models.TextField(\n        verbose_name=_(\"description\"),\n        null=True,\n        blank=True\n    )\n\n    checked = models.BooleanField(\n        verbose_name=_(\"checked\"),\n        default=False,\n    )\n\n    project = models.ForeignKey(\n        Project,\n        on_delete=models.CASCADE,\n        verbose_name=_(\"project\")\n    )\n\n    objects = ErrorManager()\n    unchecked = UncheckedManager()\n\n    @staticmethod\n    def unchecked_count(user=None):\n        if not user:\n            return Error.unchecked.count()\n\n        return Error.unchecked.scope(user).count()\n\n    @staticmethod\n    def unchecked_by_project(user):\n        total = Error.unchecked_count(user)\n\n        projects = list(Error.unchecked.scope(user).values(\n            'project__name',\n            'project__id',\n            'project__platform__id',\n        ).annotate(\n            count=Count('id')\n        ).order_by('project__platform__id', '-count'))\n\n        platforms = list(Error.unchecked.scope(user).values(\n            'project__platform__id',\n            'project__platform__name'\n        ).annotate(\n            count=Count('id')\n        ).order_by('project__platform__id', '-count'))\n\n        return {\n            'total': total,\n            'inner': platforms,\n            'outer': projects,\n        }\n\n    @staticmethod\n    def status_by_project(user):\n        total = Error.objects.scope(user).count()\n\n        projects = list(Error.objects.scope(user).values(\n            'computer__status',\n            'project__id',\n            'project__name',\n        ).annotate(\n            count=Count('id')\n        ).order_by('computer__status', '-count'))\n\n        status = list(Error.objects.scope(user).values(\n            'computer__status',\n        ).annotate(\n            count=Count('id')\n        ).order_by('computer__status', '-count'))\n\n        for item in status:\n            item['status'] = item.get('computer__status')\n            item['computer__status'] = _(dict(Computer.STATUS_CHOICES)[item.get('computer__status')])\n\n        return {\n            'total': total,\n            'inner': status,\n            'outer': projects,\n        }\n\n    def checked_ok(self):\n        self.checked = True\n        self.save()\n\n    def save(self, force_insert=False, force_update=False, using=None, update_fields=None):\n        self.description = self.description.replace(\"\\r\\n\", \"\\n\")\n\n        super().save(force_insert, force_update, using, update_fields)\n\n    class Meta:\n        app_label = 'client'\n        verbose_name = _('Error')\n        verbose_name_plural = _('Errors')", "original_line_number": 25627, "instruction_length": 1099}
{"instruction": "Create a class called \"sid\" with a state parameter for Prefix-SID. The class should be a container and should be auto-generated by the PythonClass plugin for PYANG from YANG module openconfig-network-instance. The class should be based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of the container should be represented as a class variable with a specific YANG type. The Prefix Segment-ID list should be an IGP-Prefix Segment attached to an IGP prefix. An IGP-Prefix Segment should be global (unless explicitly advertised otherwise) within the SR/IGP domain.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom operator import attrgetter\nfrom pyangbind.lib.yangtypes import RestrictedPrecisionDecimalType\nfrom pyangbind.lib.yangtypes import RestrictedClassType\nfrom pyangbind.lib.yangtypes import TypedListType\nfrom pyangbind.lib.yangtypes import YANGBool\nfrom pyangbind.lib.yangtypes import YANGListType\nfrom pyangbind.lib.yangtypes import YANGDynClass\nfrom pyangbind.lib.yangtypes import ReferenceType\nfrom pyangbind.lib.base import PybindBase\nfrom collections import OrderedDict\nfrom decimal import Decimal\nfrom bitarray import bitarray\nimport six\n\n# PY3 support of some PY2 keywords (needs improved)\nif six.PY3:\n    import builtins as __builtin__\n\n    long = int\nelif six.PY2:\n    import __builtin__\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Prefix Segment-ID list. IGP-Prefix Segment is an IGP segment attached\nto an IGP prefix. An IGP-Prefix Segment is global (unless explicitly\nadvertised otherwise) within the SR/IGP domain.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"extended-ipv4-reachability\",\n                \"prefixes\",\n                \"prefix\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"prefix-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])\n\n\nfrom . import state\n\n\nclass sid(PybindBase):\n    \"\"\"\n  This class was auto-generated by the PythonClass plugin for PYANG\n  from YANG module openconfig-network-instance-l2 - based on the path /network-instances/network-instance/protocols/protocol/isis/levels/level/link-state-database/lsp/tlvs/tlv/extended-ipv4-reachability/prefixes/prefix/subTLVs/subTLVs/prefix-sid/sid. Each member element of\n  the container is represented as a class variable - with a specific\n  YANG type.\n\n  YANG Description: Prefix Segment-ID list. IGP-Prefix Segment is an IGP segment attached\nto an IGP prefix. An IGP-Prefix Segment is global (unless explicitly\nadvertised otherwise) within the SR/IGP domain.\n  \"\"\"\n    __slots__ = (\"_path_helper\", \"_extmethods\", \"__state\")\n\n    _yang_name = \"sid\"\n\n    _pybind_generated_by = \"container\"\n\n    def __init__(self, *args, **kwargs):\n\n        self._path_helper = False\n\n        self._extmethods = False\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n        load = kwargs.pop(\"load\", None)\n        if args:\n            if len(args) > 1:\n                raise TypeError(\"cannot create a YANG container with >1 argument\")\n            all_attr = True\n            for e in self._pyangbind_elements:\n                if not hasattr(args[0], e):\n                    all_attr = False\n                    break\n            if not all_attr:\n                raise ValueError(\"Supplied object did not have the correct attributes\")\n            for e in self._pyangbind_elements:\n                nobj = getattr(args[0], e)\n                if nobj._changed() is False:\n                    continue\n                setmethod = getattr(self, \"_set_%s\" % e)\n                if load is None:\n                    setmethod(getattr(args[0], e))\n                else:\n                    setmethod(getattr(args[0], e), load=load)\n\n    def _path(self):\n        if hasattr(self, \"_parent\"):\n            return self._parent._path() + [self._yang_name]\n        else:\n            return [\n                \"network-instances\",\n                \"network-instance\",\n                \"protocols\",\n                \"protocol\",\n                \"isis\",\n                \"levels\",\n                \"level\",\n                \"link-state-database\",\n                \"lsp\",\n                \"tlvs\",\n                \"tlv\",\n                \"extended-ipv4-reachability\",\n                \"prefixes\",\n                \"prefix\",\n                \"subTLVs\",\n                \"subTLVs\",\n                \"prefix-sid\",\n                \"sid\",\n            ]\n\n    def _get_state(self):\n        \"\"\"\n    Getter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        return self.__state\n\n    def _set_state(self, v, load=False):\n        \"\"\"\n    Setter method for state, mapped from YANG variable /network_instances/network_instance/protocols/protocol/isis/levels/level/link_state_database/lsp/tlvs/tlv/extended_ipv4_reachability/prefixes/prefix/subTLVs/subTLVs/prefix_sid/sid/state (container)\n    If this variable is read-only (config: false) in the\n    source YANG file, then _set_state is considered as a private\n    method. Backends looking to populate this variable should\n    do so via calling thisObj._set_state() directly.\n\n    YANG Description: State parameters for Prefix-SID.\n    \"\"\"\n        if hasattr(v, \"_utype\"):\n            v = v._utype(v)\n        try:\n            t = YANGDynClass(\n                v,\n                base=state.state,\n                is_container=\"container\",\n                yang_name=\"state\",\n                parent=self,\n                path_helper=self._path_helper,\n                extmethods=self._extmethods,\n                register_paths=True,\n                extensions=None,\n                namespace=\"http://openconfig.net/yang/network-instance\",\n                defining_module=\"openconfig-network-instance\",\n                yang_type=\"container\",\n                is_config=False,\n            )\n        except (TypeError, ValueError):\n            raise ValueError(\n                {\n                    \"error-string\": \"\"\"state must be of a type compatible with container\"\"\",\n                    \"defined-type\": \"container\",\n                    \"generated-type\": \"\"\"YANGDynClass(base=state.state, is_container='container', yang_name=\"state\", parent=self, path_helper=self._path_helper, extmethods=self._extmethods, register_paths=True, extensions=None, namespace='http://openconfig.net/yang/network-instance', defining_module='openconfig-network-instance', yang_type='container', is_config=False)\"\"\",\n                }\n            )\n\n        self.__state = t\n        if hasattr(self, \"_set\"):\n            self._set()\n\n    def _unset_state(self):\n        self.__state = YANGDynClass(\n            base=state.state,\n            is_container=\"container\",\n            yang_name=\"state\",\n            parent=self,\n            path_helper=self._path_helper,\n            extmethods=self._extmethods,\n            register_paths=True,\n            extensions=None,\n            namespace=\"http://openconfig.net/yang/network-instance\",\n            defining_module=\"openconfig-network-instance\",\n            yang_type=\"container\",\n            is_config=False,\n        )\n\n    state = __builtin__.property(_get_state)\n\n    _pyangbind_elements = OrderedDict([(\"state\", state)])", "original_line_number": 25634, "instruction_length": 726}
{"instruction": "This code is a Python class that defines a YoupornCom object, which is a plugin for a program called pyload. To use this plugin, create an instance of the YoupornCom class and call its process() method, passing in a pyfile object. The process() method will download the file specified by the pyfile object from Youporn.com if it exists. If the file does not exist, the method will call the offline() method. The YoupornCom class also has several helper methods, including download_html(), get_file_url(), get_file_name(), and file_exists().", "input": "", "output": "# -*- coding: utf-8 -*-\n\nimport re\nfrom module.plugins.Hoster import Hoster\n\n\nclass YoupornCom(Hoster):\n    __name__ = \"YoupornCom\"\n    __type__ = \"hoster\"\n    __pattern__ = r'http://(?:www\\.)?youporn\\.com/watch/.+'\n    __version__ = \"0.2\"\n    __description__ = \"\"\"Youporn.com hoster plugin\"\"\"\n    __author_name__ = \"willnix\"\n    __author_mail__ = \"willnix@pyload.org\"\n\n    def process(self, pyfile):\n        self.pyfile = pyfile\n\n        if not self.file_exists():\n            self.offline()\n\n        pyfile.name = self.get_file_name()\n        self.download(self.get_file_url())\n\n    def download_html(self):\n        url = self.pyfile.url\n        self.html = self.load(url, post={\"user_choice\": \"Enter\"}, cookies=False)\n\n    def get_file_url(self):\n        \"\"\" returns the absolute downloadable filepath\n        \"\"\"\n        if not self.html:\n            self.download_html()\n\n        return re.search(r'(http://download\\.youporn\\.com/download/\\d+\\?save=1)\">', self.html).group(1)\n\n    def get_file_name(self):\n        if not self.html:\n            self.download_html()\n\n        file_name_pattern = r\"<title>(.*) - Free Porn Videos - YouPorn</title>\"\n        return re.search(file_name_pattern, self.html).group(1).replace(\"&amp;\", \"&\").replace(\"/\", \"\") + '.flv'\n\n    def file_exists(self):\n        \"\"\" returns True or False\n        \"\"\"\n        if not self.html:\n            self.download_html()\n        if re.search(r\"(.*invalid video_id.*)\", self.html) is not None:\n            return False\n        else:\n            return True", "original_line_number": 25938, "instruction_length": 540}
{"instruction": "This code is a Python script that loads ordered data from a folder to a PostgreSQL database using PDAL. To use this script, create an instance of the LoaderOrdered class and call its process() method. The LoaderOrdered class extends the Loader class and overrides its getFileBlockTable(), loadFromFile(), and loadFromFileSequential() methods. The loadFromFile() method loads a file into the database using PDAL, while the loadFromFileSequential() method inserts the blocks of a file into the global blocks table in the correct order. The process() method loads the files in parallel using the loadFromFile() and loadFromFileSequential() methods.", "input": "", "output": "#!/usr/bin/env python\n################################################################################\n#    Created by Oscar Martinez                                                 #\n#    o.rubi@esciencecenter.nl                                                  #\n################################################################################\nimport os, logging\nfrom pointcloud import pdalops, postgresops\nfrom pointcloud.postgres.blocks.Loader import Loader\n\nclass LoaderOrdered(Loader):\n    def getFileBlockTable(self, index):\n        return self.blockTable + '_' + str(index)\n\n    def process(self):\n        logging.info('Starting ordered data loading with PDAL (parallel by python) from ' + self.inputFolder + ' to ' + self.dbName)\n        return self.processMulti(self.inputFiles, self.numProcessesLoad, self.loadFromFile, self.loadFromFileSequential, True)\n\n    def loadFromFile(self, index, fileAbsPath):\n        # Get connection\n        connection = self.getConnection()\n        cursor = connection.cursor()\n        #Create a temporal blocks table for the blocks of the current file\n        fileBlockTable = self.getFileBlockTable(index)\n        self.createBlocksTable(cursor, fileBlockTable, self.indexTableSpace) # We use the index table space for the temporal table\n        \n        # Add point cloud format to poinctcloud_formats table\n        (columns, pcid, compression) = self.addPCFormat(cursor, self.schemaFile, fileAbsPath, self.srid)\n        connection.close()\n\n        pdalCols = []\n        for c in cols:\n            pdalCols.append(self.DM_PDAL[c])\n\n        # Get PDAL config and run PDAL\n        xmlFile = os.path.basename(fileAbsPath) + '.xml'\n        pdalops.PostgreSQLWriter(xmlFile, fileAbsPath, self.getConnectionString(), pcid, pdalCols, fileBlockTable, self.srid, self.blockSize, compression)\n        pdalops.executePDAL(xmlFile)\n        \n    def loadFromFileSequential(self, fileAbsPath, index, numFiles):\n        fileBlockTable = self.getFileBlockTable(index)\n        connection = self.getConnection()\n        cursor = connection.cursor()\n        # Insert the blocks on the global blocks table (with correct order)\n        query = \"INSERT INTO \" + self.blockTable + \" (pa) SELECT pa FROM \" + fileBlockTable + \" ORDER BY id\"\n        postgresops.mogrifyExecute(cursor, query)\n        # Drop the temporal table\n        postgresops.dropTable(cursor, fileBlockTable)\n        connection.close()", "original_line_number": 25976, "instruction_length": 645}
{"instruction": "The code provides Python wrappers around TensorFlow ops for decoding LibSVM input to tensors. To use it, call the `decode_libsvm` function with the following arguments: `input` (a `Tensor` of type `string`), `num_features` (an `int` that is `>= 1`), `dtype` (an optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.float32`), `label_dtype` (an optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`), and `name` (a name for the operation, optional). The function returns a tuple of `Tensor` objects `(label, feature_indices, feature_values, feature_shape)`, where `label` is a `Tensor` of type `label_dtype`, `feature_indices` is a `Tensor` of type `int64`, `feature_values` is a `Tensor` of type `dtype`, and `feature_shape` is a `Tensor` of type `int64`.", "input": "", "output": "\"\"\"Python wrappers around TensorFlow ops.\n\nThis file is MACHINE GENERATED! Do not edit.\nOriginal C++ source file: libsvm_ops.cc\n\"\"\"\n\nimport collections as _collections\nimport six as _six\n\nfrom tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow\nfrom tensorflow.python.eager import context as _context\nfrom tensorflow.python.eager import core as _core\nfrom tensorflow.python.eager import execute as _execute\nfrom tensorflow.python.framework import dtypes as _dtypes\nfrom tensorflow.python.framework import errors as _errors\nfrom tensorflow.python.framework import tensor_shape as _tensor_shape\n\nfrom tensorflow.core.framework import op_def_pb2 as _op_def_pb2\n# Needed to trigger the call to _set_call_cpp_shape_fn.\nfrom tensorflow.python.framework import common_shapes as _common_shapes\nfrom tensorflow.python.framework import op_def_registry as _op_def_registry\nfrom tensorflow.python.framework import ops as _ops\nfrom tensorflow.python.framework import op_def_library as _op_def_library\nfrom tensorflow.python.util.tf_export import tf_export\n\n\n_decode_libsvm_outputs = [\"label\", \"feature_indices\", \"feature_values\",\n                         \"feature_shape\"]\n_DecodeLibsvmOutput = _collections.namedtuple(\n    \"DecodeLibsvm\", _decode_libsvm_outputs)\n\n\n@tf_export('decode_libsvm')\ndef decode_libsvm(input, num_features, dtype=_dtypes.float32, label_dtype=_dtypes.int64, name=None):\n  r\"\"\"Convert LibSVM input to tensors. The output consists of\n\n  a label and a feature tensor. The shape of the label tensor\n  is the same as input and the shape of the feature tensor is\n  `[input_shape, num_features]`.\n\n  Args:\n    input: A `Tensor` of type `string`. Each string is a record in the LibSVM.\n    num_features: An `int` that is `>= 1`. The number of features.\n    dtype: An optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.float32`.\n    label_dtype: An optional `tf.DType` from: `tf.float32, tf.float64, tf.int32, tf.int64`. Defaults to `tf.int64`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A tuple of `Tensor` objects (label, feature_indices, feature_values, feature_shape).\n\n    label: A `Tensor` of type `label_dtype`. A tensor of the same shape as input.\n    feature_indices: A `Tensor` of type `int64`. A 2-D int64 tensor of dense_shape [N, ndims].\n    feature_values: A `Tensor` of type `dtype`. A 1-D tensor of any type and dense_shape [N].\n    feature_shape: A `Tensor` of type `int64`. A 1-D int64 tensor of dense_shape [ndims].\n  \"\"\"\n  _ctx = _context._context\n  if _ctx is None or not _ctx._eager_context.is_eager:\n    num_features = _execute.make_int(num_features, \"num_features\")\n    if dtype is None:\n      dtype = _dtypes.float32\n    dtype = _execute.make_type(dtype, \"dtype\")\n    if label_dtype is None:\n      label_dtype = _dtypes.int64\n    label_dtype = _execute.make_type(label_dtype, \"label_dtype\")\n    _, _, _op = _op_def_lib._apply_op_helper(\n        \"DecodeLibsvm\", input=input, num_features=num_features, dtype=dtype,\n        label_dtype=label_dtype, name=name)\n    _result = _op.outputs[:]\n    _inputs_flat = _op.inputs\n    _attrs = (\"dtype\", _op.get_attr(\"dtype\"), \"label_dtype\",\n              _op.get_attr(\"label_dtype\"), \"num_features\",\n              _op.get_attr(\"num_features\"))\n    _execute.record_gradient(\n      \"DecodeLibsvm\", _inputs_flat, _attrs, _result, name)\n    _result = _DecodeLibsvmOutput._make(_result)\n    return _result\n\n  else:\n    try:\n      _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n        _ctx._context_handle, _ctx._eager_context.device_name, \"DecodeLibsvm\",\n        name, _ctx._post_execution_callbacks, input, \"dtype\", dtype,\n        \"label_dtype\", label_dtype, \"num_features\", num_features)\n      _result = _DecodeLibsvmOutput._make(_result)\n      return _result\n    except _core._FallbackException:\n      return decode_libsvm_eager_fallback(\n          input, dtype=dtype, label_dtype=label_dtype,\n          num_features=num_features, name=name, ctx=_ctx)\n    except _core._NotOkStatusException as e:\n      if name is not None:\n        message = e.message + \" name: \" + name\n      else:\n        message = e.message\n      _six.raise_from(_core._status_to_exception(e.code, message), None)\n\n\ndef decode_libsvm_eager_fallback(input, num_features, dtype=_dtypes.float32, label_dtype=_dtypes.int64, name=None, ctx=None):\n  r\"\"\"This is the slowpath function for Eager mode.\n  This is for function decode_libsvm\n  \"\"\"\n  _ctx = ctx if ctx else _context.context()\n  num_features = _execute.make_int(num_features, \"num_features\")\n  if dtype is None:\n    dtype = _dtypes.float32\n  dtype = _execute.make_type(dtype, \"dtype\")\n  if label_dtype is None:\n    label_dtype = _dtypes.int64\n  label_dtype = _execute.make_type(label_dtype, \"label_dtype\")\n  input = _ops.convert_to_tensor(input, _dtypes.string)\n  _inputs_flat = [input]\n  _attrs = (\"dtype\", dtype, \"label_dtype\", label_dtype, \"num_features\",\n  num_features)\n  _result = _execute.execute(b\"DecodeLibsvm\", 4, inputs=_inputs_flat,\n                             attrs=_attrs, ctx=_ctx, name=name)\n  _execute.record_gradient(\n      \"DecodeLibsvm\", _inputs_flat, _attrs, _result, name)\n  _result = _DecodeLibsvmOutput._make(_result)\n  return _result\n\n_ops.RegisterShape(\"DecodeLibsvm\")(None)\n\ndef _InitOpDefLibrary(op_list_proto_bytes):\n  op_list = _op_def_pb2.OpList()\n  op_list.ParseFromString(op_list_proto_bytes)\n  _op_def_registry.register_op_list(op_list)\n  op_def_lib = _op_def_library.OpDefLibrary()\n  op_def_lib.add_op_list(op_list)\n  return op_def_lib\n# op {\n#   name: \"DecodeLibsvm\"\n#   input_arg {\n#     name: \"input\"\n#     type: DT_STRING\n#   }\n#   output_arg {\n#     name: \"label\"\n#     type_attr: \"label_dtype\"\n#   }\n#   output_arg {\n#     name: \"feature_indices\"\n#     type: DT_INT64\n#   }\n#   output_arg {\n#     name: \"feature_values\"\n#     type_attr: \"dtype\"\n#   }\n#   output_arg {\n#     name: \"feature_shape\"\n#     type: DT_INT64\n#   }\n#   attr {\n#     name: \"dtype\"\n#     type: \"type\"\n#     default_value {\n#       type: DT_FLOAT\n#     }\n#     allowed_values {\n#       list {\n#         type: DT_FLOAT\n#         type: DT_DOUBLE\n#         type: DT_INT32\n#         type: DT_INT64\n#       }\n#     }\n#   }\n#   attr {\n#     name: \"label_dtype\"\n#     type: \"type\"\n#     default_value {\n#       type: DT_INT64\n#     }\n#     allowed_values {\n#       list {\n#         type: DT_FLOAT\n#         type: DT_DOUBLE\n#         type: DT_INT32\n#         type: DT_INT64\n#       }\n#     }\n#   }\n#   attr {\n#     name: \"num_features\"\n#     type: \"int\"\n#     has_minimum: true\n#     minimum: 1\n#   }\n# }\n_op_def_lib = _InitOpDefLibrary(b\"\\n\\311\\001\\n\\014DecodeLibsvm\\022\\t\\n\\005input\\030\\007\\032\\024\\n\\005label\\\"\\013label_dtype\\032\\023\\n\\017feature_indices\\030\\t\\032\\027\\n\\016feature_values\\\"\\005dtype\\032\\021\\n\\rfeature_shape\\030\\t\\\"\\033\\n\\005dtype\\022\\004type\\032\\0020\\001:\\010\\n\\0062\\004\\001\\002\\003\\t\\\"!\\n\\013label_dtype\\022\\004type\\032\\0020\\t:\\010\\n\\0062\\004\\001\\002\\003\\t\\\"\\027\\n\\014num_features\\022\\003int(\\0010\\001\")", "original_line_number": 26027, "instruction_length": 831}
{"instruction": "Create a MuscleTendonComplex class that takes in muscle parameters, state, attachment parameters, offset correction, timestep, muscle name, and joint angle. The class contains a stepUpdateState function that updates the muscle states based on the muscle dynamics. The muscle activation is updated using the trapezoidal method, and the muscle length is updated using the same method. The class also contains a reset_state function that resets the muscle state variables. It is recommended to set the timestamp lower than 5e-4 to get decent results. The model is created based on Song's and Geyer's 2015 paper.", "input": "", "output": "import numpy as np\n\n\"\"\"\nIt is created for using MTC in Mujoco. The dynamics in this model is not continuous. The integration error will be\naccumulated overtime. And the system might get unstable if the timestep is too large. It is recommended to set the\ntimestamp lower than 5e-4 to get decent results.\n\nThe model is created based on Song's and Geyer's 2015 paper:\nSong, S. and Geyer, H., 2015. A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human\nlocomotion. The Journal of physiology, 593(16), pp.3493-3511.\n\nV0.1\nPassed basic tests. There're slightly difference compared to the simmechanics model.\n\nV0.2\n1. Verified with the simmechanics model. Difference in most of the cases can be ignored.\n2. Changed the integration method from forward Euler to trapezoid.\n3. Muscle force vce etc might vibrate/jitter if in some cases if the timestep is not low enough.\n   Need to improve this in the next version.\n   \n\"\"\"\n\nclass MuscleTendonComplex:\n    def __init__(self, paraMuscle, stateMuscle, paraMusAttach, offsetCorr, timestep, nameMuscle, angJoi):\n        self.frcmax, self.vmax, self.eref, self.lslack, self.lopt, self.tau, self.w, self.c, self.N, self.K = paraMuscle\n        self.stim, self.act, self.lmtc, self.lce, self.vce, self.frcmtc = stateMuscle\n        self.timestep = timestep\n        self.nameMuscle = nameMuscle\n        self.angJoi = angJoi\n        self.offsetCorr = offsetCorr\n        self.r, self.phiref, self.phimaxref, self.rho, self.dirAng, self.phiScale = paraMusAttach\n        self.MR =  0.01\n        self.typeMuscle = self.angJoi.size\n        nJoi = self.typeMuscle\n        self.levelArm = np.zeros(nJoi)\n\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError('Invalid muscle level arm offset correction type. ')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        self.lce = self.lmtc - self.lslack\n        self.lse = self.lmtc - self.lce\n        # unitless parameters\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        # test\n        self.lce_avg = self.lce\n        self.vce_avg = self.vce\n        self.frcmtc_avg = 0\n        self.act_avg = self.act\n        self.frame = 0\n        # self.Fse = 0.0\n        # self.Fbe = 0.0\n        # self.Fpe = 0.0\n        # self.Fce = 0.0\n\n\n    def stepUpdateState(self, angJoi):\n        \"\"\"\n        Muscle Tendon Complex Dynamics\n        update muscle states based on the muscle dynamics\n        Muscle state stim has to be updated outside before this function is called\n        \"\"\"\n        # update lmtc and level arm based on the geometry\n        self.angJoi = angJoi\n        nJoi = self.typeMuscle\n        tmpL = np.zeros(nJoi)\n        for i in range(0, nJoi):\n            if self.offsetCorr[i] == 0:\n                tmpL[i] = self.dirAng[i] * (self.angJoi[i] - self.phiref[i]) * self.r[i] * self.rho[i]\n                self.levelArm[i] = self.r[i]\n            elif self.offsetCorr[i] == 1:\n                tmp1 = np.sin((self.phiref[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmp2 = np.sin((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i])\n                tmpL[i] = self.dirAng[i] * (tmp2 - tmp1) * self.r[i] * self.rho[i] / self.phiScale[i]\n                self.levelArm[i] = np.cos((self.angJoi[i] - self.phimaxref[i]) * self.phiScale[i]) * self.r[i]\n            else:\n                raise ValueError('Invalid muscle level arm offset correction type. ')\n        self.lmtc = self.lslack + self.lopt + np.sum(tmpL)\n\n        # update muscle activation\n        # integration, forward-Euler method\n        # self.act = (self.stim - self.act) * self.timestep / self.tau + self.act\n        # integration, trapezoidal method, 2-step\n        self.act = (self.stim - self.actsubstep) * self.timestep / 2.0 / self.tau + self.actsubstep\n        self.actsubstep = (self.stim - self.act) * self.timestep / 2.0 / self.tau + self.act\n\n        # update lce and lse based on the lmtc\n        # integration, forward-Euler method\n        # self.lce = self.vce * self.timestep + self.lce\n        # integration, trapezoidal method, 2-step\n        self.lce = self.vce * self.timestep / 2.0 + self.lcesubstep\n        self.lcesubstep = self.vce * self.timestep / 2.0 + self.lce\n\n        self.lse = self.lmtc - self.lce\n        self.Lse = self.lse / self.lslack\n        self.Lce = self.lce / self.lopt\n\n        # Serial Elastic element (tendon) force-length relationship\n        if self.Lse > 1.0:\n            Fse = np.power((self.Lse - 1.0) / self.eref, 2)\n        else:\n            Fse = 0.0\n\n        # Parallel Elasticity PE\n        if self.Lce > 1.0:\n            Fpe = np.power((self.Lce - 1.0) / self.w, 2)\n        else:\n            Fpe = 0.0\n\n        # update frcmtc\n        self.frcmtc = Fse * self.frcmax\n        #self.frcmtc =  np.clip(self.frcmtc, 0, self.frcmax)\n\n        # Buffer Elasticity BE\n        if (self.Lce - (1.0 - self.w)) < 0:\n            Fbe = np.power((self.Lce - (1.0 - self.w)) / (self.w / 2), 2)\n        else:\n            Fbe = 0.0\n\n        # Contractile Element force-length relationship\n        tmp = np.power(np.absolute(self.Lce - 1.0) / self.w, 3)\n        Fce = np.exp(tmp * np.log(self.c))\n\n        #Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n        if (Fpe + Fce * self.act) < 1e-10:  # avoid numerical error\n            if (Fse + Fbe) < 1e-10:\n                Fv = 1.0\n            else:\n                Fv = (Fse + Fbe) / 1e-10\n        else:\n            Fv = (Fse + Fbe) / (Fpe + Fce * self.act)\n\n        # Contractile Element inverse force-velocity relationship\n        if Fv <= 1.0:\n            # Concentric\n            v = (Fv - 1) / (Fv * self.K + 1.0)\n        elif Fv <= self.N:\n            # excentric\n            tmp = (Fv - self.N) / (self.N - 1.0)\n            v = (tmp + 1.0) / (1.0 - tmp * 7.56 * self.K)\n        else:\n            # excentric overshoot\n            v = ((Fv - self.N) * 0.01 + 1)\n\n        self.vce = v * self.lopt * self.vmax\n        v_frac = self.vce /  self.vmax\n        mr_scale =  self.act * np.absolute(self.frcmax*self.vmax) *self.timestep\n        if self.vce <= 1:\n            self.MR =  0.01 - 0.11*(v_frac) + 0.06*np.exp(-8*v_frac)\n        else:\n            self.MR =  0.23 - 0.16*np.exp(-8*v_frac) \n        self.MR *= mr_scale\n        self.frame += 1\n        self.lce_avg = (self.lce_avg*(self.frame - 1) +  self.lce) / self.frame\n        self.vce_avg = (self.vce_avg*(self.frame - 1) +  self.vce) / self.frame\n        self.frcmtc_avg = (self.frcmtc_avg*(self.frame - 1) +  self.frcmtc) / self.frame\n        self.act_avg = (self.act_avg*(self.frame - 1) +  self.act) / self.frame\n        #self.MR = np.exp(-self.MR)\n        # print(self.MR, np.exp(-self.MR))\n        # self.Fv = Fv\n        # self.Fse = Fse\n        # self.Fbe = Fbe\n        # self.Fpe = Fpe\n        # self.Fce = Fce\n\n    def reset_state(self):\n        self.frame = 0\n        self.lce_avg = 0\n        self.frcmtc_avg = 0\n        self.act_avg = 0\n        self.vce_avg = 0", "original_line_number": 26207, "instruction_length": 608}
{"instruction": "To display a text label example using Pygame, follow these steps:\n\n1. Import the necessary modules: sys, pygame, core, and widgets.\n2. Set the window width and height.\n3. Initialize Pygame and the font.\n4. Set the screen size and clock.\n5. Set the frames per second and running variable to True.\n6. Create a panel using the widgets.Panel class with a grid size of (3, 10) and the window size.\n7. Set the panel color to (155, 155, 155, 255).\n8. Create a text label using the widgets.TextLabel class with the panel as the parent, position (1, 2), and the text to be displayed.\n9. Set the text color to white, the span to (0, 5), the border to black with a width of 8, and the margin to 10.\n10. Define a redraw function that updates the screen and draws the panel and text.\n11. Enter a while loop that runs as long as the running variable is True.\n12. Set the clock to tick at the specified frames per second.\n13. Call the redraw function.\n14. Check for any Pygame events, such as quitting the program, and exit the program if necessary.", "input": "", "output": "import sys\nsys.path.append(\"..\")\nimport pygame\nimport core\nimport widgets\n\n# Text label example.\n\nWINDOW_WIDTH = 1024\nWINDOW_HEIGHT = 728\n\npygame.init()\npygame.font.init\nscreen = pygame.display.set_mode((WINDOW_WIDTH, WINDOW_HEIGHT))\nclock = pygame.time.Clock()\nFPS = 60\nrunning = True\n\nif __name__ == \"__main__\":\t\n    panel = widgets.Panel(core.Grid((3, 10), (WINDOW_WIDTH, WINDOW_HEIGHT)), None, None, (0, 0))\n    panel.set_color((155, 155, 155, 255))\n    text = widgets.TextLabel(panel, (1, 2), core.Text(\n        \"\"\"\n        Lorem ipsum dolor sit amet, \n        consectetur adipiscing elit,\n        sed do eiusmod tempor incididunt\n        ut labore et dolore magna aliqua.\n        Ut enim ad minim veniam, quis\n        nostrud exercitation ullamco laboris\n        nisi ut aliquip ex ea commodo consequat.\n            Duis aute irure dolor in\n            reprehenderit in voluptate velit\n            esse cillum dolore eu fugiat\n            nulla pariatur. Excepteur sint\n            occaecat cupidatat non proident,\n            sunt in culpa qui officia deserunt\n            mollit anim id est laborum.\"\"\", 13, core.BLACK)\n    )\n    text.set_color(core.WHITE) # This is the color of the widget, not to be confused with the color of its text.\n    text.set_span((0, 5))\n    text.set_border(core.BLACK, 8)\n    text.set_margin(10) # Altering the margin because of the border.\n\n    def redraw():\n        pygame.display.flip()\n        screen.fill((0, 0, 0))\n        panel.draw(screen)\n        text.draw(screen)\n\nwhile (running):\n    clock.tick(FPS)\n    redraw()\n    for e in pygame.event.get():\n        if e.type == pygame.QUIT:\n            sys.exit()", "original_line_number": 26823, "instruction_length": 1034}
{"instruction": "This code tests and demonstrates the use of PeriTriaxController. To run the simulation, create a sphere pack with a given radius and number of spheres, and append the spheres to the simulation. Then, set up the engines for the simulation, including a ForceResetter, InsertionSortCollider, InteractionLoop, PeriTriaxController, and NewtonIntegrator. Finally, set the time step and run the simulation. After the simulation is complete, a view of the simulation can be displayed. The triaxDone function is used to print out information about the simulation and change the goal of the PeriTriaxController during the simulation.", "input": "", "output": "# coding: utf-8\n# 2009 © Václav Šmilauer <eudoxos@arcig.cz>\n\"Test and demonstrate use of PeriTriaxController.\"\nfrom yade import *\nfrom yade import pack,qt\nO.periodic=True\n\nO.cell.hSize=Matrix3(0.1, 0, 0,\n\t\t     0 ,0.1, 0,\n\t\t    0, 0, 0.1)\n\t\t    \nsp=pack.SpherePack()\nradius=5e-3\nnum=sp.makeCloud(Vector3().Zero,O.cell.refSize,radius,.2,500,periodic=True) # min,max,radius,rRelFuzz,spheresInCell,periodic\nO.bodies.append([sphere(s[0],s[1]) for s in sp])\n\n\nO.engines=[\n\tForceResetter(),\n\tInsertionSortCollider([Bo1_Sphere_Aabb()],verletDist=.05*radius),\n\tInteractionLoop(\n\t\t[Ig2_Sphere_Sphere_ScGeom()],\n\t\t[Ip2_FrictMat_FrictMat_FrictPhys()],\n\t\t[Law2_ScGeom_FrictPhys_CundallStrack()]\n\t),\n\t#PeriTriaxController(maxUnbalanced=0.01,relStressTol=0.02,goal=[-1e4,-1e4,0],stressMask=3,globUpdate=5,maxStrainRate=[1.,1.,1.],doneHook='triaxDone()',label='triax'),\n\t#using cell inertia\n\tPeriTriaxController(dynCell=True,mass=0.2,maxUnbalanced=0.01,relStressTol=0.02,goal=(-1e4,-1e4,0),stressMask=3,globUpdate=5,maxStrainRate=(1.,1.,1.),doneHook='triaxDone()',label='triax'),\n\tNewtonIntegrator(damping=.2),\n]\nO.dt=PWaveTimeStep()\nO.run();\nqt.View()\n\nphase=0\ndef triaxDone():\n\tglobal phase\n\tif phase==0:\n\t\tprint 'Here we are: stress',triax.stress,'strain',triax.strain,'stiffness',triax.stiff\n\t\tprint 'Now εz will go from 0 to .2 while σx and σy will be kept the same.'\n\t\ttriax.goal=(-1e4,-1e4,-0.2)\n\t\tphase+=1\n\telif phase==1:\n\t\tprint 'Here we are: stress',triax.stress,'strain',triax.strain,'stiffness',triax.stiff\n\t\tprint 'Done, pausing now.'\n\t\tO.pause()", "original_line_number": 27831, "instruction_length": 623}
{"instruction": "To move the robot arm with impedance control, use the moveImpedance(k, t) function with the desired stiffness (k) and duration (t). To move the wrist, use the moveWrist(wrist_frame, tool_frame, t, max_wrench) function with the desired wrist and tool frame, duration (t), and maximum wrench constraint. To move the tool, use the moveTool(tool_frame, t) function with the desired tool frame and duration (t). The code assumes a specific robot arm prefix, so make sure to specify the correct prefix in the command line argument.", "input": "", "output": "#!/usr/bin/env python\n\n# Copyright (c) 2014, Robot Control and Pattern Recognition Group, Warsaw University of Technology\n# All rights reserved.\n# \n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#     * Redistributions of source code must retain the above copyright\n#       notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above copyright\n#       notice, this list of conditions and the following disclaimer in the\n#       documentation and/or other materials provided with the distribution.\n#     * Neither the name of the Warsaw University of Technology nor the\n#       names of its contributors may be used to endorse or promote products\n#       derived from this software without specific prior written permission.\n# \n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL <COPYright HOLDER> BE LIABLE FOR ANY\n# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport roslib\nroslib.load_manifest('velma_controller')\n\nimport rospy\n\nfrom geometry_msgs.msg import *\nfrom cartesian_trajectory_msgs.msg import *\nimport actionlib\nfrom actionlib_msgs.msg import *\nimport tf\nimport tf_conversions.posemath as pm\nimport PyKDL\n\ndef moveImpedance(k, t):\n    global action_impedance_client\n    action_impedance_goal = CartesianImpedanceGoal()\n    action_impedance_goal.trajectory.header.stamp = rospy.Time.now() + rospy.Duration(0.2)\n    action_impedance_goal.trajectory.points.append(CartesianImpedanceTrajectoryPoint(\n    rospy.Duration(t),\n    CartesianImpedance(k,Wrench(Vector3(0.7, 0.7, 0.7),Vector3(0.7, 0.7, 0.7)))))\n    action_impedance_client.send_goal(action_impedance_goal)\n\ndef moveWrist( wrist_frame, tool_frame, t, max_wrench):\n    global action_trajectory_client\n    # we are moving the tool, so: T_B_Wd*T_W_T\n    wrist_pose = pm.toMsg(wrist_frame*tool_frame)\n\n    action_trajectory_goal = CartesianTrajectoryGoal()\n    action_trajectory_goal.trajectory.header.stamp = rospy.Time.now() + rospy.Duration(0.01)\n    action_trajectory_goal.trajectory.points.append(CartesianTrajectoryPoint(\n    rospy.Duration(t),\n    wrist_pose,\n    Twist()))\n    action_trajectory_goal.wrench_constraint = max_wrench\n    action_trajectory_client.send_goal(action_trajectory_goal)\n\ndef moveTool(tool_frame, t):\n    global action_tool_client\n    tool_pose = pm.toMsg(tool_frame)\n    action_tool_goal = CartesianTrajectoryGoal()\n    action_tool_goal.trajectory.header.stamp = rospy.Time.now()\n    action_tool_goal.trajectory.points.append(CartesianTrajectoryPoint(\n    rospy.Duration(t),\n    tool_pose,\n    Twist()))\n    action_tool_client.send_goal(action_tool_goal)\n\n\nif __name__ == '__main__':\n\n    a = []\n    for arg in sys.argv:\n        a.append(arg)\n\n    if (len(a) > 1) and ((a[1]==\"left\") or (\"right\")):\n        prefix = a[1]\n    else:\n        print \"Usage: %s prefix\"%a[0]\n        exit(0)\n\n    rospy.init_node('impedance_riser')\n\n    listener = tf.TransformListener();\n\n    action_impedance_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/cartesian_impedance\", CartesianImpedanceAction)\n    action_impedance_client.wait_for_server()\n\n    action_trajectory_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/cartesian_trajectory\", CartesianTrajectoryAction)\n    action_trajectory_client.wait_for_server()\n\n    action_tool_client = actionlib.SimpleActionClient(\"/\" + prefix + \"_arm/tool_trajectory\", CartesianTrajectoryAction)\n    action_tool_client.wait_for_server()\n\n    rospy.sleep(1.0)\n\n    # save current wrist position\n    time_now = rospy.Time.now() - rospy.Duration(1.0)\n    listener.waitForTransform('torso_base', prefix+'_arm_7_link', time_now, rospy.Duration(4.0))\n    pose = listener.lookupTransform('torso_base', prefix+'_arm_7_link', time_now)\n    T_B_W = pm.fromTf(pose)\n\n    T_W_T = PyKDL.Frame()    # tool transformation\n    print \"setting the tool to %s relative to wrist frame\"%(T_W_T)\n    # move both tool position and wrist position - the gripper holds its position\n    print \"moving wrist\"\n    # we assume that during the initialization there are no contact forces, so we limit the wrench\n    moveWrist( T_B_W, T_W_T, 2.0, Wrench(Vector3(20, 20, 20), Vector3(4, 4, 4)) )\n    print \"moving tool\"\n    moveTool( T_W_T, 2.0 )\n    rospy.sleep(2.0)\n\n    # change the stiffness\n    print \"changing stiffness for door approach\"\n    moveImpedance(Wrench(Vector3(1200.0, 1200.0, 1200.0), Vector3(300.0, 300.0, 300.0)), 4.0)\n    rospy.sleep(4.0)", "original_line_number": 27839, "instruction_length": 525}
{"instruction": "To check whether a given number is an ugly number, define a function isUlgyNumber(num) that takes a positive integer num as input and returns True if num is an ugly number, False otherwise. An ugly number is a positive number whose prime factors only include 2, 3, 5. The function recursively checks if num is divisible by 2, 3, or 5, and if so, divides num by 2, 3, or 5 and calls itself with the quotient as input. If num is not divisible by 2, 3, or 5, the function returns False. \n\nTo find the n-th ugly number, define a function ulgyNumber(N) that takes a positive integer N as input and returns a list of the first N ugly numbers. An ugly number is a positive number whose prime factors only include 2, 3, 5. The function initializes a list res of length N with all elements set to 1, and a list count of length 3 with all elements set to 0. It also initializes a list primes with the values 2, 3, and 5. The function then iterates from 1 to N-1, computing the next ugly number as the minimum of prime times res[count[j]] for each prime in primes, where j is the index of prime in primes. The function then updates count[j] to count[j]+1 for each prime that satisfies nextNum==prime*res[count[j]], and sets res[i] to nextNum. Finally, the function returns res.\n\nTo find the nth super ugly number, define a function nthSuperUglyNumber(n, primes) that takes a positive integer n and a list of k primes as input and returns the nth super ugly number. A super ugly number is a positive number whose all prime factors are in the given prime list primes of size k. The function initializes a list res of length n with all elements set to 1, and a list count of length k with all elements set to 0. The function then iterates from 1 to n-1, computing the next super ugly number as the minimum of prime times res[count[index]] for each prime in primes, where index is the index of prime in primes. The function then updates count[index] to count[index]+1 for each prime that satisfies nextNum==prime*res[count[index]], and sets res[__index] to nextNum. Finally, the function returns res[-1].", "input": "", "output": "'''\nCreated on 2015年12月11日\nhttps://leetcode.com/problems/ugly-number/\nhttps://leetcode.com/problems/ugly-number-ii/\nhttps://leetcode.com/problems/super-ugly-number/\n@author: Darren\n'''\n'''\nWrite a program to check whether a given number is an ugly number.\n\nUgly numbers are positive numbers whose prime factors only include 2, 3, 5. For example, 6, 8 are ugly while 14 is not ugly since it includes another prime factor 7.\n\nNote that 1 is typically treated as an ugly number.\n'''\ndef isUlgyNumber(num):\n    if not num:\n        return False\n    if num==1:\n        return True\n    if num%2==0:\n        return isUlgyNumber(num//2)\n    if num%3==0:\n        return isUlgyNumber(num//3)\n    if num%5==0:\n        return isUlgyNumber(num//5)\n    return False\n\nprint(isUlgyNumber(14))\n\n'''\nWrite a program to find the n-th ugly number.\n\nUgly numbers are positive numbers whose prime factors only include 2, 3, 5. For example, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12 is the sequence of the first 10 ugly numbers.\n\nNote that 1 is typically treated as an ugly number.\n'''\n\ndef ulgyNumber(N):\n    if N<1:\n        raise Exception(\"Invalid Input\")\n    if N==1:\n        return 1\n    res=[1]*N\n    count=[0]*3\n    primes=[2,3,5]\n    for i in range(1,N):\n        nextNum=min([prime*res[count[j]] for j,prime in enumerate(primes)])\n        for j,prime in enumerate(primes):\n            if nextNum==prime*res[count[j]]:\n                count[j]+=1\n        res[i]=nextNum\n    return res\n\nprint(ulgyNumber(10))\n\n'''\nWrite a program to find the nth super ugly number.\n\nSuper ugly numbers are positive numbers whose all prime factors are in the given prime list primes of size k. For example, [1, 2, 4, 7, 8, 13, 14, 16, 19, 26, 28, 32] is the sequence of the first 12 super ugly numbers given primes = [2, 7, 13, 19] of size 4.\n\nNote:\n(1) 1 is a super ugly number for any given primes.\n(2) The given numbers in primes are in ascending order.\n(3) 0 < k ≤ 100, 0 < n ≤ 106, 0 < primes[i] < 1000.\n'''\ndef nthSuperUglyNumber(n, primes):\n    \"\"\"\n    :type n: int\n    :type primes: List[int]\n    :rtype: int\n    \"\"\"\n    if n==1:\n        return 1\n    res=[1]*n\n    count=[0]*len(primes)\n    for __index in range(1,n):\n        nextNum=min([prime*res[count[index]] for index,prime in enumerate(primes)])\n        for index,prime in enumerate(primes):\n            if nextNum==prime*res[count[index]]:\n                count[index]+=1\n        res[__index]=nextNum\n    return res[-1] \n    \nn=200000\nprimes=[2,3,5,13,19,29,31,41,43,53,59,73,83,89,97,103,107,109,127,137,139,149,163,173,179,193,197,199,211,223,227,229,239,241,251,257,263,269,271,281,317,331,337,347,353,359,367,373,379,389,397,409,419,421,433,449,457,461,463,479,487,509,521,523,541,547,563,569,577,593,599,601,613,619,631,641,659,673,683,701,709,719,733,739,743,757,761,769,773,809,811,829,857,859,881,919,947,953,967,971]\nprint(nthSuperUglyNumber(n, primes))", "original_line_number": 28329, "instruction_length": 2089}
{"instruction": "To use the Vlcclient class for communication with VLC player, follow these steps:\n\n1. Import the necessary modules: threading, time, urllib, player.client.baseplayer, vlc.Meta, vlc.EventType, queue, and util.fileutil.\n\n2. Create an instance of the Vlcclient class.\n\n3. Call the start_client() method to start the threads for handling VLC events.\n\n4. Call the set_proxy() method to create a new VLC player.\n\n5. Call the play() method to start playing a specified track/station.\n\n6. Call the stop() method to stop playback.\n\n7. Call the seek() method to jump to a specified position in the track.\n\n8. Call the play_pause() method to play/pause playback.\n\n9. Call the set_volume() method to set the volume level.\n\n10. Call the mute() method to mute the audio.\n\n11. Call the current() method to return the current song.\n\n12. Call the shutdown() method to shutdown the player.\n\n13. Call the get_current_track_time() method to return the current track time.\n\n14. Call the get_current_playlist() method to return the current playlist.", "input": "", "output": "# Copyright 2016-2021 Peppy Player peppy.player@gmail.com\n# \n# This file is part of Peppy Player.\n# \n# Peppy Player is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n# \n# Peppy Player is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n# \n# You should have received a copy of the GNU General Public License\n# along with Peppy Player. If not, see <http://www.gnu.org/licenses/>.\n\nimport threading\nimport time\nimport urllib\n\nfrom player.client.baseplayer import BasePlayer\nfrom vlc import Meta\nfrom vlc import EventType\nfrom queue import Queue\nfrom util.fileutil import FILE_PLAYLIST, FILE_AUDIO\n\nclass Vlcclient(BasePlayer):\n    \"\"\" This class extends base player and provides communication with VLC player \n    using Python binding for 'libvlc' library \"\"\"\n        \n    def __init__(self):\n        \"\"\" Initializer. Starts separate threads for handling VLC events \"\"\"\n        \n        self.RADIO_MODE = \"radio\"\n        BasePlayer.__init__(self)\n        self.mode = self.RADIO_MODE\n        self.instance = None\n        self.player = None\n        self.media = None\n        self.current_track = \"\"\n        self.seek_time = \"0\"\n        self.cd_track_id = None\n        self.cd_drive_name = None\n        self.END_REACHED = \"end reached\"\n        self.TRACK_CHANGED = \"track changed\"\n        self.PAUSED = \"paused\"\n        self.player_queue = Queue()\n        self.threads_running = False\n        self.changing_volume = False\n\n    def start_client(self):\n        \"\"\" Start threads. \"\"\"\n        \n        self.threads_running = True\n        thread_1 = threading.Thread(target = self.radio_stream_event_listener)\n        thread_1.start()\n        thread_2 = threading.Thread(target = self.handle_event_queue)\n        thread_2.start()\n\n    def stop_client(self):\n        \"\"\" Stop threads \"\"\"\n\n        with self.lock:\n            self.threads_running = False\n\n    def set_proxy(self, proxy_process, proxy=None):\n        \"\"\" Create new VLC player \"\"\"\n        \n        self.instance = proxy_process\n        self.proxy = proxy\n        self.player = self.instance.media_player_new()\n        player_mgr = self.player.event_manager()\n        player_mgr.event_attach(EventType.MediaPlayerEndReached, self.player_callback, [self.END_REACHED])\n        player_mgr.event_attach(EventType.MediaPlayerPlaying, self.player_callback, [self.TRACK_CHANGED])\n    \n    def player_callback(self, event, data):\n        \"\"\" Player callback method \n        \n        :param event: event to handle\n        :param data: event data\n        \"\"\"\n        if data:\n            self.player_queue.put(data[0])\n    \n    def radio_stream_event_listener(self):\n        \"\"\" Starts the loop for listening VLC events for radio track change \"\"\"\n\n        while self.threads_running:\n            with self.lock:\n                if self.media and self.mode == self.RADIO_MODE:\n                    t = self.media.get_meta(Meta.NowPlaying)\n                    if t and t != self.current_track:\n                        self.current_track = t\n                        if self.enabled:\n                            self.notify_player_listeners({\"current_title\": t})\n            time.sleep(1)\n\n    def handle_event_queue(self):\n        \"\"\" Handling player event queue \"\"\"\n        \n        if not self.enabled:\n            return\n\n        while self.threads_running:\n            d = self.player_queue.get() # blocking line\n            if d  == self.END_REACHED:\n                self.notify_end_of_track_listeners()\n                self.player_queue.task_done()\n            elif d  == self.TRACK_CHANGED:\n                self.track_changed()\n                self.player_queue.task_done()\n                \n    def track_changed(self):\n        \"\"\" Handle track change event \"\"\"\n\n        if not self.enabled:\n            return\n        \n        if self.mode == self.RADIO_MODE: \n            return\n        \n        current = {\"source\": \"player\"}\n        current[\"state\"] = \"playing\"\n        t = self.media.get_meta(Meta.Title)\n        if t == \".\":\n            return\n        if self.cd_track_id and t.startswith(\"cdda:\"):\n            current[\"cd_track_id\"] = self.cd_track_id\n            if self.cd_tracks:\n                t = self.cd_tracks[int(self.cd_track_id) - 1].name\n            else:\n                t = self.cd_drive_name + self.cd_track_title + \" \" + self.cd_track_id\n                \n        m = self.media.get_mrl()\n        m = m[m.rfind(\"/\") + 1:]\n        m = urllib.parse.unquote(m)\n        current[\"file_name\"] = m\n        current[\"current_title\"] = t\n        current[\"Time\"] = str(self.player.get_length()/1000)\n        \n        if not self.seek_time:\n            self.seek_time = \"0\"\n        current[\"seek_time\"] = self.seek_time\n        self.notify_player_listeners(current)\n\n    def set_player_volume_control(self, flag):\n        \"\"\" Player Volume Control type setter\n\n        :param volume: True - player volume cotrol type, False - amixer or hardware volume control type\n        \"\"\"\n        BasePlayer.set_player_volume_control(self, flag)\n        if not self.player_volume_control:\n            self.set_volume(100)\n\n    def play(self, state):\n        \"\"\" Start playing specified track/station. First it cleans the playlist \n        then adds new track/station to the list and then starts playback\n        syntax for CD:\n        self.media = self.instance.media_new(\"cdda:///E:/\", (\":cdda-track=7\"))\n        \n        :param state: button state which contains the track/station info\n        \"\"\"\n        url = None\n        self.enabled = True\n\n        if state == None:\n            if self.state != None:\n                url = getattr(self.state, \"url\", None)\n            else:\n                url = None\n        else:\n            url = getattr(state, \"url\", None)\n            self.state = state    \n\n        if url == None: \n            return      \n\n        url = url.replace(\"\\\\\", \"/\").replace(\"\\\"\", \"\")\n        track_time = getattr(self.state, \"track_time\", None)\n        if track_time == None:\n            track_time = \"0\"\n        else:\n            track_time = str(track_time)\n            if \":\" in track_time:\n                track_time = track_time.replace(\":\", \".\")\n        self.seek_time = track_time\n            \n        s = getattr(self.state, \"playback_mode\", None)\n        \n        if s and s == FILE_PLAYLIST:\n            self.stop()            \n            self.mode = FILE_PLAYLIST\n            self.enabled = True\n        elif s and s == FILE_AUDIO:\n            self.mode = FILE_AUDIO\n        else:\n            self.mode = self.RADIO_MODE\n        \n        if url.startswith(\"http\") and self.mode != self.RADIO_MODE:\n            url = self.encode_url(url)\n        \n        with self.lock:\n            file_name = getattr(self.state, \"file_name\", None)\n            if file_name and file_name.startswith(\"cdda://\"):\n                parts = file_name.split()\n                self.cd_track_id = parts[1].split(\"=\")[1]                \n                self.cd_drive_name = parts[0][len(\"cdda:///\"):]\n                self.media = self.instance.media_new(parts[0], parts[1])\n            else:            \n                self.media = self.instance.media_new(url)\n            self.player.set_media(self.media)            \n            \n            self.player.play()\n            try:\n                self.player.set_time(int(float(self.seek_time)) * 1000)\n            except:\n                pass\n            \n            if self.player_volume_control and getattr(self.state, \"volume\", None) != None:\n                self.set_volume(int(self.state.volume))\n            \n    def stop(self, state=None):\n        \"\"\" Stop playback \"\"\"\n        \n        with self.lock:\n            self.enabled = False\n            self.player.stop()\n    \n    def seek(self, time):\n        \"\"\" Jump to the specified position in the track\n        \n        :param time: time position in track\n        \"\"\"\n        \n        if \":\" in time:\n            self.seek_time = self.get_seconds_from_string(time)\n        else:\n            self.seek_time = time\n        \n        with self.lock:            \n            msec = int(float(self.seek_time) * 1000)\n            t = threading.Thread(target=self.seek_method, args=[msec])\n            t.start()\n\n    def seek_method(self, msec):\n        \"\"\" Seek track thread method\n\n        :param msec: milliseconds for new position\n        \"\"\"\n        self.player.set_time(msec)\n    \n    def play_pause(self, pause_flag=None):\n        \"\"\" Play/Pause playback \n        \n        :param pause_flag: play/pause flag\n        \"\"\" \n        with self.lock:\n            self.seek_time = self.get_current_track_time()\n            self.player.pause()\n    \n    def set_volume(self, level):\n        \"\"\" Set volume.\n        \n        :param level: new volume level\n        \"\"\"\n        self.player.audio_set_volume(int(level))\n        \n        if getattr(self, \"state\", None) != None:\n            if self.state.volume == level:\n                return\n            self.state.volume = level\n\n        v = self.get_volume()\n        if v != int(level): # usually initial volume setting\n            if hasattr(self, \"volume_thread\"):\n                self.volume_thread.join()    \n            self.volume_thread = threading.Thread(target=self.set_volume_level, args=[level])\n            self.volume_thread.start()\n    \n    def set_volume_level(self, level):\n        \"\"\" Set volume level in separate thread\n        \n        :param level: volume level\n        \"\"\"\n        n = 0\n        max_n = 20\n        vol = -2\n        \n        while n < max_n and level != vol:\n            self.player.audio_set_volume(int(level))\n            time.sleep(0.1)\n            vol = self.get_volume()\n            n += 1\n    \n    def get_volume(self):\n        \"\"\"  Return current volume level \n        \n        :return: volume level or -1 if not available\n        \"\"\"\n        with self.lock:\n            return self.player.audio_get_volume()\n    \n    def mute(self):\n        \"\"\" Mute \"\"\"\n        \n        with self.lock:\n            self.player.audio_toggle_mute()\n        \n    def current(self):\n        \"\"\" Return the current song \"\"\"\n        pass\n\n    def shutdown(self):\n        \"\"\" Shutdown the player \"\"\"\n        with self.lock:\n            self.player.stop()\n        \n    def get_current_track_time(self):\n        \"\"\"  Return current track time\n        \n        :return: current track time\n        \"\"\"\n        t = self.player.get_time()/1000\n        return str(t)\n    \n    def get_current_playlist(self):\n        \"\"\"  Return current playlist\n        \n        :return: current playlist\n        \"\"\"\n        return self.playlist", "original_line_number": 28521, "instruction_length": 1027}
{"instruction": "Convert a BEL graph to a node-link JSON object using the function `to_nodelink(graph)`. Write the resulting JSON object to a file using the function `to_nodelink_file(graph, path)`, where `graph` is the BEL graph and `path` is the path to the file. Alternatively, write the JSON object to a gzip file using the function `to_nodelink_gz(graph, path)`. To read a BEL graph from a node-link JSON object, use the function `from_nodelink(graph_json_dict)`, where `graph_json_dict` is the JSON object. You can also read a BEL graph from a file containing node-link JSON using the function `from_nodelink_file(path)`, where `path` is the path to the file. To read a BEL graph from a gzip file containing node-link JSON, use the function `from_nodelink_gz(path)`, where `path` is the path to the gzip file.", "input": "", "output": "# -*- coding: utf-8 -*-\n\n\"\"\"Conversion functions for BEL graphs with node-link JSON.\"\"\"\n\nimport gzip\nimport json\nfrom io import BytesIO\nfrom itertools import chain, count\nfrom operator import methodcaller\nfrom typing import Any, Mapping, TextIO, Union\n\nfrom networkx.utils import open_file\n\nfrom .utils import ensure_version\nfrom ..constants import (\n    ANNOTATIONS, CITATION, FUSION, GRAPH_ANNOTATION_CURIE, GRAPH_ANNOTATION_LIST, GRAPH_ANNOTATION_MIRIAM, MEMBERS,\n    PARTNER_3P,\n    PARTNER_5P, PRODUCTS, REACTANTS, SOURCE_MODIFIER, TARGET_MODIFIER,\n)\nfrom ..dsl import BaseEntity\nfrom ..language import citation_dict\nfrom ..struct import BELGraph\nfrom ..struct.graph import _handle_modifier\nfrom ..tokens import parse_result_to_dsl\nfrom ..utils import hash_edge, tokenize_version\n\n__all__ = [\n    'to_nodelink',\n    'to_nodelink_file',\n    'to_nodelink_gz',\n    'to_nodelink_jsons',\n    'from_nodelink',\n    'from_nodelink_file',\n    'from_nodelink_gz',\n    'from_nodelink_jsons',\n    'to_nodelink_gz_io',\n    'from_nodelink_gz_io',\n]\n\n\ndef to_nodelink(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert this graph to a node-link JSON object.\n\n    :param graph: BEL Graph\n    \"\"\"\n    graph_json_dict = _to_nodelink_json_helper(graph)\n\n    _prepare_graph_dict(graph_json_dict['graph'])\n\n    return graph_json_dict\n\n\ndef _prepare_graph_dict(g):\n    # Convert annotation list definitions (which are sets) to canonicalized/sorted lists\n    g[GRAPH_ANNOTATION_LIST] = {\n        keyword: list(sorted(values))\n        for keyword, values in g.get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n\n    g[GRAPH_ANNOTATION_CURIE] = list(sorted(g[GRAPH_ANNOTATION_CURIE]))\n    g[GRAPH_ANNOTATION_MIRIAM] = list(sorted(g[GRAPH_ANNOTATION_MIRIAM]))\n\n\n@open_file(1, mode='w')\ndef to_nodelink_file(graph: BELGraph, path: Union[str, TextIO], **kwargs) -> None:\n    \"\"\"Write this graph as node-link JSON to a file.\n\n    :param graph: A BEL graph\n    :param path: A path or file-like\n    \"\"\"\n    graph_json_dict = to_nodelink(graph)\n    json.dump(graph_json_dict, path, ensure_ascii=False, **kwargs)\n\n\ndef to_nodelink_gz(graph, path: str, **kwargs) -> None:\n    \"\"\"Write a graph as node-link JSON to a gzip file.\"\"\"\n    with gzip.open(path, 'wt') as file:\n        json.dump(to_nodelink(graph), file, ensure_ascii=False, **kwargs)\n\n\ndef to_nodelink_jsons(graph: BELGraph, **kwargs) -> str:\n    \"\"\"Dump this graph as a node-link JSON object to a string.\"\"\"\n    return json.dumps(to_nodelink(graph), ensure_ascii=False, **kwargs)\n\n\ndef from_nodelink(graph_json_dict: Mapping[str, Any], check_version: bool = True) -> BELGraph:\n    \"\"\"Build a graph from node-link JSON Object.\"\"\"\n    pybel_version = tokenize_version(graph_json_dict['graph']['pybel_version'])\n    if pybel_version[1] < 14:  # if minor version is less than 14\n        raise ValueError('Invalid NodeLink JSON from old version of PyBEL (v{}.{}.{})'.format(*pybel_version))\n    graph = _from_nodelink_json_helper(graph_json_dict)\n    return ensure_version(graph, check_version=check_version)\n\n\n@open_file(0, mode='r')\ndef from_nodelink_file(path: Union[str, TextIO], check_version: bool = True) -> BELGraph:\n    \"\"\"Build a graph from the node-link JSON contained in the given file.\n\n    :param path: A path or file-like\n    \"\"\"\n    return from_nodelink(json.load(path), check_version=check_version)\n\n\ndef from_nodelink_gz(path: str) -> BELGraph:\n    \"\"\"Read a graph as node-link JSON from a gzip file.\"\"\"\n    with gzip.open(path, 'rt') as file:\n        return from_nodelink(json.load(file))\n\n\ndef from_nodelink_jsons(graph_json_str: str, check_version: bool = True) -> BELGraph:\n    \"\"\"Read a BEL graph from a node-link JSON string.\"\"\"\n    return from_nodelink(json.loads(graph_json_str), check_version=check_version)\n\n\ndef _to_nodelink_json_helper(graph: BELGraph) -> Mapping[str, Any]:\n    \"\"\"Convert a BEL graph to a node-link format.\n\n    :param graph: BEL Graph\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_data`\n    \"\"\"\n    nodes = sorted(graph, key=methodcaller('as_bel'))\n\n    mapping = dict(zip(nodes, count()))\n\n    return {\n        'directed': True,\n        'multigraph': True,\n        'graph': graph.graph.copy(),\n        'nodes': [\n            _augment_node(node)\n            for node in nodes\n        ],\n        'links': [\n            dict(\n                chain(\n                    data.copy().items(),\n                    [('source', mapping[u]), ('target', mapping[v]), ('key', key)],\n                ),\n            )\n            for u, v, key, data in graph.edges(keys=True, data=True)\n        ],\n    }\n\n\ndef _augment_node(node: BaseEntity) -> BaseEntity:\n    \"\"\"Add the SHA-512 identifier to a node's dictionary.\"\"\"\n    rv = node.copy()\n    rv['id'] = node.md5\n    rv['bel'] = node.as_bel()\n    for m in chain(node.get(MEMBERS, []), node.get(REACTANTS, []), node.get(PRODUCTS, [])):\n        m.update(_augment_node(m))\n    if FUSION in node:\n        node[FUSION][PARTNER_3P].update(_augment_node(node[FUSION][PARTNER_3P]))\n        node[FUSION][PARTNER_5P].update(_augment_node(node[FUSION][PARTNER_5P]))\n    return rv\n\n\ndef _recover_graph_dict(graph: BELGraph):\n    graph.graph[GRAPH_ANNOTATION_LIST] = {\n        keyword: set(values)\n        for keyword, values in graph.graph.get(GRAPH_ANNOTATION_LIST, {}).items()\n    }\n    graph.graph[GRAPH_ANNOTATION_CURIE] = set(graph.graph.get(GRAPH_ANNOTATION_CURIE, []))\n    graph.graph[GRAPH_ANNOTATION_MIRIAM] = set(graph.graph.get(GRAPH_ANNOTATION_MIRIAM, []))\n\n\ndef _from_nodelink_json_helper(data: Mapping[str, Any]) -> BELGraph:\n    \"\"\"Return graph from node-link data format.\n\n    Adapted from :func:`networkx.readwrite.json_graph.node_link_graph`\n    \"\"\"\n    graph = BELGraph()\n    graph.graph = data.get('graph', {})\n    _recover_graph_dict(graph)\n\n    mapping = []\n\n    for node_data in data['nodes']:\n        node = parse_result_to_dsl(node_data)\n        graph.add_node_from_data(node)\n        mapping.append(node)\n\n    for data in data['links']:\n        u = mapping[data['source']]\n        v = mapping[data['target']]\n\n        edge_data = {\n            k: v\n            for k, v in data.items()\n            if k not in {'source', 'target', 'key'}\n        }\n\n        for side in (SOURCE_MODIFIER, TARGET_MODIFIER):\n            side_data = edge_data.get(side)\n            if side_data:\n                _handle_modifier(side_data)\n\n        if CITATION in edge_data:\n            edge_data[CITATION] = citation_dict(**edge_data[CITATION])\n\n        if ANNOTATIONS in edge_data:\n            edge_data[ANNOTATIONS] = graph._clean_annotations(edge_data[ANNOTATIONS])\n\n        graph.add_edge(u, v, key=hash_edge(u, v, edge_data), **edge_data)\n\n    return graph\n\n\ndef to_nodelink_gz_io(graph: BELGraph) -> BytesIO:\n    \"\"\"Get a BEL graph as a compressed BytesIO.\"\"\"\n    bytes_io = BytesIO()\n    with gzip.GzipFile(fileobj=bytes_io, mode='w') as file:\n        s = to_nodelink_jsons(graph)\n        file.write(s.encode('utf-8'))\n    bytes_io.seek(0)\n    return bytes_io\n\n\ndef from_nodelink_gz_io(bytes_io: BytesIO) -> BELGraph:\n    \"\"\"Get BEL from gzipped nodelink JSON.\"\"\"\n    with gzip.GzipFile(fileobj=bytes_io, mode='r') as file:\n        s = file.read()\n    j = s.decode('utf-8')\n    return from_nodelink_jsons(j)", "original_line_number": 28736, "instruction_length": 798}
{"instruction": "The given code contains several Python functions that can be used to check the status of a xArm robot. To use these functions, import them into your Python script and use them as decorators for the functions that you want to check. For example, to check if the robot is connected before running a function, use the `xarm_is_connected` decorator. To check if the robot is ready before running a function, use the `xarm_is_ready` decorator. To log the API calls made by your script, use the `api_log` decorator.", "input": "", "output": "# !/usr/bin/env python3\n# Software License Agreement (BSD License)\n#\n# Copyright (c) 2018, UFACTORY, Inc.\n# All rights reserved.\n#\n# Author: Vinman <vinman.wen@ufactory.cc> <vinman.cub@gmail.com>\n\nimport time\nimport functools\nfrom ..core.utils.log import logger\nfrom .code import APIState\n\n\ndef check_modbus_baud(baud=2000000, _type='set', default=None):\n    def _check_modbus_baud(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            code = args[0].checkset_modbus_baud(baud)\n            if code != 0:\n                logger.error('check modbus baud is failed, code={}'.format(code))\n                return code if _type == 'set' else (code, default if default != -99 else [])\n            else:\n                return func(*args, **kwargs)\n        return decorator\n    return _check_modbus_baud\n\n\ndef xarm_is_connected(_type='set'):\n    def _xarm_is_connected(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if args[0].connected:\n                return func(*args, **kwargs)\n            else:\n                logger.error('xArm is not connect')\n                return APIState.NOT_CONNECTED if _type == 'set' else (APIState.NOT_CONNECTED, 'xArm is not connect')\n        return decorator\n    return _xarm_is_connected\n\n\ndef xarm_is_ready(_type='set'):\n    def _xarm_is_ready(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if args[0].connected and kwargs.get('auto_enable', False):\n                if not args[0].ready:\n                    args[0].motion_enable(enable=True)\n                    args[0].set_mode(0)\n                    args[0].set_state(0)\n            if args[0].connected:\n                if args[0].state_is_ready:\n                    return func(*args, **kwargs)\n                else:\n                    logger.error('xArm is not ready')\n                    logger.info('Please check the arm for errors. If so, please clear the error first. '\n                                'Then enable the motor, set the mode and set the state')\n                    return APIState.NOT_READY if _type == 'set' else (APIState.NOT_READY, 'xArm is not ready')\n            else:\n                logger.error('xArm is not connect')\n                return APIState.NOT_CONNECTED if _type == 'set' else (APIState.NOT_CONNECTED, 'xArm is not connect')\n        return decorator\n    return _xarm_is_ready\n\n\ndef xarm_is_pause(_type='set'):\n    def _xarm_is_pause(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            args[0].check_is_pause()\n            return func(*args, **kwargs)\n        return decorator\n    return _xarm_is_pause\n\n\ndef xarm_wait_until_cmdnum_lt_max(only_wait=False):\n    def _xarm_wait_until_cmdnum_lt_max(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            ret = args[0].wait_until_cmdnum_lt_max()\n            if not only_wait and ret is not None:\n                args[0].log_api_info('API -> {} -> code={}'.format(func.__name__, ret), code=ret)\n                return ret\n            return func(*args, **kwargs)\n        return decorator\n    return _xarm_wait_until_cmdnum_lt_max\n\n\ndef xarm_is_not_simulation_mode(ret=0):\n    def _xarm_is_not_simulation_mode(func):\n        @functools.wraps(func)\n        def decorator(*args, **kwargs):\n            if not args[0].check_is_simulation_robot():\n                return func(*args, **kwargs)\n            else:\n                return ret\n        return decorator\n    return _xarm_is_not_simulation_mode\n\n\ndef api_log(func):\n    @functools.wraps(func)\n    def decorator(*args, **kwargs):\n        ret = func(*args, **kwargs)\n        logger.info('{}, ret={}, args={}, kwargs={}'.format(func.__name__, ret, args[1:], kwargs))\n        return ret\n    return decorator\n\n\ndef compare_time(time1, time2):\n    try:\n        s_time = time.mktime(time.strptime(time1, '%Y-%m-%d'))\n        e_time = time.mktime(time.strptime(time2, '%Y-%m-%d'))\n        return int(s_time) - int(e_time) > 0\n    except:\n        return False\n\n\ndef compare_version(v1, v2):\n    for i in range(3):\n        if v1[i] > v2[i]:\n            return True\n        elif v1[i] < v2[i]:\n            return False\n    return False", "original_line_number": 28927, "instruction_length": 509}
{"instruction": "Create a function named `parse_file` that takes a parameter named `datafile`. The function should read the input `datafile` line by line, and for the first 10 lines (not including the header) split each line on \",\" and then for each line, create a dictionary where the key is the header title of the field, and the value is the value of that field in the row. The function should return a list of dictionaries, each data line in the file being a single list entry. Field names and values should not contain extra whitespace, like spaces or newline characters. You can use the Python string method `strip()` to remove the extra whitespace. The returned list should have 10 entries. Finally, write a test function named `test` that tests the implementation of the `parse_file` function.", "input": "", "output": "# Your task is to read the input DATAFILE line by line, and for the first 10 lines (not including the header)\n# split each line on \",\" and then for each line, create a dictionary\n# where the key is the header title of the field, and the value is the value of that field in the row.\n# The function parse_file should return a list of dictionaries,\n# each data line in the file being a single list entry.\n# Field names and values should not contain extra whitespace, like spaces or newline characters.\n# You can use the Python string method strip() to remove the extra whitespace.\n# You have to parse only the first 10 data lines in this exercise,\n# so the returned list should have 10 entries!\nimport os\n\nDATADIR = \"\"\nDATAFILE = \"beatles-diskography.csv\"\n\n\n# -*- coding: utf-8 -*-\n\ndef parse_file(datafile):\n    data = []\n    with open(datafile, \"r\") as f:\n        cnt = 0\n        key_list = []\n        value_list = []\n        for line in f:\n            #print line\n            if cnt == 0:\n                key_list = line.strip().split(',')\n            else:\n                value_list = line.strip().split(',')\n            if cnt != 0:\n                data_dict = {}\n                for i in xrange(len(key_list)):\n                    data_dict[key_list[i]] = value_list[i]\n                data.append(data_dict)\n            cnt += 1\n            if cnt > 10:\n                break\n    return data\n\n\ndef test():\n    # a simple test of your implemetation\n    datafile = os.path.join(DATADIR, DATAFILE)\n    d = parse_file(datafile)\n    firstline = {'Title': 'Please Please Me', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '22 March 1963', 'US Chart Position': '-', 'RIAA Certification': 'Platinum', 'BPI Certification': 'Gold'}\n    tenthline = {'Title': '', 'UK Chart Position': '1', 'Label': 'Parlophone(UK)', 'Released': '10 July 1964', 'US Chart Position': '-', 'RIAA Certification': '', 'BPI Certification': 'Gold'}\n\n    assert d[0] == firstline\n    assert d[9] == tenthline\n\n    \ntest()", "original_line_number": 29158, "instruction_length": 784}
{"instruction": "Create a Python function named \"Transpile\" that takes in four arguments: \"src_dir\", \"dst_dir\", \"model_filename\", and \"params_filename\". Inside the function, create a CPUPlace object and an Executor object using Paddle Fluid. Then, create a Scope object using fluid.core.Scope(). Load the inference program, feed target names, and fetch targets using fluid.io.load_inference_model() with the \"src_dir\", \"exe\", \"model_filename\", and \"params_filename\" arguments. Clone the inference program and apply the inference transpiler using fluid.InferenceTranspiler() and t.transpile(). Save the optimized inference program to \"dst_dir\" using fluid.io.save_persistables() and the \"inference_transpiler_program\" and \"params_filename\" arguments. If \"dst_dir\" does not exist, create it using os.makedirs(). Finally, create a main function that takes in arguments using argparse.ArgumentParser() and calls the \"Transpile\" function with the parsed arguments.", "input": "", "output": "import os\nimport sys\nimport argparse\nimport paddle.fluid as fluid\n\n\ndef Transpile(src_dir, dst_dir, model_filename, params_filename):\n    place = fluid.CPUPlace()\n    exe = fluid.Executor(place)\n\n    inference_scope = fluid.core.Scope()\n    with fluid.scope_guard(inference_scope):\n        print \"Loading inference_program from \", src_dir\n        [inference_program, feed_target_names,\n         fetch_targets] = fluid.io.load_inference_model(src_dir, exe, model_filename, params_filename)\n\n        inference_transpiler_program = inference_program.clone()\n        # NOTE: Applying the inference transpiler will change the inference_transpiler_program.\n        t = fluid.InferenceTranspiler()\n        # Under the with statement, inference_scope is the global scope.\n        t.transpile(inference_transpiler_program, place)\n\n        #print inference_transpiler_program\n\n        print \"Saving the optimized inference_program to \", dst_dir\n        # There is a bug in fluid.io.save_inference_model, so we can use the following code instead.\n        if not os.path.isdir(dst_dir):\n            os.makedirs(dst_dir)\n        model_path = os.path.join(dst_dir, model_filename)\n        with open(model_path, \"wb\") as f:\n            f.write(inference_transpiler_program.desc.serialize_to_string())\n        fluid.io.save_persistables(exe, dst_dir, inference_transpiler_program, params_filename)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--src_dir', help='Source directory of inference model')\n    parser.add_argument('--dst_dir', help='Dstination directory of inference model')\n    parser.add_argument('--model_filename', default=None, help='The name of model file')\n    parser.add_argument('--params_filename', default=None, help='The name of params file')\n    args = parser.parse_args()\n    Transpile(args.src_dir, args.dst_dir, args.model_filename, args.params_filename)\n\n\nif __name__ == '__main__':\n    main()", "original_line_number": 29610, "instruction_length": 942}
{"instruction": "Create a class named ErAPI with several methods to load, save, and update data of eRepublik users. The class should have a constructor method that initializes the necessary configuration settings and useful objects. The class should also have a method to calculate the rank name and damage of a user based on their rank points, strength, weapon power, level, and bonus. The class should be able to load data from a CSV file and update data from a website scraper. The class should have methods to get the ID, level, strength, rank points, citizenship, and nickname of a user. Finally, the class should have a method to write a new user to the data with their ID and nickname.", "input": "", "output": "#-*- encoding: utf-8 -*-\n\nimport csv, math, time, re, threading, sys\n\ntry:\n    from urllib.request import urlopen\nexcept ImportError:\n    from urllib import urlopen\n\nclass ErAPI():\n    # Metodo constructor, seteos basicos necesarios de configuracion, instancia objetos utiles\n    def __init__(self):\n        self.data = {}\n        # Data format: {'XXCiro|BNC': {'id': 123456, 'nick': 'XXCiro', 'level': 49, 'strength': 532.5, 'rank_points': 1233354, 'citizenship': 'Argentina'}}\n        \n        # Diccionario de puntos/rango\n        self.rank_required_points = {\n            \"Recruit\": 0,\n            \"Private\": 15,\n            \"Private*\": 45,\n            \"Private**\": 80,\n            \"Private***\": 120,\n            \"Corporal\": 170,\n            \"Corporal*\": 250,\n            \"Corporal**\": 350,\n            \"Corporal***\": 450,\n            \"Sergeant\": 600,\n            \"Sergeant*\": 800,\n            \"Sergeant**\": 1000,\n            \"Sergeant***\": 1400,\n            \"Lieutenant\": 1850,\n            \"Lieutenant*\": 2350,\n            \"Lieutenant**\": 3000,\n            \"Lieutenant***\": 3750,\n            \"Captain\": 5000,\n            \"Captain*\": 6500,\n            \"Captain**\": 9000,\n            \"Captain***\": 12000,\n            \"Major\": 15500,\n            \"Major*\": 20000,\n            \"Major**\": 25000,\n            \"Major***\": 31000,\n            \"Commander\": 40000,\n            \"Commander*\": 52000,\n            \"Commander**\": 67000,\n            \"Commander***\": 85000,\n            \"Lt Colonel\": 110000,\n            \"Lt Colonel*\": 140000,\n            \"Lt Colonel**\": 180000,\n            \"Lt Colonel***\": 225000,\n            \"Colonel\": 285000,\n            \"Colonel*\": 355000,\n            \"Colonel**\": 435000,\n            \"Colonel***\": 540000,\n            \"General\": 660000,\n            \"General*\": 800000,\n            \"General**\": 950000,\n            \"General***\": 1140000,\n            \"Field Marshal\": 1350000,\n            \"Field Marshal*\": 1600000,\n            \"Field Marshal**\": 1875000,\n            \"Field Marshal***\": 2185000,\n            \"Supreme Marshal\": 2550000,\n            \"Supreme Marshal*\": 3000000,\n            \"Supreme Marshal**\": 3500000,\n            \"Supreme Marshal***\": 4150000,\n            \"National Force\": 4900000,\n            \"National Force*\": 5800000,\n            \"National Force**\": 7000000,\n            \"National Force***\": 9000000,\n            \"World Class Force\": 11500000,\n            \"World Class Force*\": 14500000,\n            \"World Class Force**\": 18000000,\n            \"World Class Force***\": 22000000,\n            \"Legendary Force\": 26500000,\n            \"Legendary Force*\": 31500000,\n            \"Legendary Force**\": 37000000,\n            \"Legendary Force***\": 42000000,\n            \"God of War\": 50000000,\n            \"God of War*\": 100000000 ,\n            \"God of War**\": 200000000,\n            \"God of War***\": 500000000,\n            \"Titan\": 1000000000,\n            \"Titan*\": 2000000000,\n            \"Titan**\": 4000000000,\n            \"Titan***\": 10000000000}\n\n        # Lista ordenada de rangos segun importancia\n        self.rank_to_pos = [\n            \"Recruit\",\n            \"Private\",\n            \"Private*\",\n            \"Private**\",\n            \"Private***\",\n            \"Corporal\",\n            \"Corporal*\",\n            \"Corporal**\",\n            \"Corporal***\",\n            \"Sergeant\",\n            \"Sergeant*\",\n            \"Sergeant**\",\n            \"Sergeant***\",\n            \"Lieutenant\",\n            \"Lieutenant*\",\n            \"Lieutenant**\",\n            \"Lieutenant***\",\n            \"Captain\",\n            \"Captain*\",\n            \"Captain**\",\n            \"Captain***\",\n            \"Major\",\n            \"Major*\",\n            \"Major**\",\n            \"Major***\",\n            \"Commander\",\n            \"Commander*\",\n            \"Commander**\",\n            \"Commander***\",\n            \"Lt Colonel\",\n            \"Lt Colonel*\",\n            \"Lt Colonel**\",\n            \"Lt Colonel***\",\n            \"Colonel\",\n            \"Colonel*\",\n            \"Colonel**\",\n            \"Colonel***\",\n            \"General\",\n            \"General*\",\n            \"General**\",\n            \"General***\",\n            \"Field Marshal\",\n            \"Field Marshal*\",\n            \"Field Marshal**\",\n            \"Field Marshal***\",\n            \"Supreme Marshal\",\n            \"Supreme Marshal*\",\n            \"Supreme Marshal**\",\n            \"Supreme Marshal***\",\n            \"National Force\",\n            \"National Force*\",\n            \"National Force**\",\n            \"National Force***\",\n            \"World Class Force\",\n            \"World Class Force*\",\n            \"World Class Force**\",\n            \"World Class Force***\",\n            \"Legendary Force\",\n            \"Legendary Force*\",\n            \"Legendary Force**\",\n            \"Legendary Force***\",\n            \"God of War\",\n            \"God of War*\",\n            \"God of War**\",\n            \"God of War***\",\n            \"Titan\",\n            \"Titan*\",\n            \"Titan**\",\n            \"Titan***\",]\n\n        # Bandera de ejecucion, util en caso de que se decida matar de forma manual los threads para actualizar y guardar los datos\n        self.run = True\n\n        # Se paraleliza la carga de datos en un hilo nuevo, el cual es demonio del invocador en caso de \"muerte prematura\"\n        th = threading.Thread(target=self.data_loader)\n        th.daemon = True\n        th.start()\n\n    # Metodo invocador, carga datos y crea threads para guardar y actualizar informacion, solo llamado desde constructor\n    def data_loader(self):\n        self.load_data()\n\n        self.data_saver_th = threading.Thread(target=self.data_saver)\n        self.data_saver_th.daemon = True\n        self.data_saver_th.start()\n\n        self.data_updater_th = threading.Thread(target=self.data_updater)\n        self.data_updater_th.daemon = True\n        self.data_updater_th.start()\n\n    # Metodo para volcar informacion a archivo fisico, solo llamado de metodo data_loader\n    def data_saver(self):\n        while self.run:\n            self.save_data()\n\n            time.sleep(60)\n\n    # Metodo para actualizar informacion, solo llamado de metodo data_loader\n    def data_updater(self):\n        while self.run:\n            for irc_nick in self.data:\n                self.update_data(irc_nick)\n                time.sleep(30)\n\n            time.sleep(600)\n\n    # ---------------------------------------------------------------------------------- #\n    # @ PUBLIC METHODS                                                                   #\n    # ---------------------------------------------------------------------------------- #\n\n    # Metodo para actualizar informacion local del objeto desde archivo\n    def load_data(self):\n        try:\n            f = open('data/er_nick-data.csv', 'rt')\n            reader = csv.reader(f)\n            for nick_irc,id,nick_er,level,strength,rank_points,citizenship in reader:\n                self.data[nick_irc] = {'id': int(id), 'nick': nick_er, 'level': int(level), 'strength': float(strength), 'rank_points': int(rank_points), 'citizenship': citizenship}\n            f.close()\n        except:\n            pass\n\n    # Metodo para guardar informacion local del objeto en archivo\n    def save_data(self):\n        try:\n            f = open('data/er_nick-data.csv', 'wt')\n            writer = csv.writer(f)\n            for u in self.data:\n                writer.writerow([u, self.data[u]['id'], self.data[u]['nick'], self.data[u]['level'], self.data[u]['strength'], self.data[u]['rank_points'], self.data[u]['citizenship']])\n            f.close()\n        except:\n            pass\n\n    # Metodo scraper para actualizar informacion local del objeto del nick de irc especificado\n    def update_data(self, irc_nick):\n        try:\n            id = self.data[irc_nick]['id']\n\n            c = urlopen('http://www.erepublik.com/es/citizen/profile/%d' % id)\n            page = c.read()\n            c.close()\n\n            self.data[irc_nick]['nick'] = re.search('<meta name=\"title\" content=\"(.+?) - Ciudadano del Nuevo Mundo\" \\/>', page.decode('utf-8')).group(1)\n            self.data[irc_nick]['level'] = int(re.search('<strong class=\"citizen_level\">(.+?)<\\/strong>', page.decode('utf-8'), re.DOTALL).group(1))\n            self.data[irc_nick]['strength'] = float(re.search('<span class=\"military_box_info mb_bottom\">(.+?)</span>', page.decode('utf-8'), re.DOTALL).group(1).strip('\\r\\n\\t ').replace(',',''))\n            self.data[irc_nick]['rank_points'] = int(re.search('<span class=\"rank_numbers\">(.+?) \\/', page.decode('utf-8'), re.DOTALL).group(1).replace(',',''))\n            self.data[irc_nick]['citizenship'] = re.search('<a href=\"http\\:\\/\\/www.erepublik.com\\/es\\/country\\/society\\/([^ \\t\\n\\x0B\\f\\r]+?)\">', page.decode('utf-8')).group(1)\n        except:\n            pass\n\n    # Metodo para actualizar informacion local del objeto con nick de irc e id especificados, fuerza actualizacion del mismo\n    def reg_nick_write(self, nick, id):\n        if(nick.lower() in self.data.keys()):\n            self.data[nick.lower()]['id'] = int(id)\n        else:\n            self.data[nick.lower()] = {'id': int(id), 'nick': nick, 'level': 1, 'strength': 0, 'rank_points': 0, 'citizenship': ''}\n\n        self.update_data(nick.lower())        \n\n    # Metodo para obtener ID del nick de irc especificado\n    def get_id(self, nick):\n        return self.data[nick.lower()]['id']\n\n    # Metodo para obtener LEVEL del nick de irc especificado\n    def get_level(self, nick):\n        return self.data[nick.lower()]['level']\n\n    # Metodo para obtener STRENGTH del nick de irc especificado\n    def get_strength(self, nick):\n        return self.data[nick.lower()]['strength']\n\n    # Metodo para obtener RANK POINTS del nick de irc especificado\n    def get_rank_points(self, nick):\n        return self.data[nick.lower()]['rank_points']\n\n    # Metodo para obtener CITIZENSHIP del nick de irc especificado\n    def get_citizenship(self, nick):\n        return self.data[nick.lower()]['citizenship']\n\n    # Metodo para obtener NICK INGAME del nick de irc especificado\n    def get_nick(self, nick):\n        return self.data[nick.lower()]['nick']\n\n    # Metodo para obtener RANK NAME del nick de irc especificado\n    def calculate_rank_name(self, rank_points):\n        index = 0\n\n        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:\n            if(self.rank_to_pos.index(k) > index):\n                index = self.rank_to_pos.index(k)\n\n        return self.rank_to_pos[index]\n\n    # Metodo para calcular DAÑO del nick de irc especificado segun datos adicionales\n    def calculate_damage(self, rank_points, strength, weapon_power, level, bonus):\n        index = 0\n\n        for k in [key for key in self.rank_required_points.keys() if self.rank_required_points[key] < rank_points]:\n            if(self.rank_to_pos.index(k) > index):\n                index = self.rank_to_pos.index(k)\n\n        return(math.trunc(((index / 20) + 0.3) * ((strength / 10) + 40) * (1 + (weapon_power / 100)) * (1.1 if level > 99 else 1) * bonus))", "original_line_number": 29688, "instruction_length": 675}
{"instruction": "This code is a JuniperFPC modeler plugin that gathers table information from Juniper Contents tables. To use this program, import the necessary modules and call the `JuniperFPCMap` class, which maps the Juniper FPC table to a model. The `process` method of this class collects SNMP information from a device and returns a `relMap` object containing the processed data. Make sure to supply SNMP data for the `jnxContentsTable`, `jnxOperatingTable`, and `jnxContainersTable` tables. If no data is supplied, a warning message will be logged. The `relMap` object contains information about the FPC, including its state, temperature, CPU usage, uptime, and memory usage. The `FPCUpTime` value is converted from milliseconds to hours. The `FPCState` value is transformed from a numeric value to a status string using the `operatingStateLookup` dictionary. The `id` attribute of the `objectMap` is set to the FPC description and SNMP index, with spaces replaced by underscores.", "input": "", "output": "##########################################################################\n# Author:               Jane Curry,  jane.curry@skills-1st.co.uk\n# Date:                 February 28th, 2011\n# Revised:\t\tExtra debugging added Aug 23, 2011\n#\n# JuniperFPC modeler plugin\n#\n# This program can be used under the GNU General Public License version 2\n# You can find full information here: http://www.zenoss.com/oss\n#\n##########################################################################\n\n__doc__ = \"\"\"JuniperFPCMap\n\nGather table information from Juniper Contents tables\n\"\"\"\n\nimport re\nfrom Products.DataCollector.plugins.CollectorPlugin import SnmpPlugin, GetMap, GetTableMap\n\nclass JuniperFPCMap(SnmpPlugin):\n    \"\"\"Map Juniper FPC table to model.\"\"\"\n    maptype = \"JuniperFPCMap\"\n    modname = \"ZenPacks.ZenSystems.Juniper.JuniperFPC\"\n    relname = \"JuniperFP\"\n    compname = \"\"\n\n    snmpGetTableMaps = (\n        GetTableMap('jnxContentsTable',\n                    '.1.3.6.1.4.1.2636.3.1.8.1',\n                    {\n                        '.1': 'containerIndex',\n                        '.5': 'FPCType',\n                        '.6': 'FPCDescr',\n                        '.7': 'FPCSerialNo',\n                        '.8': 'FPCRevision',\n                        '.10': 'FPCPartNo',\n                        '.11': 'FPCChassisId',\n                        '.12': 'FPCChassisDescr',\n                        '.13': 'FPCChassisCLEI',\n                    }\n        ),\n        GetTableMap('jnxOperatingTable',\n                    '.1.3.6.1.4.1.2636.3.1.13.1',\n                    {\n                        '.6': 'FPCState',\n                        '.7': 'FPCTemp',\n                        '.8': 'FPCCPU',\n                        '.13': 'FPCUpTime',\n                        '.15': 'FPCMemory',\n                    }\n        ),\n        GetTableMap('jnxContainersTable',\n                    '.1.3.6.1.4.1.2636.3.1.6.1',\n                    {\n                        '.1': 'containerIndex',\n                        '.3': 'containerLevel',\n                        '.4': 'containerNextLevel',\n                        '.5': 'containerType',\n                        '.6': 'containerDescr',\n                    }\n        ),\n    )\n\n    def process(self, device, results, log):\n        \"\"\"collect snmp information from this device\"\"\"\n        log.info('processing %s for device %s', self.name(), device.id)\n        getdata, tabledata = results\n        rm = self.relMap()\n        contentsTable = tabledata.get('jnxContentsTable')\n        operatingTable = tabledata.get('jnxOperatingTable')\n        containersTable = tabledata.get('jnxContainersTable')\n\n# If no data supplied then simply return\n        if not contentsTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for contents', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n        if not operatingTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for operating system', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n        if not containersTable:\n            log.warn( 'No SNMP response from %s for the %s plugin for containers', device.id, self.name() )\n            log.warn( \"Data= %s\", tabledata )\n            return\n \n        for oid, data in contentsTable.items():\n            try:\n                om = self.objectMap(data)\n                FPCDescr = om.FPCDescr\n#                log.info(' FPCDescr is %s ' % (om.FPCDescr))\n                isaFPC = re.match(r'(.*FPC.*)', FPCDescr.upper())\n                if not isaFPC:\n                    continue\n                else:\n                    for oid1, data1 in operatingTable.items():\n                        if oid1 == oid:\n                            om.FPCState = data1['FPCState']\n                            om.FPCTemp = data1['FPCTemp']\n                            om.FPCCPU = data1['FPCCPU']\n                            om.FPCUpTime = data1['FPCUpTime']\n                            om.FPCMemory = data1['FPCMemory']\n                            for oid2, data2 in containersTable.items():\n#                                log.info( ' oid is %s - oid2 is %s - data is %s' % (oid, oid2 , data2))\n                                if oid.startswith(oid2):\n                                    om.containerDescr = data2['containerDescr']\n                                    if data2['containerLevel'] == 1:\n                                        om.containerDescr = '....' + om.containerDescr\n                                    elif data2['containerLevel'] == 2:\n                                        om.containerDescr = '........' + om.containerDescr\n                                    om.containerParentIndex = data2['containerNextLevel']\n                                    if om.containerParentIndex != 0:\n                                        for oid3, data3 in  containersTable.items():\n                                            if oid3.endswith(str(om.containerParentIndex)):\n                                                om.containerParentDescr = data3['containerDescr']\n                            om.snmpindex = oid1.strip('.')\n# Convert FPCUpTime from milliseconds to hours\n                    om.FPCUpTime = om.FPCUpTime / 1000 / 60 / 60 /24\n# Transform numeric FPCState into a status string via operatingStateLookup\n                    if (om.FPCState < 1 or om.FPCState > 7):\n                        om.FPCState = 1\n                    om.FPCState = self.operatingStateLookup[om.FPCState]\n                    om.id = self.prepId( om.FPCDescr.replace(' ','_') + '_' + str( om.snmpindex.replace('.','_') ) )\n            except (KeyError, IndexError, AttributeError, TypeError), errorInfo:\n                log.warn( ' Error in %s modeler plugin %s' % ( self.name(), errorInfo))\n                continue\n            rm.append(om)\n#            log.info('rm %s' % (rm) )\n\n        return rm\n\n    operatingStateLookup = { 1: 'Unknown',\n                             2: 'Running',\n                             3: 'Ready',\n                             4: 'Reset',\n                             5: 'RunningAtFullSpeed (Fan)',\n                             6: 'Down',\n                             7: 'Standby'\n                           }", "original_line_number": 29854, "instruction_length": 970}
{"instruction": "Create a Python script that connects to a SQLite database, creates three tables (stations, observations, and time), and populates them with data from a CSV file and a JSON API. The script should use the csv, json, requests, numpy, and sqlite3 modules. The CSV file should contain data about weather stations, and the JSON API should provide data about weather observations. The stations table should have columns for id, short, name, lon, lat, height, and json. The observations table should have columns for id, local_date_time, local_date_time_full, apparent_t, delta_t, air_temp, rain_trace, rel_hum, wind_dir, and wind_spd_kmh. The time table should have columns for id and local_date_time_full. The script should insert data into the tables using SQL queries, and should print out the queries as they are executed. Finally, the script should print out the names of the tables in the database and export each table to a CSV file.", "input": "", "output": "#!/usr/bin/env python\n\nimport csv\nimport json\nimport requests\nimport numpy as np\nimport sqlite3\n\ncsv_filename = \"testdata/vic_usgs/south_west_stations.csv\"\ndb_filename = \"testdata/vic_usgs/south_west_stations.db\"\n\n# open database\nconn = sqlite3.connect(db_filename)\nc = conn.cursor()\n\n# Create stations table\nc.execute('''CREATE TABLE IF NOT EXISTS stations \n             (id text, short text, name text, lon real, lat real, height real, json text, UNIQUE(id))''')\n\n# Create observations table\nc.execute('''CREATE TABLE IF NOT EXISTS observations \n             (id text, local_date_time text, local_date_time_full text,  \n              apparent_t real, delta_t real, air_temp real, rain_trace text, rel_hum real, wind_dir text, wind_spd_kmh real,\n              UNIQUE(id, local_date_time_full))''')\n\n# Create time point table\nc.execute('''CREATE TABLE IF NOT EXISTS time \n             (id text, local_date_time_full text, UNIQUE(local_date_time_full))''')\n\n# stations\nwith open(csv_filename, 'rb') as csvfile:\n    stations = csv.DictReader(csvfile)\n    for station in stations:\n        id = station[\"ID\"]\n        short = station[\"Short\"]\n        name = station[\"Name\"]\n        lon = float(station[\"Lon\"])\n        lat = float(station[\"Lat\"])\n        height = float(station[\"Height\"])\n        json = station[\"Json\"]\n\n        # Insert a row of data into stations\n        query_str = \"INSERT OR IGNORE INTO stations VALUES ('%s', '%s', '%s', %f, %f, %f, '%s')\" % (id, short, name, lon, lat, height, json)\n        print query_str\n        c.execute( query_str )\n\n        # Update observations table\n        r = requests.get(json)\n        stationdata = r.json()\n        data = stationdata[\"observations\"][\"data\"]\n        nrows = len(data)\n        for i in range(nrows):\n            apparent_t = data[i]['apparent_t']\n            if apparent_t is None:\n                apparent_t = 0\n\n            delta_t = data[i]['delta_t']\n            if delta_t is None:\n                delta_t = 0\n\n            air_temp = data[i]['air_temp']\n            if air_temp is None:\n                air_temp = 0\n\n            rain_trace = data[i]['rain_trace']\n            if rain_trace is None:\n                rain_trace = ''\n\n            rel_hum = data[i]['rel_hum']\n            if rel_hum is None:\n                rel_hum = 0\n\n            wind_spd_kmh = data[i]['wind_spd_kmh']\n            if wind_spd_kmh is None:\n                wind_spd_kmh = 0\n\n            query_str = \"INSERT OR IGNORE INTO observations VALUES ('%s', '%s', '%s', %0.2f, %0.2f, %0.2f, '%s', %0.2f, '%s', %0.2f)\" % \\\n                (id, data[i]['local_date_time'], data[i]['local_date_time_full'], apparent_t, delta_t, \\\n                    air_temp, rain_trace, rel_hum, data[i]['wind_dir'], wind_spd_kmh)\n            print query_str\n            c.execute(query_str)\n\n            query_str = \"INSERT OR IGNORE INTO time VALUES (%s, '%s')\" % (id, data[i]['local_date_time_full'])\n            print query_str\n            c.execute(query_str)\n\n# commit the change\nconn.commit()\n# close database\nconn.close()\n\n\n\n\n# TESTING: print out to check\n\"\"\"\nconn = sqlite3.connect(db_filename)\nc = conn.cursor()\nc.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = c.fetchall()\nprint tables\nfor table_name in tables:\n    table_name = table_name[0]\n    table = pd.read_sql_query(\"SELECT * from %s\" % table_name, conn)\n    #print table\n    table.to_csv('testdata/vic_usgs/south_west_stations_' + table_name + '.csv', index_label='index')\n# close database\nconn.close()\n\"\"\"", "original_line_number": 30228, "instruction_length": 933}
{"instruction": "Create a Python class called Tv4play that inherits from Service and OpenGraphThumbMixin. The class should have a constructor that takes a URL as an argument. The class should have a method called get that takes an options parameter. The method should use the URL to retrieve data, find the video ID, and download the video. The method should also handle authentication and geoblocking. The class should have a method called find_all_episodes that takes an options parameter. The method should use the URL to retrieve data, find all episodes, and return a list of URLs.", "input": "", "output": "# ex:ts=4:sw=4:sts=4:et\n# -*- tab-width: 4; c-basic-offset: 4; indent-tabs-mode: nil -*-\nfrom __future__ import absolute_import\nimport re\nimport os\nimport xml.etree.ElementTree as ET\nimport json\nimport copy\n\nfrom svtplay_dl.utils.urllib import urlparse, parse_qs, quote_plus\nfrom svtplay_dl.service import Service, OpenGraphThumbMixin\nfrom svtplay_dl.utils import is_py2_old, filenamify\nfrom svtplay_dl.log import log\nfrom svtplay_dl.fetcher.hls import hlsparse, HLS\nfrom svtplay_dl.fetcher.rtmp import RTMP\nfrom svtplay_dl.fetcher.hds import hdsparse\nfrom svtplay_dl.subtitle import subtitle\nfrom svtplay_dl.error import ServiceError\n\n\nclass Tv4play(Service, OpenGraphThumbMixin):\n    supported_domains = ['tv4play.se', 'tv4.se']\n\n    def __init__(self, url):\n        Service.__init__(self, url)\n        self.subtitle = None\n        self.cookies = {}\n\n    def get(self, options):\n        data = self.get_urldata()\n\n        vid = findvid(self.url, data)\n        if vid is None:\n            yield ServiceError(\"Can't find video id for %s\" % self.url)\n            return\n\n        if options.username and options.password:\n            data = self.http.request(\"get\", \"https://www.tv4play.se/session/new?https=\")\n            auth_token = re.search('name=\"authenticity_token\" ([a-z]+=\"[^\"]+\" )?value=\"([^\"]+)\"', data.text)\n            if not auth_token:\n                yield ServiceError(\"Can't find authenticity_token needed for user / password\")\n                return\n            url = \"https://www.tv4play.se/session\"\n            postdata = {\"user_name\" : options.username, \"password\": options.password, \"authenticity_token\":auth_token.group(2), \"https\": \"\"}\n            data = self.http.request(\"post\", url, data=postdata, cookies=self.cookies)\n            self.cookies = data.cookies\n            fail = re.search(\"<p class='failed-login'>([^<]+)</p>\", data.text)\n            if fail:\n                yield ServiceError(fail.group(1))\n                return\n        url = \"http://premium.tv4play.se/api/web/asset/%s/play\" % vid\n        data = self.http.request(\"get\", url, cookies=self.cookies)\n        if data.status_code == 401:\n            xml = ET.XML(data.content)\n            code = xml.find(\"code\").text\n            if code == \"SESSION_NOT_AUTHENTICATED\":\n                yield ServiceError(\"Can't access premium content\")\n            elif code == \"ASSET_PLAYBACK_INVALID_GEO_LOCATION\":\n                yield ServiceError(\"Can't downoad this video because of geoblocked.\")\n            else:\n                yield ServiceError(\"Can't find any info for that video\")\n            return\n        if data.status_code == 404:\n            yield ServiceError(\"Can't find the video api\")\n            return\n        xml = ET.XML(data.content)\n        ss = xml.find(\"items\")\n        if is_py2_old:\n            sa = list(ss.getiterator(\"item\"))\n        else:\n            sa = list(ss.iter(\"item\"))\n\n        if xml.find(\"live\").text:\n            if xml.find(\"live\").text != \"false\":\n                options.live = True\n        if xml.find(\"drmProtected\").text == \"true\":\n            yield ServiceError(\"We cant download DRM protected content from this site.\")\n            return\n\n        if options.output_auto:\n            directory = os.path.dirname(options.output)\n            options.service = \"tv4play\"\n            title = \"%s-%s-%s\" % (options.output, vid, options.service)\n            title = filenamify(title)\n            if len(directory):\n                options.output = os.path.join(directory, title)\n            else:\n                options.output = title\n\n        if self.exclude(options):\n            yield ServiceError(\"Excluding video\")\n            return\n\n        for i in sa:\n            if i.find(\"mediaFormat\").text == \"mp4\":\n                base = urlparse(i.find(\"base\").text)\n                parse = urlparse(i.find(\"url\").text)\n                if \"rtmp\" in base.scheme:\n                    swf = \"http://www.tv4play.se/flash/tv4playflashlets.swf\"\n                    options.other = \"-W %s -y %s\" % (swf, i.find(\"url\").text)\n                    yield RTMP(copy.copy(options), i.find(\"base\").text, i.find(\"bitrate\").text)\n                elif parse.path[len(parse.path)-3:len(parse.path)] == \"f4m\":\n                    streams = hdsparse(copy.copy(options), self.http.request(\"get\", i.find(\"url\").text, params={\"hdcore\": \"3.7.0\"}).text, i.find(\"url\").text)\n                    if streams:\n                        for n in list(streams.keys()):\n                            yield streams[n]\n            elif i.find(\"mediaFormat\").text == \"smi\":\n                yield subtitle(copy.copy(options), \"smi\", i.find(\"url\").text)\n\n        url = \"http://premium.tv4play.se/api/web/asset/%s/play?protocol=hls\" % vid\n        data = self.http.request(\"get\", url, cookies=self.cookies).content\n        xml = ET.XML(data)\n        ss = xml.find(\"items\")\n        if is_py2_old:\n            sa = list(ss.getiterator(\"item\"))\n        else:\n            sa = list(ss.iter(\"item\"))\n        for i in sa:\n            if i.find(\"mediaFormat\").text == \"mp4\":\n                parse = urlparse(i.find(\"url\").text)\n                if parse.path.endswith(\"m3u8\"):\n                    streams = hlsparse(i.find(\"url\").text, self.http.request(\"get\", i.find(\"url\").text).text)\n                    for n in list(streams.keys()):\n                        yield HLS(copy.copy(options), streams[n], n)\n\n    def find_all_episodes(self, options):\n        parse = urlparse(self.url)\n        show = parse.path[parse.path.find(\"/\", 1)+1:]\n        if not re.search(\"%\", show):\n            show = quote_plus(show)\n        data = self.http.request(\"get\", \"http://webapi.tv4play.se/play/video_assets?type=episode&is_live=false&platform=web&node_nids=%s&per_page=99999\" % show).text\n        jsondata = json.loads(data)\n        episodes = []\n        n = 1\n        for i in jsondata[\"results\"]:\n            try:\n                days = int(i[\"availability\"][\"availability_group_free\"])\n            except (ValueError, TypeError):\n                days = 999\n            if days > 0:\n                video_id = i[\"id\"]\n                url = \"http://www.tv4play.se/program/%s?video_id=%s\" % (\n                    show, video_id)\n                episodes.append(url)\n                if n == options.all_last:\n                    break\n                n += 1\n\n        return episodes\n\n\ndef findvid(url, data):\n    parse = urlparse(url)\n    if \"tv4play.se\" in url:\n        try:\n            vid = parse_qs(parse.query)[\"video_id\"][0]\n        except KeyError:\n            return None\n    else:\n        match = re.search(r\"\\\"vid\\\":\\\"(\\d+)\\\",\", data)\n        if match:\n            vid = match.group(1)\n        else:\n            match = re.search(r\"-(\\d+)$\", url)\n            if match:\n                vid = match.group(1)\n            else:\n                match = re.search(r\"meta content='([^']+)' property='og:video'\", data)\n                if match:\n                    match = re.search(r\"vid=(\\d+)&\", match.group(1))\n                    if match:\n                        vid = match.group(1)\n                    else:\n                        log.error(\"Can't find video id for %s\", url)\n                        return\n                else:\n                    return None\n    return vid", "original_line_number": 30262, "instruction_length": 568}
{"instruction": "This code allows running a function on a large dataset with a set of parameters. Results are extracted and saved in a way that they can easily be compared with another set. The data should be organized as the following: (names of images can be changed but must be passed as parameters to this function). The function `test_function` takes the following arguments: function to test (`-f`), dataset directory (`-d`), arguments to pass to the function that is tested (`-p`), number of CPU used for testing (`-cpu-nb`), and requirements on center, study, ... that must be satisfied by the json file of each tested subjects (`-json`). The function `generate_data_list` is used to construct the data list from the data set. The function `process_results` is used to process the results. The function `function_launcher` is used to launch the function. The function `init_worker` is used to initialize the worker. The function `get_parser` is used to initialize the parser.", "input": "", "output": "#!/usr/bin/env python\n#########################################################################################\n#\n# This function allows to run a function on a large dataset with a set of parameters.\n# Results are extracted and saved in a way that they can easily be compared with another set.\n#\n# Data should be organized as the following:\n# (names of images can be changed but must be passed as parameters to this function)\n#\n# data/\n# ......subject_name_01/\n# ......subject_name_02/\n# .................t1/\n# .........................subject_02_anything_t1.nii.gz\n# .........................some_landmarks_of_vertebral_levels.nii.gz\n# .........................subject_02_manual_segmentation_t1.nii.gz\n# .................t2/\n# .........................subject_02_anything_t2.nii.gz\n# .........................some_landmarks_of_vertebral_levels.nii.gz\n# .........................subject_02_manual_segmentation_t2.nii.gz\n# .................t2star/\n# .........................subject_02_anything_t2star.nii.gz\n# .........................subject_02_manual_segmentation_t2star.nii.gz\n# ......subject_name_03/\n#          .\n#          .\n#          .\n#\n# ---------------------------------------------------------------------------------------\n# Copyright (c) 2015 Polytechnique Montreal <www.neuro.polymtl.ca>\n# Author: Sara Dupont, Benjamin De Leener\n# Modified: 2015-09-30\n#\n# About the license: see the file LICENSE.TXT\n#########################################################################################\nimport sys\nimport commands\nimport platform\nimport signal\nfrom time import time, strftime\nfrom msct_parser import Parser\nimport sct_utils as sct\nimport os\nimport copy_reg\nimport types\nimport pandas as pd\nimport json\n\n\n# get path of the toolbox\n# TODO: put it back below when working again (julien 2016-04-04)\n# <<<\n# OLD\n# status, path_sct = commands.getstatusoutput('echo $SCT_DIR')\n# NEW\npath_script = os.path.dirname(__file__)\npath_sct = os.path.dirname(path_script)\n# >>>\n# append path that contains scripts, to be able to load modules\nsys.path.append(path_sct + '/scripts')\nsys.path.append(path_sct + '/testing')\n\n\ndef _pickle_method(method):\n    \"\"\"\n    Author: Steven Bethard (author of argparse)\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    func_name = method.im_func.__name__\n    obj = method.im_self\n    cls = method.im_class\n    cls_name = ''\n    if func_name.startswith('__') and not func_name.endswith('__'):\n        cls_name = cls.__name__.lstrip('_')\n    if cls_name:\n        func_name = '_' + cls_name + func_name\n    return _unpickle_method, (func_name, obj, cls)\n\n\ndef _unpickle_method(func_name, obj, cls):\n    \"\"\"\n    Author: Steven Bethard\n    http://bytes.com/topic/python/answers/552476-why-cant-you-pickle-instancemethods\n    \"\"\"\n    for cls in cls.mro():\n        try:\n            func = cls.__dict__[func_name]\n        except KeyError:\n            pass\n        else:\n            break\n    return func.__get__(obj, cls)\n\ncopy_reg.pickle(types.MethodType, _pickle_method, _unpickle_method)\n\n\ndef generate_data_list(folder_dataset, json_requirements=None, verbose=1):\n    \"\"\"\n    Construction of the data list from the data set\n    This function return a list of directory (in folder_dataset) in which the contrast is present.\n    :return data:\n    \"\"\"\n    data_subjects, subjects_dir = [], []\n\n    # each directory in folder_dataset should be a directory of a subject\n    for subject_dir in os.listdir(folder_dataset):\n        if not subject_dir.startswith('.') and os.path.isdir(folder_dataset + subject_dir):\n            if read_json(folder_dataset + subject_dir, json_requirements=json_requirements):\n                data_subjects.append(folder_dataset + subject_dir + '/')\n                subjects_dir.append(subject_dir)\n\n    if not data_subjects:\n        sct.printv('ERROR: No subject data were found in ' + folder_dataset + '. '\n                   'Please organize your data correctly or provide a correct dataset.',\n                   verbose=verbose, type='error')\n\n    return data_subjects, subjects_dir\n\ndef read_json(path_dir, json_requirements=None, fname_json='dataset_description.json'):\n    path_dir = sct.slash_at_the_end(path_dir, slash=1)\n    if fname_json not in os.listdir(path_dir) and json_requirements is not None:\n        accept_subject = False\n    elif json_requirements is None:\n        accept_subject = True\n    else:\n        json_file = open(path_dir+fname_json)\n        dic_info = json.load(json_file)\n        json_file.close()\n        # pass keys and items to lower case\n        dic_info = dict((k.lower(), v.lower()) for k, v in dic_info.iteritems())\n        # if no condition is not verified, accept subject\n        accept_subject = True\n        # read requirements:\n        list_conditions = json_requirements.split(',')\n        for condition in list_conditions:\n            key, val = condition.split('=')\n            key, val = key.lower(), val.lower()\n            # if key do not exist, do not accept subject\n            if key not in dic_info.keys():\n                accept_subject = False\n            # if value for this key is not the one required, do not accept subject\n            elif dic_info[key] != val:\n                accept_subject = False\n\n    return accept_subject\n\n\ndef process_results(results, subjects_name, function, folder_dataset, parameters):\n    try:\n        results_dataframe = pd.concat([result[2] for result in results])\n        results_dataframe.loc[:, 'subject'] = pd.Series(subjects_name, index=results_dataframe.index)\n        results_dataframe.loc[:, 'script'] = pd.Series([function]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'dataset'] = pd.Series([folder_dataset]*len(subjects_name), index=results_dataframe.index)\n        results_dataframe.loc[:, 'parameters'] = pd.Series([parameters] * len(subjects_name), index=results_dataframe.index)\n        return results_dataframe\n    except KeyboardInterrupt:\n        return 'KeyboardException'\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        sys.exit(2)\n\n\ndef function_launcher(args):\n    import importlib\n    # append local script to PYTHONPATH for import\n    sys.path.append(os.path.abspath(os.curdir))\n    script_to_be_run = importlib.import_module('test_' + args[0])  # import function as a module\n    try:\n        output = script_to_be_run.test(*args[1:])\n    except:\n        import traceback\n        print('%s: %s' % ('test_' + args[0], traceback.format_exc()))\n        # output = (1, 'ERROR: Function crashed', 'No result')\n        from pandas import DataFrame\n        status_script = 1\n        output_script = 'ERROR: Function crashed.'\n        output = (status_script, output_script, DataFrame(data={'status': int(status_script), 'output': output_script}, index=['']))\n    return output\n    # return script_to_be_run.test(*args[1:])\n\n\ndef init_worker():\n    signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\ndef test_function(function, folder_dataset, parameters='', nb_cpu=None, json_requirements=None, verbose=1):\n    \"\"\"\n    Run a test function on the dataset using multiprocessing and save the results\n    :return: results\n    # results are organized as the following: tuple of (status, output, DataFrame with results)\n    \"\"\"\n\n    # generate data list from folder containing\n    data_subjects, subjects_name = generate_data_list(folder_dataset, json_requirements=json_requirements)\n\n    # All scripts that are using multithreading with ITK must not use it when using multiprocessing on several subjects\n    os.environ[\"ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS\"] = \"1\"\n\n    from multiprocessing import Pool\n\n    # create datasets with parameters\n    import itertools\n    data_and_params = itertools.izip(itertools.repeat(function), data_subjects, itertools.repeat(parameters))\n\n    pool = Pool(processes=nb_cpu, initializer=init_worker)\n\n    try:\n        async_results = pool.map_async(function_launcher, data_and_params).get(9999999)\n        # results = process_results(async_results.get(9999999), subjects_name, function, folder_dataset, parameters)  # get the sorted results once all jobs are finished\n        pool.close()\n        pool.join()  # waiting for all the jobs to be done\n        results = process_results(async_results, subjects_name, function, folder_dataset, parameters)  # get the sorted results once all jobs are finished\n    except KeyboardInterrupt:\n        print \"\\nWarning: Caught KeyboardInterrupt, terminating workers\"\n        pool.terminate()\n        pool.join()\n        # return\n        # raise KeyboardInterrupt\n        # sys.exit(2)\n    except Exception as e:\n        sct.printv('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), 1, 'warning')\n        sct.printv(str(e), 1, 'warning')\n        pool.terminate()\n        pool.join()\n        # raise Exception\n        # sys.exit(2)\n\n    return results\n\n\ndef get_parser():\n    # Initialize parser\n    parser = Parser(__file__)\n\n    # Mandatory arguments\n    parser.usage.set_description(\"\")\n    parser.add_option(name=\"-f\",\n                      type_value=\"str\",\n                      description=\"Function to test.\",\n                      mandatory=True,\n                      example=\"sct_propseg\")\n\n    parser.add_option(name=\"-d\",\n                      type_value=\"folder\",\n                      description=\"Dataset directory.\",\n                      mandatory=True,\n                      example=\"dataset_full/\")\n\n    parser.add_option(name=\"-p\",\n                      type_value=\"str\",\n                      description=\"Arguments to pass to the function that is tested. Please put double-quotes if there are spaces in the list of parameters.\\n\"\n                                  \"Image paths must be contains in the arguments list.\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-json\",\n                      type_value=\"str\",\n                      description=\"Requirements on center, study, ... that must be satisfied by the json file of each tested subjects\\n\"\n                                  \"Syntax:  center=unf,study=errsm,gm_model=0\",\n                      mandatory=False)\n\n    parser.add_option(name=\"-cpu-nb\",\n                      type_value=\"int\",\n                      description=\"Number of CPU used for testing. 0: no multiprocessing. If not provided, \"\n                                  \"it uses all the available cores.\",\n                      mandatory=False,\n                      default_value=0,\n                      example='42')\n\n    parser.add_option(name=\"-log\",\n                      type_value='multiple_choice',\n                      description=\"Redirects Terminal verbose to log file.\",\n                      mandatory=False,\n                      example=['0', '1'],\n                      default_value='1')\n\n    parser.add_option(name=\"-v\",\n                      type_value=\"multiple_choice\",\n                      description=\"Verbose. 0: nothing, 1: basic, 2: extended.\",\n                      mandatory=False,\n                      example=['0', '1', '2'],\n                      default_value='1')\n\n    return parser\n\n\n# ====================================================================================================\n# Start program\n# ====================================================================================================\nif __name__ == \"__main__\":\n\n    # get parameters\n    parser = get_parser()\n    arguments = parser.parse(sys.argv[1:])\n    function_to_test = arguments[\"-f\"]\n    dataset = arguments[\"-d\"]\n    dataset = sct.slash_at_the_end(dataset, slash=1)\n    parameters = ''\n    if \"-p\" in arguments:\n        parameters = arguments[\"-p\"]\n    json_requirements = None\n    if \"-json\" in arguments:\n        json_requirements = arguments[\"-json\"]\n    nb_cpu = None\n    if \"-cpu-nb\" in arguments:\n        nb_cpu = arguments[\"-cpu-nb\"]\n    create_log = int(arguments['-log'])\n    verbose = arguments[\"-v\"]\n\n    # start timer\n    start_time = time()\n    # create single time variable for output names\n    output_time = strftime(\"%y%m%d%H%M%S\")\n    print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # build log file name\n    if create_log:\n        file_log = 'results_test_'+function_to_test+'_'+output_time\n        orig_stdout = sys.stdout\n        fname_log = file_log+'.log'\n        handle_log = file(fname_log, 'w')\n        # redirect to log file\n        sys.stdout = handle_log\n        print 'Testing started on: '+strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # get path of the toolbox\n    path_script = os.path.dirname(__file__)\n    path_sct = os.path.dirname(path_script)\n\n    # fetch true commit number and branch (do not use commit.txt which is wrong)\n    path_curr = os.path.abspath(os.curdir)\n    os.chdir(path_sct)\n    sct_commit = commands.getoutput('git rev-parse HEAD')\n    if not sct_commit.isalnum():\n        print 'WARNING: Cannot retrieve SCT commit'\n        sct_commit = 'unknown'\n        sct_branch = 'unknown'\n    else:\n        sct_branch = commands.getoutput('git branch --contains '+sct_commit).strip('* ')\n    # with open (path_sct+\"/version.txt\", \"r\") as myfile:\n    #     version_sct = myfile.read().replace('\\n', '')\n    # with open (path_sct+\"/commit.txt\", \"r\") as myfile:\n    #     commit_sct = myfile.read().replace('\\n', '')\n    print 'SCT commit/branch: '+sct_commit+'/'+sct_branch\n    os.chdir(path_curr)\n\n    # check OS\n    platform_running = sys.platform\n    if (platform_running.find('darwin') != -1):\n        os_running = 'osx'\n    elif (platform_running.find('linux') != -1):\n        os_running = 'linux'\n    print 'OS: '+os_running+' ('+platform.platform()+')'\n\n    # check hostname\n    print 'Hostname:', platform.node()\n\n    # Check number of CPU cores\n    from multiprocessing import cpu_count\n    # status, output = sct.run('echo $ITK_GLOBAL_DEFAULT_NUMBER_OF_THREADS', 0)\n    print 'CPU cores: ' + str(cpu_count())  # + ', Used by SCT: '+output\n\n    # check RAM\n    sct.checkRAM(os_running, 0)\n\n    # test function\n    try:\n        results = test_function(function_to_test, dataset, parameters, nb_cpu, json_requirements, verbose)\n        pd.set_option('display.max_rows', 500)\n        pd.set_option('display.max_columns', 500)\n        pd.set_option('display.width', 1000)\n        results_subset = results.drop('script', 1).drop('dataset', 1).drop('parameters', 1).drop('output', 1)\n        results_display = results_subset\n\n        # save panda structure\n        if create_log:\n            results_subset.to_pickle(file_log+'.pickle')\n\n        # mean\n        results_mean = results_subset[results_subset.status != 200].mean(numeric_only=True)\n        results_mean['subject'] = 'Mean'\n        results_mean.set_value('status', float('NaN'))  # set status to NaN\n        # results_display = results_display.append(results_mean, ignore_index=True)\n\n        # std\n        results_std = results_subset[results_subset.status != 200].std(numeric_only=True)\n        results_std['subject'] = 'STD'\n        results_std.set_value('status', float('NaN'))  # set status to NaN\n        # results_display = results_display.append(results_std, ignore_index=True)\n\n        # count tests that passed\n        count_passed = results_subset.status[results_subset.status == 0].count()\n        count_crashed = results_subset.status[results_subset.status == 1].count()\n        # count tests that ran\n        count_ran = results_subset.status[results_subset.status != 200].count()\n\n        # results_display = results_display.set_index('subject')\n        # jcohenadad, 2015-10-27: added .reset_index() for better visual clarity\n        results_display = results_display.set_index('subject').reset_index()\n\n        print '\\nCommand: \"' + function_to_test + ' ' + parameters\n        print 'Dataset: ' + dataset\n        # display general results\n        print '\\nGLOBAL RESULTS:'\n        elapsed_time = time() - start_time\n        print 'Duration: ' + str(int(round(elapsed_time)))+'s'\n        # display results\n        print 'Passed: ' + str(count_passed) + '/' + str(count_ran)\n        print 'Crashed: ' + str(count_crashed) + '/' + str(count_ran)\n        # build mean/std entries\n        dict_mean = results_mean.to_dict()\n        dict_mean.pop('status')\n        dict_mean.pop('subject')\n        print 'Mean: ' + str(dict_mean)\n        dict_std = results_std.to_dict()\n        dict_std.pop('status')\n        dict_std.pop('subject')\n        print 'STD: ' + str(dict_std)\n\n        # print detailed results\n        print '\\nDETAILED RESULTS:'\n        print results_display.to_string()\n        print 'Status: 0: Passed | 1: Crashed | 99: Failed | 200: Input file(s) missing | 201: Ground-truth file(s) missing'\n\n    except Exception as err:\n        print err\n\n    # stop file redirection\n    if create_log:\n        sys.stdout.close()\n        sys.stdout = orig_stdout\n        # display log file to Terminal\n        handle_log = file(fname_log, 'r')\n        print handle_log.read()", "original_line_number": 30659, "instruction_length": 966}
{"instruction": "The given code contains functions to extract features from images and detect cars in a video using a linear support vector machine (SVM) classifier. To use this code, you need to provide a list of vehicle and non-vehicle images, and then call the `extract_features` function to extract features from these images. After that, you can train a linear SVM classifier using the extracted features and labels. Once the classifier is trained, you can use the `process_image` function to detect cars in a video. The output video will be saved in the specified location.", "input": "", "output": "import matplotlib.image as mpimg\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport cv2\r\nimport glob\r\nfrom skimage.feature import hog\r\nfrom skimage import color, exposure\r\nimport random\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.preprocessing import StandardScaler\r\nfrom sklearn.svm import LinearSVC\r\nimport time\r\nfrom moviepy.editor import VideoFileClip \r\nfrom scipy.ndimage.measurements import label\r\nfrom IPython.display import HTML\r\n\r\ndef load_data(my_list):\r\n    new_list = []\r\n    for image in my_list:\r\n        img = cv2.imread(image)\r\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\r\n        new_list.append(img)\r\n    return new_list\r\n\r\n# Define a function to return HOG features and visualization\r\ndef get_hog_features(img, orient, pix_per_cell, cell_per_block, vis=False, feature_vec=True):\r\n    if vis == True:\r\n        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\r\n                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \r\n                                  visualise=True, feature_vector=False)\r\n        return features, hog_image\r\n    else:      \r\n        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\r\n                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=False, \r\n                       visualise=False, feature_vector=feature_vec)\r\n        return features\r\n\r\n\r\ndef bin_spatial(img, size=(32, 32)):\r\n    color1 = cv2.resize(img[:,:,0], size).ravel()\r\n    color2 = cv2.resize(img[:,:,1], size).ravel()\r\n    color3 = cv2.resize(img[:,:,2], size).ravel()\r\n    return np.hstack((color1, color2, color3))\r\n\r\ndef color_hist(img, nbins=32):    #bins_range=(0, 256)\r\n    # Compute the histogram of the color channels separately\r\n    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\r\n    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\r\n    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\r\n    # Concatenate the histograms into a single feature vector\r\n    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\r\n    # Return the individual histograms, bin_centers and feature vector\r\n    return hist_features\r\n            \r\n# Define a function to extract features from a list of images\r\n# Have this function call bin_spatial() and color_hist()\r\ndef extract_features(imgs, \r\n                     color_space='RGB', \r\n                     spatial_size=(32, 32),\r\n                     hist_bins=32, orient=9, \r\n                     pix_per_cell=8, \r\n                     cell_per_block=2,\r\n                     hog_channel=0,\r\n                    spatial_feat=True, \r\n                     hist_feat=True, \r\n                     hog_feat=True):\r\n    # Create a list to append feature vectors to\r\n    features = []\r\n    # Iterate through the list of images\r\n    for image in imgs:\r\n        file_features = []\r\n        # Read in each one by one\r\n        #image = mpimg.imread(file)\r\n        # apply color conversion if other than 'RGB'\r\n        if color_space != 'RGB':\r\n            if color_space == 'HSV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\r\n            elif color_space == 'LUV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\r\n            elif color_space == 'HLS':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\r\n            elif color_space == 'YUV':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\r\n            elif color_space == 'YCrCb':\r\n                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\r\n        else: feature_image = np.copy(image)      \r\n\r\n        if spatial_feat == True:\r\n            spatial_features = bin_spatial(feature_image, size=spatial_size)\r\n            file_features.append(spatial_features)\r\n        if hist_feat == True:\r\n            # Apply color_hist()\r\n            hist_features = color_hist(feature_image, nbins=hist_bins)\r\n            file_features.append(hist_features)\r\n        if hog_feat == True:\r\n        # Call get_hog_features() with vis=False, feature_vec=True\r\n            if hog_channel == 'ALL':\r\n                hog_features = []\r\n                for channel in range(feature_image.shape[2]):\r\n                    hog_features.append(get_hog_features(feature_image[:,:,channel], \r\n                                        orient, pix_per_cell, cell_per_block, \r\n                                        vis=False, feature_vec=True))\r\n                hog_features = np.ravel(hog_features)        \r\n            else:\r\n                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \r\n                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\r\n            # Append the new feature vector to the features list\r\n            file_features.append(hog_features)\r\n        features.append(np.concatenate(file_features))\r\n    # Return list of feature vectors\r\n    return features\r\n\r\ndef add_heat(heatmap, bbox_list):\r\n    # Iterate through list of bboxes\r\n    for box in bbox_list:\r\n        # Add += 1 for all pixels inside each bbox\r\n        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\r\n        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\r\n\r\n    # Return updated heatmap\r\n    return heatmap\r\n\r\ndef apply_threshold(heatmap, threshold):\r\n    # Zero out pixels below the threshold\r\n    heatmap[heatmap <= threshold] = 0\r\n    # Return thresholded map\r\n    return heatmap\r\n\r\ndef draw_labeled_bboxes(img, labels):\r\n    # Iterate through all detected cars\r\n    for car_number in range(1, labels[1]+1):\r\n        # Find pixels with each car_number label value\r\n        nonzero = (labels[0] == car_number).nonzero()\r\n        # Identify x and y values of those pixels\r\n        nonzeroy = np.array(nonzero[0])\r\n        nonzerox = np.array(nonzero[1])\r\n        # Define a bounding box based on min/max x and y\r\n        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\r\n        # Draw the box on the image\r\n        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\r\n    # Return the image\r\n    return img\r\n\r\nimg_boxes = []\r\ndef convert_color(img, conv='RGB2YCrCb'):\r\n    if conv == 'RGB2YCrCb':\r\n        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\r\n    if conv == 'BGR2YCrCb':\r\n        return cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\r\n    if conv == 'RGB2LUV':\r\n        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\r\n\r\n# Define a single function that can extract features using hog sub-sampling and make predictions\r\ndef find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):\r\n    \r\n    draw_img = np.copy(img)\r\n    #img = img.astype(np.float32)/255\r\n    \r\n    heat_map = np.zeros_like(img[:,:,0]).astype(np.float)\r\n    \r\n    \r\n    img_tosearch = img[ystart:ystop,:,:]\r\n    ctrans_tosearch = convert_color(img_tosearch, conv='RGB2YCrCb')\r\n    if scale != 1:\r\n        imshape = ctrans_tosearch.shape\r\n        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\r\n        \r\n    ch1 = ctrans_tosearch[:,:,0]\r\n    ch2 = ctrans_tosearch[:,:,1]\r\n    ch3 = ctrans_tosearch[:,:,2]\r\n\r\n    # Define blocks and steps as above\r\n    nxblocks = (ch1.shape[1] // pix_per_cell) - cell_per_block + 1\r\n    nyblocks = (ch1.shape[0] // pix_per_cell) - cell_per_block + 1 \r\n    nfeat_per_block = orient*cell_per_block**2\r\n    \r\n    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\r\n    window = 64\r\n    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\r\n    cells_per_step = 2  # Instead of overlap, define how many cells to step\r\n    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\r\n    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\r\n    \r\n    # Compute individual channel HOG features for the entire image\r\n    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\r\n    \r\n    for xb in range(nxsteps):\r\n        for yb in range(nysteps):\r\n            ypos = yb*cells_per_step\r\n            xpos = xb*cells_per_step\r\n            # Extract HOG for this patch\r\n            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \r\n            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\r\n\r\n            xleft = xpos*pix_per_cell\r\n            ytop = ypos*pix_per_cell\r\n\r\n            # Extract the image patch\r\n            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\r\n          \r\n            # Get color features\r\n            spatial_features = bin_spatial(subimg, size=spatial_size)\r\n            hist_features = color_hist(subimg, nbins=hist_bins)\r\n\r\n            # Scale features and make a prediction\r\n            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \r\n            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \r\n            test_prediction = svc.predict(test_features)\r\n            \r\n            if test_prediction == 1:\r\n                xbox_left = np.int(xleft*scale)\r\n                ytop_draw = np.int(ytop*scale)\r\n                win_draw = np.int(window*scale)\r\n                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6)\r\n                img_boxes.append(((xbox_left, ytop_draw+ystart),(xbox_left+win_draw, ytop_draw+win_draw+ystart)))\r\n                heat_map[ytop_draw+ystart:ytop_draw+win_draw+ystart, xbox_left:xbox_left+win_draw] +=1\r\n                \r\n    return draw_img, heat_map\r\n    \r\ndef process_image(img):\r\n    # Find final boxes from heatmap using label function\r\n    out_img, heatmap = find_cars(img, \r\n                      ystart=YSTART, \r\n                      ystop=YSTOP,\r\n                      scale=SCALE, \r\n                      svc = SVC, \r\n                      X_scaler = X_scaler,\r\n                      orient= ORIENTATION, \r\n                      pix_per_cell = PIX_PER_CELL,\r\n                      cell_per_block= CELL_PER_BLOCK, \r\n                      spatial_size = SPATIAL_SIZE,\r\n                      hist_bins = HIST_BINS)\r\n    labels = label(heatmap)\r\n    draw_img = draw_labeled_bboxes(np.copy(img), labels)\r\n    return draw_img   \r\n\r\nif __name__ == \"__main__\":\r\n\r\n    vehicles_images = glob.glob('../../../vehicles/vehicles/*/*.png')\r\n    non_vehicles_images = glob.glob('../../../non-vehicles/non-vehicles/*/*.png')\r\n    cars = load_data(vehicles_images)\r\n    non_cars = load_data(non_vehicles_images)\r\n\r\n        \"\"\"Parameters\"\"\"\r\n    COLOR_SPACE = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\r\n    ORIENTATION = 9  # HOG orientations\r\n    PIX_PER_CELL = 8 # HOG pixels per cell\r\n    CELL_PER_BLOCK = 2 # HOG cells per block\r\n    HOG_CHANNEL = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\r\n    SPATIAL_SIZE = (16, 16) # Spatial binning dimensions\r\n    HIST_BINS = 16    # Number of histogram bins\r\n    IS_SPATIAL_FEAT = True # Spatial features on or off\r\n    IS_HIST_FEAT = True # Histogram features on or off\r\n    IS_HOG_FEAT = True # HOG features on or off\r\n\r\n    t=time.time()\r\n\r\n    car_features = extract_features(cars, \r\n                                    color_space = COLOR_SPACE, \r\n                                    spatial_size= SPATIAL_SIZE, \r\n                                    hist_bins = HIST_BINS, \r\n                                    orient = ORIENTATION, \r\n                                    pix_per_cell = PIX_PER_CELL, \r\n                                    cell_per_block = CELL_PER_BLOCK, \r\n                                    hog_channel = HOG_CHANNEL,\r\n                                    spatial_feat = IS_SPATIAL_FEAT , \r\n                                    hist_feat = IS_HIST_FEAT,\r\n                                    hog_feat = IS_HOG_FEAT)\r\n\r\n    notcar_features = extract_features(non_cars, \r\n                                    color_space = COLOR_SPACE, \r\n                                    spatial_size= SPATIAL_SIZE, \r\n                                    hist_bins = HIST_BINS, \r\n                                    orient = ORIENTATION, \r\n                                    pix_per_cell = PIX_PER_CELL, \r\n                                    cell_per_block = CELL_PER_BLOCK, \r\n                                    hog_channel = HOG_CHANNEL,\r\n                                    spatial_feat = IS_SPATIAL_FEAT , \r\n                                    hist_feat = IS_HIST_FEAT,\r\n                                    hog_feat = IS_HOG_FEAT)\r\n\r\n    print(time.time()-t, 'Seconds to compute features...')\r\n\r\n\r\n    X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \r\n    # Fit a per-column scaler\r\n    X_scaler = StandardScaler().fit(X)\r\n    # Apply the scaler to X\r\n    scaled_X = X_scaler.transform(X)\r\n\r\n    # Define the labels vector\r\n    y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\r\n\r\n\r\n    # Split up data into randomized training and test sets\r\n    rand_state = np.random.randint(0, 100)\r\n    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)\r\n\r\n    print('Using:',orient,'orientations',pix_per_cell,'pixels per cell and', cell_per_block,'cells per block')\r\n    print('Feature vector length:', len(X_train[0]))\r\n    # Use a linear SVC \r\n    SVC = LinearSVC()\r\n    # Check the training time for the SVC\r\n    SVC.fit(X_train, y_train)\r\n    t2 = time.time()\r\n    print(round(t2-t, 2), 'Seconds to train SVC...')\r\n    # Check the score of the SVC\r\n    print('Test Accuracy of SVC = ', round(SVC.score(X_test, y_test), 4))\r\n\r\n    clip1 = VideoFileClip('../project_video.mp4')\r\n    video_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\r\n\r\n    video_output = '../output_videos/project_video.mp4'", "original_line_number": 30805, "instruction_length": 562}
{"instruction": "To run a VRP problem, do the following:\n1. Import TsplibParser, ArgParser, VRPCenter, TspPainter, and logging modules.\n2. Construct a logger with a log level of INFO and a log formatter.\n3. Define a function called \"run\" that takes a TsplibParser object as an argument.\n4. Inside the \"run\" function, create a VRPCenter object with the TsplibParser object as a parameter.\n5. Log the coordinates of each node in the TsplibParser object.\n6. Set the coordinate matrix of the TspPainter object to the cities_coord attribute of the TsplibParser object and draw the map.\n7. Log the locations of each locker in the TsplibParser object and draw the lockers on the map.\n8. Log the locations and demands of each delivery point in the TsplibParser object and calculate the total demand.\n9. Start the VRPCenter object.\n10. Define a main function that parses command line arguments using the ArgParser module and reads a TSP file using the TsplibParser module.\n11. Log the problem formulation information.\n12. Call the \"run\" function with the TsplibParser object as an argument.\n13. If running as the main module, call the main function.", "input": "", "output": "from TsplibParser import parser as tspparser\nfrom ArgParser import parser as argparser\nfrom VRPCenter import VRPCenter\nfrom TspPainter import tspPainter\nimport logging\n\n# construct the logger\nlogger = logging.getLogger(\"logger\")\nlogger.setLevel(logging.INFO)\nlogFormatter = logging.Formatter(\"%(asctime)s [%(threadName)s] [%(levelname)s]  %(message)s\")\nconsoleHandler = logging.StreamHandler()\nconsoleHandler.setFormatter(logFormatter)\nlogger.addHandler(consoleHandler)\n\ndef run(tspparser):\n    center = VRPCenter(tspparser)\n    logger.info(\"Nodes: \")\n    for i in range(1, len(tspparser.cities_coord)):\n        logger.info(\"Node \" + str(i) + \" coordinate is \" + str(tspparser.cities_coord[i][0]) + \", \" + str(tspparser.cities_coord[i][1]))\n    tspPainter.coord_mat = tspparser.cities_coord\n    tspPainter.drawMap()\n\n    logger.info(\"Lockers: \")\n    for i in range(0, len(tspparser.lockers)):\n        logger.info(tspparser.lockers[i])\n    tspPainter.drawLockers(tspparser.lockers)\n\n    logger.info(\"Delivers: \")\n    for i in range(0, len(tspparser.delivers)):\n        logger.info(tspparser.delivers[i])\n    logger.info(\"Demands: \")\n    demands = 0\n    for i in range(0, len(tspparser.demands)):\n        demands += tspparser.demands[i]\n        logger.info(\"Node {} {}\".format(i, tspparser.demands[i]))\n    logger.info(\"Total demands is: {}\".format(demands))\n\n    center.start()\n\ndef main():\n    args = argparser.parse_args()\n    tspparser.read_file(args.tsp_file[0])\n\n    logger.info(\"-------------------------------------------\")\n    logger.info(\"Problem formulation information\")\n    logger.info(\"-------------------------------------------\")\n    logger.info(\"Name: \" + tspparser.name)\n    logger.info(\"Comment: \" + tspparser.comment)\n    logger.info(\"Type: \" + tspparser.type)\n    # run vrp center\n    run(tspparser)\n\nif __name__ == \"__main__\":\n    main()", "original_line_number": 31315, "instruction_length": 1123}
{"instruction": "This code contains unit tests for the CollectionQuery class. To use this code, create a new Python file and import the unittest and CollectionQuery classes. Then, create a new TestCollectionClass class that inherits from unittest.TestCase. Within this class, define test methods that call methods of the CollectionQuery class and use assertions to verify that the expected results are returned. Use the test_archive_center method to test the archive_center method of the CollectionQuery class, the test_keyword method to test the keyword method, the test_valid_formats method to test the format method with valid formats, the test_invalid_format method to test the format method with invalid formats, the test_valid_concept_id method to test the concept_id method with valid concept IDs, and the test_invalid_concept_id method to test the concept_id method with invalid concept IDs.", "input": "", "output": "import unittest\n\nfrom cmr.queries import CollectionQuery\n\nclass TestCollectionClass(unittest.TestCase):\n\n    def test_archive_center(self):\n        query = CollectionQuery()\n        query.archive_center(\"LP DAAC\")\n\n        self.assertIn(\"archive_center\", query.params)\n        self.assertEqual(query.params[\"archive_center\"], \"LP DAAC\")\n    \n    def test_keyword(self):\n        query = CollectionQuery()\n        query.keyword(\"AST_*\")\n\n        self.assertIn(\"keyword\", query.params)\n        self.assertEqual(query.params[\"keyword\"], \"AST_*\")\n    \n    def test_valid_formats(self):\n        query = CollectionQuery()\n        formats = [\n            \"json\", \"xml\", \"echo10\", \"iso\", \"iso19115\",\n            \"csv\", \"atom\", \"kml\", \"native\", \"dif\", \"dif10\",\n            \"opendata\", \"umm_json\", \"umm_json_v1_1\" \"umm_json_v1_9\"]\n\n        for _format in formats:\n            query.format(_format)\n            self.assertEqual(query._format, _format)\n    \n    def test_invalid_format(self):\n        query = CollectionQuery()\n\n        with self.assertRaises(ValueError):\n            query.format(\"invalid\")\n            query.format(\"jsonn\")\n            query.format(\"iso19116\")\n    \n    def test_valid_concept_id(self):\n        query = CollectionQuery()\n\n        query.concept_id(\"C1299783579-LPDAAC_ECS\")\n        self.assertEqual(query.params[\"concept_id\"], [\"C1299783579-LPDAAC_ECS\"])\n        \n        query.concept_id([\"C1299783579-LPDAAC_ECS\", \"C1441380236-PODAAC\"])\n        self.assertEqual(query.params[\"concept_id\"], [\"C1299783579-LPDAAC_ECS\", \"C1441380236-PODAAC\"])\n    \n    def test_invalid_concept_id(self):\n        query = CollectionQuery()\n\n        with self.assertRaises(ValueError):\n            query.concept_id(\"G1327299284-LPDAAC_ECS\")\n        \n        with self.assertRaises(ValueError):\n            query.concept_id([\"C1299783579-LPDAAC_ECS\", \"G1327299284-LPDAAC_ECS\"])", "original_line_number": 31585, "instruction_length": 882}
{"instruction": "Create a class called RelatedManagerListSerializerField that inherits from ListSerializerField. The class should have an __init__ method that takes in a serializer, sort_by, and use_prefetch as arguments. The class should also have a pre_value method that takes in fields and exclude as arguments, and a set_value method that takes in a value as an argument. The set_value method should check if the value is None, an Iterable, a QuerySet, or a Manager, and then set the items, _native_items, and _python_items attributes accordingly. Finally, the set_value method should loop through the values and add each item to the class using the add_item method.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom collections import Iterable\ntry:\n    from django.db.models.query import QuerySet\n    from django.db.models import Manager\nexcept ImportError:\n    QuerySet = None\n    Manager = None\n\nfrom aserializer.fields import ListSerializerField\nfrom aserializer.django.utils import get_local_fields, get_related_fields\n\n\nclass RelatedManagerListSerializerField(ListSerializerField):\n\n    def __init__(self, serializer, sort_by=None, use_prefetch=False, *args, **kwargs):\n        super(RelatedManagerListSerializerField, self).__init__(serializer=serializer, sort_by=sort_by, *args, **kwargs)\n        self.use_prefetch = use_prefetch\n\n    def pre_value(self, fields=None, exclude=None, **extras):\n        super(RelatedManagerListSerializerField, self).pre_value(fields=fields, exclude=exclude, **extras)\n        self.use_prefetch = extras.get('use_prefetch', self.use_prefetch)\n\n    def set_value(self, value):\n        if value is None:\n            return\n        elif isinstance(value, Iterable):\n            values = value\n        elif isinstance(value, (QuerySet, Manager)):\n            # if using prefetch_related, we can't use only as it will re-fetch the data\n            if not self.use_prefetch and (self.only_fields or self.exclude):\n                local_fields = get_local_fields(value.model)\n                related_fields = get_related_fields(value.model)\n                only_fields = [f.name for f in local_fields]\n                if self.only_fields:\n                    only_fields = [f for f in only_fields if f in self.only_fields]\n                exclude_fields = [f.name for f in local_fields if f.name in self.exclude]\n                if exclude_fields:\n                    only_fields = [f for f in only_fields if f not in exclude_fields]\n                only_fields += [f.name for f in related_fields]\n                # .only() returns a QuerySet of RelatedDjangoModel_Deferred objects?\n                values = value.only(*only_fields)\n            else:\n                values = value.all()\n        else:\n            return\n        self.items[:] = []\n        self._native_items[:] = []\n        self._python_items[:] = []\n        for item in values:\n            self.add_item(source=item)", "original_line_number": 31675, "instruction_length": 653}
{"instruction": "Create a QuerySet class that adds extra functionality to a basic list. It should hold instances of a single type of JSSObject and use the python list API while adding some extra helper-methods on top. The class should have methods to sort by ID and name, retrieve all contained objects' data from the JSS, save all contained objects' data to the JSS, invalidate the cache datetime for all contents, and return generators of contents' names and IDs. It should also have a class method to build a QuerySet from a listing Response.", "input": "", "output": "#!/usr/bin/env python\n# Copyright (C) 2014-2017 Shea G Craig\n#\n# This program is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"queryset.py\n\nClass that adds some extra functionality to a basic list. Used as the\nresult of all queries in python-jss.\n\"\"\"\n\n\nfrom __future__ import absolute_import\nfrom collections import defaultdict\ntry:\n    import cPickle  # Python 2.X\nexcept ImportError:\n    import _pickle as cPickle  # Python 3+\nimport datetime\nimport os\n\nfrom .jssobject import DATE_FMT, Identity\n\n\nSTR_FMT = \"{0:>{1}} | {2:>{3}} | {4:>{5}}\"\n\n\nclass QuerySet(list):\n    \"\"\"A list style collection of JSSObjects.\n\n    Listing operations retrieve minimal or overview information for most\n    object types. For example, we may want to see all the Computers on\n    the JSS but that does not mean we want to do a full object GET for\n    each one.\n\n    QuerySets hold instances of a single type of JSSObject, and use the\n    python list API, while adding some extra helper-methods on top.\n    \"\"\"\n\n    def __init__(self, objects):\n        \"\"\"Construct a list of JSSObjects.\n\n        Args:\n            objects (sequence of JSSObjects):\n                Sequences must be of a single class.\n        \"\"\"\n        if objects and not len({i.__class__ for i in objects}) == 1:\n            raise ValueError\n        super(QuerySet, self).__init__(objects)\n        self.sort()\n        self.contained_class = objects[0].__class__ if objects else None\n\n    def __str__(self):\n        \"\"\"Make data human readable.\"\"\"\n        # Make ID, Name first, no matter what.\n        sort_keys = [\"id\", \"name\"]\n        if self:\n            sort_keys.extend([\n                key for key in self[0]._basic_identity.keys() if\n                key not in sort_keys])\n\n        # Build a dict of max lengths per column for table output.\n        lengths = defaultdict(int)\n        for item in self:\n            for key in sort_keys:\n                val = item._basic_identity[key] or \"\"\n                length = max(len(key), len(val))\n                if length > lengths[key]:\n                    lengths[key] = length\n\n        # Build a format string for row output.\n        format_strings = []\n        for key in sort_keys:\n            length = lengths[key]\n            format_strings.append(\"{{data[{}]:>{}}}\".format(key, length))\n\n        cached = 'cached'\n        cached_format = '| {{cached:>{}}} |'.format(len(cached))\n\n        fmt = \"| \" + \" | \".join(format_strings) + cached_format\n\n        # Begin building output with header lines.\n        # Contained class can be None\n        contained_name = self.contained_class.__name__ if self.contained_class is not None else \"Empty\"\n        results = [\"{} QuerySet\".format(contained_name)]\n        headers = {key: key for key in lengths}\n        header_line = fmt.format(data=headers, cached=\"cached\")\n        bar = len(header_line) * '-'\n        results.extend([bar, header_line, bar])\n\n        str_cached = (\n            lambda i: str(i.cached) if isinstance(i.cached, bool) else 'True')\n\n        table = [\n            fmt.format(data=item._basic_identity, cached=str_cached(item)) for\n            item in self]\n        results.extend(table)\n\n        results.append(bar)\n        return \"\\n\".join(results)\n\n    def __repr__(self):\n        \"\"\"Make data human readable.\"\"\"\n        return \"QuerySet({})\".format(super(QuerySet, self).__repr__())\n\n    def sort(self, **kwargs):\n        \"\"\"Sort list elements by ID.\n        \"\"\"\n        super(QuerySet, self).sort(key=lambda k: int(k.id))\n\n    def sort_by_name(self):\n        \"\"\"Sort list elements by name.\"\"\"\n        super(QuerySet, self).sort(key=lambda k: k.name.upper())\n\n    def retrieve_all(self):\n        \"\"\"Tell each contained object to retrieve its data from the JSS\n\n        This can take a long time given a large number of objects,\n        and depending on the size of each object.\n\n        Returns:\n            self (QuerySet) to allow method chaining.\n        \"\"\"\n        for obj in self:\n            if not obj.cached:\n                obj.retrieve()\n\n        return self\n\n    def save_all(self):\n        \"\"\"Tell each contained object to save its data to the JSS\n\n        This can take a long time given a large number of objects,\n        and depending on the size of each object.\n\n        Returns:\n            self (QuerySet) to allow method chaining.\n        \"\"\"\n        for obj in self:\n            obj.save()\n\n        return self\n\n    def invalidate(self):\n        \"\"\"Clear the cache datetime for all contents.\n\n        This causes objects to retrieve their data again when accessed.\n        \"\"\"\n        for i in self: i.cached = False\n\n    def names(self):\n        \"\"\"Return a generator of contents names\"\"\"\n        return (item.name for item in self)\n\n    def ids(self):\n        \"\"\"Return a generator of contents ids\"\"\"\n        return (item.id for item in self)\n\n    @classmethod\n    def from_response(cls, obj_class, response, jss=None, **kwargs):\n        \"\"\"Build a QuerySet from a listing Response.\"\"\"\n        response_objects = (\n            i for i in response if i is not None and i.tag != \"size\")\n\n        dicts = (\n            {child.tag: child.text for child in item} for item in\n            response_objects)\n        identities = (Identity(d) for d in dicts)\n\n        objects = [obj_class(jss, data=i, **kwargs) for i in identities]\n\n        return cls(objects)", "original_line_number": 31887, "instruction_length": 528}
{"instruction": "This code creates archives compatible with ESP32-S* ROM DFU implementation. To use it, run the script with the \"write\" command and provide the following arguments:\n- \"-o/--output-file\": the filename for storing the output DFU image (required)\n- \"--pid\": the hexadecimal product identifier (required)\n- \"--json\": an optional file for loading a \"flash_files\" dictionary with <address> <file> items\n- \"--part-size\": the partition size for larger files (default is 512KB)\n- \"files\": a list of <address> <file> pairs to be added to the DFU image. The files are split up into smaller partitions of size \"part-size\" to avoid timing-out during erasing large regions.", "input": "", "output": "#!/usr/bin/env python\n#\n# Copyright 2020-2021 Espressif Systems (Shanghai) CO LTD\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# This program creates archives compatible with ESP32-S* ROM DFU implementation.\n#\n# The archives are in CPIO format. Each file which needs to be flashed is added to the archive\n# as a separate file. In addition to that, a special index file, 'dfuinfo0.dat', is created.\n# This file must be the first one in the archive. It contains binary structures describing each\n# subsequent file (for example, where the file needs to be flashed/loaded).\n\nfrom __future__ import print_function, unicode_literals\n\nimport argparse\nimport hashlib\nimport json\nimport os\nimport struct\nimport zlib\nfrom collections import namedtuple\nfrom functools import partial\n\nfrom future.utils import iteritems\n\ntry:\n    import typing\nexcept ImportError:\n    # Only used for type annotations\n    pass\n\ntry:\n    from itertools import izip as zip  # type: ignore\nexcept ImportError:\n    # Python 3\n    pass\n\n# CPIO (\"new ASCII\") format related things\nCPIO_MAGIC = b'070701'\nCPIO_STRUCT = b'=6s' + b'8s' * 13\nCPIOHeader = namedtuple(\n    'CPIOHeader',\n    [\n        'magic',\n        'ino',\n        'mode',\n        'uid',\n        'gid',\n        'nlink',\n        'mtime',\n        'filesize',\n        'devmajor',\n        'devminor',\n        'rdevmajor',\n        'rdevminor',\n        'namesize',\n        'check',\n    ],\n)\nCPIO_TRAILER = 'TRAILER!!!'\n\n\ndef make_cpio_header(\n    filename_len, file_len, is_trailer=False\n):  # type: (int, int, bool) -> CPIOHeader\n    \"\"\" Returns CPIOHeader for the given file name and file size \"\"\"\n\n    def as_hex(val):  # type: (int) -> bytes\n        return '{:08x}'.format(val).encode('ascii')\n\n    hex_0 = as_hex(0)\n    mode = hex_0 if is_trailer else as_hex(0o0100644)\n    nlink = as_hex(1) if is_trailer else hex_0\n    return CPIOHeader(\n        magic=CPIO_MAGIC,\n        ino=hex_0,\n        mode=mode,\n        uid=hex_0,\n        gid=hex_0,\n        nlink=nlink,\n        mtime=hex_0,\n        filesize=as_hex(file_len),\n        devmajor=hex_0,\n        devminor=hex_0,\n        rdevmajor=hex_0,\n        rdevminor=hex_0,\n        namesize=as_hex(filename_len),\n        check=hex_0,\n    )\n\n\n# DFU format related things\n# Structure of one entry in dfuinfo0.dat\nDFUINFO_STRUCT = b'<I I 64s 16s'\nDFUInfo = namedtuple('DFUInfo', ['address', 'flags', 'name', 'md5'])\nDFUINFO_FILE = 'dfuinfo0.dat'\n# Structure which gets added at the end of the entire DFU file\nDFUSUFFIX_STRUCT = b'<H H H H 3s B'\nDFUSuffix = namedtuple(\n    'DFUSuffix', ['bcd_device', 'pid', 'vid', 'bcd_dfu', 'sig', 'len']\n)\nESPRESSIF_VID = 12346\n# This CRC32 gets added after DFUSUFFIX_STRUCT\nDFUCRC_STRUCT = b'<I'\n\n\ndef dfu_crc(data, crc=0):  # type: (bytes, int) -> int\n    \"\"\" Calculate CRC32/JAMCRC of data, with an optional initial value \"\"\"\n    uint32_max = 0xFFFFFFFF\n    return uint32_max - (zlib.crc32(data, crc) & uint32_max)\n\n\ndef pad_bytes(b, multiple, padding=b'\\x00'):  # type: (bytes, int, bytes) -> bytes\n    \"\"\" Pad 'b' to a length divisible by 'multiple' \"\"\"\n    padded_len = (len(b) + multiple - 1) // multiple * multiple\n    return b + padding * (padded_len - len(b))\n\n\nclass EspDfuWriter(object):\n    def __init__(self, dest_file, pid, part_size):  # type: (typing.BinaryIO, int, int) -> None\n        self.dest = dest_file\n        self.pid = pid\n        self.part_size = part_size\n        self.entries = []  # type: typing.List[bytes]\n        self.index = []  # type: typing.List[DFUInfo]\n\n    def add_file(self, flash_addr, path):  # type: (int, str) -> None\n        \"\"\"\n        Add file to be written into flash at given address\n\n        Files are split up into chunks in order avoid timing-out during erasing large regions. Instead of adding\n        \"app.bin\" at flash_addr it will add:\n        1. app.bin   at flash_addr  # sizeof(app.bin) == self.part_size\n        2. app.bin.1 at flash_addr + self.part_size\n        3. app.bin.2 at flash_addr + 2 * self.part_size\n        ...\n\n        \"\"\"\n        f_name = os.path.basename(path)\n        with open(path, 'rb') as f:\n            for i, chunk in enumerate(iter(partial(f.read, self.part_size), b'')):\n                n = f_name if i == 0 else '.'.join([f_name, str(i)])\n                self._add_cpio_flash_entry(n, flash_addr, chunk)\n                flash_addr += len(chunk)\n\n    def finish(self):  # type: () -> None\n        \"\"\" Write DFU file \"\"\"\n        # Prepare and add dfuinfo0.dat file\n        dfuinfo = b''.join([struct.pack(DFUINFO_STRUCT, *item) for item in self.index])\n        self._add_cpio_entry(DFUINFO_FILE, dfuinfo, first=True)\n\n        # Add CPIO archive trailer\n        self._add_cpio_entry(CPIO_TRAILER, b'', trailer=True)\n\n        # Combine all the entries and pad the file\n        out_data = b''.join(self.entries)\n        cpio_block_size = 10240\n        out_data = pad_bytes(out_data, cpio_block_size)\n\n        # Add DFU suffix and CRC\n        dfu_suffix = DFUSuffix(0xFFFF, self.pid, ESPRESSIF_VID, 0x0100, b'UFD', 16)\n        out_data += struct.pack(DFUSUFFIX_STRUCT, *dfu_suffix)\n        out_data += struct.pack(DFUCRC_STRUCT, dfu_crc(out_data))\n\n        # Finally write the entire binary\n        self.dest.write(out_data)\n\n    def _add_cpio_flash_entry(\n        self, filename, flash_addr, data\n    ):  # type: (str, int, bytes) -> None\n        md5 = hashlib.md5()\n        md5.update(data)\n        self.index.append(\n            DFUInfo(\n                address=flash_addr,\n                flags=0,\n                name=filename.encode('utf-8'),\n                md5=md5.digest(),\n            )\n        )\n        self._add_cpio_entry(filename, data)\n\n    def _add_cpio_entry(\n        self, filename, data, first=False, trailer=False\n    ):  # type: (str, bytes, bool, bool) -> None\n        filename_b = filename.encode('utf-8') + b'\\x00'\n        cpio_header = make_cpio_header(len(filename_b), len(data), is_trailer=trailer)\n        entry = pad_bytes(\n            struct.pack(CPIO_STRUCT, *cpio_header) + filename_b, 4\n        ) + pad_bytes(data, 4)\n        if not first:\n            self.entries.append(entry)\n        else:\n            self.entries.insert(0, entry)\n\n\ndef action_write(args):  # type: (typing.Mapping[str, typing.Any]) -> None\n    writer = EspDfuWriter(args['output_file'], args['pid'], args['part_size'])\n    for addr, f in args['files']:\n        print('Adding {} at {:#x}'.format(f, addr))\n        writer.add_file(addr, f)\n    writer.finish()\n    print('\"{}\" has been written. You may proceed with DFU flashing.'.format(args['output_file'].name))\n    if args['part_size'] % (4 * 1024) != 0:\n        print('WARNING: Partition size of DFU is not multiple of 4k (4096). You might get unexpected behavior.')\n\n\ndef main():  # type: () -> None\n    parser = argparse.ArgumentParser()\n\n    # Provision to add \"info\" command\n    subparsers = parser.add_subparsers(dest='command')\n    write_parser = subparsers.add_parser('write')\n    write_parser.add_argument('-o', '--output-file',\n                              help='Filename for storing the output DFU image',\n                              required=True,\n                              type=argparse.FileType('wb'))\n    write_parser.add_argument('--pid',\n                              required=True,\n                              type=lambda h: int(h, 16),\n                              help='Hexa-decimal product indentificator')\n    write_parser.add_argument('--json',\n                              help='Optional file for loading \"flash_files\" dictionary with <address> <file> items')\n    write_parser.add_argument('--part-size',\n                              default=os.environ.get('ESP_DFU_PART_SIZE', 512 * 1024),\n                              type=lambda x: int(x, 0),\n                              help='Larger files are split-up into smaller partitions of this size')\n    write_parser.add_argument('files',\n                              metavar='<address> <file>', help='Add <file> at <address>',\n                              nargs='*')\n\n    args = parser.parse_args()\n\n    def check_file(file_name):  # type: (str) -> str\n        if not os.path.isfile(file_name):\n            raise RuntimeError('{} is not a regular file!'.format(file_name))\n        return file_name\n\n    files = []\n    if args.files:\n        files += [(int(addr, 0), check_file(f_name)) for addr, f_name in zip(args.files[::2], args.files[1::2])]\n\n    if args.json:\n        json_dir = os.path.dirname(os.path.abspath(args.json))\n\n        def process_json_file(path):  # type: (str) -> str\n            '''\n            The input path is relative to json_dir. This function makes it relative to the current working\n            directory.\n            '''\n            return check_file(os.path.relpath(os.path.join(json_dir, path), start=os.curdir))\n\n        with open(args.json) as f:\n            files += [(int(addr, 0),\n                       process_json_file(f_name)) for addr, f_name in iteritems(json.load(f)['flash_files'])]\n\n    files = sorted([(addr, f_name.decode('utf-8') if isinstance(f_name, type(b'')) else f_name) for addr, f_name in iteritems(dict(files))],\n                   key=lambda x: x[0])  # remove possible duplicates and sort based on the address\n\n    cmd_args = {'output_file': args.output_file,\n                'files': files,\n                'pid': args.pid,\n                'part_size': args.part_size,\n                }\n\n    {'write': action_write\n     }[args.command](cmd_args)\n\n\nif __name__ == '__main__':\n    main()", "original_line_number": 32104, "instruction_length": 658}
{"instruction": "Create models for a game with a Plot model that has a URLField, an answer field with max_digits=3 and decimal_places=2, and a duration field with TimeField. Create a SliderValue model with a foreign key to the user model, a timestamp field with default timezone.now, a foreign key to the Round model, a round_order field with PositiveSmallIntegerField, and a value field with decimal_places=2, max_digits=3, and null=True. Create a Round model with a foreign key to the user model, a foreign key to the Plot model, a round_order field with PositiveSmallIntegerField, a guess field with max_digits=3 and decimal_places=2, a start_time field with auto_now=True and null=True, an end_time field with null=True, and a score field with max_digits=3 and decimal_places=2 with default Decimal(0). The Round model should also have a round_data method that returns a dictionary with various fields, and a get_guess method that returns the guess as a float or -1 if it is null. The Round model should have a unique_together constraint on the user and round_order fields. The code should import Decimal, models from django.db, settings from django.conf, and timezone from django.utils. The code should also import calculate_score from game.contrib.calculate and Control from game.control.models.", "input": "", "output": "from decimal import Decimal\nfrom django.db import models\nfrom django.conf import settings\nfrom django.utils import timezone\n\n\n# Create your models here.\nfrom game.contrib.calculate import calculate_score\nfrom game.control.models import Control\n\n\nclass Plot(models.Model):\n    plot = models.URLField()\n    answer = models.DecimalField(max_digits=3, decimal_places=2)\n    duration = models.TimeField(null=True)\n\n    def __str__(self):\n        return self.plot\n\n\nclass SliderValue(models.Model):\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n    timestamp = models.DateTimeField(default=timezone.now)\n    this_round = models.ForeignKey(\"Round\", null=True)\n    round_order = models.PositiveSmallIntegerField(null=True)\n    value = models.DecimalField(decimal_places=2, max_digits=3, null=True)\n\n    def __str__(self):\n        return \"Slider {}\".format(self.id)\n\nclass Round(models.Model):\n    user = models.ForeignKey(settings.AUTH_USER_MODEL, on_delete=models.CASCADE)\n    plot = models.ForeignKey(Plot, on_delete=models.CASCADE)\n\n    round_order = models.PositiveSmallIntegerField()\n    guess = models.DecimalField(max_digits=3, decimal_places=2, null=True)\n\n    # start time of the round\n    # end time of the round\n    start_time = models.DateTimeField(auto_now=True, null=True)\n    end_time = models.DateTimeField(null=True)\n\n    # todo: change control treatment\n    score = models.DecimalField(max_digits=3, decimal_places=2, default=Decimal(0))\n\n    def __str__(self):\n        return self.user.username\n\n    def round_data(self):\n        played_rounds = self.__class__.objects.filter(user=self.user, round_order__lte=self.round_order,\n                                                      guess__gte=Decimal(0.0))\n        score = calculate_score(played_rounds)\n        this_round = self.__class__.objects.filter(user=self.user, round_order=self.round_order,\n                                                   guess__gte=Decimal(0.0))\n        round_score = calculate_score(this_round)\n\n        data = {'username': self.user.username, 'cumulative_score': score,\n                'avatar': self.user.avatar, 'task_path': self.plot.plot, 'correct_answer': self.plot.answer,\n                'independent_guess': self.guess, 'round_id': self.round_order, 'score': round_score,\n                'game_id': None, 'condition': None, 'following': None, 'revised_guess': None\n                }\n\n        if self.user.game_type == 'c':\n            game = Control.objects.get(user=self.user)\n            data['game_id'] = game.id\n            data['condition'] = 'control'\n\n        return data\n\n    def get_guess(self):\n        return float(self.guess) if self.guess else -1\n\n    class Meta:\n        unique_together = (('user', 'round_order',),)", "original_line_number": 32255, "instruction_length": 1284}
{"instruction": "Save Levels:\n\n1. Select the building levels to save.\n2. Apply filtering beforehand to skip non-level elements.\n3. Run the script.\n4. View dependant properties such as endpoint locations, level heads, and leaders will be saved for re-use.\n5. Non-level elements will be skipped with dialog.\n6. The saved levels will be stored in a file named \"LevelPlacement\" in the temporary directory.\n7. Tested Revit API: 2020.\n8. This script is part of PyRevitPlus: Extensions for PyRevit.\n9. For more information, visit github.com/ejs-ejs or contact @ejs-ejs.", "input": "", "output": "\"\"\"\nSave Levels\n\nSave the view dependant properties - \nendpoint locations, level heads and leaders\nof the selected building levels for re-use\n\nNon-level elements will be skipped with dialog,\n so it's advisable to apply filtering beforehead\n\nTESTED REVIT API: 2020\n\n@ejs-ejs\nThis script is part of PyRevitPlus: Extensions for PyRevit\ngithub.com/ejs-ejs | @ejs-ejs\n\n--------------------------------------------------------\nRevitPythonWrapper: revitpythonwrapper.readthedocs.io\npyRevit: github.com/eirannejad/pyRevit\n\n\"\"\"\n\nimport os\nimport pickle\nfrom tempfile import gettempdir\nfrom collections import namedtuple\n\nimport rpw\nfrom rpw import doc, uidoc, DB, UI\n\nPoint = namedtuple('Point', ['X', 'Y','Z'])\n\nAxis = namedtuple('Axis', ['Name', 'Start', 'End','StartBubble', 'EndBubble', 'StartBubbleVisible', 'EndBubbleVisible'])\n\ntempfile = os.path.join(gettempdir(), 'LevelPlacement')\n\ncView = doc.ActiveView\n\n\n\nif not(cView.ViewType == DB.ViewType.Section or cView == DB.ViewType.Elevation):\n    UI.TaskDialog.Show('pyRevitPlus', 'View type \\'{}\\' not supported'.format(cView.ViewType))\n    \nelse:\n    experimental = True\n    UI.TaskDialog.Show('pyRevitPlus', 'Support for \\'{}\\' view type is experimental!'.format(cView.ViewType))\n        \n        \n    selection = rpw.ui.Selection()\n\n#if len(selection) <> 1:\n#            UI.TaskDialog.Show('pyRevitPlus', 'Select a single grid line!')\n#            exit(0);\n\n    n=0\n    LevelLines = dict()\n\n    for cLevel in selection:\n        el = cLevel.unwrap()\n        if isinstance(el, DB.Level):\n            curves=el.GetCurvesInView(DB.DatumExtentType.ViewSpecific, cView)\n            if len(curves) <> 1:\n                UI.TaskDialog.Show('pyRevitPlus', 'The level line is defind by {} curves, unable to proceed', len(curves))\n            else:\n                cLevelLine = {'Name':'', 'Start': Point(0,0,0), 'End': Point(0,0,0), 'StartBubble': False, 'StartBubbleVisible': False, 'EndBubble': False, 'EndBubbleVisible': False}\n        \n                cCurve = curves[0]\n            \n                leader0 = el.GetLeader(DB.DatumEnds.End0, cView)\n                if leader0:\n                    tmp = leader0.Elbow\n                    cLevelLine['Leader0Elbow'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader0.End\n                    cLevelLine['Leader0End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader0.Anchor\n                    cLevelLine['Leader0Anchor'] = Point(tmp.X, tmp.Y,tmp.Z)\n                \n            \n                leader1 = el.GetLeader(DB.DatumEnds.End1, cView)\n                if leader1:\n                    tmp = leader1.Elbow\n                    cLevelLine['Leader1Elbow'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader1.End\n                    cLevelLine['Leader1End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                    tmp = leader1.Anchor\n                    cLevelLine['Leader1Anchor'] = Point(tmp.X, tmp.Y,tmp.Z)\n        \n                cLevelLine['Name'] = el.Name\n        \n                tmp = cCurve.GetEndPoint(0)\n                cLevelLine['Start'] = Point(tmp.X, tmp.Y,tmp.Z)\n                tmp = cCurve.GetEndPoint(1)\n                cLevelLine['End'] = Point(tmp.X, tmp.Y,tmp.Z)\n                if el.HasBubbleInView(DB.DatumEnds.End0, cView):\n                    cLevelLine['StartBubble']=True\n                if el.HasBubbleInView(DB.DatumEnds.End1, cView):\n                    cLevelLine['EndBubble']=True\n                if el.IsBubbleVisibleInView(DB.DatumEnds.End0, cView):\n                    cLevelLine['StartBubbleVisible']=True\n                if el.IsBubbleVisibleInView(DB.DatumEnds.End1, cView):\n                    cLevelLine['EndBubbleVisible']=True\n                #if isinstance(cCurve, DB.Arc):\n                #    tmp = cCurve.Center\n                #    cLevelLine['Center'] = Point(tmp.X, tmp.Y,tmp.Z)\n            \n                LevelLines[cLevelLine['Name']] = cLevelLine\n                n += 1\n        else:\n            #if isinstance(el, DB.MultiSegmentGrid):\n            #    UI.TaskDialog.Show('pyRevitPlus', 'Skipping yet unsupported Multi-Segment grid \\'{}\\''.format(el.Name))\n            #else: \n            UI.TaskDialog.Show('pyRevitPlus', 'Skipping non- level element \\'{}\\''.format(el.Name))\n        \n    if n<>1:\n        msg = 'Saved {} level placements to {}'.format(n,tempfile)\n    else:\n        msg = 'Saved level \\'{}\\' placement to {}'.format(cLevelLine['Name'],tempfile)\n \n    if n>0:\n        with open(tempfile, 'wb') as fp:\n            pickle.dump(LevelLines, fp)\n        # close(fp)\n        UI.TaskDialog.Show('pyRevitPlus', msg)\n    else:\n        UI.TaskDialog.Show('pyRevitPlus', 'Nothing to save')", "original_line_number": 32257, "instruction_length": 545}
{"instruction": "Import the necessary modules and classes from the required files. Define a class named DescriptionParser that inherits from the DescriptionParserBase class. Define a get_content method that takes a URL as input and returns the page content. Define an __init__ method that initializes the class and sets the content of the page. Define a get_tag method that returns the tag for a given string. Define a clean method that removes unwanted characters from a string. Define a get_title method that returns the title of a given string. Define a get_original_title method that returns the original title of a given string. Define a parse_season_from_title method that extracts the season number from a given string. Define a get_episodes_num method that returns the number of episodes in a given string. Define a date_added_duration method that returns the duration since the date was added. Define a parse method that parses the page content and extracts the required information. Define a write_tvshow_nfo function that writes the TV show NFO file. Define a write_tvshow function that writes the TV show file. Define a write_tvshow_item function that writes the TV show item. Define a get_session function that returns a session. Define a download_torrent function that downloads a torrent file. Define a write_pages function that writes the pages. Define a filter function that filters the titles. Define a search_generate function that generates the search. Define a run function that runs the program.", "input": "", "output": "﻿# coding: utf-8\nimport log\nfrom log import debug\n\n\nfrom settings import Settings\nfrom base import *\nimport feedparser, urllib2, re\nfrom bs4 import BeautifulSoup\nfrom nfowriter import *\nfrom strmwriter import *\nimport requests, filesystem\n\n###################################################################################################\nclass DescriptionParser(DescriptionParserBase):\n\t#==============================================================================================\n\tdef get_content(self, url):\n\t\tpage = urllib2.urlopen(url)\n\t\treturn page\n\t\n\t#==============================================================================================\n\tdef __init__(self, url):\n\t\tInformer.__init__(self)\n\t\t\n\t\tself._dict = dict()\n\t\tself.content = self.get_content(url)\n\t\t#html_doc = '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\\n<html>' + content.encode('utf-8') + '\\n</html>'\n\t\tself.soup = BeautifulSoup(self.content, 'html.parser')\n\t\tself.OK = self.parse()\n\t\t\t\t\n\t#==============================================================================================\n\tdef get_tag(self, x):\n\t\treturn {\n\t\t\tu'Год: ': u'year',\n\t\t\tu'Жанр: ': u'genre',\n\t\t\tu'Описание: ': u'plot',\n\t\t\tu'Режиссер: ': u'director',\n\t\t\tu'Продолжительность: ': u'runtime',\n\t\t\tu'Страна: ': u'country',\n\t\t}.get(x, u'')\n\t\t\n\t#==============================================================================================\n\tdef clean(self, title):\n\t\ttry:\n\t\t\ttitle = title.split(u' ТВ-')[0]\n\t\t\ttitle = title.split(u' TV-')[0]\n\t\t\ttitle = title.split(u' [')[0]\n\t\texcept:\n\t\t\tpass\n\t\treturn title.strip()\n\t\n\t#==============================================================================================\t\n\tdef get_title(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search('^(.+?) /', full_title).group(1)\n\t\t\treturn self.clean(found)\n\t\texcept AttributeError:\n\t\t\treturn full_title\n\t\n\t#==============================================================================================\t\n\tdef get_original_title(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search('^.+? / (.+)', full_title).group(1)\n\t\t\treturn self.clean(found)\n\t\texcept AttributeError:\n\t\t\treturn full_title\n\t\n\t#==============================================================================================\n\tdef parse_season_from_title(self, title):\n\t\ttry:\n\t\t\tfound = re.search(r\"(\\d) \\[\\d+\\D+\\d+\\]\", title)\n\t\t\tif found:\n\t\t\t\ttry:\n\t\t\t\t\tself._dict['season'] = int(found.group(1))\n\t\t\t\t\treturn\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\t\tparts = title.split(u'ТВ-')\n\t\t\tif len(parts) == 1:\n\t\t\t\tparts = title.split(u'TV-')\n\t\t\tif len(parts) > 1:\n\t\t\t\tfound = re.search('([0-9]+)', parts[1]).group(1)\n\t\t\t\tself._dict['season'] = int(found)\n\t\texcept:\n\t\t\tpass\n\t\n\t#==============================================================================================\n\tdef get_episodes_num(self, full_title):\n\t\ttry:\n\t\t\tfound = re.search(' \\[([0-9]+) ', full_title).group(1)\n\t\t\treturn int(found)\n\t\texcept AttributeError:\n\t\t\treturn 1\n\n\tdef date_added_duration(self):\n\t\tul = self.soup.find('ul', class_='story_inf')\n\t\tif ul:\n\t\t\tfor li in ul.find_all('li'):\n\t\t\t\ttxt = li.get_text()\n\t\t\t\tparts = txt.split(':')\n\t\t\t\tif len(parts) > 1 and parts[0] == u'Дата':\n\t\t\t\t\tdate, t = parts[1].split(',')\t# \t\td\tu' 30-09-2012'\tunicode\n\n\t\t\t\t\tfrom datetime import datetime, timedelta\n\n\t\t\t\t\tday = timedelta(1)\n\t\t\t\t\tyesterday = datetime.today() - day\n\n\t\t\t\t\t#date = ' 30-09-2012'\n\n\t\t\t\t\tif u'Сегодня' in date:\n\t\t\t\t\t\td = datetime.today()\n\t\t\t\t\telif u'Вчера' in date:\n\t\t\t\t\t\td = yesterday\n\t\t\t\t\telse:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\td = datetime.strptime(date.strip(), '%d-%m-%Y')\n\t\t\t\t\t\texcept TypeError:\n\t\t\t\t\t\t\td = datetime.today()\n\n\t\t\t\t\tdt = datetime.today() - d\n\t\t\t\t\treturn dt\n\t\t\n\t#==============================================================================================\n\tdef parse(self):\n\t\ttag = u''\n\t\tself._dict['gold'] = False\n\t\tself._dict['season'] = 1\n\t\t\n\t\tfor title in self.soup.select('#news-title'):\n\t\t\tfull_title = title.get_text()\n\t\t\tdebug(full_title)\n\t\t\tself._dict['title'] = self.get_title(full_title)\n\t\t\tself._dict['originaltitle'] = self.get_original_title(full_title)\n\t\t\tself.parse_season_from_title(full_title)\n\t\t\tself._dict['episodes'] = self.get_episodes_num(full_title)\n\t\t\n\t\tfor b in self.soup.select('div.xfinfodata b'):\n\t\t\ttry:\n\t\t\t\ttext = b.get_text()\n\t\t\t\ttag = self.get_tag(text)\n\t\t\t\tif tag != '':\n\t\t\t\t\tspan = b.find_next_sibling('span')\n\t\t\t\t\tself._dict[tag] = span.get_text().strip()\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor div in self.soup.select('div.story_c'):\n\t\t\ttry:\n\t\t\t\ttext = div.get_text()\n\t\t\t\ttext = text.split(u'Описание:')[1]\n\t\t\t\ttext = text.split(u'Эпизоды')[0]\n\t\t\t\ttext = text.split(u'Скриншоты')[0]\n\t\t\t\ttext = text.strip()\n\t\t\t\tself._dict['plot'] = text\n\t\t\t\t#debug('---')\n\t\t\t\t#debug(text)\n\t\t\t\t#debug('---')\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor b in self.soup.select('div.story_h .rcol sup b'):\n\t\t\ttry:\n\t\t\t\ttext = b.get_text()\n\t\t\t\ttext = text.split(' ')[0]\n\t\t\t\tself._dict['rating'] = float(text) * 2\n\t\t\t\tdebug('rating: ' + str(self._dict['rating']))\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfor img in self.soup.select('span.poster img'):\n\t\t\ttry:\n\t\t\t\tself._dict['thumbnail'] = img['src'].strip()\n\t\t\t\tdebug(self._dict['thumbnail'])\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\t\t\n\t\tfanart = []\n\t\tfor a in self.soup.select('ul.clr li a'):\n\t\t\ttry:\n\t\t\t\tdebug(a['href'])\n\t\t\t\tfanart.append(a['href'].strip())\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\tif len(fanart) != 0:\n\t\t\tself._dict['fanart'] = fanart\n\t\t# else:\n\t\t# \tdt = self.date_added_duration()\n\t\t# \tif dt and dt.days <= 14:\n\t\t# \t\treturn False\n\t\t\t\n\t\tfor img in self.soup.select('div.video_info a img'):\n\t\t\ttry:\n\t\t\t\tself._dict['studio'] = img['alt'].strip()\n\t\t\t\tdebug(self._dict['studio'])\n\t\t\texcept:\n\t\t\t\tpass\n\n\t\ttags = []\n\t\tfor a in self.soup.select('a[href*=\"https://tr.anidub.com/tags/\"]'):\n\t\t\ttags.append(a.get_text().strip())\n\t\tif len(tags) > 0:\n\t\t\tself._dict['tag'] = tags\n\n\t\t\t\t\n\t\treturn True\n\n###################################################################################################\ndef write_tvshow_nfo(parser, tvshow_api, tvshow_path):\n\ttry:\n\t\tif write_tvshow_nfo.favorites:\n\t\t\tparser.Dict().get('tag', []).append('favorites')\n\texcept:\n\t\tpass\n\n\tNFOWriter(parser, tvshow_api=tvshow_api).write_tvshow_nfo(tvshow_path)\n\treturn\n\n###################################################################################################\ndef write_tvshow(content, path, settings):\n\twith filesystem.save_make_chdir_context(path, 'Anidub.write_tvshow'):\n\t\td = feedparser.parse(content)\n\n\t\tcnt = 0\n\t\tsettings.progress_dialog.update(0, 'anidub', path)\n\n\t\tfor item in d.entries:\n\t\t\twrite_tvshow_item(item, path, settings)\n\n\t\t\tcnt += 1\n\t\t\tsettings.progress_dialog.update(cnt * 100 / len(d.entries), 'anidub', path)\n\n\ndef write_tvshow_item(item, path, settings, path_out=[]):\n\tdebug('-------------------------------------------------------------------------')\n\tdebug(item.link)\n\tparser = DescriptionParser(item.link)\n\tif parser.parsed():\n\t\ttitle = parser.get_value('title')\n\t\tdebug(title)\n\t\toriginaltitle = parser.get_value('originaltitle')\n\t\tdebug(originaltitle)\n\t\tseason = parser.get_value('season')\n\n\t\tfrom downloader import TorrentDownloader\n\t\tTorrentDownloader(item.link, settings.torrents_path(), settings).download()\n\n\t\tdebug('Episodes: ' + str(parser.get_value('episodes')))\n\n\t\ttvshow_path = make_fullpath(title, '')\n\n\t\ttvshow_path = filesystem.join(path, tvshow_path)\n\t\tdebug(tvshow_path)\n\n\t\tpath_out.append(tvshow_path)\n\t\tsettings.update_paths.add(tvshow_path)\n\n\t\twith filesystem.save_make_chdir_context(tvshow_path, 'Anidub.write_tvshow_item'):\n\t\t\ttvshow_api = TVShowAPI.get_by(originaltitle, title)\n\t\t\twrite_tvshow_nfo(parser, tvshow_api, tvshow_path)\n\n\t\tseason_path = filesystem.join(tvshow_path, u'Season ' + unicode(season))\n\t\tdebug(season_path)\n\n\t\twith filesystem.save_make_chdir_context(season_path, 'Anidub.write_tvshow_item_2'):\n\n\t\t\tepisodes = tvshow_api.episodes(season)\n\n\t\t\tif len(episodes) < parser.get_value('episodes'):\n\t\t\t\tfor i in range(len(episodes) + 1, parser.get_value('episodes') + 1):\n\t\t\t\t\tepisodes.append({\n\t\t\t\t\t\t'title': title,\n\t\t\t\t\t\t'showtitle': title,\n\t\t\t\t\t\t'short': 's%02de%02d' % (season, i),\n\t\t\t\t\t\t'episode': i,\n\t\t\t\t\t\t'season': season\n\t\t\t\t\t})\n\n\t\t\tfor episode in episodes:\n\t\t\t\ttitle = episode['title']\n\t\t\t\tshortName = episode['short']\n\t\t\t\tepisodeNumber = episode['episode']\n\n\t\t\t\tif episodeNumber <= parser.get_value('episodes'):\n\t\t\t\t\tfilename = str(episodeNumber) + '. ' + 'episode_' + shortName\n\t\t\t\t\tdebug(filename)\n\n\t\t\t\t\tep = tvshow_api.Episode(season, episodeNumber)\n\t\t\t\t\tif ep:\n\t\t\t\t\t\tepisode = ep\n\n\t\t\t\t\tSTRMWriter(item.link).write(filename, season_path, episodeNumber=episodeNumber, settings=settings)\n\t\t\t\t\tNFOWriter(parser, tvshow_api=tvshow_api).write_episode(episode, filename, season_path)\n\n\telse:\n\t\tskipped(item)\n\tdel parser\n\n\ndef get_session(settings):\n\ts = requests.Session()\n\tdata = {\"login_name\": settings.anidub_login, \"login_password\": settings.anidub_password, \"login\": \"submit\"}\n\theaders = {\n\t\t'Host':\t\t\t\t\t\t\t'tr.anidub.com',\n\t\t'Origin':\t\t\t\t\t\t'https://tr.anidub.com',\n\t\t'Referer':\t\t\t\t\t\t'https://tr.anidub.com/',\n\t\t'Upgrade-Insecure-Requests':\t'1',\n\t\t'User-Agent':\t\t\t\t\t'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132'\n\t}\n\tlogin = s.post(\"https://tr.anidub.com/\", data=data, headers=headers)\n\tdebug('Login status: %d' % login.status_code)\n\tif 'login_name' in login.content:\n\t\tdebug('Login failed')\n\n\treturn s\n\t\ndef download_torrent(url, path, settings):\n\tfrom base import save_hashes\n\tsave_hashes(path)\n\n\turl = urllib2.unquote(url)\n\tdebug('download_torrent:' + url)\n\n\ts = get_session(settings)\n\n\tpage = s.get(url)\n\t#debug(page.text.encode('utf-8'))\n\tsoup = BeautifulSoup(page.text, 'html.parser')\n\n\ttry:\n\t    a = soup.select_one('#tv720 div.torrent_h a')\n\texcept TypeError:\n\t\ta = None\n\n\ttry:\n\t\tif a is None:\n\t\t\ta = soup.select_one('div.torrent_h > a')\n\texcept TypeError:\n\t\ta = None\n\n\tif a is not None:\n\t\thref = 'https://tr.anidub.com' + a['href']\n\t\tdebug(s.headers)\n\t\tr = s.get(href, headers={'Referer': url})\n\t\tdebug(r.headers)\n\t\t\n\t\tif 'Content-Type' in r.headers:\n\t\t\tif not 'torrent' in r.headers['Content-Type']:\n\t\t\t\treturn False\n\t\t\n\t\ttry:\n\t\t\twith filesystem.fopen(path, 'wb') as torr:\n\t\t\t\tfor chunk in r.iter_content(100000):\n\t\t\t\t\ttorr.write(chunk)\n\t\t\tsave_hashes(path)\n\t\t\treturn True\n\t\texcept: \n\t\t\tpass\n\n\treturn False\n\ndef write_pages(url, path, settings, params={}, filter_fn=None, dialog_title = None, path_out=[]):\n\ts = get_session(settings)\n\tif params:\n\t\tpage = s.post(url, data=params)\n\telse:\n\t\tpage = s.get(url)\n\tsoup = BeautifulSoup(page.content, 'html.parser')\n\tpage_no = 1\n\n\tcnt = 0\n\t\n\tclass Item:\n\t\tdef __init__(self, link, title):\n\t\t\tself.link = link\n\t\t\tself.title = title\n\t\n\twith filesystem.save_make_chdir_context(path, 'Anidub.write_pages'):\n\t\twhile True:\n\t\t\tif params:\n\t\t\t\tselector = soup.select('div.search_post > div.text > h2 > a')\n\t\t\telse:\n\t\t\t\tselector = soup.select('article.story > div.story_h > div.lcol > h2 > a')\n\n\t\t\tif not selector:\n\t\t\t\tbreak\n\t\n\t\t\tsettings.progress_dialog.update(0, dialog_title, path)\n\t\n\t\t\tfor a in selector:\n\t\t\t\tlog.debug(a['href'])\n\t\t\t\tlink = a['href']\n\t\t\t\ttitle = a.get_text()\n\t\t\t\tif filter_fn and filter_fn(title):\n\t\t\t\t\tcontinue\n\n\t\t\t\twrite_tvshow_item(Item(link, title), path, settings, path_out)\n\t\n\t\t\t\tcnt += 1\n\t\t\t\tsettings.progress_dialog.update(cnt * 100 / len(selector), dialog_title, path)\n\n\t\t\tif not 'favorites' in url:\n\t\t\t\tbreak\n\t\n\t\t\tpage_no += 1\n\t\t\tpage = s.get(url + 'page/%d/' % page_no)\n\t\n\t\t\tif page.status_code == requests.codes.ok:\n\t\t\t\tsoup = BeautifulSoup(page.text, 'html.parser')\n\t\t\telse:\n\t\t\t\tbreak\n\n\treturn cnt\n\n\ndef write_favorites(path, settings):\n\twrite_pages('https://tr.anidub.com/favorites/', path, settings, dialog_title=u'Избранное AniDUB')\n\n\ndef search_generate(what, settings, path_out):\n\tdef filter(title):\n\t\tif what not in title:\n\t\t\treturn True\n\n\t\treturn False\n\n\twrite_tvshow_nfo.favorites = False\n\treturn write_pages('https://tr.anidub.com/index.php?do=search', \n\t\t\t\tsettings.anime_tvshow_path(), settings, \n\t\t\t\t{'do': 'search',\n\t\t\t\t'subaction': 'search',\n\t\t\t\t'story': what.encode('utf-8')}, filter,\n\t\t\t\tdialog_title=u'Поиск AniDUB',\n\t\t\t\tpath_out=path_out)\n\t\n\n###################################################################################################\ndef run(settings):\n\tif settings.anime_save:\n\t\tif settings.anidub_rss:\n\t\t\twrite_tvshow_nfo.favorites = False\n\t\t\twrite_tvshow(settings.anidub_url, settings.anime_tvshow_path(), settings)\n\t\tif settings.anidub_favorite:\n\t\t\twrite_tvshow_nfo.favorites = True\n\t\t\twrite_favorites(settings.anime_tvshow_path(), settings)\n\n\nif __name__ == '__main__':\n\tsettings = Settings('../media_library')\n\trun(settings)", "original_line_number": 32275, "instruction_length": 1500}
{"instruction": "This code performs BLAST evaluation. To use it, follow these steps:\n\n1. Import the necessary libraries: glob, sys, csv, and tabulate from Bio.Blast.Applications.\n2. Define the following functions:\n   - carga_csv(file_name): creates a list of lists with a csv file.\n   - crea_comparacion(tabla_ref, estructura = 'star', comparacion = 'bi'): creates comparisons lists (code) depending on arguments.\n   - imprime_comparacion(listas): prints the comparison as a readable format.\n   - imprime_referencia(claves): prints the comparison as a readable format.\n   - crea_diccionario(tabla): creates a dictionary of code:organism.\n3. In the program body, prompt the user to enter the CSV file name and load the data using carga_csv().\n4. Present the CSV data using tabulate().\n5. Present the options: blast parameters, comparison parameters, run.\n6. If the user selects option 1, prompt them to enter the comparisons using the organism index and the direction (bidirectional or unidirectional). Use crea_comparacion() to create the comparison list.\n7. If the user selects option 2, prompt them to enter the desired E value for BLAST runs. Use NcbiblastpCommandline() to create the commands for comparisons.\n8. Run the commands and inform the user when the work is completed.", "input": "", "output": "#! /usr/bin/env python\n\n\"\"\" DESCRIPTION\n\n\"\"\"\n\nimport glob, sys, csv\nfrom tabulate import tabulate\nfrom Bio.Blast.Applications import NcbiblastpCommandline\n\n\"\"\"--- FUNCTIONS ---\"\"\"\n\ndef carga_csv(file_name):\n    \"\"\" creates a list of lists with a csv file \"\"\"\n    tabla = []    \n    cr = csv.reader(open(file_name,\"rb\"))\n    for row in cr:    \n        tabla.append(row)\n    return tabla      \n\ndef crea_comparacion(tabla_ref, estructura = 'star', comparacion = 'bi'):\n    \"\"\" creates comparisons lists (code) depending on arguments \"\"\"\n    lista = []\n    tabla = list(tabla_ref)\n    if estructura == 'star':\n        nodo = tabla.pop()\n        for organismo in tabla:\n            lista.append([nodo[1],organismo[1]])\n            if comparacion == 'bi':\n                lista.append([organismo[1], nodo[1]])\n    else:\n        comps = estructura.split(',')\n        for comp in comps:\n            pareja = comp.split('-')\n            query = tabla[int(pareja[0])][1]\n            db = tabla[int(pareja[1])][1]\n            lista.append([query, db])\n            if comparacion == 'bi':\n                lista.append([db, query])\n    return lista\n\ndef imprime_comparacion(listas):\n    \"\"\" prints the comparison as a readable format\"\"\"\n    print 'COMPARISONS\\n-----------\\n'\n    for lista in listas:\n        print lista[0] + ' --> ' + lista[1]\n    print '\\n'\n\ndef imprime_referencia(claves):\n    \"\"\" prints the comparison as a readable format\"\"\"\n    print 'REFERENCE\\n---------'\n    n = 0\n    for key, val in claves.items():\n        print n, '. ', key, '\\t', val\n        n=n+1\n    print '\\n'\n\ndef crea_diccionario(tabla):\n    \"\"\" creates a dictionary of code:organism\"\"\"\n    diccionario={}\n    for row in tabla:\n        diccionario[row[1]]=row[0]\n    return diccionario\n    \n\"\"\"--- PROGRAM BODY ---\"\"\"\nprint '----------------\\nBLAST EVALUATION\\n----------------'\nblast_eval = 1e-05\ncomparison_list = []\n# charge csv file\nnombre_csv = raw_input('Please enter the CSV file name: ')\norganismos = carga_csv(nombre_csv)\nreferencia = crea_diccionario(organismos)\ncomparison_list = crea_comparacion(organismos)\n# present csv data\nprint '\\nCSV data\\n--------'\nprint tabulate(organismos, headers=[\"Organism\",\"Code\", \"Genome File\", \"Database folder\"]) + '\\n'\n# present options: blast parameters, comparison parameters, run\nwhile 1:\n    \n    imprime_referencia(referencia)\n    imprime_comparacion(comparison_list)\n    print 'CHOOSE AN OPTION\\n----------------\\n1) Comparisons\\n2) Run\\n3) Quit'\n    user_in = raw_input('Option: ')\n    if user_in == '1':\n        imprime_referencia(referencia)\n        print ('Please enter the comparisons using the organism index.\\n' +\n              'Format: \"-\" between indices; \",\" between comparisons; no spaces.\\n')\n        nueva_comparacion = raw_input('Comparisons: ')\n        print 'Choose \"bi\" for bidirectional or \"uni\" for unidirectional; no quotation marks.'\n        tipo_comparacion = raw_input('Direction: ')\n        comparison_list = crea_comparacion(organismos, nueva_comparacion, tipo_comparacion)\n    elif user_in == '2':\n        blast_eval = raw_input('\\nPlease write the desired E value for BLAST runs; 1e-5 suggested.\\nE_value: ')\n        print '\\nBLAST+ commands to be runned...\\n'\n        break\n    elif user_in == '3': quit()\n    else: print ('Incorrect option, try again.\\n')\n\n# create commands for comparisons\ncomandos = []\nfor pair in comparison_list:\n    nombre = referencia[pair[0]].split()\n    comandos.append([(nombre[0]+'_'+nombre[1]+'.faa'), ('db_'+pair[1]+'/db_'+pair[1]), (pair[0]+'_'+pair[1]+'.xml')])\nprint tabulate(comandos, headers=[\"Genome file\",\"Database\", \"Product file\"]) + '\\n'\nraw_input('Press ENTER to continue')\n# run commands, inform data created\nfor comando in comandos:\n    blastp_cline = NcbiblastpCommandline(query=comando[0], db=comando[1], evalue=blast_eval ,outfmt=5, out=comando[2])\n    print 'File ' + comando[2] + ' is currently in progess...'\n    stdout, stderr = blastp_cline()\nprint 'WORK COMPLETED\\n--------------'", "original_line_number": 32812, "instruction_length": 1264}
{"instruction": "Create a Flask application with a database and email functionality. Define four models: Waterhistory, User, Application, and Act_code. The User model has a one-to-many relationship with Waterhistory, Application, and Act_code. The User model has attributes for name, email, phone, is_active, role, flat_number, password, email_verified, and phone_verified. The Waterhistory model has attributes for id, hw_counter, cw_counter, user_id, user, and month. The Application model has attributes for id, user_id, user, link, created, and is_active. The Act_code model has attributes for id, user_id, user, code, and created. The Providers_oAuth model has attributes for id, name, consumer_key, consumer_secret, request_token_params, base_url, request_token_url, access_token_method, access_token_url, and authorize_url.", "input": "", "output": "# -*- coding: utf-8 -*-\r\nfrom app import db, mail\r\nfrom sqlalchemy.orm import backref, relationship\r\nfrom datetime import datetime\r\nfrom sqlalchemy_utils import PasswordType\r\nfrom sqlalchemy.sql.schema import ForeignKey\r\nfrom flask_mail import Message\r\nfrom config import HOSTNAME\r\n\r\nclass Waterhistory(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    hw_counter = db.Column(db.Integer)\r\n    cw_counter = db.Column(db.Integer)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref=backref('history', lazy='dynamic'))\r\n    month = db.Column(db.Integer)\r\n    \r\n    def __init__(self, hw_counter, cw_counter, user):\r\n        self.hw_counter = hw_counter\r\n        self.cw_counter = cw_counter\r\n        self.user = user\r\n        self.month = datetime.utcnow().month\r\n    \r\nclass User(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    name = db.Column(db.String(80))\r\n    email = db.Column(db.String(120), unique=True)\r\n    phone = db.Column(db.String(12), unique=True)\r\n    is_active = db.Column(db.Boolean, default = True)\r\n    role = db.Column(db.String(20), default = 'user')\r\n    flat_number = db.Column(db.Integer)\r\n    password = db.Column(PasswordType(schemes=['pbkdf2_sha512', 'md5_crypt'], deprecated=['md5_crypt']))\r\n    email_verified = db.Column(db.Boolean, default = False)\r\n    phone_verified = db.Column(db.Boolean, default = False)   \r\n    \r\n    def __init__(self, name, email, phone, flat_number, password):\r\n        self.name = name\r\n        self.email = email\r\n        self.phone = phone\r\n        self.flat_number = flat_number\r\n        self.password = password\r\n        \r\n    def is_authenticated(self):\r\n        return True\r\n \r\n    def is_anonymous(self):\r\n        return False\r\n \r\n    def get_id(self):\r\n        return unicode(self.id)\r\n    \r\n    def _gen_act_code(self):\r\n        import binascii, os\r\n        act_code = binascii.hexlify(os.urandom(48))\r\n        raw = Act_code(self, act_code)\r\n        db.session.add(raw)\r\n        db.session.commit()\r\n        return act_code\r\n    \r\n    def verify_email(self, act_code = None):\r\n        if act_code is None:\r\n            act_code = self._gen_act_code()\r\n            link = 'http://' + HOSTNAME + '/activate/' + act_code\r\n            msg = Message('Hello!', recipients=[self.email], sender=('WC Project', 'wc@wc.local'))\r\n            msg.html = '<a href=\"' + link + '\">Link</a>'\r\n            mail.send(msg)\r\n            return True\r\n        \r\n        saved_codes = Act_code.query.filter_by(user = self)\r\n        \r\n        for saved_code in saved_codes:\r\n            if saved_code.code == act_code and (datetime.now() - saved_code.created).seconds <= 43200:\r\n                self.email_verified = True\r\n                db.session.commit()\r\n                return True\r\n        \r\n        def verify_phone(self, act_code):\r\n            pass\r\n        \r\n        return False\r\n        \r\n    def __repr__(self):\r\n        return 'User %r' % self.name\r\n    \r\nclass Application(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref = backref('apps', lazy = 'dynamic'))\r\n    link = db.Column(db.String(100))\r\n    created = db.Column(db.DateTime)\r\n    is_active = db.Column(db.Boolean())\r\n    \r\n    def __init__(self, user, link, created = None, is_active = True):\r\n        self.user = user\r\n        self.link = link\r\n        if created is None:\r\n            created = datetime.utcnow()\r\n        self.created = created\r\n        self.is_active = is_active\r\n        \r\nclass Act_code(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    user_id = db.Column(db.Integer, db.ForeignKey('user.id'))\r\n    user = db.relationship('User', backref = backref('codes', lazy = 'dynamic'))\r\n    code = db.Column(db.String(50))\r\n    created = db.Column(db.DateTime)\r\n    \r\n    def __init__(self, user, code):\r\n        self.user = user\r\n        self.code = code\r\n        self.created = datetime.now()\r\n    \r\nclass Providers_oAuth(db.Model):\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    name = db.Column(db.String(20))\r\n    consumer_key = db.Column(db.String(120))\r\n    consumer_secret = db.Column(db.String(120))\r\n    request_token_params = db.Column(db.String(120))\r\n    base_url = db.Column(db.String(120))\r\n    request_token_url = db.Column(db.String(120))\r\n    access_token_method = db.Column(db.String(10))\r\n    access_token_url = db.Column(db.String(120))\r\n    authorize_url = db.Column(db.String(120))", "original_line_number": 33243, "instruction_length": 813}
{"instruction": "The given code is a Python script that cleans up old images from a Google Container Registry. To use it, follow these steps:\n\n1. Install Python and the required dependencies (aiogoogle, aiohttp, and hailtop.aiogoogle).\n2. Save the code to a file named \"cleanup_gcr.py\".\n3. Open a terminal or command prompt and navigate to the directory containing the file.\n4. Run the script with the command \"python cleanup_gcr.py [project]\", where [project] is the name of your Google Cloud project.\n\nThe script will connect to your Container Registry, list all images, and delete any images that are more than 7 days old or have no associated tags. It will keep the 10 most recent images for each repository. The script will run asynchronously, using up to 8 parallel threads to speed up the cleanup process. The progress of the cleanup will be logged to the console.", "input": "", "output": "import sys\nimport time\nimport logging\nimport asyncio\nimport aiohttp\nimport hailtop.aiogoogle as aiogoogle\n\nlog = logging.getLogger(__name__)\n\n\nclass AsyncIOExecutor:\n    def __init__(self, parallelism):\n        self._semaphore = asyncio.Semaphore(parallelism)\n\n    async def _run(self, fut, aw):\n        async with self._semaphore:\n            try:\n                fut.set_result(await aw)\n            except asyncio.CancelledError:  # pylint: disable=try-except-raise\n                raise\n            except Exception as e:  # pylint: disable=broad-except\n                fut.set_exception(e)\n\n    def submit(self, aw):\n        fut = asyncio.Future()\n        asyncio.ensure_future(self._run(fut, aw))\n        return fut\n\n    async def gather(self, aws):\n        futs = [self.submit(aw) for aw in aws]\n        return [await fut for fut in futs]\n\n\nclass CleanupImages:\n    def __init__(self, client):\n        self._executor = AsyncIOExecutor(8)\n        self._client = client\n\n    async def cleanup_digest(self, image, digest, tags):\n        log.info(f'cleaning up digest {image}@{digest}')\n\n        await self._executor.gather([\n            self._client.delete(f'/{image}/manifests/{tag}')\n            for tag in tags])\n\n        await self._executor.submit(self._client.delete(f'/{image}/manifests/{digest}'))\n\n        log.info(f'cleaned up digest  {image}@{digest}')\n\n    async def cleanup_image(self, image):\n        log.info(f'cleaning up image {image}')\n\n        log.info(f'listing tags for {image}')\n\n        result = await self._executor.submit(self._client.get(f'/{image}/tags/list'))\n        manifests = result['manifest']\n        manifests = [(digest, int(data['timeUploadedMs']) / 1000, data['tag']) for digest, data in manifests.items()]\n\n        log.info(f'got {len(manifests)} manifests for {image}')\n\n        # sort is ascending, oldest first\n        manifests = sorted(manifests, key=lambda x: x[1])\n\n        # keep the most recent 10\n        manifests = manifests[:-10]\n\n        now = time.time()\n        await asyncio.gather(*[\n            self.cleanup_digest(image, digest, tags)\n            for digest, time_uploaded, tags in manifests\n            if (now - time_uploaded) >= (7 * 24 * 60 * 60) or len(tags) == 0])\n\n        log.info(f'cleaned up image  {image}')\n\n    async def run(self):\n        images = await self._executor.submit(self._client.get('/tags/list'))\n        await asyncio.gather(*[\n            self.cleanup_image(image)\n            for image in images['child']\n        ])\n\n\nasync def main():\n    logging.basicConfig(level=logging.INFO)\n\n    if len(sys.argv) != 2:\n        raise ValueError('usage: cleanup_gcr <project>')\n    project = sys.argv[1]\n\n    async with aiogoogle.ContainerClient(\n            project=project,\n            timeout=aiohttp.ClientTimeout(total=60)) as client:\n        cleanup_images = CleanupImages(client)\n        await cleanup_images.run()\n\n\nasyncio.run(main())", "original_line_number": 33274, "instruction_length": 854}
{"instruction": "The given code is a modified ShoutBox library that enables further modifications for the ShoutBox. To generate an HTML file, run the code without any arguments. To enter a new line from the command line, run \"psogen.py input Anonymous default 'Text'\". The code contains functions to generate and write HTML data to a static HTML file, read data from a file, save new ShoutBox content, and prepare a line of content. To test or generate a static HTML file, run the code with the \"__name__\" variable set to \"__main__\".", "input": "", "output": "#!/usr/bin/python\n\n# Modificated ShoutBox Library\n#   enables further modifications for the ShoutBox\n#   Run without to generate htmlfile\n#   Run the following to enter a new line from command line\n#     psogen.py input Anonymous default \"Text\"\n\nimport os, re\n\nimport messages, broadcast\n\ndatafilename = os.environ[\"SHOUTBOX_CHATFILE\"]\nhtmlfilename = os.environ[\"SHOUTBOX_GEN_HTMLFILE\"]\n\nstyle=(\"name { font-weight: bold; font-family:Tahoma } \"\n       \"data { font-family: Tahoma } \"\n       \"data.def { color: #000000 } \"\n       \"data.blue { color: #0000FF } \"\n       \"data.green { color: #008000 } \"\n       \"data.orange { color: #FF8040 } \"\n       \"data.red { color: #FF0000 }\")\n\ntry:\n     raw_dest =  os.environ[\"SHOUTBOX_BROADCAST_DESTINATIONS\"]\n     finished_dest = re.sub ( '#' , '\"' , raw_dest )\n     broadcast_destination = eval ( finished_dest )\nexcept KeyError:\n     broadcast_destination = False\n\n\n#--------------\n#  Generates Shoutbox-HTML-Frame  ...\n#           Imports:\n#               content    -   String  containing preformatted data\n#--------------\ndef generate_html(content):\n    htmlstring =   \"<html><head><meta http-equiv='cache-control' content='no-cache'><meta name='GENERATOR' content='PyShoutOut'><title>Shout-Out Data</title>\"\n    htmlstring +=  \"<style type='text/css'>\" + style  + \"</style></head><body>\"\n    htmlstring +=  content\n    htmlstring +=  \"</body></html>\"\n    return htmlstring\n\n#--------------\n#   Generates HTML Data based on given content  and write it to static html file\n#          Imports:\n#               content    -   String  containing preformatted data\n#--------------\ndef generate_html_into_file(content):\n    htmlstring = generate_html ( content )\n\n    htmlfile = open( htmlfilename , 'w' )\n    htmlfile.write( htmlstring )\n    htmlfile.close()\n\n#--------------\n# Generates HTML Data based on datafilename 's content\n#--------------\ndef generate_html_from_file():\n    old =  read_data_file()\n    generate_html_into_file( old   )\n\n#--------------\n# Generates and Displays generated HTML\n#--------------\ndef generate_html_to_display_from_file():\n    old =  read_data_file()\n    htmlstring = generate_html ( old )\n    print htmlstring\n\n#--------------\n#  Reads Data file from datafilename given name\n#--------------\ndef read_data_file():\n    datafile = open(datafilename, 'r')\n    old = datafile.read()\n    datafile.close()\n    return old\n\n#--------------\n# Function for saving new Shoubox-Content & Regenerate static HTML file -- usually called by HTML-Form\n#--------------\ndef process_form( name , indata , color ):\n    content = save_input(  name , indata , color )\n\n    if broadcast_destination == False:\n          generate_html_into_file ( content )\n\n\n#--------------\n# Acutally Saves SB-Content to datafile\n#--------------\ndef save_input( name , indata , color ):\n\n    content = prepare_line ( name, indata, color  )\n\n    if broadcast_destination != False:\n        return writeToNetwork( content , broadcast_destination )\n    else:\n        return writeToDisk ( content )\n\ndef writeToNetwork ( content , broadcast_destination ):\n        message = messages.shoutbox_message()\n\tmessage.set(content)\n        casting = broadcast.broadcast( )\n\tcasting.setDestination(broadcast_destination)\n\tcasting.set( message.get_message() )\n\tcasting.send()\n\treturn None\n\ndef writeToDisk ( content ):\n        old = read_data_file()\n        finalcontent = content  + old\n        datafile = open(datafilename, 'r+')\n        datafile.write(finalcontent)\n        datafile.close()\n\treturn finalcontent\n\n\ndef prepare_line ( name, indata, color  ):\n    datapass = re.sub(\"<\", \"&lt;\", indata)\n    data = re.sub(\">\", \"&gt;\", datapass)\n    content = \"<name>\" + name + \":</name>&nbsp;&nbsp;&nbsp;<data class='\" + color + \"'>\" + data + \"</data><br>\\n\"\n    return content\n\n#--------------\n#  Testing or Generating static HTML File\n#--------------\nif __name__ == \"__main__\":\n  import sys\n  if sys.argv.count(\"input\") >= 1 :\n     save_input(  sys.argv[2] ,  sys.argv[3] ,  sys.argv[4] )\n     generate_html_to_display_from_file()\n     print \"Entered Text.\"\n\n  generate_html_from_file ()\n  print \"Generated HTML-Shoutbox File.\"", "original_line_number": 33557, "instruction_length": 516}
{"instruction": "The given code is a Python script that defines a class called \"ManyJetsHiggsVarFiller\" which adds new variables to a ROOT tree. To use this class, you need to import it from the \"tree.gardening\" module and create an instance of it. Then, you can call the \"process\" method of the instance, passing the input and output ROOT files as arguments. The new variables that will be added to the output tree are: \"m4j\", \"m3j\", \"mW1jj\", \"mW2jj\", \"pt4j\", \"pt3j\", \"eta4j\", \"eta3j\", \"phi4j\", \"phi3j\", \"dphill4j\", \"dphill3j\", \"best1\", and \"best2\".", "input": "", "output": "from tree.gardening import TreeCloner\n\n\nimport optparse\nimport sys\nimport ROOT\nimport numpy\nimport re\nimport os.path\nimport math\nfrom math import *\nfrom array import array;\n\n\n#\n#\n#   \\  |                               |        |        |   | _)\n#  |\\/ |   _` |  __ \\   |   |          |   _ \\  __|      |   |  |   _` |   _` |   __|\n#  |   |  (   |  |   |  |   |      \\   |   __/  |        ___ |  |  (   |  (   | \\__ \\\n# _|  _| \\__,_| _|  _| \\__, |     \\___/  \\___| \\__|     _|  _| _| \\__, | \\__, | ____/\n#                      ____/                                      |___/  |___/ \n#\n#\n\n\n\n#\n# Examples:\n#\n#  cd /HWWAnalysis/ShapeAnalysis\n# source test/env.sh\n#\n# gardener.py  manyJetHiggsVar  /data2/amassiro/VBF/Data/All21Aug2012_temp_1/latino_2000_ggToH1000toWWTo2LAndTau2Nu.root    /data2/amassiro/VBF/Data/All21Aug2012_temp_2/latino_2000_ggToH1000toWWTo2LAndTau2Nu_TESTISITWORKING.root\n#\n#\n\n\nclass ManyJetsHiggsVarFiller(TreeCloner):\n\n    \n    def __init__(self):\n        pass\n\n    def help(self):\n        return '''Add new many jets system - Higgs variables'''\n\n\n    def addOptions(self,parser):\n        #description = self.help()\n        #group = optparse.OptionGroup(parser,self.label, description)\n        #group.add_option('-b', '--branch',   dest='branch', help='Name of something that is not used ... ', default='boh')\n        #parser.add_option_group(group)\n        #return group\n        pass\n\n\n    def checkOptions(self,opts):\n        pass\n\n    @staticmethod\n    def _deltamassw( jets ):\n        mW = 80.385\n        return math.fabs( mW - (jets[0] + jets[1]).M() )\n        \n\n    def process(self,**kwargs):\n        tree  = kwargs['tree']\n        input = kwargs['input']\n        output = kwargs['output']\n\n        self.connect(tree,input)\n        newbranches = ['m4j', 'm3j', 'mW1jj', 'mW2jj', 'pt4j', 'pt3j', 'eta4j', 'eta3j', 'phi4j', 'phi3j', 'dphill4j',  'dphill3j', 'best1', 'best2']\n        self.clone(output,newbranches)\n\n\n\n        m4j           = numpy.ones(1, dtype=numpy.float32)\n        m3j           = numpy.ones(1, dtype=numpy.float32)\n        mW1jj         = numpy.ones(1, dtype=numpy.float32)\n        mW2jj         = numpy.ones(1, dtype=numpy.float32)\n        pt4j          = numpy.ones(1, dtype=numpy.float32)\n        pt3j          = numpy.ones(1, dtype=numpy.float32)\n        eta4j         = numpy.ones(1, dtype=numpy.float32)\n        eta3j         = numpy.ones(1, dtype=numpy.float32)\n        phi4j         = numpy.ones(1, dtype=numpy.float32)\n        phi3j         = numpy.ones(1, dtype=numpy.float32)\n        dphill4j      = numpy.ones(1, dtype=numpy.float32)\n        dphill3j      = numpy.ones(1, dtype=numpy.float32)\n\n        best1         = numpy.ones(1, dtype=numpy.float32)\n        best2         = numpy.ones(1, dtype=numpy.float32)\n\n        self.otree.Branch('m4j'             ,  m4j           ,  'm4j/F'        )\n        self.otree.Branch('m3j'             ,  m3j           ,  'm3j/F'        )\n        self.otree.Branch('mW1jj'           ,  mW1jj         ,  'mW1jj/F'      )\n        self.otree.Branch('mW2jj'           ,  mW2jj         ,  'mW2jj/F'      )\n        self.otree.Branch('pt4j'            ,  pt4j          ,  'pt4j/F'       )\n        self.otree.Branch('pt3j'            ,  pt3j          ,  'pt3j/F'       )\n        self.otree.Branch('eta4j'           ,  eta4j         ,  'eta4j/F'      )\n        self.otree.Branch('eta3j'           ,  eta3j         ,  'eta3j/F'      )\n        self.otree.Branch('phi4j'           ,  phi4j         ,  'phi4j/F'      )\n        self.otree.Branch('phi3j'           ,  phi3j         ,  'phi3j/F'      )\n        self.otree.Branch('dphill4j'        ,  dphill4j      ,  'dphill4j/F'   )\n        self.otree.Branch('dphill3j'        ,  dphill3j      ,  'dphill3j/F'   )\n\n        self.otree.Branch('best1'           ,  best1         ,  'best1/F'   )\n        self.otree.Branch('best2'           ,  best2         ,  'best2/F'   )\n\n        nentries = self.itree.GetEntries()\n        print 'Total number of entries: ',nentries \n\n        # avoid dots to go faster\n        itree     = self.itree\n        otree     = self.otree\n\n        print '- Starting eventloop'\n        step = 5000\n        for i in xrange(nentries):\n            itree.GetEntry(i)\n\n            ## print event count\n            if i > 0 and i%step == 0.:\n                print i,'events processed.'\n\n            jetpt1 = itree.jetpt1\n            jetphi1 = itree.jetphi1\n            jeteta1 = itree.jeteta1\n\n            jetpt2 = itree.jetpt2\n            jetphi2 = itree.jetphi2\n            jeteta2 = itree.jeteta2\n\n            jetpt3 = itree.jetpt3\n            jetphi3 = itree.jetphi3\n            jeteta3 = itree.jeteta3\n\n            jetpt4 = itree.jetpt4\n            jetphi4 = itree.jetphi4\n            jeteta4 = itree.jeteta4\n\n            jet1 = ROOT.TLorentzVector()\n            jet1.SetPtEtaPhiM(itree.jetpt1, itree.jeteta1, itree.jetphi1, 0)\n            jet2 = ROOT.TLorentzVector()\n            jet2.SetPtEtaPhiM(itree.jetpt2, itree.jeteta2, itree.jetphi2, 0)\n            jet3 = ROOT.TLorentzVector()\n            jet3.SetPtEtaPhiM(itree.jetpt3, itree.jeteta3, itree.jetphi3, 0)\n            jet4 = ROOT.TLorentzVector()\n            jet4.SetPtEtaPhiM(itree.jetpt4, itree.jeteta4, itree.jetphi4, 0)\n\n            jets = [jet1,jet2,jet3,jet4]\n\n            jetSum4 = jet1 + jet2 + jet3 + jet4\n\n            jetSum3 = jet1 + jet2 + jet3\n\n            l1 = ROOT.TLorentzVector()\n            l1.SetPtEtaPhiE(itree.pt1, itree.eta1, itree.phi1, itree.pt1/sin(2*atan(exp(-itree.eta1))))\n\n            l2 = ROOT.TLorentzVector()\n            l2.SetPtEtaPhiE(itree.pt2, itree.eta2, itree.phi2, itree.pt2/sin(2*atan(exp(-itree.eta2))))\n\n            ll = ROOT.TLorentzVector()\n            ll = l1+l2;\n\n\n            mW1jj[0]    = -999\n            mW2jj[0]    = -999\n            m4j[0]      = -999\n            m3j[0]      = -999\n            pt4j[0]     = -999\n            pt3j[0]     = -999\n            eta4j[0]    = -999\n            eta3j[0]    = -999\n            phi4j[0]    = -999\n            phi3j[0]    = -999\n            dphill4j[0] = -999\n            dphill3j[0] = -999\n\n            best1[0]    = -999\n            best2[0]    = -999\n\n            if (jetpt4 > 0) :\n                m4j[0]      = jetSum4.M()\n                pt4j[0]     = jetSum4.Pt()\n                eta4j[0]    = jetSum4.Eta()\n                phi4j[0]    = jetSum4.Phi()\n                dphill4j[0] = jetSum4.DeltaPhi(ll)\n\n                # list of all possible couples\n                sjets = sorted([ (jets[i],jets[j]) for i in xrange(4) for j in xrange(4) if i<j], key=self._deltamassw)\n\n#                 for jA,jB in sjets:\n#                     print (jA+jB).M(),'->', self._deltamassw( (jA,jB) )\n\n                # choose best pair: the pair with one of the two W-candidates nearest to MW \n                best = sjets[0]\n                # the companion is made of the other 2 jets\n                other = tuple( [j for j in jets if j not in best] )\n\n                W1 = best[0] + best[1]\n                W2 = other[0]+other[1]\n\n                best1[0] = jets.index(best[0])\n                best2[0] = jets.index(best[1])\n\n                if W1.Pt() > W2.Pt() :\n                    mW1jj[0] = W1.M()\n                    mW2jj[0] = W2.M()\n                else :\n                    mW1jj[0] = W2.M()\n                    mW2jj[0] = W1.M()\n\n\n            if (jetpt3 > 0) :\n                m3j[0]      = jetSum3.M()\n                pt3j[0]     = jetSum3.Pt()\n                eta3j[0]    = jetSum3.Eta()\n                phi3j[0]    = jetSum3.Phi()\n                dphill3j[0] = jetSum3.DeltaPhi(ll)\n\n\n            otree.Fill()\n\n        self.disconnect()\n        print '- Eventloop completed'", "original_line_number": 33574, "instruction_length": 533}
{"instruction": "This code provides a CLI parser module that is responsible for parsing command models defined in cliCommands.yaml and providing functions for validation of user-input, invoking execution handle for CLI commands or macro expansions, and determining possible arg match for command auto-completion based on context. To use the module, instantiate the CLIUtil class and call its methods, such as get_match() to get a list of possible matches for a given command context, and validate_command_and_execute() to validate and execute a full command context. The module also includes methods for printing the command graph and results.", "input": "", "output": "#------------------------------------------------------------------------------\n#                            cli_parser.py\n#------------------------------------------------------------------------------\n'''\n@author : rgiyer\nDate    : October 20th, 2014\n\nThis module is responsible for parsing command model defined in\ncliCommands.yaml and providing functions for:\n       - Validation of user-input\n       - invoking execution handle for CLI commands or macro expansions\n       - determine possible arg match for command auto-completion based \n         on context\n'''\n\n# Standard Python libraries\nimport os\nimport re\nimport inspect\nimport subprocess\n\n# Packages required for openclos\nimport yaml\n\n# openclos classes\nimport util\n\n# cli related classes\nfrom cli_handle_impl import CLIImplementor\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\nclass CLICommand:\n\n    def __init__ ( self, cmd_access, cmd_handle, cmd_macro, cmd_desc ):\n        self.cmd_access = cmd_access\n        self.cmd_handle = cmd_handle\n        self.cmd_macro  = cmd_macro\n        self.cmd_desc   = cmd_desc\n\n# end class CLICommand\n\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\nclass CLIUtil:\n\n    def __init__ ( self ):\n        commandConfFile = os.path.join ( util.configLocation, \n                                         'cliCommands.yaml' )\n        self.yaml_file_stream = open ( commandConfFile, 'r' )\n        raw_graph = yaml.load ( self.yaml_file_stream )\n        self.cmd_graph = {}\n        self.indentation = 8\n        self.dump_cmd ( raw_graph )\n        self.yaml_file_stream.close ()\n        \n\n#------------------------------------------------------------------------------\n    def get_implementor_handle ( self, class_instance, handle_name ):\n        handles = inspect.getmembers ( class_instance, \n                                       predicate = inspect.ismethod )\n        for function_tuple in handles:\n            if ( handle_name == function_tuple [ 0 ] ):\n                return function_tuple [ 1 ]\n\n        # no match found\n        return 0\n\n#------------------------------------------------------------------------------\n    # Parse through the dictionary iteratively:\n    def dump_cmd ( self,\n                   cmds,\n                   cmd_root=\"\", \n                   cmd_access=\"READ\",\n                   cmd_handle=\"\",\n                   cmd_macro=\"\",\n                   cmd_desc=\"\" ):\n\n        for cmd in cmds:\n            if ( cmd_root == \"\" ):\n                cmd_compound = cmd\n            else:\n                cmd_compound = cmd_root + \"_\" + cmd\n\n            cmd_data = cmds [ cmd ]\n\n            # Get command access\n            if cmd_data.has_key ( \"Access\" ):\n                cmd_access = cmd_data [ \"Access\" ]\n\n            # Get command handler\n            if cmd_data.has_key ( \"Handle\" ):\n                cmd_handle = cmd_data [ \"Handle\" ]\n            elif ( cmd_handle != \"\" ):\n                cmd_handle = \"\"\n\n            # Get command macro\n            if cmd_data.has_key ( \"Macro\" ):\n                cmd_macro = cmd_data [ \"Macro\" ]\n            elif ( cmd_macro != \"\" ):\n                cmd_macro = \"\"\n                \n\n            # Get command description\n            if cmd_data.has_key ( \"Desc\" ):\n                cmd_desc = cmd_data [ \"Desc\" ]\n            elif ( cmd_desc != \"\" ):\n                cmd_desc = \"\"\n\n            # Parse the arguments\n            if cmd_data.has_key ( \"Args\" ):\n                cmd_args = cmd_data [ \"Args\" ]\n                self.dump_cmd ( cmd_args, \n                                cmd_compound, \n                                cmd_access,\n                                cmd_handle,\n                                cmd_macro,\n                                cmd_desc )\n\n            if cmd_data.has_key ( \"Handle\" ):\n                self.cmd_graph [ cmd_compound ] = CLICommand ( cmd_access, \n                                                               cmd_handle,\n                                                               cmd_macro,\n                                                               cmd_desc )\n                if ( len ( cmd_compound ) > self.indentation ):\n                    self.indentation = len ( cmd_compound )\n\n#------------------------------------------------------------------------------\n    def normalize_command ( self, cmd ):\n        return cmd.replace ( \" \", \"_\" )\n\n#------------------------------------------------------------------------------\n    def get_indentation ( self, cmd ):\n        return ( self.indentation + 8 - len ( cmd ) )\n\n#------------------------------------------------------------------------------\n    def suffix_macro_to_cmd ( self, macro_list, cmd ):\n        ret_cmd = []\n        for macro in macro_list:\n            ret_cmd.append ( self.normalize_command ( cmd + \"_\" + macro ) )\n        return ret_cmd\n\n#------------------------------------------------------------------------------\n    def get_macro_list ( self, class_instance, macro_txt, add_help=None ):\n        fn_macro = self.get_implementor_handle ( class_instance, macro_txt )\n        return fn_macro ( add_help )\n\n#------------------------------------------------------------------------------\n    def include_macro ( self, macro_list, ret_list ):\n        for item in macro_list:\n            ret_list.append ( item )\n\n#------------------------------------------------------------------------------\n    def string_has_enter ( self, string ):\n        if ( re.search ( \"<enter>\", string ) != None ):\n            return 1\n        else:\n            return 0\n\n#------------------------------------------------------------------------------\n    def add_enter_instruction ( self, result_list ):\n        if ( len ( result_list ) ):\n            string = result_list [ 0 ]\n            if ( self.string_has_enter ( string ) == 1 ):\n                return 0\n\n        result_list.insert ( 0, \" <enter>\" + \" \" * self.get_indentation ( \"<enter\" ) + \"Execute the current command\" )\n        \n\n#------------------------------------------------------------------------------\n    def match_macro ( self, macro_list, needle, ret_list ):\n        for haystack in macro_list:\n            if ( len ( needle ) == len ( haystack ) ):\n                if ( re.match ( needle, haystack ) != None ):\n                    self.add_enter_instruction ( ret_list )\n            elif ( len ( needle ) < len ( haystack ) ):\n                if ( re.match ( needle, haystack ) != None ):\n                    ret_list.append ( haystack )\n            else:\n                print \"\"\n\n#------------------------------------------------------------------------------\n    def option_exists ( self, consider_option, ret_list ):\n        for option in ret_list:\n            if ( re.match ( option, consider_option ) != None ):\n                return 1\n        return 0\n\n#------------------------------------------------------------------------------\n    def complete_command ( self,\n                           part_cmd, \n                           full_cmd, \n                           end_index, \n                           cmd_helper, \n                           ret_list ):\n        unmatched_string = full_cmd [ end_index: ]\n\n        # This is an adjustment for \"<space>\" before tab / ? keypress\n        if ( part_cmd [ -1 ] == \"_\" ):\n            part_cmd = part_cmd [ 0:-1 ]\n            unmatched_string = \"_\" + unmatched_string\n    \n        if ( unmatched_string [ 0 ] == \"_\" ):\n            # attach possible matches\n            possible_option = unmatched_string.replace ( \"_\", \" \" ) + ( \" \" * self.get_indentation ( full_cmd ) )\n            possible_option = possible_option + \"<\" + cmd_helper.cmd_desc + \">\"\n            ret_list.append ( possible_option )\n        else:\n            # Get part of the command from part_cmd\n            match_object = re.search ( \"_\", part_cmd )\n            while ( match_object != None ):\n                part_cmd = part_cmd [ match_object.end (): ]\n                match_object = re.search ( \"_\", part_cmd )\n        \n            # Get rest of the command from unmatched_string\n            match_object = re.search ( \"_\", unmatched_string )\n            if ( match_object != None ):\n                unmatched_string = unmatched_string [ :(match_object.end()-1)]\n\n            complete_word = part_cmd + unmatched_string\n            if ( self.option_exists ( complete_word, ret_list ) == 0 ):\n                 ret_list.append ( complete_word )\n\n        return ret_list\n        \n\n#------------------------------------------------------------------------------\n    def get_all_cmds ( self ):\n        ret_list = []\n        for cmd in self.cmd_graph:\n            cmd_str = cmd.replace ( \"_\", \" \" )\n            cmd_str = cmd_str + ( \" \" * self.get_indentation ( cmd ) ) + \"<\" + self.cmd_graph [ cmd ].cmd_desc + \">\"\n            ret_list.append ( cmd_str )\n        return ret_list\n\n#------------------------------------------------------------------------------\n# Lot of reference here to needle and haystack, needle being the current\n# command context of the CLI, and haystack being the command model dict\n# created during CLIUtil instantiation\n#------------------------------------------------------------------------------\n    def get_match ( self, cmd ):\n        if  ( len ( cmd ) == 0 or re.search ( \"[a-z|A-Z|0-9]\", cmd ) == None ):\n            return self.get_all_cmds ()\n\n        # chomp input string\n        if ( cmd [ -1 ] == \" \" ):\n            cmd = cmd [ 0:-1 ]\n\n        needle = self.normalize_command ( cmd )\n        ret_list = []\n\n        for haystack in self.cmd_graph:\n            len_haystack = len ( haystack )\n            len_needle   = len ( needle )\n            cmd_helper = self.cmd_graph [ haystack ]\n\n            # Case 1: Full command is provided, without macro expansion\n            if ( len_needle == len_haystack ):\n                # check if we have a match\n                if ( re.match ( needle, haystack ) != None ):\n                    if ( cmd_helper.cmd_macro != \"\" ):\n                        self.include_macro ( self.get_macro_list ( CLIImplementor (), cmd_helper.cmd_macro, \"add help\" ), ret_list )\n                    else:\n                        self.add_enter_instruction ( ret_list )\n\n            # Case 2: Full command is provided with macro expansion\n            elif ( len_needle > len_haystack ):\n                match_object = re.match ( haystack, needle )\n                if ( match_object != None ):\n                    # Match exists - so get the macro\n                    cmd_macro = needle [ match_object.end (): ]\n                    if ( cmd_macro [ 0 ] == \"_\" and len ( cmd_macro ) > 1 ):\n                        cmd_macro = cmd_macro [ 1: ]\n\n                    if ( cmd_helper.cmd_macro != \"\" ):\n                        cmd_macro_list = self.get_macro_list ( CLIImplementor(),\n                                                          cmd_helper.cmd_macro )\n                        self.match_macro ( cmd_macro_list, cmd_macro, ret_list )\n\n            # Case 3: Part command is provided\n            elif ( len_needle < len_haystack ):\n                match_object = re.match ( needle, haystack )\n                if ( match_object != None ):\n                    # Match exists - get rest of the command\n                    balance_cmd = haystack [ match_object.end (): ]\n                    self.complete_command ( needle, \n                                            haystack, \n                                            match_object.end (), \n                                            self.cmd_graph [ haystack ],\n                                            ret_list )\n                \n\n        return ret_list\n\n#------------------------------------------------------------------------------\n    def chomp ( self, token ):\n        match_object = re.search ( \"[a-z|A-Z|0-9]\", token )\n        if ( match_object != None ):\n            token = token [ ( match_object.end () - 1): ]\n\n        token = token [ ::-1 ]\n        match_object = re.search ( \"[a-z|A-Z|0-9]\", token )\n        if ( match_object != None ):\n            token = token [ ( match_object.end () - 1): ]\n\n        token = token [ ::-1 ]\n\n        return token\n\n#------------------------------------------------------------------------------\n    def validate_command_and_execute ( self, full_cmd_context ):\n        # We will do the validation again in case this function is called\n        # outside the CLI context\n        best_cmd_match = \"\"\n        best_cmd_args  = \"\"\n        best_cmd_handle = None\n\n        for command in self.cmd_graph:\n            match_object = re.match ( command, \n                           self.normalize_command ( full_cmd_context ) )\n            if ( match_object != None ):\n                # Okay - we found a match. Get macros if included\n                command_args = \"\"\n                # TODO - different impl here for multiple args support\n                if ( len ( full_cmd_context ) > len ( command ) ):\n                    command_args = self.chomp ( full_cmd_context [ match_object.end (): ] )\n                if ( len ( best_cmd_match ) < len ( command ) ):\n                    best_cmd_match = command\n                    best_cmd_args  = command_args\n                    best_cmd_handle = self.get_implementor_handle ( CLIImplementor (), self.cmd_graph [ command ].cmd_handle )\n\n        if ( best_cmd_handle != 0 ):\n            return best_cmd_handle ( best_cmd_args )\n        else:\n            print self.cmd_graph [ best_cmd_match ].cmd_handle + \" not implemented\"\n\n#------------------------------------------------------------------------------\n    def print_results ( self, result_list ):\n        for result in result_list:\n            print \"\\t\" + result\n\n#------------------------------------------------------------------------------\n    def print_command_graph ( self, cmd_dict ):\n        for keys in cmd_dict:\n            print keys + \"=>\"\n            cmd = cmd_dict [ keys ]\n            if ( cmd.cmd_desc != \"\" ):\n                print \"    \" + cmd.cmd_desc\n            print \"    \" + cmd.cmd_access\n            if ( cmd.cmd_macro != \"\" ):\n                fn_macro = self.get_implementor_handle ( CLIImplementor (), \n                                                        cmd.cmd_macro )\n                if ( fn_macro != 0 ):\n                    print fn_macro ()\n                else:\n                    print \"    Macro not implemented\"\n            if ( cmd.cmd_handle != \"\" ):\n                fn_handle = self.get_implementor_handle ( CLIImplementor (), \n                                                     cmd.cmd_handle )\n                if ( fn_handle != 0 ):\n                    fn_handle ()\n                else:\n                    print \"    Handler not implemented\"\n\n# end class CLIUtil\n#------------------------------------------------------------------------------\n#------------------------------------------------------------------------------\n\n#------------------------------------------------------------------------------\n#                              MAIN\n#------------------------------------------------------------------------------\ncli_util = CLIUtil ()\n    \n\nmatch_options = [ \"create cabling\",\n#                  \"create cabling-plan\",\n#                  \"create cabling-\",\n#                  \"create cabling\",\n#                  \"create cabling-plan pod\",\n#                  \"create cabling-plan pod pod_2\",\n#                  \"create\",\n#                  \"create dev\",\n#                  \"create device-config\",\n#                  \"create device-config p\",\n#                  \"create device-config pod\",\n#                  \"create device-config pod pod_1\",\n#                  \"run\",\n#                  \"update password\",\n#                  \"run r\",\n#                  \"run RE\",\n#                  \"create cab\",\n                  \"create pods\",\n                  \"create pods from\",\n                  \"create pods from-file\",\n                  \"\" ]\n\nif __name__ == '__main__':\n    for match in match_options:\n        print \"Matching results for \" + match + \" is:\"\n        cli_util.print_results ( cli_util.get_match ( match ) )\n        print \"------------------------------------------------------\"", "original_line_number": 34781, "instruction_length": 626}
{"instruction": "Create a Django project with a Board, Webhook, and Bridge model. The Board model has fields for name, webhook_activate, trello_board_id, trello_token, created_at, and updated_at. The Webhook model has fields for name, incoming_webhook_url, icon_url, username, board, created_at, and updated_at. The Bridge model has fields for webhook, board, events, created_at, and updated_at. The events field is a CharField with a max length of 700 and choices from the EVENT_CHOICES tuple. The events_as_list method returns the events field as a list.", "input": "", "output": "# -*- coding: utf-8 -*-\nfrom __future__ import unicode_literals\nfrom ast import literal_eval\n\nfrom django.db import models\nfrom django.conf import settings\n\n\nclass Board(models.Model):\n\n    name = models.CharField(max_length=100)\n    webhook_activate = models.BooleanField(default=False)\n    trello_board_id = models.CharField(max_length=100)\n    trello_token = models.CharField(max_length=100)\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        ordering = [\"name\"]\n        verbose_name_plural = \"boards\"\n\n    def __str__(self):\n        return self.name\n\n    def __unicode__(self):\n        return \"{}\".format(self.name)\n\n\nclass Webhook(models.Model):\n\n    name = models.CharField(max_length=50)\n    incoming_webhook_url = models.CharField(max_length=300, unique=True)\n\n    icon_url = models.CharField(\n        max_length=250,\n        default=\"http://maffrigby.com/wp-content/uploads/2015/05/trello-icon.png\",\n    )\n    username = models.CharField(max_length=30, default=\"Matterllo\")\n\n    board = models.ManyToManyField(Board)\n\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        ordering = [\"name\"]\n        verbose_name_plural = \"webhooks\"\n\n    def __str__(self):\n        return \"{} :: {}\".format(self.name, self.incoming_webhook_url)\n\n    def __unicode__(self):\n        return \"{} :: {}\".format(self.name, self.incoming_webhook_url)\n\n\nclass Bridge(models.Model):\n    EVENT_CHOICES = (\n        # card\n        (\"addAttachmentToCard\", \"addAttachmentToCard\"),\n        (\"addLabelToCard\", \"addLabelToCard\"),\n        (\"addMemberToCard\", \"addMemberToCard\"),\n        (\"commentCard\", \"commentCard\"),\n        (\"copyCard\", \"copyCard\"),\n        (\"createCard\", \"createCard\"),\n        (\"emailCard\", \"emailCard\"),\n        (\"moveCardFromBoard\", \"moveCardFromBoard\"),\n        (\"moveCardToBoard\", \"moveCardToBoard\"),\n        (\"removeLabelFromCard\", \"removeLabelFromCard\"),\n        (\"removeMemberFromCard\", \"removeMemberFromCard\"),\n        (\n            \"updateCard\",\n            \"updateCard (include moveCardToList, renameCard, renameCardDesc, updateCardDueDate, removeCardDueDate, archiveCard, unarchiveCard)\",\n        ),\n        # checklist\n        (\"addChecklistToCard\", \"addChecklistToCard\"),\n        (\"createCheckItem\", \"createCheckItem\"),\n        (\"updateCheckItemStateOnCard\", \"updateCheckItemStateOnCard\"),\n        # list\n        (\"archiveList\", \"archiveList\"),\n        (\"createList\", \"createList\"),\n        (\"moveListFromBoard\", \"moveCardFromBoard\"),\n        (\"moveListToBoard\", \"moveListToBoard\"),\n        (\"renameList\", \"renameList\"),\n        (\"updateList\", \"updateList\"),\n    )\n\n    webhook = models.ForeignKey(Webhook, on_delete=models.CASCADE)\n    board = models.ForeignKey(Board, on_delete=models.CASCADE)\n\n    events = models.CharField(max_length=700)\n\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n\n    class Meta:\n        verbose_name_plural = \"bridges\"\n\n    def __str__(self):\n        return \"{}::{}\".format(self.board, self.webhook)\n\n    def __unicode__(self):\n        return \"{}::{}\".format(self.board, self.webhook)\n\n    def events_as_list(self):\n        return literal_eval(self.events)", "original_line_number": 35479, "instruction_length": 539}
{"instruction": "To run a Weimar worker instance or start a Weimar graph server, use the argparse module to parse command line arguments. If starting a worker instance, use the start_worker() function from the remote.worker module and specify the number of threads running in the worker. If starting a server, specify the HyperDex coordinator IP address and port number using the hyperdex_ip and hyperdex_port arguments, respectively, and use the start_server() function from the remote.server module. If the hyperdex_ip and hyperdex_port arguments are not specified when starting a server, print an error message and exit. If the threads argument is specified when starting a server, omit it as it only refers to a worker process.", "input": "", "output": "'''\nCreated on Mar 17, 2014\n\n@author: mschilonka\n'''\nimport argparse, sys\nfrom remote import server as Server\nfrom remote import worker as Worker\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-w\", \"--worker\", help=\"Starts a weimar worker instance.\", action=\"store_true\")\n    parser.add_argument(\"-t\", \"--threads\",type=int, dest='threads', help=\"The number of threads running in one a worker (Default=3).\")\n    parser.add_argument(\"-s\", \"--server\", help=\"Starts a weimar graph server.\", action=\"store_true\")\n    parser.add_argument(\"-i\", \"--hyperdex-ip\",type=str ,dest='hyperdex_ip',  help='The HyperDex coordinator IP address. Must be specified if a server is started.')\n    parser.add_argument(\"-p\", \"--hyperdex-port\",type=int ,dest='hyperdex_port', help=\"The HyperDex coordinator port number. Must be specified if a server is started.\")\n    args = parser.parse_args()\n\n    if args.worker:\n        if(args.threads is None):\n            args.threads = 3\n        Worker.start_worker(args.threads)\n    elif args.server:\n        if(args.hyperdex_ip is None or args.hyperdex_port is None):\n            print('When starting a Weimar server, please specify the HyperDex\\'s coordinators ip and port.')\n            parser.print_help()\n            sys.exit(1)\n        if(args.threads is not None):\n            print('--threads only refers to a worker process and will be omitted.')\n        Server.start_server(args.hyperdex_ip, args.hyperdex_port)", "original_line_number": 35666, "instruction_length": 714}
{"instruction": "Create a new ZoneMixing object with the following properties: \n- Name: a string\n- Zone Name: an object-list\n- Schedule Name: an object-list\n- Design Flow Rate Calculation Method: \"Flow/Zone\"\n- Design Flow Rate: a real number\n- Flow Rate per Zone Floor Area: a real number\n- Flow Rate per Person: a real number\n- Air Changes per Hour: a real number\n- Source Zone Name: an object-list\n- Delta Temperature: 10.1\n- Delta Temperature Schedule Name: an object-list\n- Minimum Zone Temperature Schedule Name: an object-list\n- Maximum Zone Temperature Schedule Name: an object-list\n- Minimum Source Zone Temperature Schedule Name: an object-list\n- Maximum Source Zone Temperature Schedule Name: an object-list\n- Minimum Outdoor Temperature Schedule Name: an object-list\n- Maximum Outdoor Temperature Schedule Name: an object-list", "input": "", "output": "import os\nimport tempfile\nimport unittest\nimport logging\nfrom pyidf import ValidationLevel\nimport pyidf\nfrom pyidf.idf import IDF\nfrom pyidf.zone_airflow import ZoneMixing\n\nlog = logging.getLogger(__name__)\n\nclass TestZoneMixing(unittest.TestCase):\n\n    def setUp(self):\n        self.fd, self.path = tempfile.mkstemp()\n\n    def tearDown(self):\n        os.remove(self.path)\n\n    def test_create_zonemixing(self):\n\n        pyidf.validation_level = ValidationLevel.error\n\n        obj = ZoneMixing()\n        # alpha\n        var_name = \"Name\"\n        obj.name = var_name\n        # object-list\n        var_zone_name = \"object-list|Zone Name\"\n        obj.zone_name = var_zone_name\n        # object-list\n        var_schedule_name = \"object-list|Schedule Name\"\n        obj.schedule_name = var_schedule_name\n        # alpha\n        var_design_flow_rate_calculation_method = \"Flow/Zone\"\n        obj.design_flow_rate_calculation_method = var_design_flow_rate_calculation_method\n        # real\n        var_design_flow_rate = 0.0\n        obj.design_flow_rate = var_design_flow_rate\n        # real\n        var_flow_rate_per_zone_floor_area = 0.0\n        obj.flow_rate_per_zone_floor_area = var_flow_rate_per_zone_floor_area\n        # real\n        var_flow_rate_per_person = 0.0\n        obj.flow_rate_per_person = var_flow_rate_per_person\n        # real\n        var_air_changes_per_hour = 0.0\n        obj.air_changes_per_hour = var_air_changes_per_hour\n        # object-list\n        var_source_zone_name = \"object-list|Source Zone Name\"\n        obj.source_zone_name = var_source_zone_name\n        # real\n        var_delta_temperature = 10.1\n        obj.delta_temperature = var_delta_temperature\n        # object-list\n        var_delta_temperature_schedule_name = \"object-list|Delta Temperature Schedule Name\"\n        obj.delta_temperature_schedule_name = var_delta_temperature_schedule_name\n        # object-list\n        var_minimum_zone_temperature_schedule_name = \"object-list|Minimum Zone Temperature Schedule Name\"\n        obj.minimum_zone_temperature_schedule_name = var_minimum_zone_temperature_schedule_name\n        # object-list\n        var_maximum_zone_temperature_schedule_name = \"object-list|Maximum Zone Temperature Schedule Name\"\n        obj.maximum_zone_temperature_schedule_name = var_maximum_zone_temperature_schedule_name\n        # object-list\n        var_minimum_source_zone_temperature_schedule_name = \"object-list|Minimum Source Zone Temperature Schedule Name\"\n        obj.minimum_source_zone_temperature_schedule_name = var_minimum_source_zone_temperature_schedule_name\n        # object-list\n        var_maximum_source_zone_temperature_schedule_name = \"object-list|Maximum Source Zone Temperature Schedule Name\"\n        obj.maximum_source_zone_temperature_schedule_name = var_maximum_source_zone_temperature_schedule_name\n        # object-list\n        var_minimum_outdoor_temperature_schedule_name = \"object-list|Minimum Outdoor Temperature Schedule Name\"\n        obj.minimum_outdoor_temperature_schedule_name = var_minimum_outdoor_temperature_schedule_name\n        # object-list\n        var_maximum_outdoor_temperature_schedule_name = \"object-list|Maximum Outdoor Temperature Schedule Name\"\n        obj.maximum_outdoor_temperature_schedule_name = var_maximum_outdoor_temperature_schedule_name\n\n        idf = IDF()\n        idf.add(obj)\n        idf.save(self.path, check=False)\n\n        with open(self.path, mode='r') as f:\n            for line in f:\n                log.debug(line.strip())\n\n        idf2 = IDF(self.path)\n        self.assertEqual(idf2.zonemixings[0].name, var_name)\n        self.assertEqual(idf2.zonemixings[0].zone_name, var_zone_name)\n        self.assertEqual(idf2.zonemixings[0].schedule_name, var_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].design_flow_rate_calculation_method, var_design_flow_rate_calculation_method)\n        self.assertAlmostEqual(idf2.zonemixings[0].design_flow_rate, var_design_flow_rate)\n        self.assertAlmostEqual(idf2.zonemixings[0].flow_rate_per_zone_floor_area, var_flow_rate_per_zone_floor_area)\n        self.assertAlmostEqual(idf2.zonemixings[0].flow_rate_per_person, var_flow_rate_per_person)\n        self.assertAlmostEqual(idf2.zonemixings[0].air_changes_per_hour, var_air_changes_per_hour)\n        self.assertEqual(idf2.zonemixings[0].source_zone_name, var_source_zone_name)\n        self.assertAlmostEqual(idf2.zonemixings[0].delta_temperature, var_delta_temperature)\n        self.assertEqual(idf2.zonemixings[0].delta_temperature_schedule_name, var_delta_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_zone_temperature_schedule_name, var_minimum_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_zone_temperature_schedule_name, var_maximum_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_source_zone_temperature_schedule_name, var_minimum_source_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_source_zone_temperature_schedule_name, var_maximum_source_zone_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].minimum_outdoor_temperature_schedule_name, var_minimum_outdoor_temperature_schedule_name)\n        self.assertEqual(idf2.zonemixings[0].maximum_outdoor_temperature_schedule_name, var_maximum_outdoor_temperature_schedule_name)", "original_line_number": 35825, "instruction_length": 820}
{"instruction": "Create an API endpoint that lists all ranked 1v1 or global players. The endpoint should accept a string parameter 'leaderboard_type' which can be either '1v1' or 'global'. The endpoint should also accept two optional query parameters 'page[number]' and 'page[size]'. The default value for 'page[number]' should be 1 and the default value for 'page[size]' should be 5000. The endpoint should return a JSON response containing an array of player objects with their attributes such as 'id', 'login', 'mean', 'deviation', 'num_games', 'ranking', 'rating', and 'is_active'. The response should also contain a 'type' field which can be either 'ranked1v1' or 'global'. The players should be sorted by their rating in descending order. If there is an error, the endpoint should return a JSON response with an 'errors' field containing an array of error objects with their 'title' and 'status' fields.", "input": "", "output": "from faf.api import LeaderboardSchema\nfrom faf.api import LeaderboardStatsSchema\nfrom flask import request\nfrom pymysql.cursors import DictCursor\n\nfrom api import app\nfrom api.error import ApiException, ErrorCode\nfrom api.error import Error\nfrom api.query_commons import fetch_data\nfrom faf import db\n\nMAX_PAGE_SIZE = 5000\n\nSELECT_EXPRESSIONS = {\n    'id': 'r.id',\n    'login': 'l.login',\n    'mean': 'r.mean',\n    'deviation': 'r.deviation',\n    'num_games': 'r.numGames',\n    'is_active': 'r.is_active',\n    'rating': 'ROUND(r.mean - 3 * r.deviation)',\n    'ranking': '@rownum:=@rownum+1'\n}\n\nTABLE1V1 = 'ladder1v1_rating r JOIN login l on r.id = l.id, (SELECT @rownum:=%(row_num)s) n'\nTABLEGLOBAL = 'global_rating r JOIN login l on r.id = l.id, (SELECT @rownum:=%(row_num)s) n'\n\n\n@app.route('/leaderboards/<string:leaderboard_type>')\ndef leaderboards_type(leaderboard_type):\n    \"\"\"\n        Lists all ranked 1v1 or global players.\n\n        **Example Request**:\n\n        **Default Values**:\n            page[number]=1\n\n            page[size]=5000\n\n        .. sourcecode:: http\n\n           GET /leaderboards/1v1 /leaderboards/global\n           Accept: application/vnd.api+json\n\n        **Example Response**:\n\n        .. sourcecode:: http\n\n            HTTP/1.1 200 OK\n            Vary: Accept\n            Content-Type: text/javascript\n\n            {\n              \"data\": [\n                {\n                  \"attributes\": {\n                    \"deviation\": 48.4808,\n                    \"id\": \"781\",\n                    \"login\": \"Zock\",\n                    \"mean\": 2475.69,\n                    \"num_games\": 1285,\n                    \"ranking\": 1,\n                    \"rating\": 2330,\n                    \"won_games\": 946\n                  },\n                  \"id\": \"781\",\n                  \"type\": \"ranked1v1\"\n                },\n                ...\n              ]\n            }\n\n        :param page[number]: The page number being requested (EX.: /leaderboards/1v1?page[number]=2)\n        :type page[number]: int\n        :param page[size]: The total amount of players to grab by default (EX.: /leaderboards/1v1?page[size]=10)\n        :type page[size]: int\n        :param leaderboard_type: Finds players in the 1v1 or global rating\n        :type leaderboard_type: 1v1 OR global\n        :status 200: No error\n\n        \"\"\"\n    sort_field = request.values.get('sort')\n    if sort_field:\n        raise ApiException([Error(ErrorCode.QUERY_INVALID_SORT_FIELD, sort_field)])\n\n    page = int(request.values.get('page[number]', 1))\n    page_size = int(request.values.get('page[size]', MAX_PAGE_SIZE))\n    row_num = (page - 1) * page_size\n    select = SELECT_EXPRESSIONS\n\n    args = {'row_num': row_num}\n\n    rating = find_leaderboard_type(leaderboard_type, select)\n\n    return fetch_data(LeaderboardSchema(), rating['table'], rating['select'], MAX_PAGE_SIZE, request, sort='-rating',\n                      args=args, where='is_active = 1 AND r.numGames > 0')\n\n\n@app.route('/leaderboards/<string:leaderboard_type>/<int:player_id>')\ndef leaderboards_type_get_player(leaderboard_type, player_id):\n    \"\"\"\n        Gets a global or 1v1 player. Player must be active, played more than one ranked game, and must have statistics associated\n        with he/she.\n\n        **Example Request**:\n\n        .. sourcecode:: http\n\n           GET /leaderboards/1v1/781 /leaderboards/global/781\n\n        **Example Response**:\n\n        .. sourcecode:: http\n\n            HTTP/1.1 200 OK\n            Vary: Accept\n            Content-Type: text/javascript\n\n            {\n              \"data\": {\n                \"attributes\": {\n                  \"deviation\": 48.4808,\n                  \"id\": \"781\",\n                  \"login\": \"Zock\",\n                  \"mean\": 2475.69,\n                  \"num_games\": 1285,\n                  \"ranking\": 1,\n                  \"rating\": 2330,\n                  \"won_games\": 946\n                },\n                \"id\": \"781\",\n                \"type\": \"leaderboard\"\n              }\n            }\n\n        :param leaderboard_type: Finds players in the 1v1 or global rating\n        :type leaderboard_type: 1v1 OR global\n        :param player_id: Player ID\n        :type player_id: int\n\n        :status 200: No error\n        :status 404: No entry with this id was found\n\n        \"\"\"\n    select_expressions = SELECT_EXPRESSIONS.copy()\n\n    select = select_expressions\n\n    rating = find_leaderboard_type(leaderboard_type, select)\n\n    select_expressions['ranking'] = \"\"\"(SELECT count(*) FROM \"\"\" + rating['tableName'] + \"\"\"\n                                        WHERE ROUND(mean - 3 * deviation) >= ROUND(r.mean - 3 * r.deviation)\n                                        AND is_active = 1\n                                        AND numGames > 0)\n                                        \"\"\"\n\n    result = fetch_data(LeaderboardSchema(), rating['table'], rating['select'], MAX_PAGE_SIZE, request,\n                        many=False, where='r.id=%(id)s', args=dict(id=player_id, row_num=0))\n\n    if 'id' not in result['data']:\n        return {'errors': [{'title': 'No entry with this id was found'}]}, 404\n\n    return result\n\n\n@app.route(\"/leaderboards/<string:rating_type>/stats\")\ndef rating_stats(rating_type):\n    \"\"\"\n    Gets all player stats sorted by rankings.\n\n    **Example Request**:\n\n    .. sourcecode:: http\n\n       GET /leaderboards/1v1/stats /leaderboards/global/stats\n\n    **Example Response**:\n\n    .. sourcecode:: http\n\n        HTTP/1.1 200 OK\n        Vary: Accept\n        Content-Type: text/javascript\n\n        {\n          \"data\": {\n            \"attributes\": {\n              \"rating_distribution\": {\n                \"-100\": 206,\n                \"-200\": 192,\n                \"-300\": 137,\n                \"-400\": 77,\n                \"-500\": 15,\n                \"-600\": 10,\n                \"-700\": 1,\n                \"0\": 268,\n                \"100\": 282,\n                \"1000\": 122,\n                \"1100\": 86,\n                \"1200\": 72,\n                \"1300\": 55,\n                \"1400\": 42,\n                \"1500\": 35,\n                \"1600\": 25,\n                \"1700\": 15,\n                \"1800\": 14,\n                \"1900\": 7,\n                \"200\": 284,\n                \"2000\": 5,\n                \"2100\": 2,\n                \"2200\": 1,\n                \"2300\": 2,\n                \"300\": 316,\n                \"400\": 296,\n                \"500\": 239,\n                \"600\": 238,\n                \"700\": 208,\n                \"800\": 177,\n                \"900\": 140\n              }\n            },\n            \"id\": \"/leaderboards/1v1/stats\",\n            \"type\": \"leaderboard_stats\"\n          }\n        }\n\n    :status 200: No error\n\n    \"\"\"\n\n    rating = find_leaderboard_type(rating_type)\n\n    with db.connection:\n        cursor = db.connection.cursor(DictCursor)\n        cursor.execute(\"\"\"\n        SELECT\n            FLOOR((mean - 3 * deviation)/100) * 100 AS `rating`,\n            count(*) as count\n        FROM \"\"\" + rating['tableName'] + \"\"\"\n        WHERE `is_active` = 1\n            AND mean BETWEEN 0 AND 3000\n            AND deviation <= 240\n            AND numGames > 0\n        GROUP BY `rating`\n        ORDER BY CAST(`rating` as SIGNED) ASC;\n        \"\"\")\n\n        result = cursor.fetchall()\n\n    data = dict(id='/leaderboards/' + rating_type + '/stats', rating_distribution={})\n\n    for item in result:\n        data['rating_distribution'][str(int(item['rating']))] = item['count']\n\n    return LeaderboardStatsSchema().dump(data, many=False).data\n\n\ndef find_leaderboard_type(rating_type, select=None):\n    rating = {}\n\n    if rating_type == '1v1':\n        rating['table'] = TABLE1V1\n        rating['select'] = append_select_expression()\n        rating['tableName'] = 'ladder1v1_rating'\n    elif rating_type == 'global':\n        rating['table'] = TABLEGLOBAL\n        rating['select'] = select\n        rating['tableName'] = 'global_rating'\n    else:\n        raise ApiException([Error(ErrorCode.QUERY_INVALID_RATING_TYPE, rating_type)])\n\n    return rating\n\n\ndef append_select_expression():\n    select = SELECT_EXPRESSIONS.copy()\n    select['won_games'] = 'r.winGames'\n    select['lost_games'] = 'r.numGames - r.winGames'\n    select['winning_percentage'] = 'ROUND((r.winGames/r.numGames) * 100)'\n    return select", "original_line_number": 36019, "instruction_length": 892}
